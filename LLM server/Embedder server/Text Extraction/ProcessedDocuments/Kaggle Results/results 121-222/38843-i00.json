{
    "document_name": "38843-i00.docx",
    "content": [
        {
            "title": "Foreword",
            "description": "This Technical Report has been produced by the 3rd Generation Partnership Project (3GPP).\nThe contents of the present document are subject to continuing work within the TSG and may change following formal TSG approval. Should the TSG modify the contents of the present document, it will be re-released by the TSG with an identifying change of release date and an increase in version number as follows:\nVersion x.y.z\nwhere:\nx\tthe first digit:\n1\tpresented to TSG for information;\n2\tpresented to TSG for approval;\n3\tor greater indicates TSG approved document under change control.\ny\tthe second digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, etc.\nz\tthe third digit is incremented when editorial only changes have been incorporated in the document.\nIn the present document, modal verbs have the following meanings:\nshall\t\tindicates a mandatory requirement to do something\nshall not\tindicates an interdiction (prohibition) to do something\nThe constructions \"shall\" and \"shall not\" are confined to the context of normative provisions, and do not appear in Technical Reports.\nThe constructions \"must\" and \"must not\" are not used as substitutes for \"shall\" and \"shall not\". Their use is avoided insofar as possible, and they are not used in a normative context except in a direct citation from an external, referenced, non-3GPP document, or so as to maintain continuity of style when extending or modifying the provisions of such a referenced document.\nshould\t\tindicates a recommendation to do something\nshould not\tindicates a recommendation not to do something\nmay\t\tindicates permission to do something\nneed not\tindicates permission not to do something\nThe construction \"may not\" is ambiguous and is not used in normative elements. The unambiguous constructions \"might not\" or \"shall not\" are used instead, depending upon the meaning intended.\ncan\t\tindicates that something is possible\ncannot\t\tindicates that something is impossible\nThe constructions \"can\" and \"cannot\" are not substitutes for \"may\" and \"need not\".\nwill\t\tindicates that something is certain or expected to happen as a result of action taken by an agency the behaviour of which is outside the scope of the present document\nwill not\t\tindicates that something is certain or expected not to happen as a result of action taken by an agency the behaviour of which is outside the scope of the present document\nmight\tindicates a likelihood that something will happen as a result of action taken by some agency the behaviour of which is outside the scope of the present document\nmight not\tindicates a likelihood that something will not happen as a result of action taken by some agency the behaviour of which is outside the scope of the present document\nIn addition:\nis\t(or any other verb in the indicative mood) indicates a statement of fact\nis not\t(or any other negative verb in the indicative mood) indicates a statement of fact\nThe constructions \"is\" and \"is not\" do not indicate requirements.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "1\tScope",
            "description": "The application of AI/ML to wireless communications has been thus far limited to implementation-based approaches, both, at the network and the UE sides. A study on enhancement for data collection for NR and ENDC  (FS_NR_ENDC_data_collect) has examined the functional framework for RAN intelligence enabled by further enhancement of data collection through use cases, examples etc. and identify the potential standardization impacts on current NG-RAN nodes and interfaces. In SA WG2 AI/ML related study, a network functionality NWDAF (Network Data Analytics Function) was introduced in Rel-15 and has been enhanced in Rel-16 and Rel-17.\nThis study explores the benefits of augmenting the air-interface with features enabling improved support of AI/ML. The 3GPP framework for AI/ML is studied for air-interface corresponding to each target use case regarding aspects such as performance, complexity, and potential specification impact.\nThrough studying a few carefully selected use cases, assessing their performance in comparison with traditional methods and the associated potential specification impacts that enable their solutions, this studyÂ lays the foundation for future air-interface use cases leveraging AI/ML techniques.\nSufficient use cases are targeted to enable the identification of a common AI/ML framework, including functional requirements of AI/ML architecture, which could be used in subsequent projects. The study also serves identifying areas where AI/ML could improve the performance of air-interface functions.\nThe study serves identifying what is required for an adequate AI/ML model characterization and description establishing pertinent notation for discussions and subsequent evaluations. Various levels of collaboration between the gNB and UE are identified and considered.\nEvaluations to exercise the attainable gains of AI/ML based techniques for the use cases under consideration are carried out with the corresponding identification of KPIs with the goal to have a better understanding of the attainable gains and associated complexity requirements.\nFinally, specification impact are assessed in order to improve the overall understanding of what would be required to enable AI/ML techniques for the air-interface.\nThe central objective of this project is to study the 3GPP framework for AI/ML for air-interface corresponding to each target use case regarding aspects such as performance, complexity, and potential specification impact.\nThe use cases to focus include:\n-\tCSI feedback enhancement\n-\tSpatial-frequency domain CSI compression using two-sided AI model\n-\tTime domain CSI prediction using UE sided model\n-\tBeam management\n-\tSpatial-domain Downlink beam prediction for Set A of beams based on measurement results of Set B of beams\n-\tTemporal Downlink beam prediction for Set A of beams based on the historic measurement results of Set B of beams\n-\tPositioning accuracy enhancements\n-\tDirect AI/ML positioning\n-\tAI/ML assisted positioning\nNote:\tthe selection of use cases for this study solely targets the formulation of a framework to apply AI/ML to the air-interface for these and other use cases. The selection itself does not intend to provide any indication of the prospects of any future normative project.\nThis study also introduces AI/ML model terminology and description to identify common and specific characteristics for framework investigations, namely to:\n-\tCharacterize the defining stages of AI/ML related algorithms and associated complexity:\n-\tModel generation, e.g., model training (including input/output, pre-/post-process, online/offline as applicable), model validation, model testing, as applicable\n-\tInference operation, e.g., input/output, pre-/post-process, as applicable\n-\tIdentify various levels of collaboration between UE and gNB pertinent to the selected use cases, e.g.,\n-\tNo collaboration: implementation-based only AI/ML algorithms without information exchange [for comparison purposes]\n-\tVarious levels of UE/gNB collaboration targeting at separate or joint ML operation.\n-\tCharacterize lifecycle management of AI/ML model: e.g., model training, model deployment, model inference, model monitoring, model updating\n-\tDataset(s) for training, validation, testing, and inference\n-\tIdentify common notation and terminology for AI/ML related functions, procedures and interfaces\n-\tNote: \tthe work done for FS_NR_ENDC_data_collect is considered when appropriate\nFor the use cases under consideration:\n1)\tPerformance benefits of AI/ML based algorithms for the agreed use cases are evaluated:\n-\tMethodology based on statistical models (from TR 38.901 and TR 38.857 [positioning]), for link and system level simulations.\n-\tExtensions of 3GPP evaluation methodology for better suitability to AI/ML based techniques should be considered as needed.\n-\tWhether field data are optionally needed to further assess the performance and robustness in real-world environments should be discussed as part of the study.\n-\tNeed for common assumptions in dataset construction for training, validation and test for the selected use cases.\n-\tConsider adequate model training strategy, collaboration levels and associated implications\n-\tConsider agreed-upon base AI model(s) for calibration\n-\tAI model description and training methodology used for evaluation should be reported for information and cross-checking purposes\n-\tKPIs: Determine the common KPIs and corresponding requirements for the AI/ML operations. Determine the use-case specific KPIs and benchmarks of the selected use-cases.\n-\tPerformance, inference latency and computational complexity of AI/ML based algorithms should be compared to that of a state-of-the-art baseline\n-\tOverhead, power consumption (including computational), memory storage, and hardware requirements (including for given processing delays) associated with enabling respective AI/ML scheme, as well as generalization capability should be considered.\n2)\tPotential specification impact, specifically for the agreed use cases and for a common framework, is assessed:\n-\tPHY layer aspects, e.g., (RAN1)\n-\tConsidering aspects related to, e.g., the potential specification of the AI Model lifecycle management, and dataset construction for training, validation and test for the selected use cases\n-\tUse case and collaboration level specific specification impact, such as new signalling, means for training and validation data assistance, assistance information, measurement, and feedback\n-\tProtocol aspects, e.g., (RAN2) - RAN2 only starts the work after there is sufficient progress on the use case study in RAN1\n-\tConsidering aspects related to, e.g., capability indication, configuration and control procedures (training/inference), and management of data and AI/ML model, per RAN1 input\n-\tCollaboration level specific specification impact per use case\n-\tInteroperability and testability aspects, e.g., (RAN4) - RAN4 only starts the work after there is sufficient progress on use case study in RAN1 and RAN2\n-\tRequirements and testing frameworks to validate AI/ML based performance enhancements and ensuring that UE and gNB with AI/ML meet or exceed the existing minimum requirements if applicable\n-\tConsidering the need and implications for AI/ML processing capabilities definition\nNote 1:\tSpecific AI/ML models are not expected to be specified and are left to implementation. User data privacy needs to be preserved.\nNote 2:\tThe study on AI/ML for air interface is based on the current RAN architecture and new interfaces shall not be introduced.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "2\tReferences",
            "description": "The following documents contain provisions which, through reference in this text, constitute provisions of the present document.\n-\tReferences are either specific (identified by date of publication, edition number, version number, etc.) or non-specific.\n-\tFor a specific reference, subsequent revisions do not apply.\n-\tFor a non-specific reference, the latest version applies. In the case of a reference to a 3GPP document (including a GSM document), a non-specific reference implicitly refers to the latest version of that document in the same Release as the present document.\n[1]\t3GPPÂ TRÂ 21.905: \"Vocabulary for 3GPP Specifications\".\n[2]\tRP-213599: \"New SI: Study on Artificial Intelligence (AI)/Machine Learning (ML) for NR Air Interface\", Qualcomm (Moderator).\n[3]\t3GPP TR 38.901: \"Study on channel model for frequencies from 0.5 to 100 GHz\".\n[4]\t3GPP TR 38.857: \"Study on NR positioning enhancements\".\n[5]\t3GPP TR 38.802: \"Study on new radio access technology Physical layer aspects\".\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "3\tDefinitions of terms, symbols and abbreviations",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "3.1\tTerms",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the terms given in TRÂ 21.905Â [1] and the following apply. A term defined in the present document takes precedence over the definition of the same term, if any, in TRÂ 21.905Â [1].\nAI/ML-enabled Feature: refers to a Feature where AI/ML may be used.\nAI/ML Model: A data driven algorithm that applies AI/ML techniques to generate a set of outputs based on a set of inputs.\nAI/ML model delivery: A generic term referring to delivery of an AI/ML model from one entity to another entity in any manner. Note: An entity could mean a network node/function (e.g., gNB, LMF, etc.), UE, proprietary server, etc.\nAI/ML model Inference:  A process of using a trained AI/ML model to produce a set of outputs based on a set of inputs.\nAI/ML model testing: A subprocess of training, to evaluate the performance of a final AI/ML model using a dataset different from one used for model training and validation. Differently from AI/ML model validation, testing does not assume subsequent tuning of the model.\nAI/ML model training: A process to train an AI/ML Model [by learning the input/output relationship] in a data driven manner and obtain the trained AI/ML Model for inference.\nAI/ML model transfer: Delivery of an AI/ML model over the air interface in a manner that is not transparent to 3GPP signalling, either parameters of a model structure known at the receiving end or a new model with parameters. Delivery may contain a full model or a partial model.\nAI/ML model validation: A subprocess of training, to evaluate the quality of an AI/ML model using a dataset different from one used for model training, that helps selecting model parameters that generalize beyond the dataset used for model training.\nData collection: A process of collecting data by the network nodes, management entity, or UE for the purpose of AI/ML model training, data analytics and inference.\nFederated learning / federated training: A machine learning technique that trains an AI/ML model across multiple decentralized edge nodes (e.g., UEs, gNBs) each performing local model training using local data samples. The technique requires multiple interactions of the model, but no exchange of local data samples.\nFunctionality identification: A process/method of identifying an AI/ML functionality for the common understanding between the NW and the UE. Note: Information regarding the AI/ML functionality may be shared during functionality identification. Where AI/ML functionality resides depends on the specific use cases and sub use cases.\nManagement instruction: Information needed to ensure proper inference operation. This information may include selection/(de)activation/switching of AI/ML models or AI/ML functionalities, fallback to non-AI/ML operation, etc.\nModel activation: enable an AI/ML model for a specific AI/ML-enabled feature.\nModel deactivation: disable an AI/ML model for a specific AI/ML-enabled feature.\nModel download: Model transfer from the network to UE.\nModel identification: A process/method of identifying an AI/ML model for the common understanding between the NW and the UE. Note: The process/method of model identification may or may not be applicable. Note: Information regarding the AI/ML model may be shared during model identification.\nModel monitoring: A procedure that monitors the inference performance of the AI/ML model.\nModel parameter update: Process of updating the model parameters of a model.\nModel selection: The process of selecting an AI/ML model for activation among multiple models for the same AI/ML enabled feature. Note: Model selection may or may not be carried out simultaneously with model activation.\nModel switching: Deactivating a currently active AI/ML model and activating a different AI/ML model for a specific AI/ML-enabled feature.\nModel update: Process of updating the model parameters and/or model structure of a model.\nModel upload: Model transfer from UE to the network.\nNetwork-side (AI/ML) model: An AI/ML Model whose inference is performed entirely at the network.\nOffline field data: The data collected from field and used for offline training of the AI/ML model.\nOffline training: An AI/ML training process where the model is trained based on collected dataset, and where the trained model is later used or delivered for inference. Note: This definition only serves as a guidance. There may be cases that may not exactly conform to this definition but could still be categorized as offline training by commonly accepted conventions.\nOnline field data: The data collected from field and used for online training of the AI/ML model.\nOnline training: An AI/ML training process where the model being used for inference) is (typically continuously) trained in (near) real-time with the arrival of new training samples. Note: the notion of (near) real-time vs. non real-time is context-dependent and is relative to the inference time-scale. Note: This definition only serves as a guidance. There may be cases that may not exactly conform to this definition but could still be categorized as online training by commonly accepted conventions. Note: Fine-tuning/re-training may be done via online or offline training.\nReinforcement Learning (RL): A process of training an AI/ML model from input (a.k.a. state) and a feedback signal (a.k.a.  reward) resulting from the modelâs output (a.k.a. action) in an environment the model is interacting with.\nSemi-supervised learning: A process of training a model with a mix of labelled data and unlabelled data.\nSupervised learning: A process of training a model from input and its corresponding labels.\nTest encoder/decoder for TE: AI/ML model for UE encoder/gNB decoder implemented by TE.\nTwo-sided (AI/ML) model: A paired AI/ML Model(s) over which joint inference is performed, where joint inference comprises AI/ML Inference whose inference is performed jointly across the UE and the network, i.e, the first part of inference is firstly performed by UE and then the remaining part is performed by gNB, or vice versa.\nUE-side (AI/ML) model: An AI/ML Model whose inference is performed entirely at the UE.\nUnsupervised learning: A process of training a model without labelled data.\nProprietary-format models: ML models of vendor-/device-specific proprietary format, from 3GPP perspective. They are not mutually recognizable across vendors and hide model design information from other vendors when shared. Note: An example is a device-specific binary executable format.\nOpen-format models: ML models of specified format that are mutually recognizable across vendors and allow interoperability, from 3GPP perspective. They are mutually recognizable between vendors and do not hide model design information from other vendors when shared.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.2\tSymbols",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the following symbols apply:\n<symbol>\t<Explanation>\n\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.3\tAbbreviations",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the abbreviations given in TRÂ 21.905 [1] and the following apply. An abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in TRÂ 21.905Â [1].\nAI\tArtificial Intelligence\nAPI\tApplication Programming Interface\nBM\tBeam Management\nCDF\tCumulative Distribution Function\nCIR\tChannel Impulse Response\nCNN\tConvolutional Neural Network\nCQI\tChannel Quality Indicator\nCSI\tChannel State Information\nDL\tDownlink\nDP\tDelay profile\nEVM\tEvaluation Methodology\nFLOP\tFloating point Operation*\nGCS\tGeneralized Cosine Similarity\nKPI\tKey Performance Indicator\nL1-RSRP\tLayer 1 reference signal received power\nLCM\tLife Cycle Management\nLLS\tLink Level Simulations\nLMF\tLocation Management Function\nLOS\tLine-of-Sight\nLPP\tLTE Positioning Protocol\nML\tMachine Learning\nNLOS\tNon-Line-of-Sight\nNMSE\tNormalized Mean Square Error\nNRPPa\tNR Positioning Protocol A\nNW\tNetwork\nPDP\tPower Delay Profile\nPRS\tPositioning Reference Signal\nPRU\tPositioning Reference Unit\nRNN\tRecurrent Neural Network\nRI\tRank Indicator\nRU\tResource Utilization\nSGCS\tSquared Generalized Cosine Similarity\nSLS\t\tSystem Level Simulations\nSQ\tScalar Quantization\nSRS\tSounding Reference Signal\nTRP\tTransmission-Reception Point\nTxRU\tTransceiver Unit\nUE\tUser Equipment\nUPT\tUser Perceived Throughput\nVQ\tVector Quantization\n\n* Note: \tCaution is advised not to confuse FLOPs (the plural form of FLOP), which a measure of model complexity, with FLOPS (FLoating point OPerations per Second), which is a measure of compute performance of a device.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "4\tGeneral AI/ML framework",
            "description": "The purpose of this clause is to identify common notation and terminology for AI/ML related functions, procedures and interfaces.\nNote:\tThe work done for FS_NR_ENDC_data_collect is considered when appropriate.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "4.1\tDescription of AI/ML stages",
                    "description": "",
                    "summary": "",
                    "text_content": "In this clause, the defining stages of AI/ML related algorithms and associated complexity are characterized, namely:\n-\tModel generation, e.g., model training (including input/output, pre-/post-process, online/offline as applicable), model validation, model testing, as applicable\n-\tInference operation, e.g., input/output, pre-/post-process, as applicable\nIn addition, the treatment of dataset(s) for training, validation, testing, and inference is documented.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.2\tLife Cycle Management",
                    "description": "",
                    "summary": "",
                    "text_content": "In this clause, the life cycle management (LCM) of AI/ML model (e.g., model training, model deployment, model inference, model monitoring, model updating) and AI/ML functionality are characterized.\nThe following aspects, including the definition of components (if needed) and necessity, are studied in LCM:\n-\tData collection\n-\tNote: \tThis also includes associated assistance information, if applicable.\n-\tModel training\n-\tFunctionality/model identification\n-\tModel delivery/transfer\n-\tModel inference operation\n-\tFunctionality/model selection, activation, deactivation, switching, and fallback operation.\n-\tIncluding: Decision by the network (either network initiated or UE-initiated and requested to the network), decision by the UE (event-triggered as configured by the network, UEâs decision reported to the network, or UE-autonomous either with UEâs decision reported to the network or without it)\n-\tFunctionality/model monitoring\n-\tModel update\n-\t\tUE capability\nNote: \tSome aspects in the list may not have specification impact.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.2.1\tLCM Flavours",
                            "text_content": "The LCM procedure is studied for the case that an AI/ML model has a model ID with associated information and/or for the case that a given functionality is provided by some AI/ML operations. Note: Applicability of functionality-based LCM and model-ID-based LCM is a separate discussion.\nFrom RAN1 perspective, an AI/ML model identified by a model ID may be logical, and how it maps to physical AI/ML model(s) may be up to implementation. When distinction is necessary for discussion purposes, companies may use the term a logical AI/ML model to refer to a model that is identified and assigned a model ID, and physical AI/ML model(s) to refer to an actual implementation of such a model.\nFor UE-side models and UE-part of two-sided models:\n-\tFor AI/ML functionality identification\n-\tLegacy 3GPP framework of feature is taken as a starting point.\n-\tUE indicates supported functionalities/functionality for a given sub-use-case.\n-\tUE capability reporting is taken as starting point.\n-\tFor AI/ML model identification\n-\tModels are identified by model ID at the Network. UE indicates supported AI/ML models.\nIn functionality-based LCM, network indicates activation/deactivation/fallback/switching of AI/ML functionality via 3GPP signalling (e.g., RRC, MAC-CE, DCI). Models \tmay not be identified at the Network, and UE may perform model-level LCM. Whether and how much awareness/interaction NW should have about model-level LCM requires further study. For functionality identification, there may be either one or more than one Functionalities defined within an AI/ML-enabled feature, whereby AI/ML-enabled Feature refers to a Feature where AI/ML may be used. Note: UE may have one AI/ML model for the functionality, or UE may have multiple AI/ML models for the functionality.\nFor AI/ML functionality identification and functionality-based LCM of UE-side models and/or UE-part of two-sided models, functionality refers to an AI/ML-enabled Feature/FG enabled by configuration(s), where configuration(s) is(are) supported based on conditions indicated by UE capability. Correspondingly, functionality-based LCM operates based on, at least, one configuration of AI/ML-enabled Feature/FG or specific configurations of an AI/ML-enabled Feature/FG.\nAfter functionality identification, necessity, mechanisms,Â forÂ UE toÂ report updates onÂ applicableÂ functionality(es) amongÂ functionality(es) are studied, where the applicableÂ functionalities may be a subset of allÂ functionalities. Applicable functionalities can be reported by the UE.\nIn model-ID-based LCM, models are identified at the Network, and Network/UE may activate/deactivate/select/switch individual AI/ML models via model ID.\nFor AI/ML model identification and model-ID-based LCM of UE-side models and/or UE-part of two-sided models, model-ID-based LCM operates based on identified models, where a model may be associated with specific configurations/conditions associated with UE capability of an AI/ML-enabled Feature/FG and additional conditions (e.g., scenarios, sites, and datasets) as determined/identified between UE-side and NW-side.\nAfter model identification, necessity, mechanisms,Â forÂ UE toÂ report updates onÂ applicableÂ UE part/UE-side model(s), are studied, where theÂ applicableÂ models may be a subset of all identified models. Applicable models can be reported by the UE.\nHow to handle the impact of UEâs internal conditions such as memory, battery, and other hardware limitations on functionality/model operations and AI/ML-enabled Feature is to be studied.  Note: it does not preclude any existing solutions.\nFor functionality/model-ID based LCM, once functionalities/models are identified, the same or similar procedures may be used for their activation, deactivation, switching, fallback, and monitoring.\nModel ID, if needed, can be used in a Functionality (defined in functionality-based LCM) for LCM operations.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.2.2\tModel identification",
                            "text_content": "For AI/ML model identification of UE-side or UE-part of two-sided models, model identification is categorized in the following types:\n-\tType A: Model is identified to NW (if applicable) and UE (if applicable) without over-the-air signalling\n-\tThe model may be assigned with a model ID during the model identification, which may be referred/used in over-the-air signalling after model identification.\n-\tType B: Model is identified via over-the-air signalling,\n-\tType B1:\n-\tModel identification initiated by the UE, and NW assists the remaining steps (if any) of the model identification\n-\tthe model may be assigned with a model ID during the model identification\n-\tType B2:\n-\tModel identification initiated by the NW, and UE responds (if applicable) for the remaining steps (if any) of the model identification\n-\tthe model may be assigned with a model ID during the model identification\n-\tNote: \tThis study does not imply that model identification is necessary.\nOne example use case for Type B1 and B2 is model identification in model transfer from NW to UE. Another example is model identification with data collection related configuration(s) and/or indication(s) and/or dataset transfer. Note: Other example use cases are not precluded. Note: Offline model identification may be applicable for some of the example use cases.\nOnce models are identified, at least for Type A, UE can indicate supported AI/ML model IDs for a given AI/ML-enabled Feature/FG in a UE capability report as starting point. Note: model identification using capability report is not precluded for type B1 and type B2.\nModel ID may or may not be globally unique, and different types of model IDs may be created for a single model for various LCM purposes. Note: Details can be studied in the WI phase.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.2.3\tAdditional conditions",
                            "text_content": "For an AI/ML-enabled feature/FG, additional conditions refer to any aspects that are assumed for the training of the model but are not a part of UE capability for the AI/ML-enabled feature/FG. It does not imply that additional conditions are necessarily specified. Additional conditions can be divided into two categories: NW-side additional conditions and UE-side additional conditions. Note: whether specification impact is needed is a separate discussion.\nFor inference for UE-side models, to ensure consistency between training and inference regarding NW-side additional conditions (if identified), the following options can be taken as potential approaches (when feasible and necessary):\n-\tModel identification to achieve alignment on the NW-side additional condition between NW-side and UE-side\n-\tModel training at NW and transfer to UE, where the model has been trained under the additional condition\n-\tInformation and/or indication on NW-side additional conditions is provided to UE\n-\tConsistency assisted by monitoring (by UE and/or NW, the performance of UE-side candidate models/functionalities to select a model/functionality)\n-\tOther approaches are not precluded\n-\tNote: \tthe possibility that different approaches can achieve the same function is not denied\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.2.4\tScenario/configuration specific Models",
                            "text_content": "Scenario/configuration specific (including site-specific configuration/channel conditions) models may provide performance benefits in some studied use cases (i.e., when a single model cannot generalize well to multiple scenarios/configurations/sites).\n-\tAt least, when UE has limitation to store all related models, model delivery/transfer, if feasible, to UE may be beneficial, at the cost of overhead/latency associated with model delivery/transfer.\n-\tNote: On-device Finetuning/retraining, if feasible, of a single model may be an alternative to model delivery/transfer.\n-\tNote: a single model may generalize well in some studied use cases.\n-\tNote: Model transfer/delivery to UE may also face challenges, e.g., proprietary issues /burdens in some scenarios\nVarious approaches for achieving good performance across different scenarios/configurations/sites are studied, including\n-\tModel generalization, i.e., using one model that is generalizable to different scenarios/configurations/sites\n-\tModel switching, i.e., switching among a group of models where each model is for a particular scenario/configuration/site\n-\tModels in a group of models may have varying model structures, share a common model structure, or partially share a common sub-structure. Models in a group of models may have different input/output format and/or different pre-/post-processing.\n-\tModel update, i.e., using one model whose parameters are flexibly updated as the scenario/configuration/site that the device experiences changes over time. Fine-tuning is one example.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.2.5\tData collection",
                            "text_content": "Data collection may be performed for different purposes in LCM, e.g., model training, model inference, model monitoring, model selection, model update, etc. each may be done with different requirements and potential specification impact.\nFor all types of offline model training (i.e., UE- /NW-/ two-sided model training), there is no latency requirement for data collection. For model inference, when required data comes from other entities, there is a latency requirement for data collection. For (real-time) performance monitoring, when required monitoring data (e.g., performance metric) comes from other entities, there is a latency requirement for data collection.\nAt least for the use cases studied in this study item, it is assumed that the analysis/selection of the data collection frameworks should focus on the RRC_CONNECTED state (for both data generation and reporting). Analysis and potential enhancement of the non-connected state can be revisited when needed. Note that existing specification supports DL PRS measurement and UE positioning in both RRC_CONNECTED and RRC_INACTIVE state.\nAt least the following aspects, if applicable, are considered along with the corresponding specification impact:\n-\tMeasurement configuration and reporting\n-\tContents, type and format of data including:\n-\tData related to model input\n-\tData related to ground-truth\n-\tQuality of the data\n-\tOther information\n-\tSignalling of assistance information for categorizing the data\n-\tNote: The study should consider the feasibility of disclosure of proprietary information\n-\tSignalling for data collection procedure\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.3\tCollaboration levels",
                    "description": "",
                    "summary": "",
                    "text_content": "In this clause, various levels of collaboration between UE and Network are identified as found pertinent to the selected use cases, e.g.,\n-\tNo collaboration: implementation-based only AI/ML algorithms without information exchange for comparison purposes\n-\tVarious levels of UE/Network collaboration targeting at separate or joint ML operation\nThe following Network-UE collaboration levels are considered as one aspect for defining collaboration levels\n1.\tLevel x: No collaboration.\n2.\tLevel y: Signalling-based collaboration without model transfer. Note: this level includes cases without model delivery.\n3.\tLevel z: Signalling-based collaboration with model transfer.\nLevel x/y boundary is understood such as Level x is implementation-based AI/ML operation without any dedicated AI/ML-specific enhancement (e.g., LCM related signalling, RS) collaboration between network and UE. (Note: The AI/ML operation may rely on future specification not related to AI/ML collaboration. The AI/ML approaches can be used as baseline for performance evaluation for future releases.)\nLevel y/z boundary is defined based on whether model delivery over the air interface is done in a non-transparent manner to 3GPP signalling. Note: procedures other than model transfer/delivery are decoupled with collaboration Level y-z.\nTable 4.3-1 introduces different options for model delivery/transfer to UE, training location, and model delivery/transfer format combinations for UE-side models and UE-part of two-sided models:\nTable 4.3-1: Model delivery/transfer cases\n\nWhen a model of a known structure at UE (e.g., Case z4) is transferred from the Network, the new model being identified (e.g., via Type B2) has the same structure as a previously identified model at the Network and UE.\nFor model delivery/transfer to UE (for UE-side models and UE-part of two-sided models):\n-\tModel delivery/transfer to UE, if feasible, may be beneficial to handle scenario/configuration specific (including site-specific configuration/channel conditions) models (i.e., when a single model cannot generalize well to multiple scenarios/configurations/sites), to reduce the device storage requirement.\n-\tModel delivery/transfer to UE after offline compiling and/or testing may be friendlier from UEâs implementation point of view compared to the case without offline compiling and/or testing. On the other hand, the case without offline compiling and/or testing (that can update parameter with known model structure), may have benefit at least in terms of shorter model parameter update timescale.\n-\tModel transfer/delivery of an unknown structure at UE has more challenges related to feasibility (e.g. UE implementation feasibility) compared to delivery/transfer of a known structure at UE.\n-\tFor model trained at network side, Case y (w/ NW-side training) and Case z2 may incur the burden of offline cross-vendor collaboration such as sending a model to the UE-side and/or compiling a model.\n-\tFor model trained at UE side/neutral site, Case z1 and Case z3 may incur the burden of offline cross-vendor collaboration to send the trained model from the UE-side to the network, compared to Case y (w/ UE-side training) which does not have such burden.\n-\tModel storage at the 3GPP network, compared to storing the model outside the 3GPP network, may come with 3GPP network side burden on model maintenance/storage.\n-\tProprietary design disclosure concern may arise from model training and/or model storage at the network side compared to other cases (such as case y with UE side training) which does not have such issue.\n",
                    "tables": [
                        {
                            "description": "Table 4.3-1: Model delivery/transfer cases",
                            "table number": 3,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.4\tFunctional framework details",
                    "description": "",
                    "summary": "",
                    "text_content": "This section introduces the functional framework for AI/ML for NR air interface illustrated in Figure 4.4-1. The aim of this framework is to cover a general functional architecture addressing both model-ID-based LCM and functionality-based LCM, introduced in clause 4.2. Therefore, some of the functions or data/information/instruction flows (i.e., the arrows) shown in the Figure 4.4-1 might not always be relevant for a given LCM approach. As an illustrative example, consider a scenario where the network performs functionality-based LCM and where models are not identified in the network, while the UE concurrently performs model-level management (e.g., model selection/switching/(de)activation, etcâ¦). In this hypothetical case, the \"Model Training\" or \"Model Storage\" functions with their respective procedures, may be regarded as irrelevant from the networkâs perspective.\nIn clause 7, the functions and data/information/instruction flows (i.e., the arrows) depicted in Figure 4.4-1 are analysed for any standardization impact and its implications.\nNote: The functional framework and high-level procedures defined in this TR should not prevent from \"thinking beyond\" them during a normative phase if any use case requires so.\nThe figure depicts a functional framework for AI/ML in the NR Air Interface, illustrating the various components and their interactions. It includes AI/ML algorithms, data processing, and communication protocols, all designed to optimize the air interface for efficient and reliable communication.\nFigure 4.4-1: Functional framework for AI/ML for NR Air Interface\nAs seen in Figure 4.4-1, the general framework consists of the following:\n-\tData Collection is a function that provides input data to the Model Training, Management, and Inference functions.\n-\tTraining Data: Data needed as input for the AI/ML Model Training function.\n-\tMonitoring Data: Data needed as input for the Management of AI/ML models or AI/ML functionalities.\n-\tInference Data: Data needed as input for the AI/ML Inference function.\n-\tModel Training is a function that performs AI/ML model training, validation, and testing which may generate model performance metrics which can be used as part of the model testing procedure. The Model Training function is also responsible for data preparation (e.g., data pre-processing and cleaning, formatting, and transformation) based on Training Data delivered by a Data Collection function, if required.\n-\tTrained/Updated Model: In case of having a Model Storage function, this is used to deliver trained, validated, and tested AI/ML models to the Model Storage function, or to deliver an updated version of a model to the Model Storage function.\n-\tManagement is a function that oversees the operation (e.g., selection/(de)activation/switching/fallback) and monitoring (e.g., performance) of AI/ML models or AI/ML functionalities. This function is also responsible for making decisions to ensure the proper inference operation based on data received from the Data Collection function and the Inference function.\n-\tManagement Instruction: Information needed as input to manage the Inference function. Concerning information may include selection/(de)activation/switching of AI/ML models or AI/ML-based functionalities, fallback to non-AI/ML operation (i.e., not relying on inference process), etcâ¦\n-\tModel Transfer/Delivery Request: Used to request model(s) to the Model Storage function.\n-\tPerformance Feedback / Retraining Request: Information needed as input for the Model Training function, e.g., for model (re)training or updating purposes.\n-\tInference is a function that provides outputs from the process of applying AI/ML models or AI/ML functionalities, using the data that is provided by the Data Collection function (i.e., Inference Data) as an input. The Inference function is also responsible for data preparation (e.g., data pre-processing and cleaning, formatting, and transformation) based on Inference Data delivered by a Data Collection function, if required.\n-\tInference Output: Data used by the Management function to monitor the performance of AI/ML models or AI/ML functionalities.\n-\tModel Storage is a function responsible for storing trained/updated models that can be used to perform the Inference function.\n-\tNote: The Model Storage function in Figure 4.4-1 is only intended as a reference point (if any) when applicable for protocol terminations, model transfer/delivery, and related processes. It should be stressed that its purpose does not encompass restricting the actual storage locations of models. Therefore, the specification impact of all data/information/instruction flows (i.e., the arrows in Figure 4.4-1) to/from this function should be studied case by case.\n-\tModel Transfer/Delivery: Used to deliver an AI/ML model to the Inference function.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "5\tUse cases",
            "description": "Initial set of use cases includes:\n-\tCSI feedback enhancement, e.g., overhead reduction, improved accuracy, prediction [RAN1]\n-\tBeam management, e.g., beam prediction in time,Â and/orÂ spatial domainÂ for overhead and latency reduction, beam selection accuracy improvement [RAN1]\n-\tPositioning accuracy enhancements for different scenarios including, e.g., those withÂ heavy NLOS conditions [RAN1]\n-\tThe AI/ML approaches for the selected sub use cases need to be diverse enough to support various requirements on the gNB-UE collaboration levels\nNote: the selection of use cases for this study solely targets the formulation of a framework to apply AI/ML to the air-interface for these and other use cases. The selection itself does not intend to provide any indication of the prospects of any future normative project.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "5.1\tCSI feedback enhancement",
                    "description": "",
                    "summary": "",
                    "text_content": "Finalization of representative sub-use cases:\nThe following are selected as representative sub-use cases:\n-\tSpatial-frequency domain CSI compression using two-sided AI model. Note: All pre-processing/post-processing, quantization/de-quantization are within the scope of the sub use case.\n-\tThe study of AI/ML based CSI compression should be based on the legacy CSI feedback signalling framework.\n-\tTime domain CSI prediction using UE-side model.\nFor CSI compression using two-sided model use case, considered AI/ML model training collaborations include:\n-\tType 1: Joint training of the two-sided model at a single side/entity, e.g., UE-sided or Network-sided.\n-\tType 2: Joint training of the two-sided model at network side and UE side, respectively.\n-\tType 3: Separate training at network side and UE side, where the UE-side CSI generation part and the NW-side CSI reconstruction part are trained by UE side and network side, respectively.\n-\tNote: Joint training means the generation model and reconstruction model should be trained in the same loop for forward propagation and backward propagation. Joint training could be done both at single node or across multiple nodes (e.g., through gradient exchange between nodes).\n-\tNote: Separate training includes sequential training starting with UE side training, or sequential training starting with NW side training\n-\tNote: training collaboration Type 2 over the air interface for model training (not including model update) is concluded to be deprioritized in Rel-18 SI.\nFor Type 2 (Joint training of the two-sided model at network side and UE side, respectively), note that joint training includes both simultaneous training and sequential training, in which the pros and cons could be discussed separately. Further, note that Type 2 sequential training starts with NW side training.\nIn CSI compression using two-sided model use case, for discussion of training collaboration Type 1, separate columns are shown for both known model structure, and unknown model structure separately for NW-sided and UE-sided, respectively. Table 5.1-1 captures the pros/cons of training collaboration Type 1 for CSI compression using two-sided model use case.\nTable 5.1-1: Pros and Cons of training collaboration Type 1\n\nTable 5.1-2 captures the pros/cons of training collaboration Type 2 and Type 3 for CSI compression using two-sided model use case.\nTable 5.1-2: Pros and Cons of training collaboration Type 2 and Type 3\n\nFor CSI compression use case:\n-\tFor model training, training data can be generated by UE/gNB\n-\tFor NW-part of two-sided model inference, input data can be generated by UE and terminated at gNB.\n-\tFor UE-part of two-sided model inference, input data is internally available at UE.\n-\tFor performance monitoring at the NW side, calculated performance metrics (if needed) or data needed for performance metric calculation (if needed) can be generated by UE and terminated at gNB\nIn CSI compression using two-sided model use case, in order to select a CSI generation model compatible with the CSI reconstruction model used by the gNB, the following aspect has been proposed:\n-\tPairing information can be established based on model identification\nFor CSI prediction use cases:\n-\tFor model training, training data can be generated by UE.\n-\tFor UE-side model inference, input data is internally available at UE.\n-\tFor performance monitoring at the NW side, calculated performance metrics (if needed) or data needed for performance metric calculation (if needed) can be generated by UE and terminated at gNB.\n",
                    "tables": [
                        {
                            "description": "Table 5.1-1: Pros and Cons of training collaboration Type 1",
                            "table number": 4,
                            "summary": "",
                            "name": ""
                        },
                        {
                            "description": "Table 5.1-2: Pros and Cons of training collaboration Type 2 and Type 3",
                            "table number": 5,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "5.2\tBeam management",
                    "description": "",
                    "summary": "",
                    "text_content": "Finalization of representative sub-use cases:\nThe following are selected as representative sub-use cases:\n-\tBM-Case1: Spatial-domain Downlink beam prediction for Set A of beams based on measurement results of Set B of beams\n-\tConsider: Alt. 1): AI/ML model training and inference at NW side. Alt. 2): AI/ML model training and inference at UE side.\n-\tConsider: Alt. i): Set A and Set B are different (Set B is NOT a subset of Set A). Alt. ii): Set B is a subset of Set A. Note: Set A is for DL beam prediction. The codebook construction of Set A and Set B can be clarified by companies.\n-\tAI/ML model input consider: Alt 1): Only L1-RSRP measurement based on Set B; Alt.2): L1-RSRP measurement based on Set B and assistance information; Alt. 3): CIR based on Set B; Alt. 4): L1-RSRP measurement based on Set B and the corresponding DL Tx and/or Rx beam ID.\n-\tBM-Case2: Temporal Downlink beam prediction for Set A of beams based on the historic measurement results of Set B of beams\n-\tConsider: Alt. 1): AI/ML model training and inference at NW side. Alt. 2): AI/ML model training and inference at UE side.\n-\tConsider: Alt. i): Set A and Set B are different (Set B is NOT a subset of Set A). Alt. ii): Set B is a subset of Set A (Set A and Set B are not the same). Alt. iii): Set A and Set B are the same.\n-\tAI/ML model input consider: measurement results of K (Kâ¥1) latest measurement instances with the following alternatives: Alt. 1): Only L1-RSRP measurement based on Set B; Alt 2): L1-RSRP measurement based on Set B and assistance information; Alt. 3): L1-RSRP measurement based on Set B and the corresponding DL Tx and/or Rx beam ID.\n-\tF predictions for F future time instances can be obtained based on the output of AI/ML model, where each prediction is for each time instance. At least F=1.\nSet B is a set of beams whose measurements are taken as inputs of the AI/ML model.\nNote:\tBeams in Set A and Set B can be in the same Frequency Range.\nFor both sub-use cases, the following alternatives are studied for the predicted beams:\n-\tAlt.1: DL Tx beam prediction\n-\tAlt.2: DL Rx beam prediction (deprioritized)\n-\tAlt.3: Beam pair prediction (a beam pair consists of a DL Tx beam and a corresponding DL Rx beam)\nNote:\tDL Rx beam prediction may or may not have spec impact.\nThe following alternatives according to AI/ML model output are considered:\n-\tAlt.1: Tx and/or Rx Beam ID(s) and/or the predicted L1-RSRP of the N predicted DL Tx and/or Rx beams\n-\te.g., N predicted beams can be the Top-N predicted beams\n-\tAlt.2: Tx and/or Rx Beam ID(s) of the N predicted DL Tx and/or Rx beams and other information\n-\te.g., N predicted beams can be the Top-N predicted beams\n-\tAlt.3: Tx and/or Rx Beam angle(s) and/or the predicted L1-RSRP of the N predicted DL Tx and/or Rx beams\n-\te.g., N predicted beams can be the Top-N predicted beams\nNotes:\tIt is up to companies to provide other alternative(s). Beam ID is only used for discussion purposes. All the outputs are \"nominal\" and only for discussion purpose. The value of N is up to each company. All of the outputs in the above alternatives may vary based on whether the AI/ML model inference is at UE side or gNB side. The Top-N beam IDs might have been derived via post-processing of the ML-model output.\nFor BM-Case1 and BM-Case2 with a UE-side AI/ML model, the necessity and potential BM-specific conditions/additional conditions for functionality(ies) and/or model(s) are considered at least from the following aspects:\n-\tinformation regarding model inference\n-\tSet A / Set B configuration\n-\tperformance monitoring\n-\tdata collection\n-\tassistance information\nFor beam management use cases:\n-\tFor model training, training data can be generated by UE/gNB.\n-\tFor NW-side model inference, input data can be generated by UE and terminated at gNB.\n-\tFor UE-side model inference, input data is internally available at UE.\n-\tFor performance monitoring at the NW side, calculated performance metrics (if needed) or data needed for performance metric calculation (if needed) can be generated by UE and terminated at gNB.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "5.3\tPositioning accuracy enhancements",
                    "description": "",
                    "summary": "",
                    "text_content": "Finalization of representative sub-use cases:\nThe following are selected as representative sub-use cases:\n-\tDirect AI/ML positioning:\n-\tAI/ML model output: UE location\n-\te.g., fingerprinting based on channel observation as the input of AI/ML model\n-\tAI/ML assisted positioning:\n-\tAI/ML model output: new measurement and/or enhancement of existing measurement\n-\te.g., LOS/NLOS identification, timing and/or angle of measurement, likelihood of measurement\nMore specifically, the following Cases are considered for the study:\n-\tCase 1: UE-based positioning with UE-side model, direct AI/ML or AI/ML assisted positioning\n-\tCase 2a: UE-assisted/LMF-based positioning with UE-side model, AI/ML assisted positioning\n-\tCase 2b: UE-assisted/LMF-based positioning with LMF-side model, direct AI/ML positioning\n-\tCase 3a: NG-RAN node assisted positioning with gNB-side model, AI/ML assisted positioning\n-\tCase 3b: NG-RAN node assisted positioning with LMF-side model, direct AI/ML positioning\nOne-sided model whose inference is performed entirely at the UE or at the network is prioritized in Rel-18 SI.\nFor all five positioning cases (Case 1/2a/2b/3a/3b), RAN1 has not considered prioritization.\n\nFor positioning enhancement use case:\n-\tFor model training, training data can be generated by UE/PRU/gNB/LMF.\n-\tFor LMF-side model inference (Case 2b, Case 3b), input data can be generated by UE/gNB and terminated at LMF.\n-\tFor gNB-side model inference (Case 3a), input data is internally available at gNB.\n-\tFor UE-side model inference (Case 1, Case 2a), input data is internally available at UE.\n-\tFor performance monitoring at the LMF side, calculated performance metrics (if needed) or data needed for performance metric calculation (if needed) can be generated by UE/gNB and terminated at LMF.\n-\tFor performance monitoring at the gNB side, calculated performance metrics (if needed) or data needed for performance metric calculation (if needed) can be generated by at least gNB.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "6\tEvaluations",
            "description": "In this clause, performance benefits of AI/ML based algorithms for the agreed use cases in the final representative set are evaluated.\nThe evaluation methodology is based on statistical models (from TR 38.901 and TR 38.857 for positioning), for link and system level simulations.\n-\tExtensions of 3GPP evaluation methodology for better suitability to AI/ML based techniques should be considered as needed.\n-\tWhether field data are optionally needed to further assess the performance and robustness in real-world environments should be discussed as part of the study.\n-\tNeed for common assumptions in dataset construction for training, validation and test for the selected use cases.\n-\tConsider adequate model training strategy, collaboration levels and associated implications\n-\tConsider agreed-upon base AI model(s) for calibration\n-\tAI model description and training methodology used for evaluation should be reported for information and cross-checking purposes\nCommon KPIs and corresponding requirements for the AI/ML operations are to be determined. Also, use-case specific KPIs and benchmarks of the selected use-cases are to be determined.\n-\tPerformance, inference latency and computational complexity of AI/ML based algorithms should be compared to that of a state-of-the-art baseline\n-\tOverhead, power consumption (including computational), memory storage, and hardware requirements (including for given processing delays) associated with enabling respective AI/ML scheme, as well as generalization capability should be considered.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "6.1\tCommon evaluation methodology and KPIs",
                    "description": "",
                    "summary": "",
                    "text_content": "3GPP channel models (TR 38.901) are used as the baseline for evaluations. Note: additional results based on dataset other than that generated by 3GPP channel models are allowed.\nCommon KPIs (if applicable):\n-\tPerformance\n-\tIntermediate KPIs\n-\tLink and system level performance\n-\tGeneralization performance\n-\tOver-the-air Overhead\n-\tOverhead of assistance information\n-\tOverhead of data collection\n-\tOverhead of model delivery/transfer\n-\tOverhead of other AI/ML-related signalling\n-\tInference complexity, including complexity for pre- and post-processing\n-\tComputational complexity of model inference: TOPs, FLOPs, MACs\n-\tthere may be a disconnect between the actual complexity and the complexity evaluated as captured in clause 6 using these KPIs due to the platform-dependency and implementation (hardware and software) optimization solutions\n-\tComputational complexity for pre- and post-processing\n-\tModel complexity: e.g., the number of parameters and/or size (e.g., Mbyte)\n-\tComplexity shall be reported in terms of \"number of real-value model parameters\" and \"number of real-value operations\" regardless of underlying model arithmetic\n-\tTraining complexity\n-\tLCM related complexity and storage overhead\n-\tStorage/computation for training data collection\n-\tStorage/computation for training and model update\n-\tStorage/computation for model monitoring\n-\tStorage/computation for other LCM procedures, e.g., model activation, deactivation, selection, switching, fallback operation\nFor evaluation of performance monitoring approaches, the following model monitoring KPIs are considered as general guidance:\n-\tAccuracy and relevance (i.e., how well does the given monitoring metric/methods reflect the model and system performance)\n-\tOverhead (e.g., signalling overhead associated with model monitoring)\n-\tComplexity (e.g., computation and memory cost for model monitoring)\n-\tLatency (i.e., timeliness of monitoring result, from model failure to action, given the purpose of model monitoring)\nNote: Other KPIs are not precluded. Relevant KPIs may vary across different model monitoring approaches.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "6.2\tCSI feedback enhancement",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.2.1\tEvaluation assumptions, methodology and KPIs",
                            "text_content": "For the performance evaluation of the AI/ML based CSI feedback enhancement, system level simulation approach is adopted as baseline. Link level simulations are optionally adopted.\nFor calibration purposes on the dataset and/or AI/ML model across companies, companies were encouraged to align the parameters (e.g., for scenarios/channels) for generating the dataset in the simulation as a starting point.\nPerforming intermediate evaluations on AI/ML model performance can be considered to derive the intermediate KPI(s) (e.g., accuracy of AI/ML output CSI) for the purpose of AI/ML solution comparison. If realistic DL channel estimation is considered, CSI accuracy is calculated using the target CSI from ideal channel and the output CSI from the realistic channel estimation. The target CSI from ideal channel equally applies to AI/ML based CSI feedback enhancement, and the baseline codebook.\nKPIs and Evaluation metrics:\n-\tCapability/complexity: Floating point operations (FLOPs), AI/ML memory storage in terms of AI/ML model size and number of AI/ML parameters reported by companies who may select either or both\n-\tReported separately for the CSI generation part and the CSI reconstruction part (for CSI compression sub-use case)\n-\tWhen reporting the computational complexity including the pre-processing and post-processing, the complexity metric of FLOPs may be reported separately for the AI/ML model and the pre/post processing. While reporting the FLOPs of pre-processing and post-processing the following boundaries are considered:\n-\tEstimated raw channel matrix per each frequency unit as an input for pre-processing of the CSI generation part.\n-\tPrecoding vectors per each frequency unit as an output of post-processing of the CSI reconstruction part.\n-\tCSI compression: Intermediate KPIs: SGCS and/or NMSE to evaluate the accuracy of the AI/ML output CSI\n-\tFor rank>1 cases, SGCS calculation/extension methods are to be reported:\n-\tSGCS separately calculated for each layer (e.g., for K layers, K SGCS values are derived respectively, and comparison is performed per layer). Companies to ensure the correct calculation of SGCS and to avoid disorder issue of the output eigenvectors. Note: Eventual KPI can still be used to compare the performance.\n-\tThe granularity of the frequency unit for averaging operation is assumed to be:\n-\tFor 15kHz SCS: For 10MHz bandwidth: 4 RBs; for 20MHz bandwidth: 8 RBs\n-\tFor 30kHz SCS: For 10MHz bandwidth: 2 RBs; for 20MHz bandwidth: 4 RBs\n-\tOther frequency unit granularities not precluded.\n-\tCSI compression: Intermediate KPI: model monitoring mechanism is considered as:\n-\tStep 1: Generate test dataset including K test samples.\n-\tStep 2: For each of the K test samples, a bias factor of monitored intermediate KPI (KPIDiff) is calculated as a function of KPIDiff = f ( KPIActual , KPIGenie ), where KPIActual is the actual intermediate KPI, and KPIGenie is the genie-aided intermediate KPI.\n-\tKPIDiff is considered for:\n-\tCase 1: NW side monitoring of intermediate KPI, where the monitoring accuracy is evaluated for a given ground-truth CSI format (e.g., quantized ground-truth CSI with 8 bits scalar, R16 eType II-like method, etc.) or SRS measurements, where\n-\tKPIActual is calculated with the output CSI at the NW side and the given ground-truth CSI format or SRS measurements.\n-\tKPIGenie is calculated with output CSI (as for KPIActual) and the ground-truth CSI of Float32\n-\tNote: if Float32 is used for KPIActual, the monitoring accuracy is 100% if KPIActual and KPIGenie are based on the same CSI sample.\n-\tCase 2: UE side monitoring of intermediate KPI with a proxy model, where the monitoring accuracy is evaluated for the output of the proxy model at UE:\n-\tCase 2-1: the proxy model is a proxy CSI reconstruction part, and KPIActual is calculated based on the inference output of the proxy CSI reconstruction part at UE and the ground-truth CSI. Note: if the proxy CSI reconstruction model is the same as the actual CSI reconstruction model at the NW, the monitoring accuracy is 100%.\n-\tCase 2-2: the proxy model directly outputs intermediate KPI (KPIActual)\n-\tKPIGenie is calculated with the output CSI at the NW side and the same ground-truth CSI.\n-\tKPIDiff = f ( KPIActual , KPIGenie ) can take the following forms:\n-\tOption 1 (baseline for calibration): Gap between KPIActual and KPIGenie, i.e. KPIDiff = (KPIActual - KPIGenie); Monitoring accuracy is the percentage of samples for which | KPIDiff| < KPIth 1, where KPIth 1 is a threshold of the intermediate KPI gap which can take the following values: 0.02, 0.05 and 0.1.\n-\tOption 2 (optional and up to companies to report): Binary state where KPIActual and KPIGenie, have different relationships to their threshold(s), i.e., KPIDiff = (KPIActual > KPIth 2, KPIGenie < KPIth 3) OR (KPIActual < KPIth 2, KPIGenie > KPIth 3), where KPIth 2 is considered to be the same as KPIth 3. Monitoring accuracy is the percentage of samples for which KPIDiff = 0.\n-\tStep 3: Calculate the statistical result of the KPIDiff over K test samples which represents the monitoring accuracy performance.\n-\tNote:  is introduced for the evaluation and comparison purpose; it may not be available in the real network.\n-\tNote: the complexity, overhead and latency of the monitoring scheme are to be reported.\n-\tCSI prediction: Intermediate KPIs: calculated for each predicted instance if AI/ML model outputs multiple predicted instances\n-\tIf collaboration level x is reported as the benchmark, the EVM to distinguish level x and level y/z based AI/ML CSI prediction is considered from the generalization aspect, e.g., collaboration level y/z based CSI prediction is modelled as the fine-tuning case or generalization Case 1, while collaboration level x based CSI prediction is modelled as generalization Case 2 or Case 3.\n-\tThroughput including: average UPT, 5%-ile UE throughput, and CDF of UPT\nModel generalization:\nThe following cases are considered for verifying the generalization performance of an AI/ML model over various scenarios/configurations:\n-\tCase 1: The AI/ML model is trained based on training dataset from one Scenario#B/Configuration#B, and then the AI/ML model performs inference/test on a dataset from the same Scenario#B/Configuration#B\n-\tCase 2: The AI/ML model is trained based on training dataset from a different data set than Scenario#B/Configuration#B, e.g., Scenario#A/Configuration#A, Scenario#B/Configuration#A, Scenario#A/Configuration#B, and then the AI/ML model performs inference/test on Scenario#B/Configuration#B.\n-\tCase 3: The AI/ML model is trained based on training dataset constructed by mixing datasets from multiple scenarios/configurations including Scenario#B/Configuration#B and a different dataset than Scenario#B/Configuration#B, e.g., Scenario#A/Configuration#A, Scenario#B/Configuration#A, Scenario#A/Configuration#B, and then the AI/ML model performs inference/test on a dataset from a single Scenario/Configuration from Scenario#B/Configuration#B.\n-\tNote: Companies to report the ratio for dataset mixing\n-\tNote: number of the multiple scenarios/configurations can be larger than two\nTo verify the generalization performance of an AI/ML model over various scenarios, the set of scenarios are considered focusing on one or more of the following aspects:\n-\tVarious deployment scenarios (e.g., UMa, UMi, InH)\n-\tVarious outdoor/indoor UE distributions for UMa/UMi (e.g., 10:0, 8:2, 5:5, 2:8, 0:10)\n-\tVarious carrier frequencies (e.g., 2GHz, 3.5GHz)\n-\tOther aspects of scenarios are not precluded, e.g., various antenna spacing, various antenna virtualization (TxRU mapping), various ISDs, various UE speeds, etc.\n-\tCompanies to report the selected scenarios for generalization verification\nTo verify the generalization/scalability performance of an AI/ML model over various configurations (e.g., which may potentially lead to different dimensions of model input/output), the set of configurations are considered focusing on one or more of the following aspects:\n-\tVarious bandwidths (e.g., 10MHz, 20MHz) and/or frequency granularities, (e.g., size of subband)\n-\tVarious sizes of CSI feedback payloads\n-\tVarious antenna port layouts, e.g., (N1/N2/P) and/or antenna port numbers (e.g., 32 ports, 16 ports)\n-\tVarious UE speeds (e.g., 10km/h, 30km/h, 60km/h, 120km/h, etc.) for CSI prediction sub use case\n-\tOther aspects of configurations are not precluded, e.g., various numerologies, various rank numbers/layers, etc.\n-\tThe selected configurations for generalization verification\n-\tThe method to achieve generalization over various configurations to achieve scalability of the AI/ML input/output, including pre-processing, post-processing, etc\nFor evaluating the generalization/scalability over various configurations for CSI compression, to achieve the scalability over different input/output dimensions, companies to report which case(s) are evaluated from the following list:\n-\tCase 0 (benchmark for comparison): One CSI generation part with fixed input and output dimensions to 1 CSI reconstruction part with fixed input and output dimensions for each of the different input and/or output dimensions.\n-\tCase 1: One CSI generation part with scalable input and/or output dimensions to N>1 separate CSI reconstruction parts each with fixed and different output and/or input dimensions\n-\tCase 2: M>1 separate CSI generation parts each with fixed and different input and/or output dimensions to one CSI reconstruction part with scalable output and/or input dimensions\n-\tCase 3: A pair of CSI generation part with scalable input/output dimensions and CSI reconstruction part with scalable output and/or input dimensions\nFor CSI compression, to achieve the scalability over different input dimensions of CSI generation part (e.g., different bandwidths/frequency granularities, or different antenna ports), the generalization cases are elaborated as follows:\n-\tCase 1: The AI/ML model is trained based on training dataset from a fixed dimension X1 (e.g., a fixed bandwidth/frequency granularity, and/or number of antenna ports), and then the AI/ML model performs inference/test on a dataset from the same dimension X1.\n-\tCase 2: The AI/ML model is trained based on training dataset from a single dimension X1, and then the AI/ML model performs inference/test on a dataset from a different dimension X2.\n-\tCase 3: The AI/ML model is trained based on training dataset by mixing datasets subject to multiple dimensions of X1, X2,..., Xn, and then the AI/ML model performs inference/test on a single dataset subject to the dimension of X1, or X2,â¦, or Xn.\n-\tNote: For Case 2/3, the solutions to achieve the scalability between Xi and Xj, are reported by companies, including, e.g., pre-processing to angle-delay domain, padding, additional adaptation layer in AI/ML model, etc.\nFor CSI compression, to achieve the scalability over different output dimensions of CSI generation part (e.g., different generated CSI feedback dimensions), the generalization cases of are elaborated as follows\n-\tCase 1: The AI/ML model is trained based on training dataset from a fixed output dimension Y1 (e.g., a fixed CSI feedback dimension), and then the AI/ML model performs inference/test on a dataset from the same output dimension Y1.\n-\tCase 2: The AI/ML model is trained based on training dataset from a single output dimension Y1, and then the AI/ML model performs inference/test on a dataset from a different output dimension Y2.\n-\tCase 3: The AI/ML model is trained based on training dataset by mixing datasets subject to multiple dimensions of Y1, Y2,..., Yn, and then the AI/ML model performs inference/test on a single dataset of Y1, or Y2,â¦, or Yn.\n-\tNotes: For Case 1/2/3, companies to report whether the output of the CSI generation part is before quantization or after quantization. For Case 2/3, the solutions to achieve the scalability between Yi and Yj, are reported by companies, including, e.g., truncation, additional adaptation layer in AI/ML model, etc.\nModel Fine-tuning:\nFor the evaluation of the potential performance benefits of model fine-tuning of CSI feedback enhancement, which is optionally assessed, the following case is considered:\n-\tThe AI/ML model is trained based on training dataset from a different dataset than Scenario#B/Configuration#B, e.g., Scenario#A/Configuration#A, Scenario#A/Configuration#B, Scenario#B/Configuration#A, and then the AI/ML model is updated based on a fine-tuning dataset Scenario#B/Configuration#B. After that, the AI/ML model is tested on Scenario#B/Configuration#B.\n-\tIn this case, the fine-tuning dataset setting (e.g., size of dataset) is to be reported along with the improvement of performance.\nFurther details on evaluations including training collaboration types\nFor the evaluation of Type 2 (Joint training of the two-sided model at network side and UE side, respectively), following procedure is considered as an example:\n-\tFor each FP/BP loop,\n-\tStep 1: UE side generates the FP results (i.e., CSI feedback) based on the data sample(s), and sends the FP results to NW side\n-\tStep 2: NW side reconstructs the CSI based on FP results, trains the CSI reconstruction part, and generates the BP information (e.g., gradients), which are then sent to UE side\n-\tStep 3: UE side trains the CSI generation part based on the BP information from NW side\n-\tNote: the dataset between UE side and NW side is aligned.\n-\tOther Type 2 training approaches are not precluded and reported by companies\nFor the evaluations of Type 2 (Joint training of the two-sided model at network side and UE side, respectively), the following evaluation cases are considered for multi-vendors,\n-\tCase 1 (baseline): Type 2 training between one NW part model to one UE part model\n-\tCase 2: Type 2 training between one NW part model and M>1 separate UE part models.\n-\tCompanies to report the AI/ML structures for the UE part model and the NW part model\n-\tCase 3: Type 2 training between one UE part model and N>1 separate NW part models.\n-\tCompanies to report the AI/ML structures for the UE part model and the NW part model\nFor the evaluation of an example of Type 3 (Separate training at NW side and UE side), the following procedure is considered for the sequential training starting with NW side training (NW-first training):\n-\tStep1: NW sideÂ trains theÂ NWÂ side CSI generation part (which is not used for inference) and theÂ NWÂ side CSI reconstruction part jointly\n-\tStep2: After NWÂ sideÂ training is finished,Â NW sideÂ shares UE side with a set of information (e.g., dataset) that is used by the UE side to be able to train the UE side CSI generation part\n-\tCompanies to report Dataset construction, e.g., the set of information includes the input and output of the Network side CSI generation part, or includes the output of the Network side CSI generation part only, or other information if applicable. Also report the Quantization behaviour, e.g., whether the shared output of the Network side CSI generation part is before or after quantization.\n-\tStep3: UE side trains the UE side CSI generation part based on the received set of information\n-\tOther Type 3 NW-first training approaches are not precluded\nFor the evaluation of an example of Type 3 (Separate training at NW side and UE side), the following procedure is considered for the sequential training starting with UE side training (UE-first training):\n-\tStep1: UE sideÂ trains theÂ UEÂ side CSI generation part and theÂ UEÂ side CSI reconstruction part (which is not used for inference) jointly\n-\tStep2: AfterÂ UE sideÂ training is finished,Â UE sideÂ shares NW side with a set of information (e.g., dataset) that is used by the NW side to be able to train the CSI reconstruction part\n-\tCompanies to report Dataset construction, e.g., the set of information includes the input and label of the UE side CSI reconstruction part, or includes the input of the UE side CSI reconstruction part only, or other information if applicable. Also, report the Quantization behaviour, e.g., whether the shared input of the UE side CSI reconstruction part is before or after quantization.\n-\tStep3: NW side trains the NW side CSI reconstruction part based on the received set of information\n-\tOther Type 3 UE-first training approaches are not precluded\nFor the evaluation of an example of Type 3 (Separate training at NW side and UE side), the following evaluation cases for sequential training are considered for multi-vendors:\n-\tCase 1 (baseline): Type 3 training between one NW part model and one UE part model\n-\tNote 1: Case 1 can be naturally applied to the NW-first training case where 1 NW part model to M>1 separate UE part models\n-\tCompanies to report the dataset used between the NW part model and the UE part model, e.g., whether dataset for training UE part model is the same or a subset of the dataset for training NW part model\n-\tNote 2: Case 1 can be naturally applied to the UE-first training case where 1 UE part model to N>1 separate NW part models\n-\tCompanies to report the dataset used between the NW part model and the UE part model, e.g., whether dataset for training NW part model is the same or a subset of the dataset for training UE part model\n-\tCompanies to report the AI/ML structures for the combination(s) of UE part model and NW part model, which can be the same or different\n-\tCase 2: For UE-first training, Type 3 training between one NW part model and M>1 separate UE part models\n-\tNote: Case 2 can be also applied to the M>1 UE part models to N>1 NW part models\n-\tCompanies to report the AI/ML structures for the M>1 UE part models and the NW part model\n-\tCompanies to report the dataset used at UE part models, e.g., same or different dataset(s) among M UE part models\n-\tCase 3: For NW-first training, Type 3 training between one UE part model and N>1 separate NW part models\n-\tNote: Case 3 can be also applied to the N>1 NW part models to M>1 UE part models\n-\tCompanies to report the AI/ML structures for the UE part model and the N>1 NW part models\n-\tCompanies to report the dataset used at NW part models, e.g., same or different dataset(s) among N NW part models\n-\tCase 4: 1-on-1 training with joint training: benchmark/upper bound for performance comparison.\nFor the evaluation of Type 3 (Separate training at NW side and UE side), the following cases are considered for evaluations:\n-\tCase 1 (baseline): Aligned AI/ML model structure between NW side and UE side\n-\tCase 2: Not aligned AI/ML model structures between NW side and UE side\n-\tCompanies to report the AI/ML structures for the UE part model and the NW part model, e.g., different backbone (e.g., CNN, Transformer, etc.), or same backbone but different structure (e.g., number of layers)\n-\tFor the evaluation of training Type 3 under CSI compression, for the benchmark case (1-on-1 joint training) for performance comparison, the structures for the pair of NW part model/UE part model for the new case are the same with the Type 3 case to be compared, e.g., if the Type 3 is Transformer#1 for NW part model and CNN#1 for UE part model, then the benchmark case for performance comparison is also Transformer#1 for NW part model and CNN#1 for UE part model with joint training.\nEvaluation assumptions:\nTable 6.2.1-1 presents the baseline system level simulation assumptions for AI/ML based CSI feedback enhancement evaluations.\nTable 6.2.1-1: Baseline System Level Simulation assumptions for AI/ML based CSI feedback enhancement evaluations\n\nTable 6.2.1-2 presents the baseline link level simulation assumptions for AI/ML based CSI feedback enhancement evaluations.\nTable 6.2.1-2: Baseline Link Level Simulation assumptions for AI/ML based CSI feedback enhancement evaluations\n\nCSI compression sub use case specific aspects:\nFor the evaluation of the AI/ML based CSI compression sub use cases, a two-sided model is considered as a starting point, including an AI/ML-based CSI generation part to generate the CSI feedback information and an AI/ML-based CSI reconstruction part which is used to reconstruct the CSI from the received CSI feedback information. At least for inference, the CSI generation part is located at the UE side, and the CSI reconstruction part is located at the gNB side.\nFigure 6.2.1-1 provides an example for the inference procedure for CSI compression. For generating the input of CSI generation model, it may need some further pre-processing on the measured channel; for the output of the CSI reconstruction model, some further post-processing may also be applied. Besides CSI feedback of quantization output, there may also be other CSI/PMI related information transmitted. There may be other examples of merging quantization/dequantization into the inference for CSI generation/reconstruction, CSI generation model/CSI reconstruction model, respectively.\nThe figure depicts an example of the CSI compression inference procedure, which is a crucial step in the process of compressing and transmitting CSI (Cognitive Radio Information) data. CSI is a key component in the design of cognitive radio systems, enabling the efficient transmission of information between the radio and the base station. The figure illustrates the steps involved in the inference procedure, including the selection of the compression algorithm, the encoding of the CSI data, and the transmission of the compressed CSI to the base station. This figure is crucial for understanding the process of CSI compression and its impact on the performance of cognitive radio systems.\nFigure 6.2.1-1: An example of the CSI compression inference procedure\nFor the evaluation of the AI/ML based CSI compression sub use case, the following details of models are reported:\n-\tThe structure of the AI/ML model, e.g., type (CNN, RNN, Transformer, Inception, â¦), the number of layers, branches, real valued or complex valued parameters, etc.\n-\tAI/ML model input (for CSI generation part)/output (for CSI reconstruction part) types for evaluations\n-\tData pre-processing/post-processing\n-\tLoss function\n-\tSpecific quantization/dequantization method, e.g., vector quantization, scalar quantization, etc,\nFor the evaluation of the AI/ML based CSI compression sub use cases, at least the following types of AI/ML model input (for CSI generation part)/output (for CSI reconstruction part) are considered for evaluations:\n-\tRaw channel matrix, e.g., channel matrix with the dimensions of Tx, Rx, and frequency unit. Companies to report the raw channel is in frequency domain or delay domain.\n-\tPrecoding matrix. Companies to report the precoding matrix is a group of eigenvector(s) or an eType II-like reporting (i.e., eigenvectors with angular-delay domain representation).\nFor the evaluation of quantization aware/non-aware training, the following cases are considered and reported by companies:\n-\tCase 1: Quantization non-aware training, where the float-format variables are directly passed from CSI generation part to CSI reconstruction part during the training\no\tFixed/pre-configured quantization method/parameters is applied for the inference phase. Companies to report the design of the fixed/pre-configured quantization method/parameters, e.g., quantization resolution, vector quantization codebook, etc\n-\tCase 2: Quantization-aware training, where quantization/dequantization is involved in the training process\no\tCase 2-1: Fixed/pre-configured quantization method/parameters are applied during the training phase; the same quantization codebook is applied for the inference phase. Companies to report the design of the fixed/pre-configured quantization method/parameters, e.g., quantization resolution, vector quantization codebook, etc.\no\tCase 2-2: The quantization method/parameters are updated in together with the AI/ML models during the training; when training is finished, the final quantization codebook is applied for the inference phase. Companies to report how to update the quantization method/parameters during the training\n-\tQuantization methods including uniform vs non-uniform quantization, scalar versus vector quantization, and associated parameters, e.g., quantization resolution, etc.\n-\tHow to use the quantization methods are reported by companies\nConsidering performance impact of ground-truth quantization in the CSI compression, study high resolution quantization methods for ground-truth CSI, including at least the following options:\n-\tHigh resolution scalar quantization\n-\tHigh resolution codebook quantization, e.g., Rel-16 TypeII-like method with new parameters, in which case companies are to report the R16 Type II parameters with specified or new/larger values to achieve higher resolution of the ground-truth CSI labels, e.g., L,, , reference amplitude, differential amplitude, phase, etc\n-\tFloat32 adopted as the baseline/upper-bound for performance comparisons\n-\tConsider legacy values of PC6 & PC8 for performance comparison\nFor CSI compression sub use case with rank â¥ 1, AI/ML model setting to adapt to ranks/layers to be reported amongst the following options:\n-\tOption 1-1 (rank specific): Separated AI/ML models are trained per rank value and applied for corresponding ranks to perform individual inference, any specific model operates on multi-layers jointly.\n-\tOption 1-2 (rank common): A unified AI/ML model is trained and applied for adaptive ranks to perform inference, the model operates on multi-layers jointly.\n-\tOption 2 (layer specific): Separated AI/ML models are trained per layer value and applied for corresponding layers to perform individual inference.\no\tNote: input/output type is Precoding matrix\no\tCompanies to report the setting is\nï§\tOption 2-1: layer specific and rank common (different models applied for different layers; for a specific layer, the same model is applied for all rank values), or\nï§\tOption 2-2: layer specific and rank specific (different models applied for different layers; for a specific layer, different models are applied for different rank values)\n-\tOption 3 (layer common): A unified AI/ML model is trained and applied for each layer to perform individual inference.\no\tNote: input/output type is Precoding matrix\no\tCompanies to report whether the setting is\nï§\tOption 3-1: layer common and rank common (A unified AI/ML model is applied for each layer under any rank value to perform individual inference), or\nï§\tOption 3-2: layer common and rank specific (different models applied for different rank values; for a specific rank, the same model is applied for all layers)\nFor CSI compression sub use case with rank >1, for a given configured Max rank=K, the complexity of FLOPs is reported as the maximum FLOPs over all ranks each includes the summation of FLOPs for inference per layer if applicable, e.g.,\n-\tOption 1-1 (rank specific): Max FLOPs over K rank specific models.\n-\tOption 1-2 (rank common): FLOPs of the rank common model.\n-\tOption 2-1 (layer specific and rank common): Sum of the FLOPs of K models (for the rank=K).\n-\tOption 2-2 (layer specific and rank specific): Max of the FLOPs over K ranks, k=1,â¦K, each with a sum of k models.\n-\tOption 3-1 (layer common and rank common): K * FLOPs of the common model.\n-\tOption 3-2 (layer common and rank specific): Max of the FLOPs over K ranks, k=1,â¦K, each with k * FLOPs of the layer common model.\nFor CSI compression sub use case with rank >1, the storage of memory storage/number of parameters is reported as the summation of memory storage/number of parameters over all models potentially used for any layer/rank, e.g.,\n-\tOption 1-1 (rank specific)/Option 3-2 (layer common and rank specific): Sum of memory storage/number of parameters over all rank specific models.\n-\tOption 1-2 (rank common): A single memory storage/number of parameters for the rank common model.\n-\tOption 2-1 (layer specific and rank common): Sum of memory storage/number of parameters over all layer specific models.\n-\tOption 2-2 (layer specific and rank specific): Sum of memory storage/number of parameters for the specific models over all ranks and all layers in per rank.\n-\tOption 3-1 (layer common and rank common): A single memory storage/number of parameters for the common model\nFor the evaluation of CSI compression, the specific CQI determination method(s) for AI/ML can be reported by introducing an additional field in the template, e.g.,\n-\tOption 1a: CQI is calculated based on the target CSI from the realistic channel estimation.\n-\tOption 1b: CQI is calculated based on the target CSI from the realistic channel estimation and potential adjustment.\n-\tOption 1c: CQI is calculated based on traditional codebook.\n-\tOption 2a: CQI is calculated based on CSI reconstruction output, if CSI reconstruction model is available at the UE and UE can perform reconstruction model inference with potential adjustment.\no\tOption 2a-1: The CSI reconstruction part for CQI calculation at the UE same as the actual CSI reconstruction part at the NW.\no\tOption 2a-2: The CSI reconstruction part for CQI calculation at the UE is a proxy model, which is different from the actual CSI reconstruction part at the NW.\n-\tOption 2b: CQI is calculated using two stage approach, UE derives CQI using precoded CSI-RS transmitted with a reconstructed precoder.\nCSI prediction sub use case specific aspects:\nFigure 6.2.1-2 provides an example for the inference procedure for CSI prediction. For generating the input of CSI prediction model, it may need some further pre-processing on the measured channel; for the output of the CSI prediction model, some further post-processing may also be applied.\nThe figure depicts an example of the CSI prediction inference procedure, which is a crucial step in the process of determining the channel state information (CSI) required for the transmission of data. The figure illustrates the steps involved in the inference procedure, including the use of a channel state estimator (CSI) and the determination of the CSI values. The figure also shows the importance of the CSI estimation in ensuring the accuracy and reliability of the communication system.\nFigure 6.2.1-2: An example of the CSI prediction inference procedure\nFor the evaluation of the AI/ML based CSI prediction sub use case, the following details of models are reported:\n-\tThe structure of the AI/ML model, e.g., type (FCN, RNN, CNN,â¦), the number of layers, branches, format of parameters, etc.\n-\tThe input CSI type, e.g., raw channel matrix, eigenvector(s) of the raw channel matrix, feedback CSI information, etc.\n-\tIncluding assumptions on the observation window, i.e., number/time distance of historic CSI/channel measurements\n-\tThe output CSI type, e.g., channel matrix, eigenvector(s), feedback CSI information, etc.\n-\tIncluding assumptions on the prediction window, i.e., number/time distance of predicted CSI/channel\n-\tData pre-processing/post-processing\n-\tLoss function\nFor the input CSI type, both of the following types are considered for evaluations:\n-\tRaw channel matrices\n-\tEigenvector(s)\nFor SLS, spatial consistency Procedure A with 50m decorrelation distance from TR 38.901 is used (if not used, assumptions used need to be reported). UE velocity vector is assumed as fixed over time in Procedure A modelling.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.2.1-1: Baseline System Level Simulation assumptions for AI/ML based CSI feedback enhancement evaluations",
                                    "table number": 6,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 6.2.1-2: Baseline Link Level Simulation assumptions for AI/ML based CSI feedback enhancement evaluations",
                                    "table number": 7,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "6.2.2\tPerformance results",
                            "text_content": "CSI_Table 1 through CSI_Table 7 in attached Spreadsheets for CSI feedback enhancement evaluations present the performance results for:\n-\tCSI_Table 1. Evaluation results for CSI compression of 1-on-1 joint training without model generalization/scalability\n-\tCSI_Table 2. Evaluation results for CSI compression with model generalization\n-\tCSI_Table 3. Evaluation results for CSI compression with model scalability\n-\tCSI_Table 4. Evaluation results for CSI compression of multi-vendor joint training without model generalization/scalability\n-\tCSI_Table 5. Evaluation results for CSI compression of separate training without model generalization/scalability\n-\tCSI_Table 6. Evaluation results for CSI prediction without model generalization/scalability\n-\tCSI_Table 7. Evaluation results for CSI prediction with model generalization\nFor the evaluation of CSI compression of 1-on-1 joint training without model generalization/scalability, the following baselines are recommended to facilitate calibration of results:\n-\tBenchmark: R16 eType II CB;\n-\tOthers can be additionally submitted, e.g., Type I CB.\n-\tInput/Output type: Eigenvectors of the current CSI\n-\tOther can be additionally submitted, e.g., eigenvectors with additional past CSI, eType II-like input, raw channel matrix, etc.\n-\tGround-truth CSI quantization method: Float32, i.e., without quantization (baseline/upper-bound for performance comparison)\n-\tOther high resolution CSI quantization methods can be additionally submitted for comparison, e.g., R16 eType II-like method with new parameters, scalar quantization, etc.\n-\tRank/layer adaptation settings for rank>1: Option 3-1, i.e., layer common and rank common.\n-\tOther rank>1 options can be additionally submitted for comparison, e.g., Option 1-1/1-2/2-1/2-2/3-2.\n-\tQuantization method: quantization-aware training (Case 2-1 or Case 2-2)\n-\tQuantization non-aware training can be additionally submitted for comparison\n-\tSQ and/or VQ is up to companies; companies are encouraged to provide results of various cases for comparison.\n-\tPerformance metric for intermediate KPI: SGCS\n-\tNMSE can be additionally submitted\nThe CSI feedback reduction is provided for three CSI feedback overhead ranges (CSI feedback overhead A, CSI feedback overhead B, CSI feedback overhead C), where for each CSI feedback overhead range of the benchmark, it is calculated as the gap between the CSI feedback overhead of benchmark and the CSI feedback overhead of AI/ML corresponding to the same mean UPT. The various CSI feedback overhead ranges are defined as:\nCSI feedback overhead A such that A â¤ Î² Ë80 bits\nCSI feedback overhead B such that Î² Ë100 bits â¤ B â¤ Î² Ë140 bits\nCSI feedback overhead C â¥ Î² Ë230 bits\nwhere, Î² = 1 for rank = 1 and Î² = 1.5 for rank > 1\nNote:\tcompanies report the exact CSI feedback overhead considered.\nNote:\tthe CSI feedback overhead reduction and gain for mean/5%tile UPT are determined at the same payload size for benchmark scheme.\nNote:\t\"Benchmark\" means the type of Legacy CB used for comparison. \"Quantization/dequantization method\" includes the description of training awareness (Case 1/2-1/2-2), type of quantization/dequantization (SQ/VQ), etc. \"Input type\" means the input of the CSI generation part. \"Output type\" means the output of the CSI reconstruction part.\n\nFor the evaluation of CSI prediction without model generalization/scalability verification, the following baselines are recommended to facilitate calibration of results:\n-\tUE speed: 10km/h, 30km/h, 60km/h;\n-\tOthers can be additionally submitted, e.g., 120km/h.\n-\tInput/Output type: Raw channel matrix\n-\tOther can be additionally submitted, e.g., eigenvectors.\n-\tObservation window (number/distance): 5/5ms, 10/5ms\n-\tOther observation window configurations can be additionally submitted for comparison, e.g., 3/5ms, 4/5ms, 8/2.5ms, 10/4ms, etc.\n-\tPrediction window (number/distance between prediction instances/distance from the last observation instance to the 1st prediction instance): 1/5ms/5ms\n-\tOther prediction window configurations can be additionally submitted for comparison, e.g., 3/5ms/5ms, 5/5ms/5ms, 4/2.5ms/2.5ms, 5/4ms/4ms, etc.\n-\tPerformance metric for intermediate KPI: SGCS\n-\tNMSE can be additionally submitted.\n-\tSpatial consistency configuration (optional): procedure A with 50m decorrelation distance and channel updating periodicity of 1 ms.\nFor the evaluation of CSI prediction with model generalization/scalability verification, the following baselines are recommended to facilitate calibration of results:\n-\tPerformance metric for intermediate KPI: SGCS\n-\tNMSE can be additionally submitted.\nInput/output type\nFor the evaluation of CSI compression, for the type of AI/ML model input (for CSI generation part)/output (for CSI reconstruction part), a vast majority of companies adopt precoding matrix as model input/output.\nNote: For the evaluations of CSI compression with 1-on-1 joint training, 22 sources take precoding matrix without angular-delay domain conversion as the model input/output; 2 sources take precoding matrix with angular-delay domain representation as the model input/output. No company submitted explicit channel matrix as input.\nThe complexity metric in terms of FLOPs and number of parameters of AI/ML models adopted in the evaluations of CSI compression are summarized in Figure 6.2.2.1-1, where the complexity for the CSI generation part and the complexity for the CSI reconstruction part are illustrated separately.\n-\tA majority of 25 sources adopt the CSI generation model subject to the FLOPs from 10M to 800M, and 26 sources adopt the CSI reconstruction model subject to the FLOPs from 10M to 1100M.\n-\tA majority of 21 sources adopt the CSI generation model subject to the number of parameters from 1M to 13M, and 22 sources adopt the CSI reconstruction model subject to the number of parameters from 1M to 17M.\n-\tResults refer to Table 1 of clause 7.3, R1-2310450.\nThe complexity of AI/ML models in terms of FLOPs and number of parameters for CSI compression is analyzed in Figure 6.2.2.1-1, highlighting the trade-offs between model complexity and compression efficiency.\nFigure 6.2.2.1-1: Complexity of AI/ML models from evaluation results in terms of FLOPs and number of parameters for CSI compression\nSGCS performance\nFor the evaluation of AI/ML based CSI compression compared to the benchmark in terms of SGCS,\nFor Max rank 1, Layer 1,\n-\t14 sources observe the performance gain of 2.6%~ 8.8% at CSI payload X (small payload);\n-\t18 sources observe the performance gain of 0.9%~ 8.1% at CSI payload Y (medium payload);\n-\t16 sources observe the performance gain of 0.9%~ 7% at CSI payload Z (large payload);\n-\tNote: 3 sources  observe the performance gain of 0%, 10.2%~11.6% at CSI payload X (small payload), 0.9% at CSI payload Y (medium payload), -0.3% at CSI payload Z (large payload) which biases from the majority range.\nFor Max rank 2, Layer 1,\n-\t15 sources observe the performance gain of 3.9%~ 11% at CSI payload X (small payload);\n-\t13 sources observe the performance gain of 0.7%~ 4.5% at CSI payload Y (medium payload);\n-\t14 sources observe the performance gain of -0.2%~ 6.5% at CSI payload Z (large payload);\n-\tNote: 4 sources observe the performance gain of 12.7%~15.6% at CSI payload X (small payload), 5%~10.6% at CSI payload Y (medium payload), 7.1% at CSI payload Z (large payload) which biases from the majority range.\nFor Max rank 2, Layer 2, more gains are observed in general compared with Layer 1 of Max rank 2:\n-\t13 sources observe the performance gain of 5.92%~ 30.2% at CSI payload X (small payload);\n-\t13 sources observe the performance gain of 1.5%~ 23.08% at CSI payload Y (medium payload);\n-\t11 sources observe the performance gain of 4.4%~ 12.99% at CSI payload Z (large payload);\n-\tNote: 5 sources observe the performance gain of -7.4%~1.1%, 49.3% at CSI payload X (small payload), -0.3%~1.5%, 41.7% at CSI payload Y (medium payload), -0.4%~2.2%, 45.9% at CSI payload Z (large payload) which biases from the majority range.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix of the current CSI is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is SGCS for Layer 1 of Max rank 1 or Layer 1/2 of Max rank 2.\n- \tCSI payload X is â¤ 80 bits; CSI payload Y is 100 bits - 140 bits; CSI payload Z is â¥ 230 bits; where X, Y, Z are applicable per layer.\n-\tBenchmark is Rel-16 Type II codebook.\n-\tNote: Results refer to Table 5.6 of R1-2308340.\nMean UPT for FTP traffic\nFor the evaluation of AI/ML based CSI compression compared to the benchmark in terms of mean UPT under FTP traffic, more gains are achieved by Max rank 2 compared with Max rank 1 in general:\n-\tFor Max rank 1, in general the performance gain increases with the increase of RU:\n-\tFor RUâ¤39%, 7 sources observe the performance gain of 0.2%~2%\n-\t6 sources observe the performance gain of 0.29%~2% at CSI feedback overhead A (small overhead);\n-\t6 sources observe the performance gain of 0.2%~1% at CSI feedback overhead B (medium overhead)\n-\t4 sources observe the performance gain of 0.33%~1% at CSI feedback overhead C (large overhead);\n-\tFor RU 40%-69%, 7 sources observe the performance gain of 0.1%~4%\n-\t5 sources observe the performance gain of 1.09%~3% at CSI feedback overhead A (small overhead);\n-\t4 sources observe the performance gain of 0.80%~2% at CSI feedback overhead B (medium overhead);\n-\t7 sources observe the performance gain of 0.1%~4% at CSI feedback overhead C (large overhead);\n-\tFor RUâ¥70%, 9 sources observe the performance gain of 0.23%~9%\n-\t9 sources observe the performance gain of 0.38%~9% at CSI feedback overhead A (small overhead)\n-\t8 sources observe the performance gain of 0.62%~5% at CSI feedback overhead B (medium overhead)\n-\t8 sources observe the performance gain of 0.23%~6% at CSI feedback overhead C (large overhead);\n-\tNote: 5 sources observe gain of 0.1%~0.2%, 1.7%~2.51% at RUâ¤39%, 0.5%~1%, 2.34%~21.21% at RU 40%-69%, 2.51%~21.5% at RUâ¥70%, which bias from the majority ranges.\n-\tFor Max rank 2, in general the performance gain increases with the increase of RU:\n-\tFor RUâ¤39%, 8 sources observe the performance gain of -0.3%~6%\n-\t7 sources observe the performance gain of 1%~6% at CSI feedback overhead A (small overhead);\n-\t7 sources observe the performance gain of 0.5%~6% at CSI feedback overhead B (medium overhead);\n-\t8 sources observe the performance gain of -0.3%~6% at CSI feedback overhead C (large overhead);\n-\tFor RU 40%-69%, 10 sources observe the performance gain of -0.5%~10%\n-\t8 sources observe the performance gain of 3%~10% at CSI feedback overhead A (small overhead);\n-\t8 sources observe the performance gain of 1.2%~9% at CSI feedback overhead B (medium overhead)\n-\t10 sources observe the performance gain of -0.5%~9% at CSI feedback overhead C (large overhead)\n-\tFor RUâ¥70%, 11 sources observe the performance gain of -0.2%~15%\n-\t11 sources observe the performance gain of 5%~15% at CSI feedback overhead A (small overhead);\n-\t11 sources observe the performance gain of 3%~9% at CSI feedback overhead B (medium overhead);\n-\t10 sources observe the performance gain of -0.2%~12% at CSI feedback overhead C (large overhead);\n-\tNote: 5 sources observe gain of 0.3%, 7%~30% at RUâ¤39%, 1%, 18%~23% at RU 40%-69%, 12.71%~26.8% at RUâ¥70%, which bias from the majority ranges.\n-\tFor Max rank 4:\n-\tFor RUâ¤39%, 2 sources observe the performance gain of -4%~6%\n-\t2 sources observe the performance gain of 2.5%~6% at CSI feedback overhead A (small overhead);\n-\t1 source observes the performance gain of 6% at CSI feedback overhead B (medium overhead);\n-\t2 sources observe the performance gain of -4%~0% at CSI feedback overhead C (large overhead);\n-\tFor RU 40%-69%, 3 sources observe the performance gain of -1.8%~12.22%\n-\t3 sources observe the performance gain of 3%~12.22% at CSI feedback overhead A (small overhead);\n-\t2 sources observe the performance gain of 7.04%~11% at CSI feedback overhead B (medium overhead);\n-\t3 sources observe the performance gain of -1.8%~8.19% at CSI feedback overhead C (large overhead);\n-\tFor RUâ¥70%, 3 sources observe the performance gain of -1%~17%\n-\t3 sources observe the performance gain of 3%~17% at CSI feedback overhead A (small overhead);\n-\t2 sources observe the performance gain of 6.64%~17% at CSI feedback overhead B (medium overhead);\n-\t3 sources observe the performance gain of -1%~8.40% at CSI feedback overhead C (large overhead);\n-\tNote: 1 source observes significant gain or significant loss under Max rank 4 due to specific CQI/RI selection method (e.g., Option 1a/2a) for AI/ML and/or CQI/RI determination method for eType II benchmark.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix of the current CSI is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is mean UPT for Max rank 1, Max rank 2, or Max rank 4.\n-\tBenchmark is Rel-16 Type II codebook.\n-\tNote: Results refer to Table 5.12 of R1-2308342.\n5% UPT for FTP traffic\nFor the evaluation of AI/ML based CSI compression compared to the benchmark in terms of 5% UPT under FTP, more gains are achieved by Max rank 2 compared with Max rank 1 in general:\n-\tFor Max rank 1, in general the performance gain increases with the increase of RU:\n-\tFor RUâ¤39%, 3 sources observe the performance gain of 0.8%~3%\n-\t3 sources observe the performance gain of 1.72%~3% at CSI feedback overhead A (small overhead);\n-\t3 sources observe the performance gain of 0.80%~1.2% at CSI feedback overhead B (medium overhead);\n-\t3 sources observe the performance gain of 1.68%~3% at CSI feedback overhead C (large overhead);\n-\tFor RU 40%-69%, 6 sources observe the performance gain of 0.1%~7%\n-\t6 sources observe the performance gain of 2.8%~7% at CSI feedback overhead A (small overhead);\n-\t3 sources observe the performance gain of 1.22%~2.7% at CSI feedback overhead B (medium overhead);\n-\t3 sources observe the performance gain of 0.1%~3.25% at CSI feedback overhead C (large overhead);\n-\tFor RUâ¥70%, 8 sources observe the performance gain of 0.85%~20.43%\n-\t8 sources observe the performance gain of 4%~20.43% at CSI feedback overhead A (small overhead);\n-\t7 sources observe the performance gain of 1%~10.13% at CSI feedback overhead B (medium overhead);\n-\t8 sources observe the performance gain of 0.85%~8% at CSI feedback overhead C (large overhead);\n-\tNote: 4 sources observe gain of 0% and 5.6%~5.7% at RUâ¤39%, 4.2%~5.8%  at RU 40%-69%, 23%~50% at RUâ¥70%, which bias from the majority ranges.\n-\tFor Max rank 2, in general the performance gain increases with the increase of RU:\n-\tFor RUâ¤39%, 8 sources observe the performance gain of -2%~5%\n-\t5 sources observe the performance gain of 1.1%~5% at CSI feedback overhead A (small overhead);\n-\t6 sources observe the performance gain of -2%~3% at CSI feedback overhead B (medium overhead);\n-\t7 sources observe the performance gain of -0.5%~5% at CSI feedback overhead C (large overhead);\n-\tFor RU 40%-69%, 8 sources observe the performance gain of -4%~13%\n-\t6 sources observe the performance gain of 7%~13% at CSI feedback overhead A (small overhead);\n-\t7 sources observe the performance gain of 0.3%~8% at CSI feedback overhead B (medium overhead);\n-\t6 sources observe the performance gain of -4%~8% at CSI feedback overhead C (large overhead);\n-\tFor RUâ¥70%, 9 sources observe the performance gain of -1.3%~24%\n-\t6 sources observe the performance gain of 10.26%~24% at CSI feedback overhead A (small overhead);\n-\t6 sources observe the performance gain of 9%~15.02% at CSI feedback overhead B (medium overhead);\n-\t8 sources observe the performance gain of -1.3%~13.67% at CSI feedback overhead C (large overhead);\n-\tNote: 7 sources observe gain of 4.4%~13% at RUâ¤39%, -8%~-2%, 10%~25.6% at RU 40%-69%, -10%~-8.1% at RUâ¥70%, which bias from the majority ranges.\n-\tFor Max rank 4:\n-\tFor RUâ¤39%, 2 sources observe the performance gain of -1.6%~10%\n-\t2 sources observe the performance gain of 8%~10% at CSI feedback overhead A (small overhead);\n-\t1 source observes the performance gain of 5% at CSI feedback overhead B (medium overhead);\n-\t2 sources observe the performance gain of -1.6%~1% at CSI feedback overhead C (large overhead);\n-\tFor RU 40%-69%, 3 sources observe the performance gain of -1.7%~23%\n-\t3 sources observe the performance gain of 5%~17% at CSI feedback overhead A (small overhead);\n-\t2 sources observe the performance gain of 6.17%~23% at CSI feedback overhead B (medium overhead);\n-\t3 sources observe the performance gain of -1.7%~9.47% at CSI feedback overhead C (large overhead);\n-\tFor RUâ¥70%, 3 sources observe the performance gain of 2%~31%\n-\t3 sources observe the performance gain of 5.8%~31% at CSI feedback overhead A (small overhead);\n-\t2 sources observe the performance gain of 10.2%~30% at CSI feedback overhead B (medium overhead);\n-\t3 sources observe the performance gain of 2%~15% at CSI feedback overhead C (large overhead);\n-\tNote: 1 source observes significant gain or significant loss under Max rank 4 due to specific CQI/RI selection method (e.g., Option 1a/2a) for AI/ML and/or CQI/RI determination method for eType II benchmark\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table\n-\tPrecoding matrix of the current CSI is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is 5% UPT for Max rank 1, Max rank 2, or Max rank 4.\n-\tBenchmark is Rel-16 Type II codebook.\n-\tResults refer to Table 5.13 of R1-2308342.\nMean UPT for full buffer\nFor the evaluation of AI/ML based CSI compression compared to the benchmark, in terms of mean UPT under full buffer, more gains are achieved by Max rank 2 compared with Max rank 1 in general:\n-\tFor Max rank 1, 8 sources observe the performance gain of 1.1%~11%\n-\t6 sources observe the performance gain of 6%~11% at CSI feedback overhead A (small overhead);\n-\t6 sources observe the performance gain of 3%~7% at CSI feedback overhead B (medium overhead);\n-\t8 sources observe the performance gain of 1.1%~11% at CSI feedback overhead C (large overhead);\n-\tFor Max rank 2, 9 sources observe the performance gain of 0.2%~15%\n-\t9 sources observe the performance gain of 4%~15% at CSI feedback overhead A (small overhead);\n-\t9 sources observe the performance gain of 2%~10% at CSI feedback overhead B (medium overhead);\n-\t9 sources observe the performance gain of -0.2%~14% at CSI feedback overhead C (large overhead);\n-\tNote: For Max rank 4, 1 source observes gain of 7.44%~9.95% over CSI feedback overhead A/B/C.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix of the current CSI is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tBenchmark is Rel-16 Type II codebook.\n-\tNote: Results refer to Table 5.7 of R1-2308340.\n5% UPT for full buffer\nFor the evaluation of AI/ML based CSI compression compared to the benchmark in terms of 5% UPT under full buffer,\n-\tFor Max rank 1, 5 sources observe the performance gain of 0%~20.9%\n-\t5 sources observe the performance gain of 2.5%~20.9% at CSI feedback overhead A (small overhead);\n-\t5 sources observe the performance gain of 2.3%~17.4% at CSI feedback overhead B (medium overhead);\n-\t4 sources observe the performance gain of 0%~6.62% at CSI feedback overhead C (large overhead);\n-\tFor Max rank 2, 6 sources observe the performance gain of -7%~14.9%\n-\t6 sources observe the performance gain of 4.1%~14.9% at CSI feedback overhead A (small overhead);\n-\t5 sources observe the performance gain of 0.3%~4% at CSI feedback overhead B (medium overhead);\n-\t6 sources observe the performance gain of -7%~6.03% at CSI feedback overhead C (large overhead);\n-\tNote: For Max rank 4, 1 source observes gain of 3.59%~6.15% over CSI feedback overhead A/B/C.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table\n-\tPrecoding matrix of the current CSI is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tBenchmark is Rel-16 Type II codebook.\n-\tNote: Results refer to Table 5.8 of R1-2308340.\nCSI feedback reduction\nFor the evaluation of AI/ML based CSI compression, compared to the benchmark, in terms of CSI feedback reduction,\n-\tFor Max rank = 1,\n-\tFor CSI feedback overhead A (small overhead), 1 source observes the CSI feedback reduction of 10.24% for FTP traffic;\n-\tFor CSI feedback overhead B (medium overhead), 3 sources observe the CSI feedback reduction of 15.62%~60% for FTP traffic, and 2 sources observe the CSI feedback reduction of 37%~66% for full buffer;\n-\tFor CSI feedback overhead C (large overhead), 2 sources observe the CSI feedback reduction of 14.37%~55% for FTP traffic, and 2 sources observes the CSI feedback reduction of 50%~53% for full buffer;\n-\tNote: For CSI feedback overhead C (large overhead), 1 source observes CSI feedback reduction of 75% for FTP traffic.\n-\tFor Max rank = 2,\n-\tFor CSI feedback overhead A (small overhead), 3 sources observe the CSI feedback reduction of 20.83%~54% for FTP traffic, and 1 source observes the CSI feedback reduction of 56% for full buffer;\n-\tFor CSI feedback overhead B (medium overhead), 3 sources observe the CSI feedback reduction of 22.22%~52% for FTP traffic, and 2 sources observe the CSI feedback reduction of 52% for full buffer;\n-\tFor CSI feedback overhead C (large overhead), 3 sources observe the CSI feedback reduction of 10%~58.33% for FTP traffic, and 2 sources observe the CSI feedback reduction of 22%~54% for full buffer;\n-\tNote: For CSI feedback overhead B (medium overhead), 1 source observes CSI feedback reduction of up to ~83% for FTP traffic using particular VQ codebook solution.\n-\tFor Max rank = 4,\n-\tFor CSI feedback overhead A (small overhead), 2 sources observe the CSI feedback reduction of 50%~79% for FTP traffic, and 1 source observes the CSI feedback reduction of 70.53% for full buffer;\n-\tFor CSI feedback overhead B (medium overhead), 2 sources observe the CSI feedback reduction of 36.10%~78% for FTP traffic, and 1 source observes the CSI feedback reduction of 47.74% for full buffer;\n-\tFor CSI feedback overhead C (large overhead), 2 sources observe the CSI feedback reduction of 8%~58% for FTP traffic, and 1 source observes the CSI feedback reduction of 42.59% for full buffer;\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix of the current CSI is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is CSI feedback overhead reduction for Max rank 1/2/4.\n-\tBenchmark is Rel-16 Type II codebook.\n-\tNote: Results refer to Table 5.30 of R1-2308344.\nMonitoring for intermediate KPI, NW side monitoring\nFor the evaluation of intermediate KPI based monitoring mechanism for CSI compression, for monitoring Case 1, in terms of monitoring accuracy with Option 1,\n-\tFor ground-truth CSI format of R16 eType II CB, monitoring accuracy is increased with the increase of the resolution for the ground-truth CSI (number of bits for each sample of ground-truth CSI) in general, with the impact of increased overhead, wherein\n-\tfor ground-truth CSI format of R16 eType II CB with PC#6, 4 sources observe KPIDiff as 13.2%~71.6%/ 28.5%~100%/ 68.4%~100% for KPIth_1=0.02/0.05/0.1, respectively.\n-\tNote: two sources observed averaging on the test samples improves the monitoring accuracy.\n-\tfor ground-truth CSI format of R16 eType II CB with PC#8, 5 sources observe KPIDiff as 21%~43.0%/ 48.1%~79.1%/ 79.8%~97.1% for KPIth_1=0.02/0.05/0.1, respectively.\n-\tfor ground-truth CSI format of R16 eType II CB with new parameter of 580-750bits CSI payload size, 2 sources observe KPIDiff as 35.4%~63%/ 77.9%~93.0%/ 99.5%~99.9% for KPIth_1=0.02/0.05/0.1, respectively, which have 12.7%~20%/ 13.9%~29.8%/ 8%~31.1% gain over PC#8.\n-\tfor ground-truth CSI format of R16 eType II CB with new parameter of around 1000bits CSI payload size, 4 sources observe KPIDiff as 34.9%~89%/ 82.9%~100%/ 99.9%~100% for KPIth_1=0.02/0.05/0.1, respectively, which have 12.2%~68%/ 18%~43.62%/ 2.9%~31% gain over PC#8 from 3 sources and 4.67%~10.6%/ 0%~5.88%/ 0%~0.49% gain over PC#6 from 1 source.\n-\tfor ground-truth CSI format of R16 eType II CB with new parameter of around 1600bits CSI payload size, 2 sources observe KPIDiff as 89.1%~97%/ 99.9%~100%/ 100% for KPIth_1=0.02/0.05/0.1, respectively, which have 76%/33%/3% gain over PC#8 from 1 source.\n-\tFor ground-truth CSI format of 4 bits scalar quantization, 2 sources observe KPIDiff as 9.4%~47%/ 96.3%~100%/ 100% for KPIth_1=0.02/0.05/0.1, respectively.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tTime independency is assumed over the test samples for monitoring\n-\tPrecoding matrix is used as the model input.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is monitoring accuracy for Layer 1.\n-\tNote: Results refer to Table 5.21 of R1-2308343.\nMonitoring for intermediate KPI, UE side monitoring\nFor the evaluation of intermediate KPI based monitoring mechanism for CSI compression, for Case 2, in terms of monitoring accuracy with Option 1,\n-\tFor Case 2-1 subject to generalization Case 1 for the proxy model, 5 sources observe KPIDiff as 31%~84%/ 65.63%~99.8%/ 95%~100% for KPIth_1=0.02/0.05/0.1, respectively;\n-\tCompared with monitoring Case 1 with ground-truth CSI format of R16 eType II CB with new parameter of around 1000bits CSI payload size,\n-\t2 sources observe +0.99%~+4.07% gain at KPIth_1=0.02;\n-\t3 sources observe -6.03%~-58%/ -0.2%~-24%/ 0%~-5% degradation for KPIth_1=0.02/0.05/0.1, respectively;\n-\tCompared with monitoring Case 1 with ground-truth CSI format of R16 eType II CB with new parameter of around 1600bits CSI payload size, 2 sources observe -16.35%~-66%/ -0.4%~-24%/ 0%~-24% degradation for KPIth_1=0.02/0.05/0.1, respectively.\n-\tNote: For Case 2-1 subject to generalization Case 2 for the proxy model, 2 sources observe -1.77%~-37.42% / -1.07%~-23.93%/ -0.16%~-14% compared with generalization Case 1 with the same testing scenario.\n-\tNote: For Case 2-2, 1 source observes KPIDiff as 61%~72.1%/ 91.2%~96.6%/ 99.2%~99.75% under generalization Case 1 for the proxy model, and 60%~71.3%/ 90.4%~99.3%/ 99%~100% under generalization Case 3 for the proxy model, for KPIth_1=0.02/0.05/0.1, respectively.\n-\tNote: for Case 2-1, 1 source observes that if different model backbone is adopted for proxy model as compared to the NW part model, it has negative impact to the monitoring performance.\n-\tNote: for the complexity and overhead analysis:\n-\tCase 2-1/Case 2-2 have smaller air-interface overhead for UE report for monitoring compared with Case 1. Overhead of proxy model from LCM perspective, if any, is not evaluated.\n-\tThe complexity aspect for Case 1, Case 2-1 and Case 2-2  is not evaluated.\n-\tNote: \"Generalization Case 1\" means the proxy model is trained based on training dataset from one Scenario#B, and then tested for monitoring on a dataset from the same Scenario#B. \"Generalization Case 2\" means the proxy model is trained based on training dataset from one Scenario#A, and then tested for monitoring on a dataset from a different Scenario#B. \"Generalization Case 3\" means the proxy model is trained based on mixing datasets from multiple scenarios including Scenario#B, and then tested for monitoring on the dataset from Scenario#B.\n-\tNote: two sources observed averaging on the test samples improves the monitoring accuracy.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tTime independency is assumed over the test samples for monitoring.\n-\tPrecoding matrix is used as the model input.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is monitoring accuracy for Layer 1.\n-\tNote: Results refer to Table 5.22 of R1-2308343.\nQuantization methods, quantization awareness for training\nFor the comparison of quantization methods for CSI compression, quantization non-aware training (Case 1) is in general inferior to the quantization aware training (Case 2-1/2-2), and may lead to lower performance than the benchmark:\n-\tFor scalar quantization, compared with benchmark,\n-\t-2.4%~-43.2% degradations are observed for  quantization non-aware training (Case 1) from 6 sources.\n-\t3.9%~8.64% gains are observed for quantization aware training with fixed/pre-configured quantization method/parameters (Case 2-1) from 5 sources, which are 17.3%~83.2% gains over  quantization non-aware training (Case 1) from 5 sources and 7.56%~11.55%  gains over  quantization non-aware training (Case 1) from 1 source.\n-\tNote: 0.72% gains are observed for Case 2-1 from 1 source due to SQ parameter chosen without matching latent distribution, which achieves 13.9% gains over Case 1.\n-\t8.91% gains are observed for quantization aware training with jointly updated quantization method/parameters (Case 2-2) from 1 source, which are 23.1% gains over  quantization non-aware training (Case 1) from 1 source.\n-\tFor vector quantization, compared with benchmark,\n-\t-2%~-10% degradations are observed for  quantization non-aware training (Case 1) from 1 source.\n-\t5.64%~7.55% gains are observed for quantization aware training with fixed/pre-configured quantization method/parameters (Case 2-1) from 3 sources, which are 3%~21.6% gains over  quantization non-aware training (Case 1) from 3 sources.\n-\t4.6%~13.01% gains are observed for quantization aware training with jointly updated quantization method/parameters (Case 2-2) from 7 sources, which are 10.7%~30% gains over  quantization non-aware training (Case 1) from 4 sources and 3.66%~9.8% gains over  quantization non-aware training (Case 1) from 2 sources.\n-\tIn general, Case 2-2 outperforms Case 2-1 with 0.46%~5.1% gains, as observed by 6 sources.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is SGCS for Layer 1.\n-\tBenchmark is Rel-16 Type II codebook.\n-\tNote: Results refer to Table 5.14 of R1-2308342.\nQuantization methods, quantization format\nFor the comparison of quantization methods for CSI compression, in general vector quantization (VQ) has comparable performance with scalar quantization (SQ):\n-\tFor SQ and VQ under the same training case, it is\n-\tobserved by 3 sources that VQ under Case 2-1 has -1%~-4.5% degradation over SQ under Case 2-1,\n-\tobserved by 1 source that VQ under Case 2-1 has 1.1% gain over SQ under Case 2-1, and\n-\tobserved by 3 sources that VQ under Case 2-2 has 0.7%~3.8% gain over SQ under Case 2-2.\n-\tNote: VQ under Case 2-1 has 8% gains over SQ under Case 2-1 as observed from 1 source due to SQ parameter chosen without matching latent distribution.\n-\tFor SQ and VQ across training cases, it is\n-\tobserved by 6 sources that VQ under Case 2-2 has 0.46%~4% gain over SQ under Case 2-1, and\n-\tobserved by 1 source that VQ under Case 2-2 has -1.3% degradation over SQ under Case 2-1.\n-\tobserved by 1 source that VQ under Case 2-1 has -2.9%~-6.4% degradation over SQ under Case 2-2.\n-\tNote: in general, more companies observing gain of VQ over SQ than companies observing loss.\n-\tNote: it is observed by 1 source that combined SQ and VQ under Case 2-2 has minor gain of 0.2% over VQ only under Case 2-2.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is SGCS for Layer 1.\n-\tBenchmark is Rel-16 Type II codebook.\n-\tNote: Results refer to Table 5.15 of R1-2308342.\nHigh resolution ground-truth CSI for training\nFor the evaluation of high-resolution quantization of the ground-truth CSI for the training of CSI compression, compared to the upper-bound of Float32, quantized high resolution ground-truth CSI can achieve significant overhead reduction with minor performance loss if the parameters are appropriately selected.\n-\tFor high resolution scalar quantization,\n-\tFloat16 achieves 50% overhead reduction and -0.6% or less performance loss from 2 sources\n-\t8 bits scalar quantization achieves 75% overhead reduction and -0.14%~-0.9% performance loss from 2 sources\n-\tFor high resolution R16 eType II-like quantization,\n-\tR16 eType II CB with legacy parameters can achieve significant overhead reduction while with performance loss compared to Float32, wherein:\n-\tPC#6 achieves around 99% overhead reduction with -1.4% ~-1.7% performance loss from 2 sources, and -3%~-9.5% performance loss from 4 sources.\n-\tPC#8 achieves around 98% overhead reduction with 0% ~-1.7% performance loss from 3 sources, and -2.9%~-5.5% performance loss from 5 sources.\n-\tFor R16 eType II CB with new parameters:\n-\tR16 eType II CB with new parameter of 1000-1400bits CSI payload size achieves 95%~97.5% overhead reduction (3~4.1 times overhead compared to PC8) with performance gain of 0.7%~4.3% over PC#8 from 4 sources.\n-\tR16 eType II CB with new parameter of 1500-2100bits CSI payload size achieves 94%~96.2% overhead reduction (4.8~6.1 times overhead compared to PC8) with performance gain of 1.3%~5.4% over PC#8 from 3 sources.\n-\tNote: it is observed by 1 source that using R16 eType II-like quantization with legacy PC may achieve close performance to Float32 by dataset dithering.\n-\tNote: the new parameters include at least one from the follows:\n-\tL= 8, 10, 12;\n-\tpv = 0.8, 0.9, 0.95;\n-\treference amplitude = 6 bits, 8 bits; differential amplitude = 4bits; phase = 5 bits, 6 bits;\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table\n-\tPrecoding matrix is used as the model input.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is SGCS for Layer 1.\n-\tNote: Results refer to Table 5.18 of R1-2308342.\nGeneralization over deployment scenarios\nFrom the results for the generalization verification of AI/ML based CSI compression over various deployment scenarios compared to the generalization Case 1 where the AI/ML model is trained with dataset subject to a certain deployment scenario#B and applied for inference with a same deployment scenario#B,\n-\tFor generalization Case 2, generalized performance may be achieved for certain combinations of deployment scenario#A and deployment scenario#B but not for others:\n-\tIf deployment scenario#A is UMi & deployment scenario#B is UMa, deployment scenario#A is UMa & deployment scenario#B is UMi, or deployment scenario#A is UMa & deployment scenario#B is InH:\n-\t14 sources observe that generalized performance can be achieved:\n-\tFor deployment scenario#A is UMi & deployment scenario#B is UMa, 9 sources observe less than -1.6% degradation or positive gain.\n-\tFor deployment scenario#A is UMa & deployment scenario#B is UMi, 10 sources observe less than -1.5% degradation or positive gain.\n-\tFor deployment scenario#A is UMa & deployment scenario#B is InH, 2 sources observe less than -0.6% degradation or positive gain.\n-\t13 sources observe that moderate/significant degradations are suffered under generalization Case 2:\n-\tFor deployment scenario#A is UMi & deployment scenario#B is UMa, 10 sources observe -1.69%~-21.1% degradation.\n-\tFor deployment scenario#A is UMa & deployment scenario#B is UMi, 9 sources observe -1.7%~-8.1% degradation.\n-\tFor deployment scenario#A is UMa & deployment scenario#B is InH, 3 sources observe -1.74%~-31.6% degradation.\n-\tIf deployment scenario#A is InH & deployment scenario#B is Uma/UMi, significant performance degradations are observed under generalization Case 2:\n-\tFor deployment scenario#A is InH & deployment scenario#B is UMa, 5 sources observe -5.55%~ -27.7% degradation.\n-\tFor deployment scenario#A is InH & deployment scenario#B is UMi, 3 sources observe -8.63%~-20% degradation\n-\tFor generalization Case 3, generalized performance of the AI/ML model can be achieved (0%~-4% loss or positive gain) for deployment scenario#B subject to any of UMa, UMi, and InH, if the training dataset is constructed with data samples subject to multiple deployment scenarios including deployment scenario#B, as observed by 15 sources.\n-\tMinor loss (0%~-1.6%) are observed by 15 sources.\n-\tModerate loss (-1.69%~-4%) are observed by 8 sources.\n-\tPositive gains are observed by 10 sources.\n-\tNote: Significant degradations of up to -6.7% are observed by 2 sources for deployment scenario#B subject to UMa, and by 2 sources for deployment scenario#B subject to UMi.\n-\tNote: For generalization Case 2, if deployment scenario#A is UMi & deployment scenario#B is InH, 3 sources observe different trends, where significant performance degradations of -27.8%~-32.86% are observed by two sources, while moderate performance degradations of -1.44%~-2.41% are observed by another source.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is SGCS in linear value for layer 1/2.\n-\tNote: Results refer to Table 5.1 of R1-2308340.\nGeneralization over UE distributions\nFor the generalization verification of AI/ML based CSI compression over various UE distributions compared to the generalization Case 1 where the AI/ML model is trained with dataset subject to a certain UE distribution#B and applied for inference with a same UE distribution#B,\n-\tFor generalization Case 2, generalized performance may be achieved for some certain combinations of UE distribution#A and UE distribution#B but not for others\n-\tIf UE distribution#A is Outdoor & UE distribution#B is Indoor, 7 sources observe that moderate/significant degradations of -1.9%~-11.5% degradation are suffered,\n-\tNote: 1 source observes minor degradation of -0.48%~-0.93% for partial cases.\n-\tIf UE distribution#A is Indoor & UE distribution#B is Outdoor, 7 sources observe minor loss of less than -1.11% degradation or positive gain\n-\tFor generalization Case 3, generalized performance of the AI/ML model can be achieved (0%~-1.54% loss or positive gain) for UE distribution#B subject to any of Outdoor and Indoor, if the training dataset is constructed with data samples subject to multiple UE distributions including UE distribution#B, as observed by 6 sources.\n-\tMinor loss (0%~-1.54%) are observed by 5 sources.\n-\tPositive gains are observed by 4 sources.\n-\tNote: Moderate degradations of up to -3.9% are still observed by 2 sources for UE distribution#B  subject to Indoor.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is SGCS in linear value for layer 1/2.\n-\tNote: Results refer to Table 5.9 of R1-2308340.\nGeneralization over carrier frequencies\nFor the generalization verification of AI/ML based CSI compression over various carrier frequencies compared to the generalization Case 1 where the AI/ML model is trained with dataset subject to a certain carrier frequency#B and applied for inference with a same carrier frequency#B,\n-\tFor generalization Case 2, generalized performance may be achieved in general\n-\tIf carrier frequency#A is 3.5/4GHz & carrier frequency#B is 2GHz, 3 sources observe generalized performance of less than -0.8% degradation.\n-\tIf carrier frequency#A is 2GHz & carrier frequency#B is 3.5/4GHz, 5 sources observe generalized performance of less than -1.06% degradation or positive gain.\n-\tNote: 2 sources observes significant degradations up to -6.6%.\n-\tFor generalization Case 3, generalized performance of the AI/ML model may be achieved (0%~-1.2% loss or positive gain) for carrier frequency#B subject to any of 2GHz and 3.5/4GHz, if the training dataset is constructed with data samples subject to multiple carrier frequencies including carrier frequency#B, as observed by 4 sources.\n-\tMinor loss (0%~-1.2%) are observed by 4 sources.\n-\tPositive gains are observed by 4 sources.\n-\tNote: Significant degradations of up to -4.9% are still observed by 1 source for carrier frequency#B subject to 3.5/4GHz\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is SGCS in linear value for layer 1.\n-\tAntenna layouts are assumed as the same over the different frequency carriers.\n-\tNote: Results refer to Table 5.2 of R1-2308340.\nGeneralization over TxRU mappings\nFor the generalization verification of AI/ML based CSI compression over various TxRU mappings, compared to the generalization Case 1 where the AI/ML model is trained with dataset subject to a certain TxRU mapping#B and applied for inference with a same TxRU mapping#B,\n-\tFor generalization Case 2, significant degradations are suffered in general from the perspective of the layouts of antenna ports, as observed by 2 sources:\n-\tFor TxRU mapping#A is [2,8,2] & TxRU mapping#B is [4,4,2] or TxRU mapping#A is [8,2,2] & TxRU mapping#B is [4,4,2], 2 sources observe -13%~-36.1% degradation.\n-\tFor TxRU mapping#A is [4,4,2] & TxRU mapping#B is [2,8,2] or TxRU mapping#A is [8,2,2] & TxRU mapping#B is [2,8,2], 2 sources observe -7%~-23.6% degradation.\n-\tFor TxRU mapping#A is [4,4,2] & TxRU mapping#B is [8,2,2] or TxRU mapping#A is [2,8,2] & TxRU mapping#B is [8,2,2], 1 source observes -19%~-27% degradation.\n-\tFor generalization Case 2, generalized performance may be achieved for some certain combinations of TxRU mapping#A and TxRU mapping#B but not for others, from the perspective of the layouts of antenna element mapping, as observed by 2 sources:\n-\tFor TxRU mapping#A is 8x8x2 & TxRU mapping#B is 2x8x2, 2 sources observe minor/moderate degradation of -0.6%~-2.5%.\n-\tFor TxRU mapping#A is 2x8x2 & TxRU mapping#B is 8x8x2, 1 source observes moderate degradation of -3%.\n-\tFor generalization Case 3, generalized performance of the AI/ML model can be achieved (0%~-4.4% loss or positive gain) for TxRU mapping#B subject to any of [2,8,2], [4,4,2], and [8,2,2] from the perspective of the layouts of antenna ports, or subject to any of 8x8x2 and 2x8x2 from the perspective of the layouts of antenna element mapping, if the training dataset is constructed with data samples subject to TxRU mappings including TxRU mapping#B, as observed by 4 sources.\n-\tMinor loss (0%~-2%) are observed by 4 sources.\n-\tModerate loss (-2.5%~-4.4%) are observed by 1 source.\n-\tPositive gains are observed by 1 source.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is SGCS in linear value for layer 1.\n-\t[x,y,z] for TxRU mapping: Vertical port number, Horizontal port number, polarization\n-\tAxBxC for TxRU mapping: AxBxC antenna elements virtualized to [2,8,2]\n-\tNote: Results refer to Table 5.19 of R1-2308342.\nScalability over CSI payload sizes\nFor the scalability verification of AI/ML based CSI compression over various CSI payload sizes, compared to the generalization Case 1 where the AI/ML model is trained with dataset subject to a certain CSI payload size#B and applied for inference with a same CSI payload size#B,\n-\tFor generalization Case 2, significant performance degradations are observed in general, as -5.3%~-14.7% degradations are observed by 2 sources.\n-\tGeneralized performance of the AI/ML model can be achieved (-0%~-5.9%loss) under generalization Case 3 for the inference on CSI payload size#B, if the training dataset is constructed with data samples subject to multiple CSI payload sizes including CSI payload size#B, and an appropriate scalability solution is performed to scale the dimension of the AI/ML model, shown by 13 sources (10 sources showing -0%~-2.2% loss, 7 sources showing -2.3%~-5.9% loss, 5 sources showing positive gain). The scalability solution is adopted as follows:\n-\tPre/post-processing of truncation/padding, adopted by 6 sources, showing -0% ~-5.9% loss or positive gain.\n-\tVarious quantization granularities, adopted by 1 source, showing -0.7% loss or positive gain.\n-\tAdaptation layer in the AL/ML model, adopted by 6 sources, showing -0%~-4.78% loss or positive gain.\n-\t\tNote: Significant degradations of up to -14.22% are still observed by 2 sources for generalization Case 3.\n-\tGeneralized performance of the AI/ML model can also be achieved by finetuning models on CSI payload size#B, showing loss 0%~-2.2% by 2 sources\nThe above results are based on the following assumptions:\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tInput/output scalability dimension Case 3 is adopted: A pair of CSI generation part with scalable input/output dimensions and CSI reconstruction part with scalable output and/or input dimensions.\n-\tThe performance metric is SGCS in linear value for layer 1/2.\n-\tNote: Results refer to Table 5.10 of R1-2308340.\nScalability over bandwidths\nFor the scalability verification of AI/ML based CSI compression over various bandwidths, compared to the generalization Case 1 where the AI/ML model is trained with dataset subject to a certain bandwidth#B and applied for inference with a same bandwidth#B,\n-\tFor generalization Case 2, if bandwidth#A is 20MHz & bandwidth#B is 10MHz, or bandwidth#A is 10MHz & bandwidth#B is 20MHz, or bandwidth#A is 10MHz & bandwidth#B is 5MHz:\n-\t2 sources observe that generalized performance can be achieved:\n-\tFor bandwidth#A is 20MHz & bandwidth#B is 10MHz, 1 source observes less than -1.28% degradation.\n-\tFor bandwidth#A is 10MHz & bandwidth#B is 20MHz, 2 sources observe less than -1.1% degradation.\n-\t1 source observe that moderate/significant degradations are suffered under generalization Case 2:\n-\tFor bandwidth#A is 10MHz & bandwidth#B is 5MHz, 1 source observes larger than -2.5% degradation.\n-\tFor generalization Case 3, 3 sources observe that generalized performance of the AI/ML model can be achieved (0%~-2.97% loss) for bandwidth#B subject to each of 10MHz/52RB and 20MHz and 48RB, if the training dataset is constructed with data samples subject to multiple bandwidths including bandwidth#B.\n-\tMinor loss (0%~-1.7%) are observed by 2 sources.\n-\tModerate loss (-1.91%~-2.97%) are observed by 2 sources.\n-\tPositive gains are observed by 2 sources.\n-\tNote: Significant loss (-5.4%) is observed by 1 source.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is SGCS in linear value for layer 1/2.\n-\tNote: Results refer to Table 5.31 of R1-2308344.\nScalability over Tx port numbers\nFor the scalability verification of AI/ML based CSI compression over various Tx port numbers compared to the generalization Case 1 where the AI/ML model is trained with dataset subject to a certain Tx port number#B and applied for inference with a same Tx port number#B,\n-\tFor generalization Case 2, significant performance degradations are observed in general, if Tx port number#A is 32 & Tx port number#B is 16, as -3.37%~-21.8% degradations are observed by 4 sources\n-\tFor generalization Case 3, generalized performance of the AI/ML model can be achieved (0%~-3.94% loss or positive gains) for Tx port number#B subject to any of 16 and 32, if the training dataset is constructed with data samples subject to multiple Tx port numbers including Tx port number#B, and an appropriate scalability solution is performed to scale the dimension of the AI/ML model, as observed by 9 sources.\n-\tMinor loss (0%~-1.6%) are observed by 8 sources.\n-\tModerate loss (-2.02%~-3.94%) are observed by 4 sources.\n-\tPositive gains are observed by 5 sources.\n-\tNote: Significant degradations of up to -9.76% are still observed by 2 sources for deployment scenario#B subject to 32 ports, and for deployment scenario#B subject to 16 ports\n-\tNote: Pre/post-processing of truncation/padding is adopted by 6 sources, and adaptation layer in the AL/ML model is adopted by 1 source.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\t1-on-1 joint training is assumed.\n-\tThe performance metric is SGCS in linear value for layer 1/2/3/4.\n-\tNote: Results refer to Table 5.3 of R1-2308340.\n1 NW part model to M>1 UE part models\nFor the evaluation of Type 2 training between 1 NW part model and M>1 separate UE part models (Case 2), as compared to joint training between 1 NW part model and the 1 UE part model,\n-\t7 sources observe minor degradation of -0%~-1.67% or positive gain;\n-\t3 sources observe moderate degradation of -2.5%~-6.5%.\n-\tNote: among the above sources, 5 sources adopt simultaneous training, while 1 source adopts sequential training starting with NW side training.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\tThe performance metric is SGCS for Layer 1.\n-\tSame pair of NW part model and UE part model between 1-on-1 joint training and Type 2 training.\n-\tM=2, 3, or 4 are considered.\n-\tNote: Results refer to Table 5.23 of R1-2308343.\n1 UE part model to N>1 NW part models\nFor the evaluation of Type 2 training between 1 UE part model and N>1 separate NW part models (Case 3), as compared to joint training between 1 NW part model and the 1 UE part model,\n-\t2 sources observe minor degradation of -0%~-0.8% or positive gain;\n-\t1 source observe moderate degradation of -1.4%~-4.2%.\n-\tNote: among the above sources, 1 source adopts simultaneous training.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\tThe performance metric is SGCS for Layer 1.\n-\tSame pair of NW part model and UE part model between 1-on-1 joint training and Type 2 training.\n-\tN=2, 3, or 4 are considered.\n-\tNote: Results refer to Table 5.24 of R1-2308343.\nNW first training, 1 NW part model to 1 UE part model, same backbone\nFor the evaluation of Type 3 NW first separate training with dataset sharing manner for CSI compression for the pairing of 1 NW to 1 UE (Case 1), as compared to 1-on-1 joint training between the NW part model and the UE part model,\n-\tFor the NW first separate training case where the same backbone is adopted for both the NW part model and the UE part model, minor degradation is observed for both the cases where the shared output of the Network side CSI generation part is before or after quantization:\n-\tFor the case where the shared output of the Network side CSI generation part is after quantization, 9 sources observe -0%~-0.5% degradation, 10 sources observe -0.5%~-1% degradation, and 2 sources observe -1%~-1.3% degradation.\n-\tFor the case where the shared output of the Network side CSI generation part is before quantization, 6 sources observe -0%~-0.8% degradation, and 1 source observes -1%~-1.5% degradation.\n-\tNote: the dataset sharing behaviour from above sources follows the example of the agreement \"the set of information includes the input and output of the Network side CSI generation part, or includes the output of the Network side CSI generation part only\".\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\tThe performance metric is SGCS for Layer 1/2.\n-\tSame size of training dataset for benchmark, NW part training and the UE part training\n-\tSame pair of NW part model and UE part model between 1-on-1 joint training and NW first separate training.\n-\tQuantization/dequantization method/parameters between NW side and UE side are aligned.\n-\tNote: Results refer to Table 5.16 of R1-2308342.\nImpact of shared dataset under 1 NW part model to 1 UE part model\nFor the evaluation of Type 3 NW/UE first separate training with dataset sharing manner for CSI compression for the pairing of 1 NW to 1 UE (Case 1), as compared to the case where the same set of dataset is applied for training the NW part model and training the UE part model, if the dataset#2 applied for training the UE/NW part model is a subset of the dataset#1 applied for training the NW/UE part model,\n-\tIf the dataset#2 is appropriately selected, minor additional performance degradation can be achieved, as -0%~-0.59% gap is observed from 3 sources.\n-\tIf the dataset#2 has a significantly reduced size compared to dataset#1, moderate/significant additional performance degradation may occur, as -0.6%~-4.83% gap is observed from 4 sources.\n-\tNote: the dataset sharing behaviour from above sources follows the example of the agreement where \"the set of information includes the input and output of the Network side CSI generation part, or includes the output of the Network side CSI generation part only\".\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\tThe performance metric is SGCS for Layer 1/2.\n-\tNote: Results refer to Table 5.4 of R1-2308340.\nNW first training, 1 NW part model to 1 UE part model, different backbones\nFor the evaluation of Type 3 NW first separate training with dataset sharing manner for CSI compression, for the pairing of 1 NW to 1 UE (Case 1), as compared to 1-on-1 joint training between the NW part model and the UE part model,\n-\tFor the NW first separate training case where different backbones are adopted for the NW part model and the UE part model, more degradations are observed in general than the situation where the same backbone is adopted for the NW part model and the UE part model.\n-\tFor the case where the shared output of the Network side CSI generation part is after quantization, 3 sources observe minor degradation of -0%~-1.02%, and 3 sources observe moderate degradation of -1.46%~-5.1%.\n-\tFor the case where the shared output of the Network side CSI generation part is before quantization, 2 sources observe minor degradation of -0%~-0.1%, 1 source observes moderate degradation of -2.03%.\n-\tNote: the dataset sharing behaviour from above sources follows the example of the agreement where \"the set of information includes the input and output of the Network side CSI generation part, or includes the output of the Network side CSI generation part only\".\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\tThe performance metric is SGCS for Layer 1/2.\n-\tSame size of training dataset for benchmark, NW part training and the UE part training\n-\tSame pair of NW part model and UE part model between 1-on-1 joint training and NW first separate training.\n-\tQuantization/dequantization method/parameters between NW side and UE side are aligned.\n-\tNote: Results refer to Table 5.16 of R1-2308342.\nNW first training, 1 UE part model to N>1 NW part models\nFor the evaluation of Type 3 NW first separate training with dataset sharing manner for CSI compression, for the pairing between 1 UE part model and N>1 separate NW part models (Case 3), when taking 1-on-1 joint training between the NW part model and the UE part model as benchmark, larger performance loss is observed in general than the case of NW first separate training with 1 UE part model and 1 NW part model pairing (Case 1):\n-\t6 sources observe minor loss of -0%~-1.6% compared to the 1-on-1 joint training.\n-\t3 sources observe moderate loss of -1.9%~-6.64% compared to the 1-on-1 joint training.\n-\t5 sources observe significant loss of -37.9%~-87% compared to the 1-on-1 joint training.\n-\tNote: as opposed to companies which observe significant loss, the minor loss observed by other companies may due to the fact that special handling (e.g., adaptation layer) is performed to pair with N>1 NW part models during the training at the UE side.\n-\tNote: the dataset sharing behaviour from above sources follows the example of the agreement, where \"the set of information includes the input and output of the Network side CSI generation part, or includes the output of the Network side CSI generation part only\".\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\tThe performance metric is SGCS for Layer 1.\n-\tSame size of training dataset for benchmark, NW part training and the UE part training\n-\tSame pair of NW part model and UE part model between 1-on-1 joint training and NW first separate training.\n-\tQuantization/dequantization method/parameters between NW side and UE side are aligned.\n-\tN=2, 3, or 4 are considered.\n-\tNote: Results refer to Table 5.20 of R1-2308342.\nUE first training, 1 NW part model to 1 UE part model, same backbone\nFor the evaluation of Type 3 UE first separate training with dataset sharing manner for CSI compression for the pairing of 1 NW to 1 UE (Case 1), as compared to 1-on-1 joint training between the NW part model and the UE part model,\n-\tFor the UE first separate training case where the same backbone is adopted for both the UE part model and the NW part model, minor degradation is observed in general for both the cases where the shared input of the UE side CSI reconstruction part is before or after quantization:\n-\tFor the case where the shared input of the UE side CSI reconstruction part is after quantization, 9 sources observe -0%~-0.42% degradation, 2 sources observe -0.7%~-0.9% degradation, and 3 sources observe -1.05%~-1.8% degradation.\n-\tFor the case where the shared input of the UE side CSI reconstruction part is before quantization, 3 sources observe -0%~-0.8% degradation, and 2 sources observe -1.3%~-2.9% degradation.\n-\tNote: the dataset sharing behaviour from above sources follows the example of the agreement where \"the set of information includes the input and label of the UE side CSI reconstruction part, or includes the input of the UE side CSI reconstruction part only\".\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\tThe performance metric is SGCS for Layer 1/2.\n-\tSame size of training dataset for benchmark, NW part training and the UE part training\n-\tSame pair of NW part model and UE part model between 1-on-1 joint training and UE first separate training.\n-\tQuantization/dequantization method/parameters between NW side and UE side are aligned.\n-\tNote: Results refer to Table 5.17 of R1-2308342.\nUE first training, 1 NW part model to 1 UE part model, different backbones\nFor the evaluation of Type 3 UE first separate training with dataset sharing manner for CSI compression, for the pairing of 1 NW to 1 UE (Case 1), as compared to 1-on-1 joint training between the NW part model and the UE part model,\n-\tFor the UE first separate training case where different backbones are adopted for the NW part model and the UE part model, more degradations are observed in general than the situation where the same backbone is adopted for the NW part model and the UE part model.\n-\tFor the case where the shared input of the UE side CSI reconstruction part is after quantization, 5 sources observe minor degradation of -0.23%~-1.07%, and 1 source observes moderate degradation of -1.74%~-1.88%.\n-\tFor the case where the shared input of the UE side CSI reconstruction part is before quantization, 1 source observes moderate degradation of -1.58%~-2.73%.\n-\tNote: the dataset sharing behaviour from above sources follows the example of the agreement, where \"the set of information includes the input and label of the UE side CSI reconstruction part, or includes the input of the UE side CSI reconstruction part only\".\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\tThe performance metric is SGCS for Layer 1/2.\n-\tSame size of training dataset for benchmark, NW part training and the UE part training\n-\tSame pair of NW part model and UE part model between 1-on-1 joint training and UE first separate training.\n-\tQuantization/dequantization method/parameters between NW side and UE side are aligned.\n-\tNote: Results refer to Table 5.17 of R1-2308342.\nUE first training, M>1 UE part models to 1 NW part model\nFor the evaluation of Type 3 UE first separate training with dataset sharing manner for CSI compression, for the pairing between M>1 separate UE part models and 1 NW part model (Case 2), when taking 1-on-1 joint training between the NW part model and the UE part model as benchmark, larger performance loss is observed in general than the case of UE first separate training with 1 UE part model and 1 NW part model pairing (Case 1):\n-\t8 sources observe minor loss of -0%~-1.82% compared to 1-on-1 joint training.\n-\t4 sources observe moderate loss of -2.17%~-4.96% compared to 1-on-1 joint training.\n-\t2 sources observe significant loss of -11.56%~-73.7% compared to 1-on-1 joint training.\n-\tNote: 1 source observes other UE first separate training implementations may achieve better performance.\n-\tNote: the dataset sharing behaviour from above sources follows the example of the agreement, where \"the set of information includes the input and output of the Network side CSI generation part, or includes the output of the Network side CSI generation part only\".\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tPrecoding matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\tThe performance metric is SGCS for Layer 1.\n-\tSame size of training dataset for benchmark, NW part training and the UE part training\n-\tSame pair of NW part model and UE part model between 1-on-1 joint training and UE first separate training.\n-\tQuantization/dequantization method/parameters between NW side and UE side are aligned.\n-\tM=2, 3, or 4 are considered.\n-\tNote: Results refer to Table 5.25 of R1-2308343.\nThe complexity values in terms of FLOPs and number of parameters of AI/ML models adopted in the evaluations of CSI prediction are summarized in Figure 6.2.2.6-1.\n-\tResults refer to Table 2 of clause 7.3, R1-2310450.\nThe complexity of AI/ML models in terms of FLOPs and number of parameters for CSI prediction is depicted in Figure 6.2.2.6-1, illustrating the intricate nature of these models in the context of AI/ML.\nFigure 6.2.2.6-1: Complexity of AI/ML models from evaluation results in terms of FLOPs \nand number of parameters for CSI prediction.\nSGCS performance, impact of input type\nFor the AI/ML based CSI prediction, compared with the benchmark of the nearest historical CSI:\n-\tspatial consistency is not adopted in 15 sources, wherein:\n-\t15 sources observe the gain of 0.46% ~ 44.8% using raw channel matrix as input, wherein\n-\t4 sources observe the gain of 0.46%~6.3%.\n-\t14 sources observe the gain of 7.57%~26.47%.\n-\t5 sources observe the gain of 29.03%~44.8%.\n-\t4 sources observe the gain of 2.24% ~ 19.4% using precoding matrix as input, which is in general worse than using raw channel matrix as input\n-\tspatial consistency is adopted in 4 sources, all of which use raw channel matrix as input, wherein\n-\t3 sources observe the gain of 1.7%~35.51%.\n-\t1 source observe the gain of 76.6%.\n-\t1 source observe the loss of -5.5%.\nThe above results are based on the following assumptions:\n-\tThe observation window considers to start as early as 15ms~50ms.\n-\tA future 4ms or 5ms instance from the prediction output is considered for calculating the metric.\n-\tUE speed includes 10km/h, 30km/h, and 60km/h. The same fixed UE speed is assumed for both training and inference.\n-\tThe performance metric is SGCS in linear value for layer 1.\n-\tNote: Results refer to Table 5.26 of R1-2308344.\nSGCS performance, impact of UE speed\nFor the AI/ML based CSI prediction, compared to the Benchmark#1 of the nearest historical CSI, in terms of SGCS, from UE speed perspective, in general the gain of AI/ML based solution is related with the UE speed:\n-\tFor 10km/h UE speed, 6 sources observe 2.4%~12.5% gain (2.4%~12.5% gain for 5 sources who do not adopt spatial consistency, and 8.7% gain for 1 source who adopts spatial consistency), 1 source observes 21.93% gain (who does not adopt spatial consistency).\n-\tFor 30km/h UE speed, 1 source observes loss of -5.5% (who adopts spatial consistency), 3 sources observe 6%~10.43% gain (who do not adopt spatial consistency), 8 sources observe 12.65%~33% gain (14.65%~33% gain for 7 sources who do not adopt spatial consistency, and 12.65% gain for 1 source who adopts spatial consistency), and 3 sources observe 41.75%~ 76.6% gain (41.75%~ 44.8% gain for 2 sources who do not adopt spatial consistency, and 76.6% gain for 1 source who adopts spatial consistency), which are in general larger than 10km/h UE speed.\n-\tFor 60km/h UE speed, 3 sources observe 0.46%~2.6% gain (0.46%~2.3% gain for 2 sources who do not adopt spatial consistency, and 1.7%~2.6% gain for 1 source who adopts spatial consistency), 7 sources observe 9.1%~20.6% gain (9.1%~20.6% gain for 6 sources who do not adopt spatial consistency, and 13.8% gain for 1 source who adopts spatial consistency), 1 source observe 29.03% gain, which are in general smaller than 30km/h UE speed.\nThe above results are based on the following assumptions:\n-\tThe observation window considers to start as early as 15ms~50ms.\n-\tA future 4ms or 5ms instance from the prediction output is considered for calculating the metric.\n-\tRaw channel matrix is considered as model input\n-\tThe performance metric is SGCS in linear value for layer 1.\n-\tNo post processing is considered.\n-\tThe same fixed UE speed is assumed for both training and inference.\n-\tNote: Results refer to Table 5.27 of R1-2308344.\nSGCS performance, impact of observation window\nFor the AI/ML based CSI prediction, compared to the Benchmark#1 of the nearest historical CSI, in terms of SGCS, from observation window length perspective, in general the gain of AI/ML based solution is slightly increased with the increase of the length for the observation window:\n-\tWhen the observation window is increased from 5/5ms to 8/5ms, the gain over benchmark is increased by 0.28%~2.19%, as observed by 2 sources.\n-\tWhen the observation window is increased from 5/5ms to 15/5ms, the gain over benchmark is increased by 5.59%~10.32%, as observed by 1 source.\n-\tWhen the observation window is increased from 4/5ms to 8/5ms and 10/5ms, the gain over benchmark is increased by 0.96%~4.23% and 1%~4.42%, respectively, as observed by 2 sources.\nThe above results are based on the following assumptions:\n-\tThe UE speed is 30km/h.\n-\tA future 4ms or 5ms instance from the prediction output is considered for calculating the metric.\n-\tRaw channel matrix is considered as model input\n-\tThe performance metric is SGCS in linear value for layer 1.\n-\tNo post processing is considered.\n-\tNote: Results refer to Table 5.32 of R1-2308344.\nSGCS performance, impact of prediction window\n-\tFor the AI/ML based CSI prediction, compared to the Benchmark#1 of the nearest historical CSI, in terms of SGCS, from prediction window length perspective, in general the gain of AI/ML based solution is related with the prediction length in terms of the distance to the applicable time of the predicted CSI:\n-\tWhen the prediction length is increased from 10ms to 15ms, the gain over benchmark is reduced (gap from -1.13%~-51%), as observed by 3 sources.\n-\tWhen the prediction length is increased from 2.5ms/3ms to 5ms, the gain over benchmark is increased (gap from +5.85%~+13%), as observed by 2 sources.\n-\tWhen the prediction length is increased from 5ms to 10ms, 5 sources observe the gain over benchmark is reduced (gap from -1%~-12.1%) while 2 sources observe the gain over benchmark is increased (+11.65%~+45.5%).\nThe above results are based on the following assumptions:\n-\tThe UE speed is 30km/h.\n-\tThe observation window considers to start as early as 15ms~50ms.\n-\tRaw channel matrix is considered as model input.\n-\tThe performance metric is SGCS in linear value for layer 1.\n-\tNo post processing is considered.\n-\tNote: Results refer to Table 5.33 of R1-2308344.\nMean UPT\nFor the AI/ML based CSI prediction, in terms of mean UPT, gains are observed compared to both Benchmark#1 of the nearest historical CSI and Benchmark#2 of a non-AI/ML based CSI prediction approach:\n-\tCompared to the benchmark of the nearest historical CSI:\n-\tFor FTP traffic:\n-\t4 sources observe 1.2%~4.9% gain;\n-\t2 sources observe 5.3%~10.58% gain;\n-\t2 sources observe 15.1% ~23.5% gain.\n-\t1 source observes loss of -1.3%~-13.8%.\n-\tFor full buffer traffic:\n-\t1 source observes 2%~3% gain;\n-\t2 sources observe 7.6%~15.6% gain.\n-\tCompared to the benchmark of an auto-regression/Kalman filter based CSI prediction:\n-\tFor FTP traffic:\n-\t3 sources observe 0.7%~7.0% gain;\n-\t2 sources observe loss of -0.1%~-2.4%.\n-\t1 source observe loss of -3%~-17%.\n-\tFor full buffer traffic:\n-\t2 sources observes 0.6%~2.78% gain.\n-\t1 source observes 8.1%~11.5% gain.\nThe above results are based on the following assumptions:\n-\tThe same fixed UE speed of 30km/h or 60km/h is assumed for both training and inference\n-\tThe observation window considers to start as early as 15ms~50ms.\n-\tA future 4ms or 5ms instance from the prediction output is considered for calculating the metric.\n-\tRaw channel matrix is considered as model input\n-\tThe performance metric is mean UPT for Max rank 1.\n-\tNo post processing is considered.\n-\tNote: Results refer to Table 5.28 of R1-2308344.\n5% UPT\nFor the AI/ML based CSI prediction, in terms of 5% UPT, gains are observed compared to both Benchmark#1 of the nearest historical CSI and Benchmark#2 of a non-AI/ML based CSI prediction approach:\n-\tCompared to the benchmark of the nearest historical CSI:\n-\tFor FTP traffic:\n-\t4 sources  observe 1% ~9.7% gain;\n-\t5 sources observe 10%~26.4% gain;\n-\t1 source observes loss of -11.6%~-14%;\n-\tFor full buffer traffic:\n-\t3 sources observe 3.5%~35.3% gain;\n-\tCompared to the benchmark of an auto-regression/Kalman filter based CSI prediction:\n-\tFor FTP traffic:\n-\t3 sources observe 0.18%~17.58% gain;\n-\t1 source observes -8.2%~-12.4% degradation;\n-\tFor full buffer traffic:\n-\t1 source observes 6.7% ~15.4% gain.\n-\t1 source observes -2% degradation\nThe above results are based on the following assumptions:\n-\tThe same fixed UE speed of 30km/h or 60km/h is assumed for both training and inference\n-\tThe observation window considers to start as early as 15ms~50ms.\n-\tA future 4ms or 5ms instance from the prediction output is considered for calculating the metric.\n-\tRaw channel matrix is considered as model input\n-\tThe performance metric is 5% UPT for Max rank 1.\n-\tNo post processing is considered.\n-\tNote: Results refer to Table 5.29 of R1-2308344.\nGeneralization over UE speeds\nFor the generalization verification of AI/ML based CSI prediction over various UE speeds compared to the generalization Case 1 where the AI/ML model is trained with dataset subject to a certain UE speed#B and applied for inference with a same UE speed#B,\n-\tFor generalization Case 2, generalized performance may be achieved for certain combinations of UE speed#A and UE speed#B but not for others:\n-\tIf UE speed#B is 10 km/h & UE speed#A is 30 km/h, 2 sources observe a generalized performance of less than -1.4% degradation.\n-\tNote: 1 company still observes significant degradation (-11.3%~-13.4% loss).\n-\tIf UE speed#B is either 30 km/h or 60 km/h or 120 km/h, or if UE speed#B is 10km/h and UE speed#A is either 60km/h or 120km/h, 11 sources observe that moderate/significant performance degradations are suffered:\n-\tFor UE speed#B is 10 km/h & UE speed#A is either 60 km/h or 120 km/h, 1 source observes moderate degradation (-2.3% loss), 3 sources observe significant degradation (-5.5%~-61% loss).\n-\tFor UE speed#B is 30 km/h & UE speed#A is either 10 km/h, 60 km/h or 120 km/h, 2 sources observe moderate degradation (-2.01%~-4.62% loss), 9 sources observe significant degradation (-5%~-72.37% loss).\n-\tFor UE speed#B is 60 km/h & UE speed#A is either 10 km/h, 30 km/h or 120 km/h, 1 source observes moderate degradation (-3% loss), 10 sources observe significant degradation (-7.8%~-76.85% loss).\n-\tFor UE speed#B is 120 km/h & UE speed#A is either 30 km/h or 60 km/h, 1 source observes moderate degradation (-3.4% loss), 5 sources observe significant degradation (-7.55%~-56.3% loss).\n-\tFor generalization Case 3, generalized performance of the AI/ML model can be achieved in general (0%~-4.45% loss) for UE speed#B subject to any of 10 km/h, 30 km/h, 60 km/h and 120 km/h, if the training dataset is constructed with data samples subject to multiple UE speeds including UE speed#B, as observed by 11 sources.\n-\tFor UE speed#B is 10 km/h, minor loss (-0.2%~-1.7%) are observed by 4 sources.\n-\tFor UE speed#B is 30 km/h, minor loss (-0.2%~-1.34%) or positive gain are observed by 5 sources, moderate loss (-4.07%~-4.2%) are observed by 2 sources.\n-\tFor UE speed#B is 60 km/h, minor loss (-0.05%~-2%) are observed by 4 sources, moderate loss (-3.76%~-4.65%) are observed by 2 sources.\n-\tFor UE speed#B is 120 km/h, moderate loss (-2%~-4.45%) are observed by 4 sources.\n-\tNote: For generalization Case 3, 6 sources observe significant performance degradations (-5%~-43.6% loss) for UE speed#B subject to 10 km/h, 30 km/h, 60 km/h, but compared with generalization Case 2, in general the performance is still improved.\nThe above results are based on the following assumptions besides the assumptions of the agreed EVM table:\n-\tRaw channel matrix is used as the model input.\n-\tTraining data samples are not quantized, i.e., Float32 is used/represented.\n-\tThe performance metric is SGCS in linear value for layer 1/2/3/4.\n-\tNo spatial consistency is considered.\n-\tNote: Results refer to Table 5.5 of R1-2308340.\nThe following aspects have been studied for the evaluation on AI/ML based CSI compression in Rel-18:\n-\tFrom the perspective of basic performance gain over non-AI/ML benchmark (assuming 1 on 1 joint training without considering generalization),\no\tIt has been studied with corresponding observations on:\nï§\tthe metrics of SGCS, mean UPT, 5% UPT, CSI feedback overhead reduction\nï§\tthe benchmark of R16 Type II codebook\no\tIt has been studied but is lack of observations on:\nï§\tthe metric of NMSE\nï§\tthe benchmarks of Type I codebook and R17 Type II codebook\no\tIt has been studied with corresponding observations on complexity but without comparison with non-AI/ML.\n-\tFrom the perspective of AI/ML solutions (assuming 1 on 1 joint training without considering generalization),\no\tIt has been studied with corresponding observations on: model input/output type, monitoring for intermediate KPI (including NW side monitoring and UE side monitoring), quantization methods (including quantization awareness for training, and quantization format), and high resolution ground-truth CSI for training, with the metric of SGCS.\no\tIt has been studied but is lack of observations on: the options of CQI/RI calculation, and the options of rank>1 solution\n-\tFrom the perspective of generalization over various scenarios (assuming 1 on 1 joint training),\no\tIt has been studied with corresponding observations on (with the metric of SGCS):\nï§\tthe scenarios including various deployment scenarios, various outdoor/indoor UE distributions, various carrier frequencies, and various TxRU mappings\nï§\tthe approach of dataset mixing (generalization Case 3)\no\tIt has been studied but is lack of observations on:\nï§\tother aspects of scenarios\nï§\tthe approach of fine-tuning\n-\tFrom the perspective of scalability over various configurations (assuming 1 on 1 joint training),\no\tIt has been studied with corresponding observations on (with the metric of SGCS):\nï§\tthe configurations including various bandwidths/frequency granularities, various CSI feedback payloads, and various antenna port numbers\nï§\tthe approach of dataset mixing (generalization Case 3), and the approach of fine-tuning for CSI feedback payloads\nï§\tthe scalability solutions\no\tIt has been studied but is lack of observations on:\nï§\tother aspects of configurations\nï§\tthe approach of fine-tuning for configurations other than CSI feedback payloads\n-\tFrom the perspective of multi-vendor joint training (without considering generalization),\no\tIt has been studied with corresponding observations on (with the metric of SGCS):\nï§\tjoint training between 1 NW part model and M>1 UE part models, and joint training between 1 UE part model and N>1 NW part models\no\tIt has been studied but is lack of observations on:\nï§\tjoint training between N>1 NW part models and M>1 UE part models\nï§\tperformance comparison between simultaneous training and sequential training\n-\tFrom the perspective of separate training (without considering generalization),\no\tIt has been studied with corresponding observations on (with the metric of SGCS):\nï§\tNW first training, including 1 NW part model to 1 UE part model with same backbone and with different backbones, and 1 UE part model to N>1 NW part models\nï§\tUE first training, including 1 NW part model to 1 UE part model with same backbone and with different backbones, and 1 NW part model to M>1 UE part models\nï§\tImpact of shared dataset under 1 NW part model to 1 UE part model for NW first training and UE first training\no\tIt has been studied but is lack of observations on:\nï§\tthe metric of air-interface overhead of information (e.g., dataset) sharing\nThe following aspects have been studied for the evaluation on AI/ML based CSI prediction:\n-\tFrom the perspective of basic performance gain over non-AI/ML benchmark (without considering generalization),\no\tIt has been studied with corresponding observations on:\nï§\tthe metrics of SGCS, mean UPT, 5% UPT;\nï§\tthe benchmarks of nearest historical CSI and auto-regression/Kalman filter based CSI prediction.\nï·\tNote: the benchmark of level x based CSI prediction is represented by generalization cases.\no\tIt has been studied but is lack of observations on:\nï§\tthe impact of modeling spatial consistency\nï§\tthe metrics of NMSE\no\tIt has been studied with corresponding observations on complexity but without comparison with non-AI/ML\n-\tFrom the perspective of AI/ML solutions (without considering generalization),\no\tIt has been studied with corresponding observations on (with the metric of SGCS and the benchmark of nearest historical CSI): impact of input type, impact of UE speed, impact of prediction window, impact of observation window\n-\tFrom the perspective of generalization over various scenarios,\no\tIt has been studied with corresponding observations on (with the metric of SGCS):\nï§\tthe scenario including various UE speeds\nï§\tthe approach of dataset mixing (generalization Case 3)\no\tIt has been studied but is lack of observations on:\nï§\tvarious deployment scenarios, various carrier frequencies, and other aspects of scenarios.\nï§\tthe approach of fine-tuning\n-\tFrom the perspective of scalability over various configurations, it has been studied but is lack of observations.\nBased on the evaluation for CSI compression, the following high-level observations are provided:\n-\tFrom the perspective of basic performance gain over non-AI/ML benchmark, AI/ML based CSI compression outperforms Rel-16 eType II CB in general under 1-on-1 joint training and generalization Case 1, where\no\t0.2%~2%/-0.3%~6%/-4%~6% gains of mean UPT as shown in Figure 6.2.2.8-1 through Figure 6.2.2.8-3 are observed for Max rank 1/2/4, respectively, under RUâ¤39%.\no\t0.1%~4%/-0.5%~10%/-1.8%~12.22% gains of mean UPT as shown in Figure 6.2.2.8-4 through Figure 6.2.2.8-6 are observed for Max rank 1/2/4, respectively, under RU40%-69%.\no\t0.23%~9%/-0.2%~15%/-1%~17% gains of mean UPT as shown in Figure 6.2.2.8-7 through Figure 6.2.2.8-9 are observed for Max rank 1/2/4, respectively, under RUâ¥70%.\nThe figure depicts the mean UPT gain and the maximum Rank 1 (RU) values for a specific source index, with x-axis indicating the index of the source. The figure is used to analyze the performance of a specific source in a network, highlighting the impact of the source on the network's performance.\nFigure 6.2.2.8-1: Mean UPT gain, Max Rank 1 (RUâ¤39%), x-axis means index of source\n\nThe figure depicts the mean UPT gain and the maximum Rank 2 (RU) values for a specific index of source, with x-axis indicating the index of the source. The figure is used to analyze the performance of a specific source in a network, with RU values indicating the reliability of the source.\nFigure 6.2.2.8-2: Mean UPT gain, Max Rank 2 (RUâ¤39%), x-axis means index of source\n\nThe figure depicts the mean UPT gain and the maximum Rank 4 (RU â¤39%) of a 5G network, with x-axis indicating the index of the source. The figure illustrates the impact of various factors on the UPT gain, such as the number of sources, the distance between them, and the presence of obstacles. The maximum Rank 4 indicates the highest possible UPT gain, which is crucial for achieving high data rates in 5G networks.\nFigure 6.2.2.8-3: Mean UPT gain, Max Rank 4 (RUâ¤39%), x-axis means index of source\n\nThe figure depicts the mean UPT gain and the maximum Rank 1 (RU40%-69%) of a specific source, with x-axis indicating the index of the source. The figure is labeled as \"Figure 6.2.2.8-4: Mean UPT gain, Max Rank 1 (RU40%-69 % ),\" and is part of a larger set of figures that provide detailed information about the signal propagation in a 5G network.\nFigure 6.2.2.8-4: Mean UPT gain, Max Rank 1 (RU40%-69%), x-axis means index of source\n\nThe figure depicts the mean UPT gain and the maximum Rank 2 (RU40%-69%) of a specific source, with x-axis indicating the index of the source. The figure is used to analyze the performance of a specific source in a network, highlighting its performance in terms of UPT gain and Rank 2.\nFigure 6.2.2.8-5: Mean UPT gain, Max Rank 2 (RU40%-69%), x-axis means index of source\n\nThe figure depicts the mean UPT gain and the maximum Rank 4 (RU40%-69%) of a specific source, with x-axis indicating the index of the source. The figure is used to analyze the performance of a specific source in a network, highlighting its performance in terms of UPT gain and Rank 4.\nFigure 6.2.2.8-6: Mean UPT gain, Max Rank 4 (RU40%-69%), x-axis means index of source\n\nThe figure depicts the mean UPT gain and the maximum Rank 1 (RUâ¥70%) of a wireless communication system, with x-axis indicating the index of the source. The figure illustrates the performance of the system under different conditions, such as different signal strengths and distances between the source and the receiver.\nFigure 6.2.2.8-7: Mean UPT gain, Max Rank 1 (RUâ¥70%), x-axis means index of source\n\nThe figure depicts the mean UPT gain and the maximum Rank 2 (RUâ¥70%) of a signal source, with x-axis indicating the index of the source. The figure is used to analyze the performance of a signal source in a 5G network, highlighting the importance of signal quality and the use of beamforming techniques to mitigate interference.\nFigure 6.2.2.8-8: Mean UPT gain, Max Rank 2 (RUâ¥70%), x-axis means index of source\n\nThe figure depicts the mean UPT gain and the maximum Rank 4 (RUâ¥70%) of a signal source, with x-axis indicating the index of the source. The figure is used to analyze the performance of a signal source in a 5G network, highlighting the importance of signal quality and the use of advanced signal processing techniques to improve the overall system performance.\nFigure 6.2.2.8-9: Mean UPT gain, Max Rank 4 (RUâ¥70%), x-axis means index of source\n-\tFrom the perspective of CSI feedback overhead reduction over non-AI/ML, AI/ML based CSI compression achieves CSI feedback reduction compared with Rel-16 eType II CB in general under 1-on-1 joint training and generalization Case 1, where 4 sources observe the CSI feedback overhead reduction of 10.24%~60%/10%~58.33%/8%~79% for Max rank 1/2/4, respectively, under FTP traffic.\n-\tFrom the perspective of AI/ML complexity, a majority of 25 sources adopt the CSI generation model subject to the computational complexity in units of FLOPs from 10M to 800M, and 26 sources adopt the CSI reconstruction model subject to the FLOPs from 10M to 1100M. The actual model complexity may differ from the model complexity in the evaluation with respect to platform-dependent optimization on model implementations. In addition, the complexity between AI/ML and non-AI/ML benchmark is not compared.\n-\tFrom the perspective of model input/output type, it is more beneficial by considering precoding matrix as the model input (for CSI generation part)/output (for CSI reconstruction part) than explicit channel matrix.\n-\tFrom the perspective of intermediate KPI based monitoring,\no\tFor the monitoring at NW side, increased monitoring accuracy can be achieved by considering R16 eType II CB with new/larger parameter(s) as the ground-truth CSI format for monitoring. On the other hand, the new/larger parameter(s) would lead to increased air-interface overhead compared to R16 eType II CB with legacy parameters.\no\tFor the monitoring at UE side, performance can be monitored with smaller air-interface overhead by considering proxy model at UE compared with monitoring at NW side. On the other hand, the monitoring accuracy may be impacted by the design/robustness of the proxy model.\no\tNote: the complexity aspect for Case 1, Case 2-1 and Case 2-2 is not evaluated.\n-\tFrom the perspective of quantization methods for CSI feedback,\no\tFor the quantization awareness for training, it is beneficial to consider quantization aware training with fixed/pre-configured quantization method/parameters (Case 2-1) or jointly updated quantization method/parameters (Case 2-2) to avoid severe performance degradation. In particular, it is more beneficial in performance for Case 2-2 over Case 2-1 under vector quantization format (VQ).\no\tFor the quantization format, VQ format achieves comparable performance with scalar quantization format (SQ) in general, where VQ achieves better performance than SQ in some cases while worse in some other cases.\n-\tFrom the perspective of high resolution ground-truth CSI for training, compared to unquantized ground-truth CSI (e.g., Float32), taking R16 eType II CB with new/larger parameter(s) as the ground-truth CSI format for training data collection can achieve significant overhead reduction without causing severe performance degradation; taking scalar quantization format for training data collection can achieve moderate overhead reduction without causing severe performance degradation. On the other hand, the R16 eType II CB with new/larger parameter(s) would lead to increased overhead compared to R16 eType II CB with legacy parameters\no\tFor ground-truth CSI format, 5 sources observe R16 eType II CB with new/larger parameter(s) outperforms R16 eType II CB with legacy parameter, while one source observes R16 eType II CB with legacy parameter is already close to Float32 with particular dataset processing technique.\no\tNote: the complexity aspect is not evaluated.\n-\tFrom the perspective of generalization over scenarios, or scalability over configurations that have been evaluated, compared to generalization Case 1 where the AI/ML model is trained with dataset subject to a certain scenario#B/configuration#B and applied for inference with a same scenario#B/configuration#B,\no\tFor generalization Case 2 where the AI/ML model is trained with dataset from a different scenario#A/configuration#A, generalized performance may be achieved for some certain combinations of scenario#A/configuration#A and scenario#B/configuration#B but not for others.\no\tFor generalization Case 3 where the training dataset is constructed with data samples subject to more than one scenario/configuration (evaluations studied up to four scenarios/configurations) including scenario#B/configuration#B, generalized performance of the AI/ML model can be achieved.\no\tIn particular, appropriate scalability solution (e.g., truncation/padding, adaptive quantization granularities, adaptation layer in the AI/ML model) may need to be performed to scale the dimensions of the AI/ML model when the training dataset includes data samples subject to configuration#A which has different input/output dimension than configuration#B.\n-\tFrom the perspective of training collaboration types, compared to 1-on-1 joint training, both multi-vendor joint training and separate training with procedures given in clause 6.2.1 may suffer performance loss.\no\tIn particular, for multi-vendor joint training, minor or moderate degradation is observed.\no\tIn particular, for separate training with procedure given in clause 6.2.1, the performance loss depends on the factors such as backbone alignment, and multi-vendor training behavior:\nï§\tFor separate training of 1 NW part model and 1 UE part model, under both NW first training and UE first training, if backbones are aligned between the two sides, minor degradation is observed; otherwise, additional degradation is observed, leading to minor or moderate performance degradation.\nï§\tFor NW first training with 1 UE part model to N>1 NW part models, or UE first training with 1 NW part model to M>1 UE part models, additional degradation is observed, leading to minor, moderate, or significant performance degradation, depending on the training approach.\nï§\tAs a note, other procedures of separate training are not extensively evaluated.\nBased on the evaluation for CSI prediction, the following high-level observations are provided:\n-\tFrom the perspective of basic performance gain over non-AI/ML benchmark, under the same UE speed for training and inference,\no\tAI/ML based CSI prediction outperforms the benchmark of the nearest historical CSI in general, where the majority of sources observe up to 10.6% gain in terms of mean UPT.\no\tfor AI/ML based CSI prediction over non-AI/ML based CSI prediction, 3 sources observe 0.7%~7% gain while 2 sources observe performance loss of -0.1%~-17% in terms of mean UPT.\n-\tFrom the perspective of AI/ML complexity, a majority of 16 sources adopt the model subject to the computational complexity in units of FLOPs from 0.1M to 1000M. The actual model complexity may differ from the model complexity in the evaluation with respect to platform-dependent optimization on model implementations. In addition, the complexity between AI/ML and non-AI/ML benchmark is not compared.\n-\tFrom the perspective of model input/output type, it is more beneficial in performance by considering raw channel matrix as the model input than precoding matrix\n-\tFrom the perspective of AI/ML solutions, the gain of AI/ML based CSI prediction over the benchmark of the nearest historical CSI is impacted by the observation window length, prediction window length, and UE speed\n-\tFrom the perspective of generalization over UE speeds that have been evaluated, compared to generalization Case 1 where the AI/ML model is trained with dataset subject to a certain UE speed#B and applied for inference with a same UE speed#B,\no\tFor generalization Case 2 where the AI/ML model is trained with dataset from a different UE speed#A, generalized performance may be achieved for some certain combinations of UE speed#A and UE speed#B but not for others\no\tFor generalization Case 3 where the training dataset is constructed with data samples subject to multiple UE speeds including UE speed#B, generalized performance of the AI/ML model can be achieved in general\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.3\tBeam management",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.3.1\tEvaluation assumptions, methodology and KPIs",
                            "text_content": "Figure 6.3.1-1 provides an example for the inference procedure for beam management for BM-Case1 and BM-Case2. Measurements based on Set B of beams are used as model input. In addition, beam ID information may be also provided as input to the AI/ML model. Based on model output (e.g., probability of each beam in Set A to be the Top-1 beam, predicted L1-RSRPs), Top-1/N beam(s) among Set A of beams can be predicted and/or potentially with predicted L1-RSRPs (depending on the labelling). In the evaluation, for BM-Case 1, the measurements of Set B (otherwise stated) are used as model input to predict Top-1/N beams from Set A, and for BM-Case2, the measurements from historic time instance(s) are used as model input for temporal DL beam prediction of beams from Set A. In the evaluation, the cases that Set A and Set B are different (Set B is NOT a subset of Set A), and Set B is a subset of Set A for both BM-Case1 and BM-Case2, and case that Set A and Set B are the same for BM-Case2 are considered. And the performance of DL Tx beam prediction and DL Tx-Rx beam pair prediction is evaluated.\nFor both BM-Case1 and BM-Case2, UE can report the prediction result to NW based on the output of a UE-side model, or NW can predict the Top-1/N beam(s) based on the reported measurements of Set B for a NW-side model.\nThe figure depicts an inference procedure for beam management in a 5G network, illustrating the steps involved in determining the optimal beam pattern for a given signal. The figure shows a series of steps, including the selection of a beamforming matrix, the calculation of the beamforming gain, and the selection of the optimal beam pattern. The figure is a visual representation of the complex process involved in beam management in a 5G network, highlighting the importance of this technique in ensuring reliable and efficient communication.\nFigure 6.3.1-1: An example of the inference procedure for beam management.\n\nFor dataset construction and performance evaluation (if applicable) in the AI/ML for beam management use case, system level simulation approach is adopted as baseline. Link level simulation is optionally adopted.\nKPIs:\n-\tModel complexity and computational complexity.\nBeam prediction accuracy related KPIs, including:\n-\tTop-1 genie-aided Tx beam considers the following definitions:\n-\tOption A (baseline), the Top-1 genie-aided Tx beam is the Tx beam that results in the largest L1-RSRP over all Tx and Rx beams\n-\tOption B (optional), the Top-1 genie-aided Tx beam is the Tx beam that results in the largest L1-RSRP over all Tx beams with specific Rx beam(s)\n- Specific Rx beam(s) are to be reported. Note: specific Rx beams are a subset of all Rx beams.\n-\tTop-1 genie-aided Tx-Rx beam pair considers the following definitions:\n-\tOption A: The Tx-Rx beam pair that results in the largest L1-RSRP over all Tx and Rx beams\n-\tOther options not precluded and can be reported\n-\tAverage L1-RSRP difference of Top-1 predicted beam:\n-\tThe difference between the ideal L1-RSRP of Top-1 predicted beam and the ideal L1-RSRP of the Top-1 genie-aided beam\n-\tBeam prediction accuracy (%):\n-\tTop-1 (%): the percentage of \"the Top-1 genie-aided beam is Top-1 predicted beam\"\n-\tTop-K/1 (%): the percentage of \"the Top-1 genie-aided beam is one of the Top-K predicted beams\"\n-\tTop-1/K (%) (Optional): the percentage of \"the Top-1 predicted beam is one of the Top-K genie-aided beams\"\n-\tWhere K >1 and values can be reported\n-\tCDF of L1-RSRP difference for Top-1 predicted beam\n-\tBeam prediction accuracy (%) with 1dB margin for Top-1 beam\n-\tThe beam prediction accuracy (%) with 1dB margin is the percentage of the Top-1 predicted beam \"whose ideal L1-RSRP is within 1dB of the ideal L1-RSRP of the Top-1 genie-aided beam\"\n-\tOther beam prediction accuracy related KPIs are not precluded and can be reported\nImpact of quantization error of imputed L1-RSRP (for training and inference) is to be studied. Existing quantization granularity of L1-RSRP (i.e., 1dB for the best beam, 2dB for the difference to the best beam) is the starting point for evaluation at least for NW-sided model.\nThe performance impact of the relative L1-RSRP measurement error can be optionally evaluated for both DL Tx beam and beam pair prediction, where the relative L1-RSRP measurement error can be modelled as noise among beams as a starting point:\n-\tAdditive Gaussian noise with 95% of the density function within the measurement accuracy range, and/or uniformly distributed noise for the error due to baseband and/or RF impairment.\n-\tOther modelling methods are not precluded and can be reported by companies.\n-\tCompaniesâ report includes how to model the measurement error and the measurement accuracy range in training and test data and labels.\n-\tCompaniesâ report includes the baseline performance with the relative L1-RSRP measurement error\nSystem performance related KPIs, including:\n-\tUE throughput: CDF of UE throughput, average and 5%-ile UE throughput\n-\tRS overhead reduction for BM-Case1:\n-\tOption 1: \"RS \" OH reduction[%]=1-N/M\n-\twhere N is the number of beams (pairs) (with reference signal (SSB and/or CSI-RS)) required for measurement for AI/ML\n-\twhere M is the total number of beams (pairs) to be predicted\n-\tOption 2: \"RS \" OH reduction[%]=1-N/M\n-\twhere N is the total number of beams (pairs) (with reference signal (SSB and/or CSI-RS)) required for measurement for AI/ML, including the beams (pairs) required for additional measurements before/after the prediction if applicable\n-\twhere M is the total number of beams (pairs) (with reference signal (SSB and/or CSI-RS)) required for measurement for baseline scheme, including the beams (pairs) required for additional measurements before/after the prediction if applicable\n-\tCompanies report the assumption on additional measurements\n-\tRS overhead reduction for BM-Case2, when Top-1 and Top-K beam (pairs) are inferred:\n-\t\"RS \" OH reduction[%]=1-N/M\n-\twhere N is the total number of beams (pairs) (with reference signal (SSB and/or CSI-RS)) required for measurement for AI/ML, including the beams (pairs) required for additional measurements before/after the prediction if applicable.\n-\twhere M is the total number of beams (pairs) (with reference signal (SSB and/or CSI-RS)) required for measurement for baseline scheme\n-\tCompanies report the assumption on additional measurements.\n-\tCompanies report the assumption on baseline scheme.\n-\tCompanies report the assumption on T1 and T2.\n-\tOther System performance related KPIs are not precluded and can be reported by companies\nTo calculate the measurement/RS overhead reduction and summarize results for BM-Case 2, at least when Top-1 beam (pair) is inferred:\n-\tCase A: based on number of measurements/RSs and prediction time.Â An example is shown in Figure 6.3.1-2.\n-\twhere T2 is the time duration for beam prediction\n-\twhere Mt is the number of time instances for measurement as AI/ML inputs with a periodicity of Tper\n-\twhere Pt is the number of time instance(s) for prediction with a periodicity of Tper in T2\n-\tIn this case, the non-AI baseline is Option 1 (measured all the beams at each time instance(s) for prediction with a periodicity of Tper in T2)\n-\tFor Set B= Set A, the RS overhead reduction is 1-Mt/(Mt+Pt).\n-\tFor Set B (N beams, same number in each time instance) is a subset of Set A (M beams), the RS overhead reduction is\n-\tN*Mt/(M*(Mt+Pt)) if no sliding window\n-\t1-N/M if considering sliding window\n-\tCase B: based on a periodicity T of the required reference signals for measurements to achieve a certain beam prediction accuracy. An example is shown in Figure 6.3.1-3.\n-\tFor non-AI baseline (Option 2), every T=X ms reference signals for measurements are needed\n-\tFor AI, every T=Y ms, reference signals for measurements are needed\n-\tIn this case,\n-\tFor Set B = Set A, the RS overhead reduction is 1-X/Y.\n-\tFor Set B (N beams) is a subset of Set A (M beams), the RS overhead reduction is [1-XN/(YM)].\n-\tCase B+: based on Y times of a given minimal periodicity Tper of the reference signals for measurements. An example is shown in Figure 6.3.1-4.\n-\tFor non-AI baseline (Option 1), UE measures all the reference signals of Set A every Tper\n-\tFor AI, UE measures the reference signals of Set B every Y times of Tper\n-\tIn this case, prediction time is defined as the time from each measurement instance to the latest prediction instance before the next measurement instance.\n-\tIn this case, the non-AI baseline is Option 1 (measured all the beams at each time instance(s) for prediction with a periodicity of Tper, which is reported by companies)\n-\tFor Set B= Set A, the RS overhead reduction is 1-1/Y.\n-\tFor Set B (N beams) is a subset of Set A (M beams), the RS overhead reduction is 1-N/(YM).\n\nThe figure depicts a simplified representation of a 5G network, illustrating the signal propagation in a scenario where the base station (gNB) is located in a building and the user equipment (UE) is located outdoors. The diagram highlights the presence of multiple paths, including NLOS (Non-Line-of-Sight) and LOS (Line-of-Sight), which are essential for signal quality and interference management. Beamforming techniques are employed to mitigate interference, ensuring a reliable and efficient communication network.\nFigure 6.3.1-2: Example for Case A\nThe figure depicts a simplified representation of a 3GPP case B scenario, illustrating the various elements involved in a 3GPP case B network. The figure includes a base station (BS), a mobile station (MS), a mobile terminal (MT), and a mobile edge computing (MEC) node. The BS is equipped with a 3GPP radio interface, while the MS and MT are equipped with 3GPP radio interfaces as well. The MEC node is equipped with a 3GPP radio interface and a 3GPP radio interface for the MEC. The figure also includes a 3GPP radio interface for the MEC and a 3GPP radio interface for the MEC. The figure also includes a 3GPP radio interface for the MEC and a 3GPP radio interface for the MEC. The figure also includes a 3GPP radio interface for the MEC and a 3GPP radio interface for the MEC. The figure also includes a 3GPP radio interface for the MEC and a 3GPP radio interface for the MEC. The figure also includes a 3GPP radio interface for the MEC and a 3GPP radio interface for\nFigure 6.3.1-3: Example for Case B\nThe figure depicts a simplified representation of a 5G network, illustrating the signal propagation in a scenario where buildings are NLOS (Non-Line-of-Sight) and the signal is reflected off of these structures. The diagram highlights the role of base stations (gNB) and user equipment (UE) in the network, as well as the presence of scatterers. The figure also illustrates the use of beamforming techniques to mitigate interference, which is crucial for ensuring reliable communication in a 5G network.\nFigure 6.3.1-4: Example for Case B+\nFor both BM-Case1 and BM-Case2 when Set B is a subset of or different from Set A, a certain RS/measurement overhead is assumed to summarize the evaluation results for Top-1(%) beam prediction accuracy. With additional measurements among predicted Top-K beam (pairs) (i.e., with additional RS/measurement overhead), Top-1 beam (pair) can be obtained by finding a best beam (pair) among the K predicted beams (pairs) with the beam prediction accuracy of Top-K/1(%) if no genie-aid Top-1 beam change out of the K predicted beam (pairs) during the additional measurements.\nNote: This is to explain the potential implications and relations of Top-1(%) and Top-K/1(%) beam prediction accuracy metrics defined in evaluations agenda item with regards to RS overhead and additional measurement. The corresponding specification impact is a separate discussion.\nOther KPIs, including:\n-\tUCI report overhead (e.g., number of UCI reports and UCI payload size) and/or UCI overhead reduction for inference of AI/ML model can be reported, at least for NW side beam prediction\n-\tUCI overhead reduction = 1- Total UCI payload size for AI/ML/Total UCI payload size of baseline.\n-\tCompanies expected to report detailed assumption of UCI for AI/ML and baseline, e.g., including quantization mechanism.\n-\tLatency reduction:\n-\t(1 â (Total transmission time of N beams) / (Total transmission time of M beams))\n-\twhere N is the number of beams (with reference signal (SSB and/or CSI-RS)) in the input beam set required for measurement\n-\twhere M is the total number of beams\n-\tPower consumption reduction\nFor AI/ML models, which provide L1-RSRP as the model output, the accuracy of predicted L1-RSRP is to be evaluated. Companies optionally report average (absolute value)/CDF of the predicted L1-RSRP difference, where the predicted L1-RSRP difference is defined as the difference between the predicted L1-RSRP of Top-1 predicted beam and the ideal L1-RSRP of the same beam.\nModel generalization:\nIn the context of model generalization, scenarios may mean various deployment scenarios, various outdoor/indoor UE distributions, various UE mobility assumptions. Similarly, configurations may mean various UE parameters, various gNB settings, Various Set B of beam(pairs). The selected scenarios/configurations for generalization verification may consider the AI model inference node (e.g., @UE or @gNB) and use case (e.g., BM-Case1, or BM-Case2). Specifically, the following generalizations could be considered and clause 6.3.2 presents those which have been actually simulated by companies:\n-\tScenarios:\n-\tVarious deployment scenarios, e.g., UMa, UMi and others; e.g., 200m ISD or 500m ISD and others; e.g., same deployment, different cells with different configuration/assumption; e.g., gNB height and UE height;\n-\tVarious outdoor/indoor UE distributions, e.g., 100%/0%, 20%/80%, and others\n-\tVarious UE mobility, e.g., 3km/h, 30km/h, 60km/h and others\n-\tConfigurations (parameters and settings):\n-\tVarious UE parameters, e.g., number of UE Rx beams (including number of panels and UE antenna array dimensions)\n-\tVarious gNB settings, e.g., DL Tx beam codebook (including various Set A of beam(pairs) and gNB antenna array dimensions)\n-\tVarious Set B of beam (pairs)\n-\tT1 for measurement /T2 for prediction for BM-Case2\n-\tOther scenarios/configurations(parameters and settings) are not precluded and can be reported\nCompanies to report the selected scenarios/configurations for generalization verification. Note: other approaches for achieving good generalization performance for AI/ML-based schemes are not precluded.\nThe following cases are considered for verifying the generalization performance of an AI/ML model over various scenarios/configurations as a starting point:\n-\tCase 1: The AI/ML model is trained based on training dataset from one Scenario#A/Configuration#A, and then the AI/ML model performs inference/test on a dataset from the same Scenario#A/Configuration#A\n-\tCase 2: The AI/ML model is trained based on training dataset from one Scenario#A/Configuration#A, and then the AI/ML model performs inference/test on a different dataset than Scenario#A/Configuration#A, e.g., Scenario#B/Configuration#B, Scenario#A/Configuration#B\n-\tCase 3: The AI/ML model is trained based on training dataset constructed by mixing datasets from multiple scenarios/configurations including Scenario#A/Configuration#A and a different dataset than Scenario#A/Configuration#A, e.g., Scenario#B/Configuration#B, Scenario#A/Configuration#B, and then the AI/ML model performs inference/test on a dataset from a single Scenario/Configuration from the multiple scenarios/configurations, e.g.,  Scenario#A/Configuration#A, Scenario#B/Configuration#B, Scenario#A/Configuration#B.\n-\tNotes: Companies to report the ratio for dataset mixing. Number of the multiple scenarios/configurations can be larger than two.\n-\tThe following case for generalization verification, can be optionally considered by companies:\n-\tCase 2A: The AI/ML model is trained based on training dataset from one Scenario#A/Configuration#A, and then the AI/ML model is updated based on a fine-tuning dataset different than Scenario#A/Configuration#A, e.g., Scenario#B/Configuration#B, Scenario#A/Configuration#B. After that, the AI/ML model is tested on a different dataset than Scenario#A/Configuration#A, e.g., subject to Scenario#B/Configuration#B, Scenario#A/Configuration#B.\n-\tCompanies to report the fine-tuning dataset setting (e.g., size of dataset) and the improvement of performance.\nFurther details on evaluation assumptions\nThe following options are studied on the selection of Set B of beams (pairs):\n-\tOption 1: Set B is fixed across training and inference\n-\tOption 2: Set B is variable (e.g., different beams (pairs) patterns in each time instance/report/measurement during training and/or inference)\n-\tOpt 2A: Set B is changed following a set of pre-configured patterns\n-\tOpt 2B: Set B is randomly changed among pre-configured patterns\n-\tOpt 2C: Set B is randomly changed among Set A beams (pairs)\n-\tOpt 2D: Set B is a subset of measured beams (pairs) Set C (including Set B = Set C), e.g. Top-K beams(pairs) of Set C\n-\tThe number of beams(pairs) in Set B can be fixed or variable\n-\tCompanies report the number of pre-configured patterns used in the evaluation for Option 2: Set B is variable if applicable (e.g. Opt A and Opt B)\n-\tNote: BM-Case1 and BM-Case2 may be considered for different option.\n-\tNote: This does not preclude the alternative that Set B is different from Set A.\nFor the evaluation of Option 2: Set B is variable (e.g., different beams (pairs) patterns in each time instance/report/measurement during training and/or inference), study the following options as AI/ML model inputs:\n-\tAlt 1: Implicit information of Tx beam ID and/or Rx beam ID\n-\te.g., measurements of Set B of beams together with default values (e.g., 0) for the beams not in Set B are used as AI inputs in a certain order/ matrix/ vector. Detailed assumption can be reported.\n-\tAlt 2: Tx beam ID and/or Rx beam ID is used as inputs of AI/ML explicitly.\nFor the purpose of DL Tx beam prediction evaluations, consider the following options for Rx beam as AI/ML model input for training and/or inference if applicable:\n-\tOption 1:  Measurements of the \"best\" Rx beam with exhaustive beam sweeping for each model input sample.\n-\tCompanies expected to report how to select the \"best\" Rx beam(s).\n-\tOption 2: Measurements of specific Rx beam(s).\n-\tCompanies expected to report how to select specific Rx beam(s).\n-\tOption 3: Measurements of random Rx beam(s) per model input sample.\n-\tOption 4:  Measurements of quasi-optimal Rx beam (i.e., not all the measurements as inputs of AI/ML are from the \"best\" Rx beam) with less measurement/RS overhead compared to exhaustive Rx beam sweeping.\n-\tIdentify the quasi-optimal Rx beams to be utilized for measuring Set B/Set C based on the previous measurements. Companies can report the time information and beam type (e.g., whether the same Tx beam(s) in Set B) of the reference signal to use. Companies expected to report the measurement/RS overhead together with the beam prediction accuracy, as well as, how to find the quasi-optimal Rx beam with \"previous measurement\".\n-\tOther options are not precluded and can be reported by companies.\nPerformance with different types of labels are studied considering the following:\n-\tOption 1a: Top-1 beam(pair) in Set A\n-\tOption 1b: Top-K beam (pair)s in Set A\n-\tOption 2a: L1-RSRPs per beam of all the beams(pairs) in Set A\n-\tOption 2b: Top-K beam(pair)s in Set A and the corresponding L1-RSRPs\n-\tOption 2c: Top-1 beam(pair) in Set A and the corresponding L1-RSRP\nEvaluation assumptions:\nTable 6.3.1-1 presents the baseline system level simulation assumptions for AI/ML in beam management evaluations.\nTable 6.3.1-1: Baseline System Level Simulation assumptions for AI/ML in beam management evaluations\n\nFor temporal beam prediction, the following options are considered as a starting point for UE trajectory model. Companies report further changes or modifications from those. Other options are not precluded. UE orientation can be independently modelled from UE moving trajectory. Other UE orientation model is not precluded:\n-\tOption 1: Linear trajectory model with random direction change.\n-\tUE moving trajectory: UE will move straight along the selected direction to the end of an time interval, where the length of the time interval is provided by using an exponential distribution with average interval length, e.g., 5s, with granularity of 100 ms.\n-\tUE moving direction change: At the end of the time interval, UE will change the moving direction with the angle difference A_diff from the beginning of the time interval, provided by using a uniform distribution within [-45Â°, 45Â°].\n-\tUE moves straight within the time interval with the fixed speed.\n-\tOption 2: Linear trajectory model with random and smooth direction change.\n-\tUE moving trajectory: UE will change the moving direction by multiple steps within an time internal, where the length of the time interval is provided by using an exponential distribution with average interval length, e.g., 5s, with granularity of 100 ms.\n-\tUE moving direction change: At the end of the time interval, UE will change the moving direction with the angle difference A_diff from the beginning of the time interval, provided by using a uniform distribution within [-45Â°, 45Â°].\n-\tThe time interval is further broken into N sub-intervals, e.g. 100ms per sub-interval, and at the end of each sub-interval, UE change the direction by the angle of A_diff/N.\n-\tUE moves straight within the time sub-interval with the fixed speed.\n-\tOption 3: Random direction straight-line trajectories.\n-\tInitial UE location, moving direction and speed: UE is randomly dropped in a cell, and an initial moving direction is randomly selected, with a fixed speed.\n-\tThe initial UE location should be randomly drop within the following blue area:\n\n\nwhere d1 is the minimum distance that UE should be away from the BS.\n-\tEach sector is a cell and that the cell association is geometry based.\n-\tDuring the simulation, inter-cell handover or switching should be disabled.\nFor training data generation:\n-\tFor each UE moving trajectory: the total length of the UE trajectory can be set as T seconds if it is in time, or set as D meter if it is in distance.\n-\tThe trajectory sampling interval granularity depends on UE speed.\n-\tUE can move straight along the entire trajectory, or\n-\tUE can move straight during the time interval, where the time interval is provided by using an exponential distribution with average interval length ÎT\n-\tUE may change the moving direction at the end of the time interval. UE will change the moving direction with the angle difference A_diff from the beginning of the time interval, provided by using a uniform distribution within [-45Â°, 45Â°]\n-\tIf the UE trajectory hits the cell boundary (the red line), the trajectory should be terminated.\n-\tIf the trajectory length (in time) is less than the length of observation window + prediction window, the trajectory should be discarded.\n-\tThe length of observation window + prediction window is not fixed and companies can report their values.\nFor AI/ML in beam management evaluation, RAN1 does not attempt to define any common AI/ML model as a baseline.\nTable 6.3.1-2 presents the baseline link level simulation assumptions for AI/ML in beam management evaluations.\nTable 6.3.1-2: Baseline Link Level Simulation assumptions for AI/ML in beam management evaluations\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.3.1-1: Baseline System Level Simulation assumptions for AI/ML in beam management evaluations",
                                    "table number": 8,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 6.3.1-2: Baseline Link Level Simulation assumptions for AI/ML in beam management evaluations",
                                    "table number": 9,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "6.3.2\tPerformance results",
                            "text_content": "BM_Table 1 through BM_Table 5 in attached Spreadsheets for Beam Management evaluations present the performance results for:\n-\tBM_Table 1: Evaluation results for BMCase-1 without generalization\n-\tBM_Table 2: Evaluation results for BMCase-2 without generalization\n-\tBM_Table 3: Evaluation results for BMCase-1 with generalization for DL Tx beam prediction\n-\tBM_Table 4. Evaluation results for BMCase-1 with generalization for beam pair prediction\n-\tBM_Table 5. Evaluation results for BMCase-2 with generalization for DL Tx beam and beam pair prediction\nIn the evaluation, SLS are used for data generation for training/inference unless otherwise stated.\nIn the following performance results, Top-K/1(%) is used for Top-K DL Tx beam prediction accuracy or Top-K beam pair prediction accuracy.\nFigure 6.3.2-1 and Table 6.3.2-1 illustrate model parameter (M) and computational complexity in FLOPs (M) for BM-Case 1 and BM-Case 2, Tx beam prediction and beam pair prediction respectively, according to the reported assumption in BM_Table 1 and BM_Table 2.\nNote: Optimization of AI/ML model (e.g., in terms of model/computational complexity) was not discussed in the study.\n\nFigure 6.3.2-1: Complexity of AI/ML models from evaluation results \nin terms of FLOPs and number of parameters for BM cases\n\nTable 6.3.2-1: AI/ML model complexity/computation complexity \nused in the evaluations for AI/ML in beam management\n\nIn the following performance results, Top-K/1(%) is used for Top-K DL Tx beam prediction accuracy or Top-K beam pair prediction accuracy.\nBM-Case1: Spatial-domain Downlink beam prediction for Set A of beams based on measurement results of Set B of beams\nNote that in the following evaluations, ideal measurements are assumed\n-\tBeams could be measured regardless of their SNR.\n-\tNo measurement error.\n-\tMeasured in a single-time instance (within a channel-coherence time interval).\n-\tNo quantization for the L1-RSRP measurements.\n-\tNo constraint on UCI payload overhead for full report of the L1-RSRP measurements of Set B for NW-side models are assumed.\nFor BM-Case1 DL Tx beam prediction, when Set B is a subset of Set A, AI/ML can provide good beam prediction performance with less measurement/RS overhead comparing to using all measurements of Set A (which provides 100% beam prediction performance as non-AI baseline Option 1) without considering generalization aspects with the measurements from the best Rx beam without UE rotation.\n-\t(A)With measurements ofÂ fixed Set BÂ of beams that ofÂ 1/4Â of Set A of beams\n-\tTop-1 DL Tx beam prediction accuracy:\n-\tevaluation results from 9 sources indicate that, AI/ML can achieve about 70%~80% beam prediction accuracy\n-\tevaluation results from 9 sources indicate that, AI/ML can achieve about 80%~90% beam prediction accuracy\n-\tevaluation results from 7 sources indicate that, AI/ML can achieve more than 90% beam prediction accuracy\n-\tevaluation results from 1 source indicates that AI/ML can achieve about 60% beam prediction accuracy when the DL Tx beam grid is generated with oversampling\n-\tNote: 1 source reported that, AI/ML can achieve more than 90% beam prediction accuracy for 100% outdoor UE, and AI/ML can achieve less than 80% beam prediction accuracy for 80% indoor and 20% outdoor. All other results are with the assumption of 80% indoor and 20% outdoor.\n-\tNote: 1 source reported that, AI/ML can achieve 97.3% beam prediction accuracy with the measurements from the best Rx beam based on the best Tx beam in Set A, and AI/ML can achieve 76.4% beam prediction accuracy with the measurements from the best Rx beam of on the best Tx beam in Set B, and 1 source reported that using the best Rx beam in Set A and Set B have similar performance, i.e., 84.84% and 84.59% respectively.\n-\tNon-AI baseline Option 2 (exhaustive beam sweeping in Set B of beams) can achieve about 25% beam prediction accuracy.\n-\tTop-1 DL Tx beam with 1dB margin:\n-\tevaluation results from 15 sources indicate that, AI/ML can achieve more than or about 90% beam prediction accuracy.\n-\tevaluation results from 3 sources indicate that, AI/ML can achieve about 80% beam prediction accuracy, wherein 1 source assumed the L1-RSRP of the Top-1 predicted beam is measured with the best Rx beam searched from the best Tx beam in set B.\n-\tTop-K(=2) DL Tx beam prediction accuracy\n-\tevaluation results from 7 sources indicate that, AI/ML can achieve 80%- 90% beam prediction accuracy.\n-\tevaluation results from 14 sources indicate that, AI/ML can achieve more than 90% beam prediction accuracy.\n-\tThe beam prediction accuracy increases with K.\n-\tevaluation results from indicate that Top-2 DL beam prediction accuracy can be more than 95%\n-\tevaluation results from 2 sources indicate that Top-3 DL beam prediction accuracy can be more than 95%\n-\tevaluation results from 3 sources indicate that Top-4 DL beam prediction accuracy can be more than 95%\n-\tevaluation results from 4 sources indicate that Top-5 DL beam prediction accuracy can be more than 95%\n-\tAverage L1-RSRP difference of Top-1 predicted beam\n-\tevaluation results from 17 sources indicate that it can be below or about 1dB\n-\tevaluation results from 2 sources indicate that it can be 2.6~2.7dB with the assumption that the L1-RSRP of the Top-1 predicted beam is measured with the best Rx beam searched from the best Tx beam in set B\n-\tAverage predicted L1-RSRP difference of Top-1 beam\n-\tevaluation results from 5 sources indicate that it can be below or about 1dB\n-\tevaluation results from 1 source indicates that it is about 2dB\n-\tNote that this is assumed that all the L1-RSRPs of Set A of beams are used as the label in AI/ML training phase (e.g., regression AI/ML model)\n-\tUE average throughput\n-\tevaluation results from 3 sources indicate that AI/ML achieves 96%~99% of the UE average throughput of the BM-Case1 baseline option 1 (exhaustive search over Set A beams).\n-\tevaluation results from 1 source indicates that non-AI baseline option 2 (exhaustive search over Set B beams) achieves 89% of the UE average throughput of the BM-Case1 baseline option 1 (exhaustive search over Set A beams).\n-\tUE 5%ile throughput\n-\tevaluation results from 2 sources indicate that, AI/ML achieves 95~97% of the UE 5%ile throughput of the BM-Case1 baseline option 1 (exhaustive search over Set A beams).\n-\t(B) With measurements ofÂ fixed Set BÂ of beams that ofÂ 1/8Â of Set A of beams\n-\tTop-1 DL Tx beam prediction accuracy:\n-\tevaluation results from 7 sources indicate that, AI/ML can achieve about 50% beam prediction accuracy\n-\tevaluation results from 4 sources indicate that, AI/ML can achieve about 60%~70% beam prediction accuracy\n-\tevaluation results from 5 sources indicate that, AI/ML can achieve about 70%~80% beam prediction accuracy.\n-\tevaluation results from 4 sources indicate that, AI/ML can achieve more than 80% beam prediction accuracy\n-\tNote: 1 source reported that, AI/ML can achieve 89% beam prediction accuracy with the measurements from the best Rx beam based on the best Tx beam in Set A, and AI/ML can achieve 67.6% beam prediction accuracy with the measurements from the best Rx beam of on the best Tx beam in Set B.\n-\tNon-AI baseline Option 2 (exhaustive beam sweeping in Set B of beams) can achieve about 12.5% beam prediction accuracy\n-\tTop-1 DL Tx beam prediction with 1dB margin\n-\tevaluation results from 7 sources indicate that, AI/ML can achieve 70%-80% beam prediction accuracy\n-\twherein 1 source assumed the L1-RSRP of the Top-1 predicted beam is measured with the best Rx beam searched from the best Tx beam in set B.\n-\tevaluation results from 1 source indicate that, AI/ML can achieve 80%-90% beam prediction accuracy\n-\tevaluation results from 5 sources indicate that, AI/ML can achieve more than 90% beam prediction accuracy\n-\tTop-K(=2) DL Tx beam prediction accuracy\n-\tevaluation results from 6 sources indicate that, AI/ML can achieve about 70%~ 80% beam prediction accuracy\n-\tevaluation results from 5 sources indicate that, AI/ML can achieve 80%~90% beam prediction accuracy\n-\tevaluation results from 4 sources indicate that, AI/ML can achieve 90% beam prediction accuracy for Top-2 DL Tx beam.\n-\tThe beam prediction accuracy increases with K.\n-\tevaluation results from 3 sources indicate that Top-3 DL beam prediction accuracy can be more than 95%\n-\tevaluation results from 4 sources indicate that Top-5 DL beam prediction accuracy can be more than 90%\n-\tAverage L1-RSRP difference of Top-1 predicted beam\n-\tevaluation results from 8 sources indicate that it can be below or about 1dB\n-\tevaluation results from 4 sources indicate that it can be 1dB~2dB\n-\tevaluation results from 1 source indicates that it can be 3.4dB with the assumption that the L1-RSRP of the Top-1 predicted beam is measured with the best Rx beam searched from the best Tx beam in set B\n-\tAverage predicted L1-RSRP difference of Top-1 beam\n-\tevaluation results from 5 sources indicates that it can be 0.8~1.5dB\n-\tNote that 4 sources assumed that all the L1-RSRPs of Set A of beams are used as the label in AI/ML training phase (e.g., regression AI/ML model) and 1 source assumed that only the L1-RSRP of the Top-1 beam in Set A is used as the label in training phase and the result is 0.82 dB.\n-\tUE average throughput\n-\tevaluation results from 1 source indicates that AI/ML achieves 98% of the UE average throughput of the BMCase1 baseline option 1 (exhaustive search over Set A beams).\n-\tevaluation results from 1 source indicates that AI/ML achieves 85% of the UE average throughput of the BMCase1 baseline option 1 (exhaustive search over Set A beams).\n-\tUE 5%ile throughput\n-\tevaluation results from 1 source indicates that, AI/ML achieves 84% of the UE 5%ile throughput of the BMCase1 baseline option (exhaustive search over Set A beams).\n-\tevaluation results from 1 source indicates that, AI/ML achieves 70% of the UE 5%ile throughput of the BMCase1 baseline option (exhaustive search over Set A beams).\nFor BM-Case1 DL Tx beam prediction, when Set B is different than Set A, with measurements of Set B of wide beams that are 1/4 or 1/6 or 1/8 of Set A beams, AI/ML can provide good beam prediction performance with less measurement/RS overhead comparing to using all measurements of Set A (which provides 100% beam prediction performance as non-AI baseline Option 1) without considering generalization aspects with the measurements from the best Rx beam without UE rotation.\n-\tTop-1 DL Tx beam\n-\tevaluation results from 3 sources indicate that, AI/ML can achieve more than 80% beam prediction accuracy from 5 sources indicate that, AI/ML can achieve more than 55% beam prediction accuracy\n-\t2 sources reported more than 80% beam prediction accuracy with 100% outdoor UEs, and more than 60% beam prediction accuracy with 20% outdoor UEs.\n-\tEvaluation results from 1 source shows that, with limited measurements (e.g., 1 or 4) of narrow beams in Set A=32, AI/ML can increase 15% or 30% beam prediction accuracy [respectively] compared with 55% beam prediction accuracy with measurement of wide beams only.\n-\tTop-1 DL Tx beam with 1dB margin\n-\tevaluation results from 4 sources indicate that, AI/ML can achieve more than 85% beam prediction accuracy\n-\tevaluation results from 3 sources indicate that, AI/ML can achieve 57%~77% beam prediction accuracy\n-\tOne source reported more than 86% beam prediction accuracy with 100% outdoor UEs, and more than 70% beam prediction accuracy with 20% outdoor UEs.\n-\tTop-K(=3) DL Tx beam\n-\tevaluation results from 3 sources indicate that, AI/ML can achieve more than 95% beam prediction accuracy\n-\tevaluation results from 3 sources indicate that, AI/ML can achieve 85~94% beam prediction accuracy\n-\tevaluation results from 1 source indicates that Top-5 DL beam prediction accuracy can be more than 90%.\n-\tAverage L1-RSRP difference of Top-1 predicted beam\n-\tevaluation results from 4 sources indicate that, the average L1-RSRP difference can be less or about 1dB\n-\tUE average throughput\n-\tevaluation results from 1 source indicates that, AI/ML achieves 99% of the UE average throughput of the BMCase1 baseline option 1 (exhaustive search over Set A beams)\n-\tUE 5%ile throughput\n-\tevaluation results from 1 source indicates that, AI/ML achieves 94% of the of the BMCase1 baseline option 1(exhaustive search over Set A beams)\nFor BM-Case1 DL Tx-Rx beam pair prediction, when Set B is a subset of Set A, AI/ML can provide good beam prediction performance with less measurement/RS overhead comparing to using all measurements of Set A (which provides 100% beam prediction performance as non-AI baseline Option 1) without considering generalization aspects and without UE rotation.\n-\t(A) With measurements of fixed Set B of beam pairs that of 1/4 of Set A of beam pairs\n-\tTop-1 beam pair prediction accuracy:\n-\tevaluation results from 8 sources indicate that, AI/ML can achieve about 50%~70% prediction accuracy\n-\tevaluation results from 4 source indicate that, AI/ML can achieve 70%~80% prediction accuracy\n-\tevaluation results from 5 sources indicate that, AI/ML can achieve about 80%~90% prediction accuracy\n-\tevaluation results from 1 source indicates that, AI/ML can achieve more than 90% prediction accuracy\n-\tNote: in the above evaluation and the rest of other KPIs, most of the sources used measurements from all Rx beams of a certain set of Tx beams, except 3 sources who use measurements from half of Rx beams of a certain set of Tx beams.\n-\tThe results from 3 sources indicate 60%~68% prediction accuracy in terms of Top-1 beam pair prediction accuracy.\n-\t1 source additionally reports that, AI/ML can achieve 76.46% and 56.12% beam prediction accuracy with the measurements from all Rx beams and half of Rx beams of a certain set of Tx beams respectively.\n-\tNon-AI baseline Option 2 (exhaustive beam sweeping in Set B of beam pairs) can achieve about 25% prediction accuracy.\n-\tTop-1 beam pair prediction accuracy with 1dB margin:\n-\tevaluation results from 5 sources indicate that, AI/ML can achieve more than 70% prediction accuracy\n-\tevaluation results from 2 sources indicate that, AI/ML can achieve 80%~ about 90% prediction accuracy\n-\tevaluation results from 6 sources indicate that, AI/ML can achieve more than 90% prediction accuracy.\n-\tNote: 1 source reported that, AI/ML can achieve 91.6% and 74.57% beam prediction accuracy with 1dB margin with the measurements from all Rx beams of a certain set of Tx beams and with half of Rx beams of a certain set of Tx beams respectively.\n-\tTop-K(=2) beam pair prediction accuracy\n-\tevaluation results from 2 sources indicate that, AI/ML can achieve 65%- 75% prediction accuracy.\n-\tevaluation results from 6 sources indicate that, AI/ML can achieve 80%- 90% prediction accuracy\n-\tevaluation results from 4 sources indicate that, AI/ML can achieve more than 90% prediction accuracy\n-\tNote: 1 source reported that, AI/ML can achieve 91.34% and 78.06% Top-K(=2) beam prediction accuracy with the measurements from all Rx beams and half of Rx beams of a certain set of Tx beams respectively.\n-\tThe beam prediction accuracy increases with K.\n-\tevaluation results from 1 source indicate that Top-3 beam pair prediction accuracy can be more than 95%\n-\tevaluation results from 4 sources indicate that Top-4 beam pair prediction accuracy can be [more than 95%\n-\tevaluation results from 2 sources indicate that Top-5 beam pair prediction accuracy can be more than 95%\n-\tevaluation results from 1 source indicate that Top-10 beam pair prediction accuracy can be more than 95% for 32 Tx and 4 Rx with results from half Rx\n-\tAverage L1-RSRP difference of Top-1 predicted beam pair\n-\tevaluation results from 13 sources indicate that it can be below or about 1dB\n-\tevaluation results from 1 source indicate that it can be about 1.5dB\n-\tNote: 1 source reported that it can be 0.716dB and 1.611dB with the measurements from all Rx beams and half of Rx beams of a certain set of Tx beams respectively.\n-\tPredicted L1-RSRP difference of Top-1 beam pair\n-\t3 sources indicate that it can be below or about 1dB\n-\tNote that this is assumed that all the L1-RSRPs of Set A of beams are used as the label in AI/ML training phase (e.g., regression AI/ML model)\n-\t(B) With measurements of fixed Set B of beam pairs that of 1/8 of Set A of beam pairs\n-\tTop-1 beam pair prediction accuracy:\n-\tevaluation results from 4 sources indicate that, AI/ML can achieve about 50% prediction accuracy\n-\tevaluation results from 4 sources indicate that, AI/ML can achieve about 60%~70% prediction accuracy\n-\tevaluation results from 6 sources indicate that, AI/ML can achieve about 70%~80% prediction accuracy\n-\tNote: in the above evaluation and the rest of other KPIs, most of the sources used measurements from all Rx beams of a certain set of Tx beams, except 7 sources who use measurements from half of Rx beams of a certain set of Tx beams.\n-\tNon-AI baseline Option 2 (exhaustive beam sweeping in Set B of beam pairs) can achieve about 12.5% prediction accuracy\n-\tTop-1 beam pair prediction with 1dB margin\n-\tevaluation results from 4 sources indicate that, AI/ML can achieve 60%-70% prediction accuracy\n-\tevaluation results from 1 source indicate that, AI/ML can achieve 70%-80% prediction accuracy\n-\tevaluation results from 4 sources indicate that, AI/ML can achieve 80%-90% prediction accuracy\n-\tTop-K(=2) beam pair prediction accuracy\n-\tevaluation results from 4 sources indicate that, AI/ML can achieve about 70%- 80% prediction accuracy.\n-\tevaluation results from 6 sources indicate that, AI/ML can achieve 80%- 90% prediction accuracy\n-\tevaluation results from 2 sources indicate that, AI/ML can achieve more than 90% prediction accuracy\n-\tThe beam prediction accuracy increases with K.\n-\tevaluation results from 1 source indicate that Top-3 beam pair prediction accuracy can be 96%\n-\tevaluation results from 1 source indicate that Top-4 beam pair prediction accuracy can be 96%\n-\tevaluation results from 1 source indicate that Top-5 beam pair prediction accuracy can be 91%\n-\tevaluation results from 1 source indicate that Top-5 beam pair prediction accuracy can be 94%\n-\tAverage L1-RSRP difference of Top-1 predicted beam pair\n-\tevaluation results from 5 sources indicate that it can be below or about 1dB\n-\tevaluation results from 5 sources indicate that it can be 1dB~2dB\n-\tAverage predicted L1-RSRP difference of Top-1 beam pair\n-\tevaluation results from 2 sources indicate that it can be 0.7~1.3dB\n-\tNote that this is assumed that all the L1-RSRPs of Set A of beams are used as the label in AI/ML training phase (e.g., regression AI/ML model).\n-\t(C) With measurements of fixed Set B of beams that of 1/16 of Set A of beams\n-\tTop-1 beam pair prediction accuracy\n-\tevaluation results from 5 sources indicate that, AI/ML can achieve less than 50% or about 50% prediction accuracy\n-\tevaluation results from 2 source indicate that, AI/ML can achieve about 55%~57% prediction accuracy\n-\tevaluation results from 3 sources indicate that, AI/ML can achieve about 60%~70% prediction accuracy\n-\tevaluation results from 1 source indicate that, AI/ML can achieve about 70%~80% prediction accuracy\n-\tNote: in the above evaluation and the rest of other KPIs, some 6 sources used measurements from all Rx beams of a certain set of Tx beams, and some other 6 sources use measurements from half or fourth of Rx beams of a certain set of Tx beams.\n-\tNon-AI baseline Option 2 (exhaustive beam sweeping in Set B of beam pairs) can achieve about 6.25% prediction accuracy\n-\tTop-1 beam pair prediction with 1dB margin\n-\tevaluation results from 4 sources indicate that, AI/ML can achieve less than 50% or about 50% prediction accuracy\n-\tevaluation results from 1 source indicate that, AI/ML can achieve more than 50%~60% prediction accuracy\n-\tevaluation results from 3 sources indicate that, AI/ML can achieve about 60%-70% prediction accuracy\n-\tevaluation results from 2 sources indicate that, AI/ML can achieve 72%~85% prediction accuracy\n-\tTop-K(=2) beam pair prediction accuracy\n-\tevaluation results from 3 sources indicate that, AI/ML can achieve less than 60% prediction accuracy.\n-\tevaluation results from 5 sources indicate that, AI/ML can achieve about 70%- 80% prediction accuracy\n-\tevaluation results from 1 source indicate that, AI/ML can achieve more than 85% prediction accuracy\n-\tThe beam prediction accuracy increases with K.\n-\tAverage L1-RSRP difference of Top-1 predicted beam pair\n-\tevaluation results from 3 sources indicate that it can be 1dB~2dB\n-\tevaluation results from 2 sources indicate that it can be 2dB~3dB\n-\tevaluation results from 2 sources indicate that it can be more than 3dB\n-\tevaluation results from 1 source indicate that it can be about 6dB\n-\tPredicted L1-RSRP difference of Top-1 beam pair\n-\tevaluation results from 2 sources indicates that it can be about 2.5dB\n-\tNote that this is assumed that all the L1-RSRPs of Set A of beams are used as the label in AI/ML training phase (e.g., regression AI/ML model).\n-\tNote: in the above evaluations, 8 sources assumed 4 Rx, other sources assumed 8 Rx.\nFor BM-Case1 beam pair prediction, when Set B is different to Set A, with measurements of Set B of Tx wide beams that are 1/4 or 1/8 of Set A beams, evaluation results from 1 source indicate that AI/ML can provide good beam prediction performance with less measurement/RS overhead compared to using all measurements of Set A (which provides 100% beam prediction performance as non-AI baseline Option 1) without considering generalization and without UE rotation.\n-\tFor Top-1 beam pair prediction accuracy, evaluation results from 1 source indicate that, AI/ML can achieve about 92.7%/92.5% beam prediction accuracy for 1/4 and 1/8 overhead respectively.\n-\tFor Top-1 beam prediction accuracy with 1dB margin, evaluation results from 1 source indicate that, AI/ML can achieve about 97.6%/97.3% beam prediction accuracy for 1/4 and 1/8 overhead respectively.\nBM-Case2: Temporal Downlink beam prediction for Set A of beams based on the historic measurement results of Set B of beams.\nFor BM-Case2, when Set B = Set A, for DL Tx beam prediction with the measurements from the best Rx beam or Tx-Rx beam pair prediction, without considering generalization aspects, with the following assumptions:\n-\tUE speed: 30km/h (unless otherwise stated)\n-\tPrediction time: 80ms/160ms/320ms/640ms/800ms/others\n-\tWith UE rotation and without UE rotation\n-\tSet B is the same as Set A in each time instance for measurement\nNote that ideal measurements are assumed.\n-\tBeams could be measured regardless of their SNR.\n-\tNo measurement error.\n-\tNo quantization for the L1-RSRP measurements.\n-\tNo constraint on UCI payload overhead for full report of the L1-RSRP measurements of Set B for NW-side models are assumed.\n(A) For Tx DL beam prediction, based on most of the evaluation results, AI/ML provides some beam prediction accuracy gain for prediction time larger than or equal to 160ms, and some evaluation results show AI/ML may have similar performance or some degradation for 80ms or 160ms prediction time comparing with non-AI baseline (Option 2, sample and hold based on the previous measurements) with same RS/measurement overhead without UE rotation. For the longer the prediction time, the higher gain of beam prediction accuracy can be achieved by AI/ML:\n-\tFor 80ms prediction time, evaluation results from 1 source show that AI/ML may have similar performance or may decrease about 4% beam prediction accuracy, evaluation results from 2 sources show that AI/ML may have similar performance or may decrease 0.4%~1% beam prediction accuracy, evaluation results from 1 source show that AI/ML can increase about 1%~2% prediction accuracy in terms of Top-1 beam prediction accuracy,\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 40ms. And it can decrease 4% beam prediction accuracy comparing with 98.23% achieved by non-AI baseline (Option 2-2) with 32 Tx beams\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 80ms/160ms. And it may decrease up to 0.4~1% beam prediction accuracy comparing with about 80%/78.7% achieved by non-AI baseline (Option 2) with 32 Tx beams.\n-\twherein, 1 source used measurements from 8 time instances with measurement periodicity of 40ms. And it can decrease about 0.5% beam prediction accuracy comparing with 67.4% achieved by non-AI baseline (Option 2) with 64 Tx beams\n-\twherein, 1 source used measurements from 5 time instances with measurement periodicity of 80ms. And it can increase 1% beam prediction accuracy gain comparing with 78.5% and 76.2% achieved by non-AI baseline (Option 2) with 32 Tx beams for 30km/h and 60km/h respectively.\n-\tFor 160ms prediction time, evaluation results from 3 sources show that AI/ML may have similar performance or may decrease 1%~5% beam prediction accuracy in terms of Top-1 beam prediction accuracy, evaluation results from 3 sources show that AI/ML can increase 1%~2% prediction accuracy, evaluation results from 3 sources show that AI/ML can increase 4%~5% prediction accuracy and evaluation results from 2 sources show that AI/ML can increase about 10% prediction accuracy in terms of Top-1 beam prediction accuracy.\n-\twherein, 1 source used measurements from 3 time instances with measurement periodicity of 80ms. And AI/ML does not provide beam prediction accuracy gain comparing with 83.9% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 40ms. And it can decrease 5% beam prediction accuracy comparing with 97.18% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 80ms/160ms/240ms/320ms. And it may decrease up to 2% beam prediction accuracy comparing with about 73.8%~80.9%% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 6 time instances with measurement periodicity of 40ms. And it can increase 4% beam prediction accuracy comparing with achieved 64.4% by non-AI baseline (Option 2) with 60km/h UE speed and 32 Tx beams\n-\twherein, 1 source used measurements from 2 time instances with measurement periodicity of 160ms. And it can increase 4% beam prediction accuracy comparing with 52% achieved by non-AI baseline (Option 2) with 64 Tx beams\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 160ms. And it can increase 5% beam prediction accuracy comparing with 61.2% achieved by non-AI baseline (baseline 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 2 time instances with measurement periodicity of 80ms. And it can increase 1.9% beam prediction accuracy comparing with 93.2% achieved by non-AI baseline (baseline 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 5 time instances with measurement periodicity of 160ms. And it can increase 10.8% beam prediction accuracy comparing with achieved 82.2% by non-AI baseline (Option 2) with 30km/h UE speed and 32 Tx beams\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 40ms. And it can increase 1% beam prediction accuracy comparing with 85.8% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 8 time instances with measurement periodicity of 40ms. And it can increase about 2% beam prediction accuracy comparing with 67.4% achieved by non-AI baseline (Option 2) with 64 Tx beams\n-\twherein, 1 source used measurements from 3 time instances with measurement periodicity of 160ms. And it can increase about 9.2% and about 4.6% beam prediction accuracy comparing with 51.36% and 45.76% achieved by non-AI baseline (Option 2) with 30km/h and 60km/h UE speed respectively with 64 Tx beams\n-\tFor 320ms prediction time, evaluation results from 7 sources show that AI/ML can increase about up to 3%~8% prediction accuracy, and evaluation results from 2 sources show that AI/ML can increase about 18.5%~23.5% prediction accuracy in terms of Top-1 beam prediction accuracy\n-\twherein, 1 source used measurements from 2 time instances with measurement periodicity of 160ms. And it can increase 6% beam prediction accuracy comparing with 39.7% achieved by non-AI baseline (Option 2) with 64 Tx beams.\n-\twherein, 1 source used measurements from 6 time instances with measurement periodicity of 80ms. And it can increase 8% beam prediction accuracy comparing with achieved 55.5% by non-AI baseline (Option 2) with 60km/h UE speed and for 32 Tx beams\n-\twherein, 1 source used measurements from 3 time instances with measurement periodicity of 160ms. And it can increase 18.5% and 23.5% beam prediction accuracy comparing with 42.78% and 34.53% achieved by non-AI baseline (Option 2) with 30km/h and 60km/h UE speed respectively and for 64 Tx beams.\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 320ms. And it can increase 3.5% beam prediction accuracy comparing with 60.82% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 2 time instances with measurement periodicity of 80ms. And it can increase 3.2% beam prediction accuracy comparing with 90.1% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 5 time instances with measurement periodicity of 160ms. And it can increase 18.4% beam prediction accuracy comparing with 74.4% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 80ms. And it can increase 4.2% beam prediction accuracy comparing with 79.4% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 80ms/160ms/320ms/400ms /480ms/640ms. And it can increase up to 3.4% beam prediction accuracy comparing with about 69.5~78.5% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 8 time instances with measurement periodicity of 40ms. And it can increase about 3% beam prediction accuracy comparing with 29.1% achieved by non-AI baseline (Option 2) with 64 Tx beams\n-\tFor 640ms prediction time, evaluation results from 5 sources show that AI/ML can increase 4.5~8% prediction accuracy, and evaluation results from 1 source show that AI/ML can increase up to 14.3% prediction accuracy in terms of Top-1 beam prediction accuracy, and evaluation results from 1 source show that AI/ML can increase up to 28.5% prediction accuracy in terms of Top-1 beam prediction accuracy\n-\twherein, 1 source used measurements from 2 time instances with measurement periodicity of 160ms. And it can increase 8% beam prediction accuracy comparing with 35.2% achieved by non-AI baseline (Option 2) with 64 Tx beams\n-\twherein, 1 source used measurements from 6 time instances with measurement periodicity of 160ms. And it can increase 14.3% beam prediction accuracy comparing with achieved 41.8% by non-AI baseline (Option 2) with 60km/h UE speed and 32 Tx beams\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 320ms. And it can increase 4.5% beam prediction accuracy comparing with 58% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 2 time instances with measurement periodicity of 80ms. And it can increase 5.4% beam prediction accuracy comparing with 84.4% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 5 time instances with measurement periodicity of 160ms. And it can increase 28.5% beam prediction accuracy comparing with 63.9% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 160ms. And it can increase 7.8% beam prediction accuracy comparing with 67.9% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 160ms/320ms/640ms/800ms/960ms/1280ms. And it can increase up to 8.2% beam prediction accuracy comparing with about 62.7~74.3% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\tFor 800ms prediction time, in terms of Top-1 beam prediction accuracy\n-\tevaluation results from 1 source show that AI/ML can increase about 3.5% prediction accuracy comparing with 34.6% achieved by non-AI baseline (Option 2) with 64 Tx beams with measurements from 2 time instances in measurement periodicity of 160ms\n-\tevaluation results from 1 source show that AI/ML can increase about 33.7% prediction accuracy comparing with achieved 58.6% by non-AI baseline (Option 2) 32 Tx beams with measurements from 5 time instances with measurement periodicity of 160ms\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 800ms/1600ms. And it can increase up to 9.1% beam prediction accuracy comparing with about 61.5~66.5% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\tFor 960ms prediction time, in terms of Top-1 beam prediction accuracy\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 960ms/1920ms. And it can increase up to 10.6% beam prediction accuracy comparing with about 60.1~64.4% achieved by non-AI baseline (Option 2) with 32 Tx beams\n-\tFor 1280ms prediction time, in terms of Top-1 beam prediction accuracy\n-\tevaluation results from 1 source show that AI/ML can increase about 12.7% beam prediction accuracy comparing with 54.3% achieved by non-AI baseline (Option 2) with 32 Tx beams with measurements from 4 time instances with measurement periodicity of 320ms.\n-\tevaluation results from 1 source show that AI/ML can increase about 4%~13.4% beam prediction accuracy comparing with 54%~66.8% achieved by non-AI baseline (Option 2) with 32 Tx beams with measurements from 4 time instances with measurement periodicity of 320ms~2560ms.\n-\tevaluation results from 1 source show that AI/ML can increase up to 17.6% prediction accuracy for 3200ms prediction time.\n-\tevaluation results from 1 source show that AI/ML can increase up to 19.1% prediction accuracy for up to 12.8s prediction time.\n-\tBeam prediction accuracy gain in terms of Top-K prediction accuracy or Top-1 prediction accuracy with 1dB error is similar as or smaller than the beam prediction accuracy gain in terms of Top-1 prediction accuracy.\n-\tFor the prediction time no larger than 1280ms, AI/ML and non-AI baseline (Option 2) can provide similar average L1-RSRP error, which are less than 1dB.\n(B) For Tx DL beam prediction, based on the evaluation from 2 sources, AI/ML can provide some beam prediction accuracy gain comparing with non-AI baseline (Option 2, sample-and-hold) with UE rotation and the performance of AI/ML compared to baseline (Option 2, sample-and-hold) improves with the increase of measurement periodicity:\n-\tFor 160ms/800ms/1200ms/1600ms prediction time, evaluation results from 1 source show about 2%/8%/10%/13% prediction accuracy increase comparing with 74%/60%/53%/47.7% achieved by non-AI baseline (Option 2) with 32 Tx beam respectively in terms of Top-1 beam prediction accuracy, with measurements from 4 time instances in measurement periodicity of 160ms/800ms/ 1200ms/1600ms respectively.\n-\tIn the evaluation, UE rotation is modelled every 20ms with a rotation speed uniformly distributed within {0, 60} RPM, and the rotation direction is {1/4 of data with randomly to left or right in horizontal, 1/4 of data always to left, 1/4 of data always to right, 1/4 of data to left and right in turn} with random initial directly.\n-\tFor 160ms/320ms/480ms/960ms prediction time, evaluation results from 1 source show that AI/ML can increase 2%/3%/4.2%/7.3% Top-1 beam prediction accuracy compared to non-AI baseline (Option 2) with 78%/75.5%/73%/66.3% beam prediction accuracy with 12 Tx with measurement periodicity of 200ms/360ms/520ms/1000ms.\n-\tIn the evaluation, UE rotation is modelled every 40ms with constant 10 RPM rotation speed in all three rotational axes, with rotational direction chosen uniformly at random among the three axes.\n(C) For Tx DL beam prediction (without UE rotation unless otherwise stated), AI/ML can provide good beam prediction accuracy with the less measurements/RS overhead:\n-\tUnder the assumption of setting Case A, decent beam prediction accuracy can be achieved with 1/5~1/2 measurement/RS overhead reduction comparing the non-AI baseline (Option 1, with 100% prediction accuracy)\n-\tevaluation results from 1 source show that AI/ML can achieve 57% beam prediction accuracy, while non-AI baseline (Option 2) can only achieve 52% beam prediction accuracy in term of Top-1 beam prediction accuracy for 160ms prediction time,\n-\t1/3 RS/measurement overhead reduction can be obtained with measurements from 2 time instances with measurement periodicity of 160ms.\n-\tWhen prediction time increased to 320ms or larger, >50% Top-1 beam prediction accuracy is lower than 50% even with the help of AI/ML although it still can provide some gain compared with non-AI baseline (Option2).\n-\tevaluation results from 1 source show that AI/ML can achieve 60%~71% beam prediction accuracy in terms of Top-1 beam prediction accuracy for 40ms up to 240ms prediction time\n-\t3/7 RS/measurement overhead reduction can be obtained with measurements from 8 time instances with measurement periodicity of 40ms.\n-\tWhen prediction time increased to 280ms or larger, >50% Top-1 beam prediction accuracy is lower than 50% even with the help of AI/ML\n-\tevaluation results from 1 source show that AI/ML can achieve 60.5% beam prediction accuracy in terms of Top-1 beam prediction accuracy for up to 320ms prediction time\n-\t2/5 RS/measurement overhead reduction can be obtained with measurements from 3 time instances with measurement periodicity of 160ms.\n-\tevaluation results from 1 source show that AI/ML can achieve 86.8%/83.6%/75.7%/67% beam prediction accuracy in terms of Top-1 beam prediction accuracy for up to 160ms/320ms/640ms/1280ms prediction time, respectively\n-\t1/2 RS/measurement overhead reduction can be obtained with measurements from 4 time instances with measurement periodicity of 40ms/80ms/160ms/320ms, respectively.\n-\tevaluation results from 1 source show that AI/ML can achieve 92% beam prediction accuracy in terms of Top-1 beam prediction accuracy for 160ms up to 800ms prediction time\n-\t1/2 RS/measurement overhead reduction can be obtained with measurements from 5 time instances with measurement periodicity of 160ms.\n-\tevaluation results from 1 source show that AI/ML can achieve 64%~68%/56%~63%/ 47%~56% beam prediction accuracy in terms of Top-1 beam prediction accuracy for 160ms/320ms/ 640ms prediction time respectively\n-\t2/5 RS/measurement overhead reduction can be obtained with measurements from 5 time instances with measurement periodicity of 40ms/80ms/160ms respectively.\n-\tevaluation results from 1 source show that AI/ML can achieve 62%~66% beam prediction accuracy in terms of Top-1 beam prediction accuracy for 160ms to 640ms prediction time\n-\t1/5 RS/measurement overhead reduction can be obtained with measurements from 4 time instances with measurement periodicity of 160ms to 640ms.\n-\tevaluation results from 1 source show that AI/ML can achieve 58.0%~80.1% beam prediction accuracy in terms of Top-1 beam prediction accuracy for 160ms to 12800ms prediction time\n-\tup to 1/2 RS/measurement overhead reduction can be obtained with measurements from 4 time instances with measurement periodicity of 160ms to 3200ms.\n-\tUnder the assumption of setting Case B, evaluation results from 2 sources indicate that a certain beam prediction accuracy can be achieved with 1/2 ~ 7/10 measurement/RS overhead reduction comparing with non-AI schemes (Option 2)\n-\tevaluation results from 1 source show that AI/ML can provide 1/2 RS/measurement overhead reduction with UE rotation:\n-\tAI/ML can achieve ~65% beam prediction accuracy, while non-AI baseline (Option 2) can only achieve 48% beam prediction accuracy in term of Top-1 beam prediction accuracy for 1600ms prediction time/measurement periodicity\n-\tWith non-AI baseline (Option 2), similar prediction accuracy (~65% of Top-1 beam prediction accuracy) can be achieved with 800ms prediction time /measurement periodicity.\n-\tIn the evaluation, UE rotation is modelled every 20ms with a rotation speed of RPM = 60 R/M, and the rotation direction is {1/4 of data with randomly to left or right in horizontal, 1/4 of data always to left, 1/4 of data always to right, 1/4 of data to left and right in turn} with random initial directly.\n-\tevaluation results from 1 source show that AI/ML can provide 7/10 RS/measurement overhead reduction without UE rotation:\n-\tAI/ML can achieve ~64% beam prediction accuracy, while non-AI baseline (Option 2) can only achieve 46% beam prediction accuracy in term of Top-1 beam prediction accuracy for 3200ms prediction time\n-\tWith non-AI baseline (Option 2), similar prediction accuracy (~64% of Top-1 beam prediction accuracy) can be achieved with 960ms prediction time.\n-\tUnder the assumption of setting Case B+, based on the evaluation results from 2 sources, good beam prediction accuracy can be achieved by AI/ML with measurement/RS overhead reduction compared to the non-AI baseline (Option 1, with 100% prediction accuracy) for which minimal periodicity of measurement is Tper\n-\tevaluation results from 1 source with Tper = 40ms show that AI/ML can provide 80%/88.9%/92.3%/96% RS/measurement overhead reduction:\n-\tAI/ML can achieve 80%/78.5%/77.2%/73.6% beam prediction accuracy in terms of Top-1 beam prediction accuracy with 160ms/320ms/480ms/960ms prediction time 200ms/360ms/520ms/ 1000ms measurement periodicity.\n-\tIn the evaluation, UE rotation is modelled every 40ms with constant 10 RPM rotation speed in all three rotational axes, with rotational direction chosen uniformly at random among the three axes.\n-\tevaluation results from 1 source with Tper = 160ms~3200ms show that AI/ML can provide 80% RS/measurement overhead reduction:\n-\tAI/ML can achieve 50%~73% beam prediction accuracy in terms of Top-1 beam prediction accuracy with 640ms to 12800ms prediction time (4 prediction time instance) /800ms to 16000ms measurement periodicity (4 measurement time instance) without UE rotation.\n(D) For beam pair prediction, AI/ML may or may not provide beam prediction accuracy gain comparing with non-AI baseline (Option 2) for 160ms or less prediction time without UE rotation. For the longer the prediction time, the higher gain of beam prediction accuracy can be achieved by AI/ML:\n-\tFor 160ms prediction time, evaluation results from 2 sources show AI/ML can provide similar performance or increase up to 1% prediction accuracy gain, evaluation results from 1 source show AI/ML may decrease 8% prediction accuracy, and evaluation results from 1 source show AI/ML can increase 13.8% prediction accuracy, in terms of Top-1 beam prediction accuracy.\n-\tevaluation results from 1 source show that AI/ML decrease 8% prediction accuracy in terms of Top-1 beam prediction accuracy with measurements from 4 time instances with measurement periodicity of 160ms comparing with 68.1% achieved by non-AI baseline (Option 2) with 32 Tx beams and 8 Rx beams.\n-\tevaluation results from 1 source show that AI/ML can increase 0.1% beam prediction accuracy in terms of Top-1 beam prediction accuracy with measurements from 4 time instances with measurement periodicity of 40ms comparing with 81.3% achieved by non-AI baseline (Option 2) with 32 Tx beams and 4 Rx beams.\n-\tevaluation results from 1 source show that AI/ML decrease 0.1%~1% prediction accuracy in terms of Top-1 beam prediction accuracy with measurements from 4 time instances with measurement periodicity of 80ms~320ms comparing with 80.7%~83.4% achieved by non-AI baseline (Option 2) with 32 Tx beams and 8 Rx beams.\n-\tevaluation results from 1 source show that AI/ML can increase 13.8% prediction accuracy in terms of Top-1 beam prediction accuracy with measurements from 5 time instances with measurement periodicity of 160ms comparing with 78.1% achieved by non-AI baseline (Option 2) with 32 Tx beams and 8 Rx beams.\n-\tFor 320ms prediction time, evaluation results from 2 sources show that AI/ML can increase less than 3% prediction accuracy in terms of Top-1 beam prediction accuracy, and evaluation results from 1 source show that AI/ML can increase 22.5% prediction accuracy in terms of Top-1 beam prediction accuracy\n-\twherein, 1 source used measurements from 4 time instances with measurement periodicity of 80ms~640ms. With one AI/ML model to predict the beam at one or multiple time instances including 320ms, AI/ML may increase [less than 2%] beam prediction accuracy comparing with 78.8%~81.2% achieved by non-AI baseline (Option 2)\n-\tWherein, 1 source used measurements from 4 time instances with measurement periodicity of 80ms and it shows that AI/ML can increase 2.8% beam prediction accuracy in terms of Top-1 beam prediction accuracy comparing with 74.5% achieved by non-AI baseline (Option 2).\n-\tWherein, 1 source used measurements from 5 time instances with measurement periodicity of 160ms and it shows that AI/ML can increase 22.5% prediction accuracy in terms of Top-1 beam prediction accuracy comparing with 69.2% achieved by non-AI baseline (Option 2) with 32 Tx beams and 8 Rx beams.\n-\tFor 640ms prediction time, evaluation results from 2 sources show that AI/ML may be able to increase up to 7.5% prediction accuracy, and evaluation results from 1 source show that AI/ML can increase 34% prediction accuracy in terms of Top-1 beam prediction accuracy\n-\twherein, 1 source used measurements from 4 time instances\n-\tWith one AI/ML model to predict the beam at 640ms with 640/1280ms as measurement periodicity, AI/ML can increase 6%/3.5% beam prediction accuracy comparing with 74.1%/73.5% achieved by non-AI baseline (Option 2).\n-\tWith one AI/ML model to predict the beam at multiple prediction time instances (with two or more of 160ms 320ms, 480ms, 640ms) with different measurement periodicities (e.g., 160ms, 320ms, 800ms, 960ms), AI/ML can increase [0.7%~3.5%] beam prediction accuracy. From the evaluation results, the more target predicted time instances, the less performance gain can be obtained from AI/ML.\n-\tWherein, 1 source used measurements from 4 time instances with measurement periodicity of 160ms and it shows that AI/ML can increase 7.5% beam prediction accuracy in terms of Top-1 beam prediction accuracy comparing with 63.3% achieved by non-AI baseline (Option 2)\n-\tWherein, 1 source used measurements from 5 time instances with measurement periodicity of 160ms and it shows that AI/ML can increase 34% prediction accuracy in terms of Top-1 beam prediction accuracy comparing with 57.16% achieved by non-AI baseline (Option 2) with 32 Tx beams and 8 Rx beams.\n-\tFor 800ms prediction time,\n-\tevaluation results from 1 source show that AI/ML can to increase 6.7%~7.5% prediction accuracy in terms of Top-1 beam prediction accuracy\n-\twherein, measurements from 4 time instances with 800ms/1600ms as measurement periodicity were used and AI/ML can increase 6.7%/7.5% beam prediction accuracy respectively comparing with 72.9%/69.2% achieved by non-AI baseline (Option 2).\n-\tevaluation results from 1 source show that AI/ML can to increase 39.4% prediction accuracy in terms of Top-1 beam prediction accuracy\n-\twherein, measurements from 5 time instances with 160ms as measurement periodicity were used and AI/ML can increase 39.4% beam prediction accuracy comparing with 51.2% achieved by non-AI baseline (Option 2) with 32 Tx beams and 8 Rx beams.\n-\tFor 960ms prediction time,\n-\tevaluation results from 1 source show that AI/ML may increase 12.8% beam prediction accuracy in terms of Top-1 beam prediction accuracy\n-\tWherein measurements from 5 time instances with measurement periodicity of 160ms, and predictions of 95 time instances with prediction periodicity of 10ms are assumed. AI/ML has 12.8% of beam prediction accuracy improvement in terms of Top 1 beam prediction accuracy comparing with 57.5% achieved by non-AI baseline (Option 2).\n-\tevaluation results from 1 source show that AI/ML may be able to increase up to 8.5% prediction accuracy in terms of Top-1 beam prediction accuracy\n-\tmeasurements from 4 time instances with measurement periodicity of 960ms/1920ms were used respectively, with one model to predict single /multiple prediction time instances. AI/ML can increase 8.1%/8.5% beam prediction accuracy respectively comparing with 71.3%/67.7% achieved by non-AI baseline (Option 2).\n-\tFor 1200ms/1600ms/2400ms/3200ms/40000ms prediction time, evaluation results from 1 source show that AI/ML may be able to increase up to 8.8%/ up to 10.7%/ up to 10.2%/up to 11.3%/up to 20.4% prediction accuracy in terms of Top-1 beam prediction accuracy respectively\n-\tmeasurements from 4 time instances were used with 1200ms/1600ms /1200ms/1600ms/4000ms as measurement periodicity respectively\n(E)For beam pair prediction, based on the evaluation results from 3 sources, AI/ML may or may not provide beam prediction accuracy gain comparing with non-AI baseline (Option 2) with UE rotation:\n-\tFor 160ms prediction time, in terms of Top-1 beam prediction accuracy\n-\tevaluation results from 1 source show that AI/ML may decrease 10% prediction accuracy with measurements from 4 time instances with measurement periodicity of 160ms. In this case, non-AI baseline (option 2) can achieve 51.09% beam prediction accuracy.\n-\tIn the evaluation, UE rotation is modelled every 20ms with a rotation speed uniformly distributed within {0, 60} RPM, and the rotation direction is {1/4 of data with randomly to left or right in horizontal, 1/4 of data always to left, 1/4 of data always to right, 1/4 of data to left and right in turn} with random initial directly.\n-\tFor 200ms prediction time, in terms of Top-1 beam prediction accuracy with 10 RPM rotation speed in all three rotational axes, with rotational direction chosen uniformly at random among the three axes\n-\tevaluation results from 1 source show that AI/ML can increase [1%~1.6%] prediction accuracy with measurement periodicity of 240ms with different AI/ML models. In this case, non-AI baseline (option 2) can achieve 67.4% beam prediction accuracy\n-\tFor 200ms prediction time, in terms of Top-1 beam prediction accuracy with 100 RPM rotation speed in all three rotational axes, with rotational direction chosen uniformly at random among the three axes\n-\tevaluation results from 1 source show that AI/ML can increase 23%~30% prediction accuracy with measurement periodicity of 240ms with different AI/ML models. In this case, non-AI baseline (option 2) can only achieve 17% beam prediction accuracy.\n-\tFor 500ms prediction time, in terms of Top-1 beam prediction accuracy with 10 RPM rotation speed to fixed a direction\n-\tevaluation results from 1 source show that AI/ML can increase 6%/8%/11% prediction accuracy with measurements from 1/2/5 time instances in measurement periodicity of 100ms respectively\n-\tevaluation results from 1 source show that AI/ML can increase 11%/11.5%/12.5% prediction accuracy with measurements from 1/2/5 time instances in measurement periodicity of 50ms respectively\n-\tFor 800ms prediction time, in terms of Top-1 beam prediction accuracy\n-\tevaluation results from 1 source show that AI/ML may decrease 6% prediction accuracy with measurements from 4 time instances with measurement periodicity of 800ms. In this case, non-AI baseline (option 2) can achieve 30.19% prediction accuracy.\n-\tIn the evaluation, UE rotation is modelled every 20ms with a rotation speed uniformly distributed within {0, 60} RPM, and the rotation direction is {1/4 of data with randomly to left or right in horizontal, 1/4 of data always to left, 1/4 of data always to right, 1/4 of data to left and right in turn} with random initial directly.\n(F) For beam pair prediction, (without UE rotation unless otherwise stated), AI/ML can provide good beam prediction accuracy with the less measurements/RS overhead:\n-\tUnder assumption of setting Case A, decent beam prediction accuracy can be achieved with up to 1/2 measurement/RS overhead comparing with no time domain prediction.\n-\tevaluation results from 1 source show that AI/ML can achieve 81.4%/77.3%/70.8%/61.8% beam prediction accuracy in terms of Top-1 beam prediction accuracy for up to 160ms/320ms/640ms/1280ms prediction time, respectively\n-\t1/2 RS/measurement overhead reduction can be obtained with measurements from 4 time instances with measurement periodicity of 40ms/80ms/160ms/320ms.\n-\tevaluation results from 1 source show that AI/ML can achieve 90%-92% beam prediction accuracy in terms of Top-1 beam prediction accuracy for 160ms up to 800ms prediction time\n-\t1/2 RS/measurement overhead reduction can be obtained with measurements from 5 time instances with measurement periodicity of 160ms.\n-\tevaluation results from 1 source show that AI/ML can achieve 79%~84% beam prediction accuracy in terms of Top-1 beam prediction accuracy for 80ms to 640ms prediction time without UE rotation for beam pair\n-\tup to 1/2 RS/measurement overhead reduction can be obtained with measurements from 4 time instances with measurement periodicity of 80ms or 160ms.\n-\tevaluation results from 1 source show that AI/ML can achieve 71.9% /67.4%/64.4% for 30km/h /60km/h /90km/h beam prediction accuracy respectively in terms of Top-1 beam prediction accuracy for 800ms prediction time.\n-\t1/2 RS/measurement overhead reduction can be obtained with measurements from 5 time instances with measurement periodicity of 160ms.\n-\tUnder assumption of setting Case B, based on the evaluation from 2 sources a certain beam prediction accuracy can be achieved with 1/2 or 3/5 measurement/RS overhead reduction comparing with non-AI schemes with 30km/h respectively\n-\tevaluation results from 1 source show that AI/ML can provide 1/2 or 2/3 or 3/4 RS/measurement overhead reduction without UE rotation for 30km/h /60km/h /90km/h respectively\n-\tAI/ML can achieve 70.3%/77.1%/79.8% beam prediction accuracy with 30km/h /60km/h /90km/h respectively, while non-AI baseline (Option 2) can only achieve 57.2%/36%/36% beam prediction accuracy in term of Top-1 beam prediction accuracy for 960ms/960ms/640ms prediction time/measurement periodicity for 30km/h /60km/h /90km/h respectively.\n-\tWith non-AI baseline (Option 2), similar prediction accuracy (76.7% of Top-1 beam prediction accuracy) can be achieved with 480ms/320ms/160ms measurement periodicity for 30km/h /60km/h /90km/h respectively.\n-\tevaluation results from 1 source show that AI/ML can provide 3/5 RS/measurement overhead reduction without UE rotation\n-\tAI/ML can achieve 77.6% beam prediction accuracy, while non-AI baseline (Option 2) can only achieve 66.9% beam prediction accuracy in term of Top-1 beam prediction accuracy for 1600ms prediction time.\n-\tWith non-AI baseline (Option 2), similar prediction accuracy (74.1% of Top-1 beam prediction accuracy) can be achieved with 640ms prediction time.\n-\tUnder the assumption of setting Case B+, based on the evaluation from 1 source decent beam prediction accuracy can be achieved with 80% measurement/RS overhead comparing the non-AI baseline (Option 1, with 100% prediction accuracy) with Tper =160ms to 960ms as minimal periodicity of measurement\n-\tevaluation results from 1 source show that AI/ML can provide 80% RS/measurement overhead reduction:\n-\tAI/ML can achieve 68%~77% beam prediction accuracy in terms of Top-1 beam prediction accuracy with 640ms to 3840ms prediction time (4 prediction time instance) /800ms to 4800ms measurement periodicity (4 measurement time instance) without UE rotation.\nFor BM-Case2, when Set B patten is a subset of Set A in each time instance, for DL Tx beam prediction with the measurements from the best Rx beam or Tx-Rx beam pair prediction, without considering generalization aspects, with the following assumptions:\n-\tUE speed: 30km/h (unless otherwise stated)\n-\tPrediction time: 40ms/80ms/160ms/320ms/640ms/others\n-\tWith and without UE rotation\n-\tFixed Set B patterns or preconfigured Set B pattens in each measurement instances (unless otherwise stated)\nNote that ideal measurements are assumed:\n-\tBeams could be measured regardless of their SNR.\n-\tNo measurement error.\n-\tNo quantization for the L1-RSRP measurements.\n-\tNo constraint on UCI payload overhead for full report of the L1-RSRP measurements of Set B for NW-side models are assumed.\nNote: In some evaluations results, non-AI baseline (Option 2) may have better performance in terms of Top-1 beam prediction accuracy than the ratio of Set B/Set A. This is because the Top-1 beam distribution among Set A of beams are not uniform while the Set B pattern may be well designed or happen to be the beams that have high probability to be the Top-1 beam.\nNote: non-AI baseline Option 2: sample and hold based on the measurements in the last time instance (unless otherwise stated)\n(A) For Tx DL beam prediction without UE rotation, AI/ML can provide good beam prediction accuracy and gain comparing with non-AI baseline (Option 2) with same RS/measurement overhead:\n-\tWith measurements ofÂ fixed Set BÂ or variable Set B with pre-configured patterns of beams that ofÂ 1/2Â of Set A of beams in one time instance,\n-\t1/2 RS overhead in spatial domain can be achieved comparing with non-AI baseline (Option 1) assuming all Set A of beams needs to be measured at each time instances for measurement and prediction. More RS overhead can be achieved considering additional temporal domain RS overhead reduction.\n-\tTop-1 DL Tx beam prediction accuracy:\n-\tevaluation results from 1 source show that AI/ML can achieve 86.4%/83.5% prediction accuracy for prediction time 40ms/160ms, with 32 Tx beam in Set A, and Set B is different in each time instance.\n-\twherein, measurements from 3 time instances with measurement periodicity of 80ms are used.\n-\twherein, 80.5%/70% prediction accuracy can be achieved by non-AI baseline (Option 2) with assumption that the selection of 1/2 of beams selected in baseline are the most frequently used in the evaluated scenario.\n-\tevaluation results from 1 source show that AI/ML can achieve 94.5%/93.7%/92.1% prediction accuracy for prediction time 80ms/160ms/320ms with 32 Tx beam in Set A, and Set B is the same in each time instance.\n-\twherein, measurements from 2 time instances with measurement periodicity of 80ms are used\n-\twherein, 71%/69.9%/68% prediction accuracy can be achieved by non-AI baseline with the assumption that 16 Tx beams are measured in total and preferred beam pattern is used.\n-\twhere the Rx beam of best beam pair within Set A is assumed to obtained the measurement of Set B.\n-\tevaluation results from 1 source show that AI/ML can achieve 67.1%/65.01% prediction accuracy for prediction time 80ms with 32 Tx beam in Set A for 30km/h/60km/h respectively, and Set B is the same in each time instance.\n-\twherein, measurements from 5 time instances with measurement periodicity of 80ms are used\n-\twherein, 44.35%/44.29% prediction accuracy can be achieved for 30km/h/60km/h respectively by non-AI baseline (Option 2)\n-\tevaluation results from 1 source show that AI/ML can achieve 75.34% prediction accuracy for prediction time 160ms with 32 Tx beams in Set A for 30km/h, and Set B is the same in each time instance.\n-\twherein, measurements from 4 time instances with measurement periodicity of 160ms are used\n-\twherein, 44.36% prediction accuracy can be achieved for 30km/h by non-AI baseline (Option 2).\n-\tWith measurements ofÂ fixed Set BÂ or variable Set B with pre-configured patterns of beams that ofÂ 1/4Â of Set A of beams in one time instance,\n-\t1/4 RS overhead in spatial domain can be achieved comparing with non-AI baseline (Option 1) assuming all Set A of beams needs to be measured at each time instances for measurement and prediction. More RS overhead can be achieved considering additional temporal domain RS overhead reduction.\n-\tTop-1 DL Tx beam prediction accuracy:\n-\tevaluation results from 1 source show that AI/ML can achieve 93.4%/92.4%/90.5% and 91.3%/90.6%/89.1% prediction accuracy for prediction time 80ms/160ms/320ms, with 32 Tx beam in Set A, and Set B is different and same in each time instance respectively\n-\twherein, measurements from 2 instances with measurement periodicity of 80ms are used respectively.\n-\tWherein, 70.5%/69.4%/67.4% and 42.5%/42.2%/41.5% prediction accuracy can be achieved by non-AI baseline (Option 2) with the assumption that 16 Tx beams are measured in total and preferred beam pattern is used.\n-\tWhere the Rx beam of best beam pair within Set A is assumed to obtained the measurement of Set B.\n-\tevaluation results from 1 source show that AI/ML can achieve 56.4%/52.7% prediction accuracy for prediction time 80ms/160ms, with 64 Tx beam in Set A and Set B is the same in each time instance\n-\twherein, measurements from 2 time instances with measurement periodicity of 80ms/160ms are used respectively\n-\twherein, 63.25%/58.45% prediction accuracy can be achieved by non-AI baseline (Option 1) when measuring Set A during observation and then applying sample-and-hold\n-\tevaluation results from 1 source show that AI/ML can achieve 83.15%/79.53%/79.43% prediction accuracy for prediction time 40ms/80ms/160ms, with 32 Tx beam in Set A and Set B is the same in each time instance\n-\twherein, measurements from 4 time instances with measurement periodicity of 40ms are used,\n-\t32.8%/32.8%/32.7% prediction accuracy can be achieved by non-AI baseline (Option 2)\n-\tWherein, the Rx beam of best beam pair within Set A is assumed to obtained the measurement of Set B.\n-\tevaluation results from 1 source show that AI/ML can achieve 88%~90% prediction accuracy for prediction time 160ms/320ms/480ms/640ms/800ms, with 32 Tx beam in Set A and Set B is the same in each time instance\n-\twherein, measurements from 5 time instances with measurement periodicity of 160ms are used,\n-\t16%~22% prediction accuracy can be achieved by non-AI baseline (Option 2)\n-\tWhere the best Rx beam for each Tx beam within Set B is assumed to obtained the measurement of Set B.\n-\tevaluation results from 1 source show that AI/ML can achieve 88%/86%/ 82% prediction accuracy for prediction time 40ms/160ms/320ms, with 32 Tx beam in Set A and Set B is the same in each time instance\n-\twherein, measurements from 8 time instances with measurement periodicity of 40ms are used,\n-\t36.2%/35.8%/35.3% prediction accuracy can be achieved by non-AI baseline (Option 2) on the best Tx beam with highest L1-RSRP in the all time instances\n-\tfor random Set B pattern (Set B/Set A=1/4ï¼the SetB is randomly changed in Set A in each time instance), compared to the above case, for Top-1 beam prediction accuracy, evaluation results show about 6% beam prediction accuracy degradation.\n-\twherein, the Rx beam of best beam pair within Set B is assumed to obtained the measurement of Set B\n-\tevaluation results from 1 source show that AI/ML can achieve 73.8%/73.3% and 76.9%/73.08% prediction accuracy for prediction time 160ms/320ms, with 32 Tx beam in Set A, and Set B is the same and different in each time instance respectively\n-\twherein, measurements from 4 time instances with measurement periodicity of 160ms/320ms are used respectively,\n-\t24%/24.7% and 18.1%/17% prediction accuracy can be achieved for same and different Set B pattern respectively with non-AI baseline (Option 2)\n-\tevaluation results from 1 source show that AI/ML can achieve 61.9%/56.35% prediction accuracy for prediction time 80ms with 32 Tx beam in Set A for 30km/h/60km/h respectively, and Set B is the same in each time instance.\n-\twherein, measurements from 5 time instances with measurement periodicity of 80ms are used\n-\twherein, 20.3%/22% prediction accuracy can be achieved for 30km/h/60km/h respectively by non-AI baseline (Option 2)\n-\tevaluation results from 1 source show that AI/ML can achieve 61.7%~55.6% prediction accuracy for prediction time 80ms~960ms, with 32 Tx beam in Set A, and Set B is the same in each time instance\n-\twherein, measurements from 4 time instances with measurement periodicity of equal to or 2 times of the prediction time are used respectively,\n-\t18.6%~8.8% prediction accuracy can be achieved for same Set B pattern with non-AI baseline (Option 2) based on the measurements of the last time instance\n-\tNote: RS overhead reduction\n-\tUnder the assumption of setting Case A, AI/ML can achieve 57.8%~61.0% beam prediction accuracy in terms of Top-1 beam prediction accuracy for 160ms to 960ms prediction time\n-\tup to 4/5 RS/measurement overhead reduction can be obtained with measurements from 4 time instances with measurement periodicity of 160ms to 960ms.\n-\tUnder the assumption of setting Case B, AI/ML can provide more than 90% RS/measurement overhead reduction:\n-\tAI/ML can achieve 58% beam prediction accuracy, while non-AI baseline (Option 2) can only achieve 10% beam prediction accuracy in term of Top-1 beam prediction accuracy for 960ms prediction time\n-\twith non-AI baseline (Option 2), 18.6% of Top-1 beam prediction accuracy can be achieved with 80ms prediction time.\n-\tUnder the assumption of setting Case B+, AI/ML can provide 87.5% RS/measurement overhead reduction:\n-\tAI/ML can achieve 55.6%~59.5% beam prediction accuracy in terms of Top-1 beam prediction accuracy with 160ms to 960ms prediction time 320ms to 1920ms measurement periodicity (4 measurement time instance).\n-\tevaluation results from 1 source show that AI/ML can achieve 67.25% prediction accuracy for prediction time 160ms with 32 Tx beams in Set A for 30km/h, and Set B is the same in each time instance.\n-\twherein, measurements from 4 time instances with measurement periodicity of 160ms are used\n-\twherein, 23.95% prediction accuracy can be achieved for 30km/h by non-AI baseline (Option 2).\n-\tWith measurements ofÂ fixed Set BÂ or variable Set B with pre-configured patterns of beams that ofÂ 1/8Â of Set A of beams in one time instance,\n-\t1/8 RS overhead in spatial domain can be achieved comparing with non-AI baseline (Option 1) assuming all Set A of beams needs to be measured at each time instances for measurement and prediction. More RS overhead can be achieved considering additional temporal domain RS overhead reduction.\n-\tTop-1 DL Tx beam prediction accuracy:\n-\tevaluation results from 1 source show that AI/ML can achieve 67.4%/67.8%/ 70%/66.9%/67.5%/64.9%/62.9% prediction accuracy for prediction time 160ms/320ms/480ms/ 640ms/800ms/960ms, with 32 Tx beam in Set A, and Set B is the same in each time instance.\n-\twherein, measurements from 8 time instances with measurement periodicity of 160ms are used\n-\t9%/8.9%/8.8%/8.7%/8.5%/8.4% prediction accuracy can be achieved by non-AI scheme (Option 2)\n-\tevaluation results from 1 source show that AI/ML can achieve 94%/93.5%/92.6%/90.7% prediction accuracy for prediction time 40ms/80ms/160ms/320ms, with 32 Tx beam in Set A, and Set B is different in each time instance respectively\n-\twherein, measurements from 4 time instances with measurement periodicity of 40ms is used.\n-\twherein, 70.7%/70.2%/69.1%/67.2% prediction accuracy can be achieved by non-AI baseline (Option 2) with the assumption that 16 Tx beams are measured in total and preferred beam pattern is used.\n-\twhere the Rx beam of best beam pair within Set A is assumed to obtained the measurement of Set B.\n-\tevaluation results from 1 source show that AI/ML can achieve 76.1%/75.2%/70.7% prediction accuracy for prediction time 40ms/80ms/160ms, with 32 Tx beam in Set A and Set B is the same in each time instance\n-\twherein, measurements from 4 time instances with measurement periodicity of 40ms are used,\n-\t18.0%/17.9%/17.8% prediction accuracy can be achieved by non-AI baseline (Option 2)\n-\twherein the Rx beam of best beam pair within Set A is assumed to obtained the measurement of Set B.\n-\tevaluation results from 1 source show that AI/ML can achieve 81.7%/81.1%/80.6% prediction accuracy for prediction time 40ms/160ms/320ms, with 32 Tx beam in Set A and Set B is the same in each time instance\n-\twherein, measurements from 8 time instances with measurement periodicity of 40ms are used,\n-\t30.7%/30.4%/30% prediction accuracy can be achieved by non-AI baseline (Option 2) based on the best Tx beam with highest L1-RSRP in all the time instances\n-\tfor random Set B pattern (SetB/SetA=1/8ï¼the SetB is randomly changed in Set A in each time instance), compared to the above case, for Top-1 beam prediction accuracy, evaluation results show about 5% beam prediction accuracy degradation.\n-\twherein, the Rx beam of best beam pair within Set B is assumed to obtained the measurement of Set B\n-\tevaluation results from 1 source show that AI/ML can achieve 56.91% prediction accuracy for prediction time 160ms with 32 Tx beams in Set A for 30km/h, and Set B is the same in each time instance.\n-\twherein, measurements from 4 time instances with measurement periodicity of 160ms are used\n-\twherein, 18.75% prediction accuracy can be achieved for 30km/h by non-AI baseline (Option 2).\n(B) For Tx DL beam prediction with UE rotation, based on evaluation from 2 sources, AI/ML can provide good beam prediction accuracy and gain comparing with non-AI baseline (Option 2) with same RS/measurement:\n-\tWith measurements ofÂ fixed Set BÂ of beams that ofÂ 1/3Â of Set A of beams in one time instance. (Note that more RS overhead can be achieved considering additional temporal domain RS overhead reduction)\n-\t1/3 RS overhead in spatial domain can be achieved comparing with non-AI baseline (Option 1) assuming all Set A of beams needs to be measured at each time instances for measurement and prediction. More RS overhead can be achieved considering additional temporal domain RS overhead reduction.\n-\tEvaluation results from 1 source show that AI/ML can achieve\n-\t77.5% Top-1 beam prediction accuracy for 160ms prediction time and 200ms measurement periodicity wherein, 33.4% prediction accuracy can be achieved by non-AI baseline (Option 2), and 43.3% beam prediction accuracy can be achieved by a combination of spatial interpolation (radial basis function interpolation) followed by sample-and-hold.\n-\tUnder the assumption of Case B+, 93.3% RS overhead reduction can be achieved compared to non-AI baseline (Option 1) assuming all Set A of beams needs to be measured every 40ms at each time instances for measurement and prediction.\n-\tWherein, UE rotation is modelled every 40ms with constant 10 RPM rotation speed in all three rotational axes, with rotational direction chosen uniformly at random among the three axes.\n-\tWith measurements ofÂ variable Set BÂ (with preconfigured Set B pattern in each time instances) of beams that ofÂ 1/3Â of Set A of beams in one time instance,\n-\t1/3 RS overhead in spatial domain can be achieved comparing with non-AI baseline (Option 1) assuming all Set A of beams needs to be measured at each time instances for measurement and prediction. More RS overhead can be achieved considering additional temporal domain RS overhead reduction.\n-\tEvaluation results from 1 source show that AI/ML can achieve\n-\t78%/76%/73.8%/68.6% Top-1 beam prediction accuracy for 160ms/320ms/480ms/960ms prediction time and 200ms/360ms/520ms/1000ms measurement periodicity\n-\twherein, 71.5%/63%/56.5%/45.3% prediction accuracy can be achieved by non-AI baseline (Option 2), in which for each prediction instance, the latest measurement for each beam in Set A is used as the predicted value for that beam.\n-\twherein, Set B patterns in Set A/Set B consecutive time slots partition Set A.\n-\tUnder the assumption of Case B+, 93.3%/96.3%/97.4%/98.7% RS overhead reduction can be achieved compared to non-AI baseline (Option 1) assuming all Set A of beams needs to be measured every 40ms at each time instances for measurement and prediction for 160ms/320ms/480ms/960ms prediction time.\n-\tWherein, UE rotation is modelled every 40ms with constant 10 RPM rotation speed in all three rotational axes, with rotational direction chosen uniformly at random among the three axes.\n-\tWith measurements ofÂ fixed Set BÂ of beams that ofÂ 1/4Â of Set A of beams in one time instance,\n-\t1/4 RS overhead in spatial domain can be achieved comparing with non-AI baseline (Option 1) assuming all Set A of beams needs to be measured at each time instances for measurement and prediction. More RS overhead can be achieved considering additional temporal domain RS overhead reduction.\n-\tTop-1 DL Tx beam prediction accuracy:\n-\tevaluation results from 1 source show that AI/ML can achieve 71.8%/57.3% prediction accuracy for prediction time 160ms/320ms, with 32 Tx beam in Set A, and Set B is the same in each time instance respectively\n-\twherein, measurements from 4 time instances with measurement periodicity of 160ms/320ms are used respectively,\n-\t24.3%/14.2% prediction accuracy can be achieved for same and different Set B pattern respectively with non-AI baseline (Option 2)\n-\tWherein, UE rotation is modelled every 20ms with a rotation speed uniformly distributed within {0, 60} RPM, and the rotation direction is {1/4 of data with randomly to left or right in horizontal, 1/4 of data always to left, 1/4 of data always to right, 1/4 of data to left and right in turn} with random initial directly.\n(C) For beam pair prediction without UE rotation, based on evaluation of most sources, AI/ML can provide good beam prediction accuracy and gain comparing with non-AI baseline (Option 2) with same RS/measurement overhead.\n-\tWith measurements ofÂ fixed Set B or variable Set B with preconfigured pattern in each time instanceÂ of beams that ofÂ 1/4Â of Set A of beams in one time instance,\n-\t1/4 RS overhead can be achieved comparing with non-AI baseline (Option 1) assuming all Set A of beams needs to be measured at each time instances for measurement and prediction. More RS overhead can be achieved considering additional temporal domain RS overhead reduction.\n-\tTop-1 beam pair prediction accuracy:\n-\tevaluation results from 1 source show that AI/ML can achieve 76.3%/74.7%/72% prediction accuracy for prediction time 40ms/80ms/160ms, with 32 Tx beams and 8 Rx beams in Set A, and Set B is the same in each time instance\n-\twherein, measurements from 4 time instances with measurement periodicity of 40ms are used\n-\t32.7%/32.6%/32.5% prediction accuracy can be achieved by non-AI baseline (Option 2)\n-\tevaluation results from 1 source show that AI/ML can achieve 88%~90% prediction accuracy for prediction time 160ms/320ms/480ms/640ms/800ms, with 32 Tx beams and 8 Rx beams in Set A, and Set B is the same in each time instance\n-\twherein, measurements from 5 time instances with measurement periodicity of 160ms are used\n-\t19%~23% prediction accuracy can be achieved by non-AI baseline (Option 2)\n-\tevaluation results from 1 source show that AI/ML can achieve 80.97%/80.17%/75.86% prediction accuracy for prediction time 40ms/80ms/160ms, with 32 Tx beam and 4 Rx beam in Set A, and Set B is the same in each time instance\n-\twherein, measurements from 4 time instances with measurement periodicity of 40ms are used,\n-\t38.6%/38.0%/37.2% prediction accuracy can be achieved by non-AI baseline (Option 2)\n-\tevaluation results from 1 source show that AI/ML can achieve 63.2%/~57.7% prediction accuracy for prediction time 80ms~960ms, with 32 Tx beam and 8 Rx beam in Set A, and Set B is the same in each time instance\n-\twherein, measurements from 4 time instances with measurement periodicity same as or 2 times of the prediction time are used\n-\t22.3%~10.7% prediction accuracy can be achieved by non-AI baseline (Option 2)\n-\tRS overhead reduction\n-\tUnder the assumption of setting Case A, AI/ML can achieve 58.1%~62.0% beam prediction accuracy in terms of Top-1 beam prediction accuracy for 160ms to 960ms prediction time, up to 4/5 RS/measurement overhead reduction can be obtained with measurements from 4 time instances with measurement periodicity of 160ms to 960ms.\n-\tUnder the assumption of setting Case B, AI/ML can provide more than 90% RS/measurement overhead reduction:\n-\tAI/ML can achieve 58.1% beam prediction accuracy, while non-AI baseline (Option 2) can only achieve 12.7% beam prediction accuracy in term of Top-1 beam prediction accuracy for 960ms prediction time\n-\tWith non-AI baseline (Option 2), 22.3% of Top-1 beam prediction accuracy can be achieved with 80ms prediction time.\n-\tUnder the assumption of setting Case B+, AI/ML can provide 87.5% RS/measurement overhead reduction:\n-\tAI/ML can achieve 57.1%~60.7% beam prediction accuracy in terms of Top-1 beam prediction accuracy with 160ms to 960ms prediction time /320ms to 1920ms measurement periodicity (4 measurement time instance).\n-\tevaluation results from 1 source show that AI/ML can achieve 48.2%/51.6% prediction accuracy for prediction time 160ms, with 32 Tx beam and 8 Rx beam in Set A, and Set B is the same and different in each time instance respectively\n-\twherein, measurements from 4 time instances with measurement periodicity of 160ms are used,\n-\t16.2%/22.9% prediction accuracy can be achieved by non-AI baseline (Option 2) based on the measurements of the last time instance\n-\tWith measurements ofÂ fixed Set BÂ of beams that ofÂ 1/8 of Set A of beams in one time instance,\n-\t1/8 RS overhead can be achieved comparing with non-AI baseline (Option 1) assuming all Set A of beams needs to be measured at each time instances for measurement and prediction. More RS overhead can be achieved considering additional temporal domain RS overhead reduction.\n-\tTop-1 beam pair prediction accuracy:\n-\tevaluation results from 1 source show that AI/ML can achieve 76.7%/74.1%/73.6% prediction accuracy for prediction time 40ms/160ms/320ms, with 256 (32Tx*8Rx) beam pairs in Set A and Set B (4Tx*8Rx) is the same in each time instance\n-\twherein, measurements from 8 time instances with measurement periodicity of 40ms are used,\n-\t30.1%/29.7%/29.1% prediction accuracy can be achieved by non-AI baseline (Option 2) based on the measurements in all time instances\n-\tevaluation results from 1 source show that AI/ML can achieve 77.0%/76.2%/72.0% and 74.2%/73.0%/69.8% prediction accuracy for prediction time 40ms/80ms/160ms, with 32 Tx beams and 4 Rx beams in Set A, and Set B is the same in each time instance with all measurements from all Rx beams and half of Rx beams respectively\n-\twherein, measurements from 4 time instances with measurement periodicity of 40ms are used,\n-\t9.88%/9.60%/8.95% and 14.57%/14.45%/14.27% prediction accuracy can be achieved by non-AI baseline (Option 2) for the case with all Rx beams and half of Rx beams respectively\n-\tWith measurements ofÂ fixed Set BÂ or variable Set B with pre-configured pattern in each time instance of beams that ofÂ 1/16Â of Set A of beams in one time instance,\n-\t1/16 RS overhead can be achieved comparing with non-AI baseline (Option 1) assuming all Set A of beams needs to be measured at each time instances for measurement and prediction. More RS overhead can be achieved considering additional temporal domain RS overhead reduction.\n-\tTop-1 beam pair prediction accuracy:\n-\tevaluation results from 1 source show that AI/ML can achieve 50.58%/48.71%/44.33% and 63.94%/63.31%/60.49% prediction accuracy for 40ms/80ms/160ms prediction time with 32 Tx beam in Set A, and Set B is the same in each time instance with {8 Tx and 2 Rx} and {4 Tx and all Rx} respectively.\n-\twherein, measurements from 4 time instances with measurement periodicity of 40ms are used\n-\t8.96%/8.91%/8.89% and 4.7%/4.56%/4.3% prediction accuracy can be achieved by non-AI scheme (Option 2) for the case with from all Rx beams and half of Rx beams respectively\n-\tevaluation results from 1 source show that AI/ML can achieve 89.1% / 86.4%/ 82.9% prediction accuracy for prediction time 40ms/160ms/320ms, with 256 (32Tx*8Rx) beam pairs in Set A and Set B (2Tx*8Rx) is different in each time instance\n-\twherein, measurements from 8 time instances with measurement periodicity of 40ms are used,\n-\t69.4%/67.8%/66% prediction accuracy can be achieved by non-AI baseline (Option 2) based on the measurements in all time instances\n(D) For beam pair prediction with UE rotation, evaluations from 2 sources show AI/ML can provide 44% or 15% beam prediction accuracy gain comparing with non-AI baseline (Option 2) with same RS/measurement overhead, with 78% or 30%~35% Top-1 beam prediction accuracy respectively.\n-\tWith measurements ofÂ fixed Set BÂ or variable Set B with pre-configured pattern in each time instance of beams that ofÂ 1/4 of Set A of beams in one time instance,\n-\t1/4 RS overhead can be achieved comparing with non-AI baseline (Option 1) assuming all Set A of beams needs to be measured at each time instances for measurement and prediction. More RS overhead can be achieved considering additional temporal domain RS overhead reduction.\n-\tTop-1 beam pair prediction accuracy:\n-\tevaluation results from 1 source show that AI/ML can achieve 35.02%/29.2% prediction accuracy for prediction time 40ms/160ms, with 32 Tx beam and 8 Rx beam in Set A, and Set B is the same and different in each time instance respectively\n-\twherein, measurements from 4 time instances with measurement periodicity of 40ms/160ms are used,\n-\t19.7%/15.6% prediction accuracy can be achieved by non-AI baseline (Option 2) based on the measurements of the last time instance\n-\tUE rotation is modelled every 20ms with a rotation speed uniformly distributed within {0, 60} RPM, and the rotation direction is {1/4 of data with randomly to left or right in horizontal, 1/4 of data always to left, 1/4 of data always to right, 1/4 of data to left and right in turn} with random initial directly.\n-\tWith measurements ofÂ variable Set B with pre-configured patterns in each time instance of beams that ofÂ 1/16Â of Set A of beams in one time instance,\n-\t1/16 RS overhead can be achieved comparing with non-AI baseline (Option 1) assuming all Set A of beams needs to be measured at each time instances for measurement and prediction. More RS overhead can be achieved considering additional temporal domain RS overhead reduction.\n-\tTop-1 beam pair prediction accuracy:\n-\tevaluation results from 1 source show that AI/ML can achieve 78.1% prediction accuracy for prediction time 40ms with 32 Tx beams and 8 Rx beams in Set A, Set B is different in each time instance and 10 RPM rotation speed to fixed a direction\n-\twherein, measurements from 3 time instances with measurement periodicity of 40ms or 80ms are used\n-\t42.4%/42.5% prediction accuracy can be achieved by non-AI scheme (Option 2).\nPerformance with quantization:\nAt least for BM-Case1 for inference of DL Tx beam with L1-RSRPs of all beams in Set B, existing quantization granularity of L1-RSRP (i.e., 1 dB for the best beam, 2 dB for the difference to the best beam) causes a minor loss   in beam prediction accuracy compared to unquantized L1-RSRPs of beams in Set B.\n-\tEvaluation results from 13 sources show less than 5% beam prediction accuracy degradation in terms of Top-1 beam prediction accuracy.\n-\tNote: 1 source uses the data without quantization for training and data with quantization for inference. Other sources use the same quantization scheme for data for training and inference.\nAt least for BM-Case1 for inference of DL Tx beam with L1-RSRPs of all beams in Set B,\n-\tEvaluation results from 4 sources show that, with 1dB quantization step for the absolute L1-RSRP of the best beam and 4dB quantization step differential L1-RSRP report with the existing quantization range, less than 5% beam prediction accuracy degradation in terms of Top-1 beam prediction accuracy compared to unquantized L1-RSRPs of beams in Set B.\n-\tSame quantization scheme is used for the input data for training and inference.\n-\tNote: 1 source used quantized L1-RSRPs with the same quantization scheme as labels in training.\n-\tNote: 1 source used unquantized L1-RSRPs as labels in training.\n-\tNote: 1 source used unquantized L1-RSRPs to determine Top-1 beam id as labels in training.\nPerformance with measurement error\nFor BM-Case1 DL Tx beam prediction (unless otherwise stated), when Set B is a subset (1/4 unless otherwise stated) of Set A, without differentiating BB errors and RF errors modelled as truncated Gaussian distribution (unless otherwise stated),\n-\tConsidering Â±2 dB relative measurement error,\n-\tevaluation results from 3 sources show that the beam prediction accuracy degrades 6%~10%in terms of Top-1 beam prediction accuracy comparing to the one without measurement error. And 1 source shows that 95%ile of L1-RSRP diff can be about 1.4~2dB, 1 source shows that average L1-RSRP diff can be lower than 1dB.\n-\tevaluation results from 1 source show that\n-\tfor DL Tx beam prediction, the beam prediction accuracy degrades 28.8% in terms of Top-1 beam prediction accuracy comparing to the one without measurement error, [and average L1-RSRP diff can be about 7.3dB.\n-\tfor Tx-Rx beam pair prediction when Set B is 1/8 of Set A, the beam prediction accuracy degrades 2.4% in terms of Top-1 beam prediction accuracy comparing to the one without measurement error, and average L1-RSRP diff can be about 5.8dB\n-\twherein the measurement error is modelled as uniformed distribution.\n-\tevaluation results from 1 source show that considering different relative measurement error range in model training (Â±2 dB, Â±0 dB), similar (less than 1% difference) Top-1 beam prediction accuracy can be achieved\n-\tConsidering Â±3 or Â±4 dB relative measurement error,\n-\tevaluation results from 4 sources show that the beam prediction accuracy degrades 14% (with 3dB error) ~20% (with 4dB error) in terms of Top-1 beam prediction accuracy comparing to the one without measurement error. And 1 source shows that the 95%ile of L1-RSRP diff can be about 2~3.2dB. 1 source shows that average L1-RSRP diff can be lower than 1dB.\n-\tevaluation results from 1 source show that considering different relative measurement error range in model training (0dB, Â±2 dB, Â±4 dB), similar (less than 1% difference) Top-1 beam prediction accuracy can be achieved, and average L1-RSRP diff can be lower than 1dB when Â±2 dB or Â±4 dB relative measurement error is considered in model training\n-\tConsidering up to Â±5 dB relative measurement error when Set B is 1/8 of Set A,\n-\tevaluation results from 1 source show that the beam prediction accuracy degrades 13.6% in terms of Top-1 beam prediction accuracy comparing to the one without measurement error for DL Tx beam prediction.\n-\tConsidering Â±6 dB relative measurement error,\n-\tevaluation results from 3 sources show that the beam prediction accuracy degrades 22%~30% in terms of Top-1 beam prediction accuracy comparing to the one without measurement error. And the 95%ile of L1-RSRP diff can be about 3.1~7.5dB.\n-\tevaluation results from 1 source show that he L1-RSRP difference in 90%ile degrades 7dB for the AI/ML model, compared to baseline 1 and 2 that degrades 3 dB respectively 1 dB at the same percentile.\n-\tevaluation results from 1 source show that for both DL Tx beam prediction and beam pair prediction, the beam prediction accuracy degrades 42~48% in terms of Top-1 beam prediction accuracy comparing to the one without measurement error. And the average L1-RSRP diff can be about 1.6dB.\n-\tHowever, comparing with the global search of all beams in Set A with the same measurement error level, for DL Tx beam prediction the beam prediction accuracy degrades less than 1% in terms of Top-1 beam prediction accuracy, and for Tx-Rx beam pair prediction the beam prediction accuracy degrades about 7% in terms of Top-1 beam prediction accuracy.\n-\tNote: in this evaluation, measurement errors are considered in training and inference phase only for AI inputs with idea labels in training phase.\n-\tevaluation results from 1 source show that\n-\tfor DL Tx beam prediction, the beam prediction accuracy degrades 32.4% in terms of Top-1 beam prediction accuracy comparing to the one without measurement error, [and average L1-RSRP diff can be about 8.34dB.\n-\tfor Tx-Rx beam pair prediction, the beam prediction accuracy degrades 5.2% in terms of Top-1 beam prediction accuracy comparing to the one without measurement error, [and average L1-RSRP diff can be about 6.4dB.\n-\tevaluation results from 1 source show that considering different relative measurement error range in model training (0dB, Â±2 dB, Â±6 dB), similar less or than 2% Top-1 beam prediction accuracy can be achieved, and average L1-RSRP diff can be lower than 1dB when Â±6 dB relative measurement error is considered in model training\nFor BM-Case1 DL Tx beam prediction or Tx-Rx beam pair prediction, when Set B is a subset (1/4 unless otherwise stated) of Set A, with separately modelled BB error and/or RF errors modelled as truncated Gaussian distribution (unless otherwise stated),\n-\tConsidering Â±3 relative measurement error for BB and RF respectively,\n-\tevaluation results from 1 source show that for DL Tx beam prediction and beam pair prediction with Set B is Â¼ of Set A, the beam prediction accuracy degrades 42% and 38% respectively in terms of Top-1 beam prediction accuracy comparing to the one without measurement error. And the average of L1-RSRP diff is about 1.1dB and 2.16dB respectively.\n-\tHowever, comparing with the global search of all beams in Set A with the same measurement error level, for DL Tx beam prediction the beam prediction accuracy degrades about 2 % in terms of Top-1 beam prediction accuracy, and for Tx-Rx beam pair prediction the beam prediction accuracy degrades about 8% in terms of Top-1 beam prediction accuracy.\n-\tNote: in this evaluation, measurement errors are considered in training and inference phase only for AI inputs with idea labels in training phase.\n-\tevaluation results from 1 source show that for both DL Tx beam prediction with Set B is 1/4 of Set A and beam pair prediction with Set B is 1/16 Set A, the beam prediction accuracy degrades 4.3% and 6.3% respectively in terms of Top-1 beam prediction accuracy comparing to the one without measurement error. And the average of L1-RSRP diff becomes 0.7dB and 2.18dB larger respectively.\n-\tNote: in this evaluation, for DL Tx beam prediction, the measurements of Set B from each Rx beam of all Rx beams were used as AI inputs to obtain Top-K beams, followed by Top-K beam sweeping with that given Rx beam. This procedure repeats over all Rx beams, to obtain the best Tx beam at all Rx beams.\n-\tConsidering 3.3 dB for standard deviation in relative measurement error without truncation for RF only, evaluations results from 1 source show with AI/ML:\n-\twith a common measurement error for all Tx beams at a given Rx beam:\n-\tTop-1 beam prediction accuracy with 1 dB margin performance has slight performance degradation (less than 0.2%) than that without measurement error.\n-\twith independent measurement errors for all Tx beams,\n-\tTop-1 beam prediction accuracy with 1 dB margin has 10% and 20% performance degradation than that without measurement error for Set B/Set A = 1/2 and 1/4 respectively.\n-\twherein, measurement errors are only considered in inference inputs\nNote that:\n-\tIn the above results, measurement errors are considered in both training (input data and label) and inference phase (except the ground-truth) unless otherwise stated.\n-\tBeams could be measured regardless of their SNR.\n-\tMeasured in a single-time instance (within a channel-coherence time interval).\n-\tNo quantization for the L1-RSRP measurements.\n-\tNo constraint on UCI payload overhead for full report of the L1-RSRP measurements of Set B for NW-side models are assumed.\nPerformance with different Rx beam assumption for DL Tx beam prediction\nAt least for BM-Case1 when Set B is a subset of Set A, and for DL Tx beam prediction, with the measurements of the \"best\" Rx beam with exhaustive beam sweeping for each model input sample, AI/ML provides the better performance than with measurements of random Rx beam(s).\n-\tEvaluation results from 12 sources show 20%~50% degradation with random Rx beam(s) comparing with the \"best\" Rx beam in terms of Top-1 prediction accuracy.\n-\tEvaluation results from 1 source shows 12% degradation with measurement of random Rx compared with measurement of best Rx in term of Top-1 beam prediction accuracy.\nComparing performance with non-AI baseline option 2 (based on the measurement from Set B of beams), with measurements of random Rx beam(s) as AI/ML inputs:\n-\tEvaluation results from 7 sources show that AI/ML can still provide 7%~44% beam prediction accuracy gain in terms of Top-1 beam prediction accuracy.\nNote: In both training and inference, measurements of random Rx beams are used as AI/ML inputs.\nFor BM-Case 1 DL Tx beam prediction without UE rotation, for Top-1 beam prediction accuracy, compared to the best Rx beams obtained from one shot measurements, with quasi-optimal Rx beam performance degradation is observed:\n-\tevaluation results from 1 source show 2% beam prediction accuracy degradation when Set B = 1/2 Set A and 7% beam prediction accuracy improvement when Set B = 1/4 or 1/8 Set A, when using the best Rx beams obtained from previous exhaustive sweeping (20ms ago) of all beams in Set A, comparing with using the best Rx beam for each Tx beams in Set B obtained from current exhaustive sweeping, without considering UE rotation for 3km/h UE speed. Such beam prediction accuracy improvement may not exist when considering UE rotation and higher UE speed.\n-\tevaluation results from 1 source show 2.5% beam prediction accuracy degradation using the best Rx of each Tx beams obtained from previous exhaustive sweeping (20ms ago) than using the best Rx of each Tx beams obtained from current exhaustive sweeping, without considering UE rotation for 3km/h UE speed.\n-\tevaluation results from 1 source shows 6.6%/6.9%/32.1%/45% degradation using a stochastic model in which the UE Rx beam is randomly selected with average probability that the best Rx beam is selected equal to 87.1%/75.1%/34.3%/10.9% compared to using the best Rx of each Tx beams obtained from current exhaustive sweeping, without considering UE rotation\n-\tevaluation results from 1 source show 13% beam prediction accuracy degradation, with the assumption of the best Rx beam for each Tx beam obtained from previous exhaustive sweeping over all beams in Set A in a SSB-like structure (in the past 160ms for each Rx beam with every 20ms a burst of Set A of beams) without considering UE rotation for 3km/h UE speed.\n-\tevaluation results from 1 source show 3%~11% beam prediction accuracy degradation, with the assumption of the best Rx beam obtained from one specific Tx beam which is 1st Tx beam in Set B.\n-\tevaluation results from 1 source show 12% beam prediction accuracy degradation with the assumption of the best Rx beams obtained from one specific Rx beam which is the best between the same Rx beam for different panels.\n-\tIn addition, evaluation results from 3 sources show 1%~4% and 6%~12% beam prediction accuracy degradation, with the assumption of the best Rx beam is used for 90% and 80% of the model input samples and random Rx beam for the remaining samples respectively.\n-\tEven though, AI/ML can still provide better performance than non-AI baseline option 2 (exhaustive beam sweeping in Set B of beams), e.g., 50%~60% beam prediction accuracy difference in terms of Top-1 beam prediction accuracy based on the evaluation results from 2 sources, where non-AI baseline option 1 (exhaustive beam sweeping in Set A of beams) provides 100% prediction accuracy.\nFor BM-Case 2 DL Tx beam prediction with UE rotation, for Top-1 beam prediction accuracy, with quasi-optimal Rx beam selection:\n-\tevaluation results from 1 source show 5~11% beam prediction accuracy improvement given the assumption of the best Rx beams obtained from previous round-robin sweep of beam pair links from beams in Set A, compared to sample-and-hold baselines.\n-\tIn the evaluation, UE rotation is modelled every 40ms with constant 10 RPM rotation speed in all three rotational axes, with rotational direction chosen uniformly at random among the three axes.\nPerformance with different label options\nDifferent label options may lead to different data collection overhead for training. At least for BMCase-1, for (Option 1a) Top-1 beam(pair) in Set A as the label and (Option 2a) all L1-RSRPs per beam of all the beams(pairs) in Set A as the label, with the comparable model complexity and computation complexity, the results across companies and the observed performance delta are summarized as below:\n-\tFor Top 1 beam (pair) prediction accuracy,\n-\tevaluation results from 7 sources show that an AI/ML model with Top-1 beam(pair) in Set A as the label (Option 1a) can provide better performance (e,g, 2~7% or 12%~18% higher for Top 1 beam prediction accuracy) than an AI/ML model with all L1-RSRPs per beam of all the beams(pairs) in Set A as the label (Option 2a)\n-\tevaluation results from 1 source show that similar or slightly worse (e,g, 2% higher for Top 1 beam prediction accuracy)) can be achieved with Option 1a than Option 2a\n-\tFor Top-K beam (pair) prediction accuracy or Top-1 beam prediction accuracy with 1dB margin,\n-\tevaluation results from  2 sources show that Option 1a can provide similar performance than Option 2a\n-\tevaluation results from 1 source show that Option 2a can provide 5%~12% better performance than Option 1a for Top-2/-4 beam pair prediction accuracy.\n-\tevaluation results from 1 source show that show that Option 1a can provide 2%~5% better performance than Option 2a for Top-2/-6 beam pair prediction accuracy.\n-\tevaluation results from 1 source show that show that Option 1a can provide 2%~7% /1%~5% better performance than Option 2a for Top-2/-4 beam prediction accuracy for DL Tx beam prediction.\n-\tevaluation results from 1 source show that show that Option 1a can provide <1% or 9%~17% better performance than Option 2a for Top-2/-3 beam prediction accuracy for DL Tx beam prediction for Set B=1/2 Set A or Set B =1/4 or 1/8 Set A.\n-\tDetailed assumptions and results are listed as below:\n-\tevaluation results from one source show that for both DL Tx beam prediction and beam pair prediction with Set B is Â¼ of Set A, with Top-1 beam in Set A as the label, AI/ML can provide 2%~3% higher beam prediction accuracy in terms of Top-1 beam prediction accuracy comparing to the one with all L1-RSRPs per beam of all the beams as the label with comparable model complexity. The Top-K beam prediction accuracy is comparable for DL Tx beam prediction; however, the Top-K beam prediction accuracy is slightly better (<1%) with all L1-RSRPs as the label. The average L1-RSRP difference is similar (about 1.5dB) in the two cases.\n-\tevaluation results from one source show that for Tx beam prediction with Set B is 1/2 Set A and Set B is 1/4 Set A, with Top-1 beam in Set A as the label, AI/ML can provide 2%-5% higher beam prediction accuracy in terms of Top-1 beam prediction accuracy comparing to the one with all L1-RSRPs per beam of all the beams as the label with comparable model complexity. The Top- 1 beam with 1dB error and Top-K beam prediction accuracy is comparable for DL Tx beam prediction.\n-\tevaluation results from one source show that for beam pair prediction with Set B is 1/8 or 1/16of Set A, with Top-1 beam in Set A as the label, AI/ML can provide 4%-6% higher beam prediction accuracy in terms of Top-1 beam prediction accuracy comparing to the one with all L1-RSRPs per beam of all the beams as the label even with larger model complexity.\n-\tevaluation results from one source show that for beam pair prediction with Set B is Â¼ Set A, with Top-1 beam in Set A as the label, AI/ML can provide 12% higher beam prediction accuracy in terms of Top-1 beam prediction accuracy comparing to the one with all L1-RSRPs of all the beams as the label with comparable model complexity. However, labelling with all L1-RSRPs can provide 5% and 12 % better for Top-3 or Top-4 beam prediction accuracy comparing with labelling with Top-1 beam ID.\n-\tevaluation results from one source show that for beam pair prediction with Set B is Â¼ Set A, with Top-1 beam in Set A as the label, AI/ML can provide 15% higher beam prediction accuracy in terms of Top-1 beam prediction accuracy comparing to the one with all L1-RSRPs per beam of all the beams as the label with comparable model complexity. The average L1-RSRP difference is similar (about 0.4dB) in the two cases.\n-\tevaluation results from one source show that for DL Tx beam prediction with Set B is Â¼ of Set A, with Top-1 beam in Set A as the label, AI/ML can provide similar beam prediction accuracy in terms of Top-1 beam prediction accuracy comparing to the one with all L1-RSRPs per beam of all the beams as the label. Using Top-1 beam as the label can provide 2%/5% better performance for Top-2/-6 beam prediction. The average L1-RSRP difference is similar (about 1dB) in the two cases.\n-\tevaluation results from one source show that for beam pair prediction with Set B is 1/16 of Set A, with Top-1 beam in Set A as the label, 2% beam prediction accuracy degradation in terms of Top-1 beam prediction accuracy is achieved comparing to the one with all L1-RSRPs per beam of all the beams as the label.\n-\tevaluation results from one source show that for Tx beam prediction with Set B is 1/4 of Set A or 1/8 of Set A or 1/16 of Set A, with Top-1 beam in Set A as the label, AI/ML can provide comparable or up to 7% higher beam prediction accuracy in terms of Top-K (K=1, 2, 4) beam prediction accuracy comparing to the one with all L1-RSRPs per beam of all the beams as the label with comparable model complexity. However, the performance of average L1-RSRP difference of Top-1 predicted beam and beam prediction accuracy with 1dB margin for Top-1 beam is comparable or better with all L1-RSRPs per beam of all the beams as the label.\n-\tEvaluation results from one source show that for Tx beam prediction with Set B is 1/2 Set A, with Top-1 beam in Set A as the label,Â AI/ML can provide <1% higher beam prediction accuracy in terms of Top-K (K=1,2,3) beam prediction accuracy comparing to the one with all L1-RSRPs per beam of all the beams as the label with comparable model complexity. With Set B is 1/4 Set A and 1/8 Set A and Top-1 beam in Set A as the label, AI/ML can provide 10-18% higher beam prediction accuracy in terms of Top-K (K=1,2,3) beam prediction accuracy comparing to the one with all L1-RSRPs per beam of all the beams as the label with comparable model complexity.\nIn addition, 1 source show good performance with Top-K beam(pair)s in Set A and the corresponding L1-RSRPs as the label (Option 2b) can be achieved with two separate AI models. In the evaluation, one classification model (with Top-1/K beam(s) in Set A as the label(s)) is used to predict the Top-1/K beam and another regression model (with L1-RSRP(s) of Top-1/K beam(s) in Set A as the label(s)) is used to predict L1-RSRP(s).\nNote: The performance for beam predication accuracy with AI/ML may also depend on some other aspects, e.g., AI/ML model architecture choice, model training parameters (e.g., hyperparameter tuning), loss function corresponding to optimizing certain KPI(s). Assumptions on loss function are not indicated in the evaluations above.\nNote: ideal measurements are assumed\n-\tBeams could be measured regardless of their SNR.\n-\tNo measurement error.\n-\tMeasured in a single-time instance (within a channel-coherence time interval).\n-\tNo quantization for the L1-RSRP measurements.\n-\tNo constraint on UCI payload overhead for full report of the L1-RSRP measurements of Set B for NW-side models are assumed.\nPerformance with different Set B pattern assumptions\nFor BMCase-1 and for a fixed Set B pattern option, Set B pattern will affect the beam prediction accuracy with AI/ML for both DL Tx beam prediction and beam pair prediction.\nAt least for BM-Case1 (unless otherwise stated) DL Tx beam with the measurements from the best Rx beam, and/or beam pair prediction, when Set B is a subset of Set A without considering other generalization aspects and without UE rotation.\n-\t(Opt 2B) For the case that Set B of beam(pair)s is changed among pre-configured patterns, compared to the case that Set B is fixed across training and inference (Opt 1), for Top-1 beam prediction accuracy\n-\tevaluation results from 14 sources show no more than 10% or about 10% beam prediction accuracy degradation, wherein 2 sources used up to 24 pre-configured patterns and the rest of sources use 3 ~ 5 patterns;\n-\tAI/ML still can provide better performance (e.g., >30%) of Top-1 beam prediction unless otherwise stated) than non-AI baseline option 2 (exhaustive beam sweeping in Set B of beams).\n-\tNote: the above performance can also be treated as training with mixed patterns of Set B of beam, and testing with mixed patterns Set B of beams.\n-\t(Opt 2C) For the case that Set B of beam(pair)s is randomly changed in Set A of beams, compared to the case that Set B is fixed across training and inference (Opt 1), for Top-1 beam prediction accuracy\n-\tevaluation results from 2 sources show 10%~20% beam prediction accuracy degradation.\n-\tevaluation results from 7 sources show 20%~50% beam prediction accuracy degradation.\n-\tAI/ML still can provide better performance (e.g., >25% of Top-1 beam prediction unless otherwise stated) than non-AI baseline option 2 (exhaustive beam sweeping in Set B of beams):\n-\t(Opt 2D) For the case that Set B of beams (pairs) is a subset of measured beams (pairs) Set C (where Set C is fixed across training and inference), compared to the case with all measurements of measured beam Set C as AI inputs\n-\twith Top K=1/2 of the measurements of Set C,\n-\tFor Top-1 beam prediction accuracy\n-\tevaluation results from 5 sources show less than 4% the beam prediction accuracy degradation\n-\tevaluation results from 3 sources show about 7% the beam prediction accuracy degradation\n-\tevaluation results from 1 source show <1% and 7% beam prediction accuracy degradation with measuring 1/2 and 1/4 of Set A of beams respectively.\n-\tevaluation results from 1 source show about 12% the beam prediction accuracy\n-\tNote: all the above results are for DL Tx beam prediction\n-\tFor NW-side model, 1/2 UCI reporting overhead for inference inputs can be saved without considering quantization impact.\n-\tIn the above evaluation, 5 sources use L1-RSRPs of Top-4 measurements of 8 beams in Set C for 32 Tx beams in Set A.\n-\tIn the above evaluation, 3 sources use L1-RSRPs of Top-8 measurements of 16 beams in Set C for 64 Tx beams in Set A\n-\tIn the above evaluation, 1 source uses L1-RSRPs of Top-4/-8 measurements of 8/16 beams in Set C for 32 Tx beams in Set A.\n-\twith Top K=1/4 of the measurements of Set C,\n-\tFor Top-1 beam prediction accuracy\n-\tevaluation results from 2 sources show 8~10% beam prediction accuracy degradation.\n-\tevaluation results from 1 source show 15% beam prediction accuracy degradation.\n-\tevaluation results from 1 source show 2% beam prediction accuracy degradation with measuring 1/2 of Set A of beams respectively.\n-\tNote: all the above results are for DL Tx beam prediction\n-\tFor NW-side model, 3/4 UCI reporting overhead for inference inputs can be saved without considering quantization impact.\n-\tIn the above evaluation, 1 source uses L1-RSRPs of Top-4 measurements of 16 beams in Set C for 32 Tx beams in Set A.\n-\tIn the above evaluation, 2 sources use L1-RSRPs of Top-4 measurements of 16 beams in Set C for 64 Tx beams in Set A.\n-\twith Top K=1/8 of the measurements of Set C,\n-\tevaluation results from 1 source show 7.5% beam prediction accuracy degradation in terms of Top-1 beam prediction accuracy for beam pair prediction.\n-\tFor NW-side model, 7/8 UCI reporting overhead for inference input can be saved.\n-\tIn the evaluation, 1 resource uses L1-RSRPs of Top-16 measurements of 128 beams in Set C for 64 Tx beams and 8 Rx beams in Set A.\n-\twith Top K=1/6 of the measurements of Set C, for BM-Case 2, evaluation results [from 1 source] show 3.5% improvement in beam prediction accuracy compared to non-AI/ML baseline (Option 2, sample-and-hold) whose beam prediction accuracy is 78.2%.\n-\twith the reported measurements within a given gap of [5dB/ 10dB/ 14dB~20dB] to the best beam in Set C, evaluation results from 6 sources show 15%~28% / 4%~16.4%/ 2%~6% respectively beam prediction accuracy degradation.\n-\t1 source Samsung simulated for BM-Case 2, and filled in the unreported measurements in Set C as (L1-RSRP of the best Rx beam in Set Câ14dB) as the inputs for AI/ML.\n-\twith Top-M measurements in Set C or with the reported measurements within a given gap to the best beam in Set C (when Set C is larger than Set B), comparing with the case that using a smaller number of beams in Set B as the fixed pattern, the results show that comparable or better beam prediction accuracy can be achieved with the same reporting overhead or numbers of measurements as of AI inputs but larger measurement overhead.\n-\tevaluation results from 1 source show similar Top-1 beam prediction accuracy for the case using the measurements of Top 8 beams of 16 beams in Set C and 64 beams in Set A comparing with using 8 fixed beams in Set B.\n-\tevaluation results from 1 source show 16.5% and 43% gain in terms of Top-1 beam prediction accuracy for the case of using the measurements of Top 4 beams of 8 or 16 beams in Set C and 32 beam in Set A respectively comparing with using 4 fixed beams in Set B.\n-\tevaluation results from 1 source show about 8% gain in terms of Top-1 beam prediction accuracy for the case using the measurements of Top 4 beams of 8 beams in Set C and 32 beams in Set A comparing with using 4 fixed beams in Set B.\n-\tevaluation results from 1 source show about 12.5% gain in terms of Top-1 beam prediction accuracy for the case using the measurements of Top 4 beams of 8 beams in Set C and 32 beams in Set A comparing with using 4 fixed beams in Set B.\n-\tevaluation results from 1 source show about 18% gain in terms of Top-1 beam prediction accuracy for the case using the measurements of Top 8 beams of 16 beams in Set C and 64 beams in Set A comparing with using 4 beams in Set B.\n-\tevaluation results from 1 source show similar Top-1 beam prediction accuracy for the case using the measurements of Top 4 beams of 8 beams in Set C and 32 beams in Set A comparing with using 4 fixed beams in Set B\n-\tevaluation results from 1 source show 17% gain in terms of Top-1 beam prediction accuracy for the case of using the measurements of Top 8 beams of 16 beams in Set C and 64 beams in Set A comparing with using 8 fixed beams in Set B. .\n-\tevaluation results from 1 source show 12% gain in terms of Top-1 beam prediction accuracy for the case of using the measurements of Top 4 beams of 8 in Set C and 32 beam in Set A comparing with using 4 fixed beams in Set B respectively.\n-\tThe beam prediction accuracy increases with the number of measurements of Set B.\n-\tAI/ML still can provide better performance (e.g., >30% of Top-1 beam prediction unless otherwise stated) than non-AI baseline option 2 (exhaustive beam sweeping in Set B of beams).\n-\tNote that ideal measurements are assumed\n-\tBeams could be measured regardless of their SNR.\n-\tNo measurement error.\n-\tMeasured in a single-time instance (within a channel-coherence time interval).\n-\tNo quantization for the L1-RSRP measurements.\n-\tNo constraint on UCI payload overhead for full report of the L1-RSRP measurements of Set B for NW-side models are assumed.\n-\tThis observation is based on Set B patterns that were chosen by each company.\n-\tImplicit or explicit information of Tx beam ID and/or Rx beam ID are used as AI/ML model inputs\nThe following generalization aspects were evaluated for at least BMCase-1 when Set B is a subset of Set A (and BMCase-2 if stated),\n-\tScenarios\n-\tVarious deployment scenarios,\n-\te.g., UMa, UMi\n-\te.g., 200m ISD or 500m ISD\n-\tVarious outdoor/indoor UE distributions, e.g., 100%/0%, 20%/80%, and others\n-\tVarious UE mobility (for BMCase-2 only),\n-\te.g., 30km/h, 60km/h and others\n-\tConfigurations (parameters and settings)\n-\tVarious UE parameters,\n-\te.g., UE codebook\n-\te.g., UE antenna array dimensions\n-\te.g., different number beams in a seen UE codebook when inference using a subset of Rx beams of training\n-\tVarious gNB settings,\n-\te.g., DL Tx beam codebook\n-\te.g., gNB antenna array dimensions\n-\tVarious Set A of beam(pairs)\n-\tVarious Set B of beam (pairs)\nNote: the following are assumed in the simulation unless otherwise stated\n-\tFor DL Tx beam prediction, the measurements from best Rx beam are used.\n-\tFixed Set B pattern.\n-\tWithout UE Rotation.\n-\tBeams could be measured regardless of their SNR.\n-\tNo measurement error.\n-\tMeasured in a single-time instance (within a channel-coherence time interval).\n-\tNo quantization for the L1-RSRP measurements.\n-\tNo constraint on UCI payload overhead for full report of the L1-RSRP measurements of Set B for NW-side models are assumed.\n-\tObservations are applicable for both Tx beam and beam pair.\n-\tThe evaluation results are from BM-Case 1 and similar observation are expected for BM-Case 1 when Set B is different from Set A.\nNote that, in the following evaluation, model switching is not evaluated for generalization performance.\nCompanies have provided evaluation results which show that Case 3 and/or Case 2A can provide better performance than Case 2. In most of the cases/evaluations, Case 3 has performance degradation than Case 1. From the evaluation results from some companies and for some scenarios, Case 3 may have similar or slightly higher performance than Case 1:\n-\t2 sources: for various UE distribution with same or double training data size,\n-\t1 source: for different ISDs with triple training data size.\n(A) For some cases, Case 2 have some performance degradation than Case 1 in most of the cases/evaluations. In Case 2, AI/ML still can provide better performance than non-AI baseline option 2 (based on the measurements from Set B of beams):\n-\tFor various deployment scenarios: UMa/UMi (with the assumption of same down tilt, same or different NLOS probability, same or different ISD, same or different antenna height)\n-\t(Case 2) For generalization Case 2 compared to Case 1,\n-\tWith the assumption of same ISD, antenna height and same NLOS probability for UMa/UMi, evaluation results from 4 sources show less than 5% degradation, evaluation results from 4 sources show 5%~10% degradation\n-\twherein 1 source assumed different UE distribution with same ISD, antenna height, its results show 5%~17% and less than 5% degradation for 100% outdoor UE and 80%/20% in/outdoor UE, respectively, for different combinations of Set B and Set A (i.e., different ratio of Set B/Set A and Set B could be either subset of Set A or different from Set A) for Top-1 beam prediction accuracy, for DL Tx beam prediction.\n-\tWith the assumption of different antenna height for UMa/UMi,\n-\tevaluation results from 1 source show about 13% degradation for Top-1 beam prediction accuracy, for DL Tx beam prediction with same ISD\n-\tevaluation results from 1 source show 16%, and 18% degradation for Top-1 beam prediction accuracy, for DL Tx beam and beam pair prediction respectively, with different ISD\n-\tevaluation results from 1 source show about 13% degradation for Top-1 beam prediction accuracy, for both DL Tx beam and beam pair prediction with same ISD, different antenna heights and NLOS probabilities\n-\t(Case 3) For generalization Case 3 compared to Case 1, the evaluation results from 5 sources show less than 5% degradation, and the evaluation results from 1 source show 8% degradation for Top-1 beam prediction accuracy, for DL Tx beam and/or beam pair prediction.\n-\twherein 1 source assumed different ISD and antenna height and the results show about 8% degradation for Top-1 beam prediction accuracy for both DL Tx beam and beam pair prediction.\n-\tVarious deployment scenarios: ISD 200m/ISD 500m\n-\t(Case 2) For generalization Case 2 compared to Case 1, evaluation results from 3 sources show about 1%~2% degradation, evaluation results from 2 sources show ~9% degradation for Top-1 beam prediction accuracy for DL Tx beam and/or beam pair prediction.\n-\t(Case 3) For generalization Case 3 compared to Case 1, the evaluation results from 1 source show slightly better (1%~2% for Top-1 beam prediction accuracy) performance compared to Case 1 with triple size of training data for DL Tx beam prediction, and, the evaluation results from 1 source show about 1% degradation on Top-1 beam prediction accuracy for beam pair prediction with the same size of training data.\n-\tVarious deployment scenarios: 100% outdoor/20%outdoor\n-\t(Case 2) For generalization Case 2 compared to Case 1, evaluation results from 4 sources show less than 5% degradation, evaluation results from 3 sources show 5%~10% degradation, evaluation results from 3 sources show 10%~25% degradation for Top-1 beam prediction accuracy for DL Tx beam and/or beam pair prediction.\n-\tIn addition, 1 source evaluated the scenario with 80% outdoor/20% outdoor, and its evaluation results show about 20% degradation for Top-1 beam prediction accuracy for DL Tx beam prediction.\n-\tIn addition, 1 source evaluated the scenario with 100% outdoor/0% outdoor, and its evaluation results show 10%~25% degradation for Top-1 beam prediction accuracy for DL Tx beam prediction.\n-\tIn addition, evaluation results from 1 source show that the performance degradation becomes larger with smaller ratio of Set B/Set A.\n-\twherein, 1 source evaluated the scenario with ISD=200 in UMa for different combinations of Set B and Set A (i.e., different ratio of Set B/Set A and Set B could be either subset of Set A or different from Set A) and the results show 10%~17% degradation for Top-1 beam prediction accuracy for DL Tx beam prediction.\n-\t(Case 2A) For generalization Case 2A compared to Case 1, evaluation results from 1 source show 1%~6% degradation for Top-1 beam prediction accuracy for DL Tx beam prediction.\n-\twherein, 1 source evaluated the scenario ISD=200 in UMa for different number of epochs and number of data used for finetuning and the results show 1%~6% degradation for Top-1 beam prediction accuracy for DL Tx beam prediction.\n-\tIn addition, 1 source evaluated the scenario with 80% outdoor/20% outdoor, and its evaluation results show 3%~8% degradation for Top-1 beam prediction accuracy for DL Tx beam prediction.\n-\t(Case 3) For generalization Case 3 compared to Case 1, the evaluation results from 4 sources show less than 2% degradation, and the evaluation results from 2 sources show 10% degradation for Top-1 beam prediction accuracy compared to Case 1. However, the evaluation results from 1 source show slightly better (about 1% for Top-1 beam prediction accuracy) performance compared to Case 1 with double size of training data.\n-\tIn additional, 1 source evaluated the scenario with 80% outdoor/20% outdoor, and its evaluation results show slightly better (about 4% for Top-1 beam prediction accuracy) performance compared to Case 1 with same training data size for DL Tx beam prediction.\n-\tIn additional, the evaluation results from 1 source show that for generalization from 100% outdoor to 20% outdoor, 7% degradation for Top-1 beam prediction accuracy compared to Case 1. For generalization from 20% outdoor to 100% outdoor, about 4% degradation for Top-1 beam prediction accuracy compared to Case 1.\n-\tFor DL Tx beam prediction only, various UE parameters: different UE codebooks, and/or different UE antenna array dimensions\n-\t(Case 2) For generalization Case 2 compared to Case 1, for Top-1 beam prediction accuracy\n-\tevaluation results from 2 sources show less than 1% performance with different UE codebooks.\n-\tevaluation results from 1 source show about 4% degradation, with different UE codebook, different number of Rx elements and panel location.\n-\tevaluation results from 1 source show about 10% degradation with both different number of UE Rx beams, different number of Rx elements, and about 5% degradation with both different number of UE Rx beams (where in Configuration #A, UE Rx beams are subset of UE Rx beams in Configuration #B), and same number of Rx elements,\n-\t(Case 3) For generalization Case 3 compared to Case 1, the evaluation results from 1 source show 1~2.5% degradation with different number of UE Rx beams, different number of Rx elements and panel location, and evaluation results from 1 source show about 7.5% degradation with both different number of UE Rx beams, different number of Rx elements, for Top-1 beam prediction accuracy.\n-\tFor beam pair prediction only, various UE parameters: different number of beams in a seen UE codebook when inference using a subset of Rx beams of training\n-\t(Case 2) For generalization Case 2 compared to Case 1, evaluation results from 2 sources show 2%~15% degradation Top-1 beam prediction accuracy\n-\twherein, evaluation results from 1 source show 2% with different number of beams in a seen UE codebook for Top-1 beam prediction accuracy based on the assumption that training by 8 Rx beam and inference by 4 of 8 Rx beam.\n-\twherein, evaluation results from 1 source show 15% degradation with different number of beams in a seen UE codebook for Top-1 beam prediction accuracy based on the assumption that training by 4 Rx beam and inference by 2 of 4 Rx beam.\n(B) For some cases, Case 2 have significant performance degradation than Case 1 in most of the cases/ evaluations. In Case 2, AI/ML can provide comparable or worse performance than non-AI baseline option 2 (based on the measurements from Set B of beams)\n-\tVarious deployment scenarios: UMa/UMi (With the assumption of different ISD, antenna height, down tilt and NLOS probability)\n-\t(Case 2) For generalization Case 2 compared to Case 1, evaluation results from 3 sources show 20%~35% degradation for Top-1 beam prediction accuracy compared to Case 1, for DL Tx beam and/or beam pair prediction.\n-\t(Case 3) For generalization Case 3 compared to Case 1, the evaluation results from 2 sources show less than 5% degradation,\n-\tVarious configurations (parameters and settings): different gNB antenna array dimensions, and/or DL Tx beam codebook\n-\tNote: different DL Tx beam codebooks will result in various Set A of beam(pairs)\n-\t(Case 2) For generalization Case 2 compared to Case 1, evaluation results from 2 source show 15%~40% degradation, evaluation results from 5 sources show 30%~50% degradation, evaluation results from 2 sources show about 60% degradation, evaluation results from 1 source show about 70% degradation, for Top-1 beam prediction accuracy for DL Tx beam and/or beam pair prediction. 1 source shows BM-AI can perform worse than the conventional approachâs with mismatched set A design.\n-\tWherein 1 source show 15%-40% degradation for Top-1 beam accuracy assuming same DL Tx codebook (pointing angles) and different beam width, and 50%-60% degradation for Top-1 beam accuracy assuming different DL Tx codebooks (pointing angles) and same beam width for Tx beam and pair prediction\n-\twherein 2 sources assumed different Tx beam codebooks have different horizontal angles but the same gNB array/beamwidth and the results show about 56% degradation for Top-1 beam prediction accuracy with same training data size for DL Tx beam prediction.\n-\twherein 1 source assumed different Tx beam codebooks have different horizonal beam angles and the different gNB array/beamwidth and the results show about 57% degradation for Top-1 beam prediction accuracy with same training data size for beam pair prediction.\n-\twherein 2 sources assumed different Tx beam codebooks have the same beam pointing angles but have different beamwidth (due to different gNB array sizes) and the results show about 30% degradation for Top-1 beam prediction accuracy.\n-\tevaluation results from 1 source show performance degradation in terms of the top-1 beam accuracy from 73.9% to 34.2% at 4 beams in Set B, from 88.6% to 63.9% at 8 beams in set B, from 97.8% to 88.4% at 16 beams in set B.\n-\tevaluation results from 5 sources show better performance than non-AI baseline option 2 (based on the measurements from Set B of beams). However, evaluation results from 5 sources similar or even worse performance than non-AI baseline option 2 (based on the measurements from Set B of beams).\n-\t(Case 2A) For generalization Case 2A compared to Case 1, evaluation results from 1 source show 16%~20% for Top-1 beam prediction accuracy for DL Tx beam prediction with the assumption that different Tx beam codebooks have different horizontal angles but the same gNB array/beamwidth.\n-\t(Case 3) For generalization Case 3 compared to Case 1, the evaluation results from 6 sources show less than 5% degradation, and the evaluation results from 2 sources show 10%~15% degradation for Top-1 beam prediction accuracy compared to Case 1. Evaluation results from 1 source show there is 2%~32% degradation for Top-1 beam with 1 dB margin.\n-\tWherein, 1 source assumes different beamwidth and double training data size\n-\tFor Tx-Rx beam pair prediction only, various UE parameters: different UE codebooks, and/or different UE antenna array dimensions\n-\tNote: different UE Rx beam codebooks will result in various Set A of beam pairs for beam pair prediction\n-\t(Case 2) For generalization Case 2 compared to Case 1, evaluation results from 4 sources show large degradation (i.e., >40%) with different number of elements (different beamwidth) and different UE codebooks for Top-1 beam prediction accuracy.\n-\twherein, evaluation results from 1 source show 12% and 52% degradation with UE codebook is different for Top-1 beam prediction accuracy with 1x4 Rx beam and with 2x2 Rx beam pattern and 1x4 Rx beam respectively.\n-\t(Case 3) For generalization Case 3 compared to Case 1, evaluation results from 1 source show less than 5% degradation, and evaluation results from 1 source show 16%~26% degradation for Top-1 beam prediction accuracy, with different number of elements and/or different number of UE Rx\n-\tVarious Set B of beams: different fixed Set B pattern\n-\t(Case 2) For generalization Case 2 compared to Case 1, evaluation results from 9 sources show large degradation with different Set B pattern (different number and/or same number different Set B pattern) for DL Tx beam prediction and/or beam pair prediction.\n-\tevaluation results from 1 source show 13~21% degradation with same evenly spaced in beam(pair) ID dimension without providing beam ID information as AI/ML inputs.\n-\tevaluation results from 1 source show 20%~40% degradation with different number of beams in Set B for BMCase-2\n-\tevaluation results from 1 source show the AI-BM performance can be worse than the conventional approachâs with mismatched set B design.\n-\t(Case 3) For generalization Case 3 compared to Case 1,\n-\tevaluation results from 5 sources show less than or about 5% degradation.\n-\tevaluation results from 1 source show 14% degradation without providing beam ID information as AI/ML inputs.\n-\tevaluation results from 1 source show 3%~10% degradation with different number of beams in Set B for BMCase-2\n-\tevaluation results from 1 source show 8-10% degradation with different Set B pattern.\n(C) For BMCase-2, various UE mobility, different companies reported different observation for Case 2. In Case 2, AI/ML still can provide comparable or worse performance than non-AI baseline option 2 (based on the measurements from Set B of beams)]\n-\tFor various UE mobility for BMCase-2: 30km/h / 60km/h / 90km/h 120km/h\n-\t(Case 2) For generalization Case 2 compared to Case 1,\n-\tevaluation results from 3 sources show significant degradation i.e., >30% in terms of Top 1 prediction accuracy, and evaluation results from 1 source show about 19%~49% degradation for prediction time 160ms~800ms.\n-\tevaluation results from 4 sources show >6% performance degradation in terms of Top 1 prediction accuracy and evaluation results from 3 sources show about 10~18% degradation\n-\t(Case 3) For generalization Case 3 compared to Case 1, for Top-1 beam prediction accuracy\n-\tthe evaluation results from 3 sources show 3~7% degradation for Top-1 beam prediction accuracy\n-\tthe evaluation results from 1 source show 8~14% degradation for Top-1 beam prediction accuracy\n-\tthe evaluation results from 1 source show <17% degradation for Top-1 beam prediction accuracy by training with same size of training data mixed of 30km/h, 60km/h and 90km/h.\n-\tthe evaluation results from 1 source show about 1% degradation for Top-1 beam prediction accuracy for 30km/h and 60km/h, and show about 4%/8% degradation for Top-1 beam prediction accuracy for 30km/h and 90km/h.\n-\tthe evaluation results from 1 source show comparable performance for Top-1 beam prediction accuracy for 30km/h and 60km/h\n-\tthe evaluation results from 3 sources show slightly better (1%~2% for Top-1 beam prediction accuracy) performance compared to Case 1 with double or triple size of training data for DL Tx beam prediction.\nSummary of evaluations and results for BM-Case1\nFor BM-Case1 when Set B is a subset of Set A or when Set B is different than Set A, without UE rotation, AI/ML can achieve good performance with measurements of fixed Set B that is 1/4 or 1/8 of Set A of beam measured with best Rx beam for DL Tx beam prediction, and with measurements of fixed Set B that is 1/4 or 1/8 or 1/16 of Set A for beam pair prediction. In addition, based on the evaluation results from 2 or 3 sources, for BM-Case1 DL Tx beam prediction, with 1/4 or 1/8 measurement/RS overhead, 96%~99% or 85%~98% of UE average throughput and 95%~97% or 70%~84% of UE 5%ile throughput of non-AI baseline option 1 (exhaustive search over Set A beams) can be achieved according to the predicted beam from AI/ML. Note that, ideal measurements are assumed in the evaluations (in clause 6.3.2.1): beam could be measured regardless their SNR, no measurement error, and measurements obtained in a single-time instance (within a channel-coherence time interval), no quantization and no constraint on UCI payload (for NW-side model).\nWith some realistic consideration (in clause 6.3.2.3):\n-\tExisting quantization granularity of L1-RSRP causes a minor loss in beam prediction accuracy compared to unquantized L1-RSRPs of beams in Set B at least for BM-Case1 for inference of DL Tx beam prediction.\n-\tMeasurement errors degrade the beam prediction performance with AI/ML, while measurement errors also degrade the performance with non-AI baseline (both option 1 and option 2).\n-\tFor DL Tx beam prediction, with the measurements from quasi-optimal Rx beam, some performance degradation (e.g., 2% to up to12% Top-1 beam prediction accuracy loss based on most of results) is observed comparing to with measurements from best Rx beam. If the measurements are from random Rx beam, large performance degradation is observed.\nIn addition, comparing with fixed Set B (Opt 1), in case of with Set B changed among pre-configured patterns (Opt 2B), some performance degradation (e.g., no more than or about 10% Top-1 beam prediction accuracy loss based on most of results) is observed; in case of with Set B randomly changed in Set A of beams (Opt 2C), large degradation (e.g, 20%~50% Top-1 beam prediction accuracy loss based on most of results) is observed. With reduced number of measurements of a fixed set of beams (Set C) as inputs of AI/ML (Opt 2D), some performance degradation (e.g., <10% Top-1 beam prediction accuracy loss based on most of results) is observed, comparing with using all measurements from Set C, in the meanwhile, UCI reporting overhead for inference inputs can be reduced (e.g., 1/2 to 7/8 UCI reporting overhead reduction) comparing with reporting all measurements of the fixed beam Set C.\nMoreover, the performance with different label options has been evaluated which may lead to different data collection overhead for training (for both BM-Case1 and BM-Case2).\nSummary of evaluations and result for BM-Case2\nEvaluation results for BM-Case2 when Set B= Set A for DL Tx beam prediction with the measurements from the best Rx beam and beam pair prediction are summarized in Table 6.3.2.5-1 and Table 6.3.2.5-2, without considering generalization aspects.\nTable 6.3.2.5-1: Summary of the evaluation results for BM-Case2 \nwhen Set B=Set A for DL Tx beam prediction\n\nTable 6.3.2.5-2: Summary of the evaluation results for BM-Case2 \nwhen Set B=Set A for beam pair prediction\n\nFor BM-Case2 when Set B is a subset of Set A for DL Tx beam prediction with the measurements from the best Rx beam, without considering generalization aspects, AI/ML can achieve good prediction accuracy with 1/2, 1/3, 1/4, 1/8 RS overhead in spatial domain, for the case Set B is fixed or variable with pre-configured patterns of beams with or without UE rotation. More RS/measurements overhead reduction can be achieved considering overhead reduction in time domain.\nFor BM-Case2 when Set B is a subset of Set A for beam pair prediction, without considering generalization aspects\n-\twithout UE rotation, AI/ML can achieve good prediction accuracy with 1/4, 1/8, 1/16 RS overhead in spatial domain, for the case Set B is fixed or variable with pre-configured patterns of beams.\n-\twith UE rotation, from 2 sources, AI/ML can provide 15% or 44% prediction accuracy gain with 1/4, 1/16 RS overhead in spatial domain comparing with non-AI baseline (option 2), for the case Set B is fixed or variable with pre-configured patterns of beams. However, the Top-1 beam prediction accuracy may or may not be good enough.\n-\tMore RS/measurements overhead reduction can be achieved considering overhead reduction in time domain.\nNote that, ideal measurements are assumed in the above evaluations (for BM-Case2): beam could be measured regardless their SNR, no measurement error, no quantization and no constraint on UCI payload (for NW-side model). With measurement error, quantization or measurements results from quasi-optimal Rx beam for DL Tx beam prediction, similar observations are observed (for some cases) or expected as for BM-Case1.\nReduced measurement overhead can reduce measurement latency for beam prediction in some configurations.\nSummary of evaluations and results for generalization\nDifferent location of AI/ML model (e.g., NW side model, or UE side model) may have different generalization requirements:\nFor NW side model,\n-\tgeneralization performance with various gNB settings and various Set B of beams may not be an issue since the gNB settings are most likely to be fixed or limited to a given gNB (at least seen by AI/ML before)\n-\tfor DL Tx beam prediction, generalization performance with various unseen UE parameters is acceptable at least with the measurement from the best or fixed Rx beam.\n-\tTx-Rx beam pair prediction, generalization performance with various UE parameters, i.e., different number of beams in a seen UE codebook when inference using a subset of Rx beams of training is acceptable.\n-\tfor Tx-Rx beam pair prediction, the significant generalization performance degradation with unseen various UE parameters (i.e., different UE codebooks, and/or different UE antenna array dimensions) can be improved to achieve less than 5% degradation (2 sources) and 16%~26% degradation (1 source) in terms of Top-1 beam prediction accuracy with the model training with mixed data compared to generalization performance Case 1.\n-\tNote: with same amount of data for training for different scenarios for Case 3\n-\tAlternatively, AI/ML model can be trained for different scenarios and rely on model switching based on applicable scenario which would improve generalization performance.\nFor UE side model,\n-\tgeneralization performance with unseen various UE parameters may not be an issue\n-\tthe significant generalization performance degradation with unseen various gNB setting (i.e., different gNB antenna array dimensions, and/or DL Tx beam codebook) or unseen various Set B of beam(pairs) can be improved to achieve\n-\t(for gNB setting) less than 5% (6 sources), 10%~15% (2 sources), and 2%~32% (1 source) degradation in terms of Top-1 beam prediction accuracy compared with the model training with mixed data to generalization performance Case 1, and 16%~20% (1 source) degradation in terms of Top-1 beam prediction accuracy compared with the model finetune to generalization performance Case 1.\n-\t(for Set B of beam(pairs)) less than 10% (all 7 sources) degradation in terms of Top-1 beam prediction accuracy compared with the model training with mixed data to generalization performance Case 1.\n-\tNote: For gNB setting, generalization performance Case 3 may depend on how different the gNB settings are across training and inference\n-\tNote: with same amount of data for training for different scenarios for Case 3\n-\tAlternatively, AI/ML model can be trained for different scenarios and rely on model switching based on applicable scenario which would improve generalization performance.\nAt least for BMCase-1, AI/ML (without considering model switching) has some performance degradation with some unseen scenarios including:\n-\tFor DL Tx beam prediction,\n-\tdeployment scenarios: different ISD, UMi/UMa (at least with same down tilt)\n-\tvarious outdoor/indoor UE distributions\n-\tvarious UE parameters: different UE codebooks, and different UE antenna array dimensions.\n-\tNote: at least with the measurement from the best Rx beam.\n-\tFor beam pair prediction\n-\tdeployment scenarios: different ISD, UMi/UMa (at least with same down tilt)\n-\tvarious outdoor/indoor UE distributions\n-\tvarious UE parameters: when inference using a subset of Rx beams of training.\nHowever, the AI/ML (without considering model switching) has significant performance degradation with some other unseen scenarios, including:\n-\tFor DL Tx beam prediction,\n-\tdeployment scenarios: UMi/UMa (at least with the assumption of different ISD, antenna height, down tilt and NLOS probability)\n-\tvarious gNB setting: different gNB antenna array dimensions, and DL Tx beam codebook\n-\tvarious Set B patterns\n-\tvarious Set A patterns\n-\tFor beam pair prediction\n-\tvarious UE parameters: different UE codebooks, and different UE antenna array dimensions\n-\tdeployment scenarios: with the assumption of different ISD, antenna height, down tilt and NLOS probability\n-\tvarious gNB setting: different gNB antenna array dimensions, and DL Tx beam codebook\n-\tvarious Set B patterns\n-\tvarious Set A patterns\nIn order to let AI/ML model see the data from a new setting which causes performance loss, the AI/ML model can be trained with mixed data or finetuned with the data from the new setting to improve the generalization performance. Alternatively, AI/ML model can be trained for different scenarios and rely on model switching based on applicable scenario which would improve generalization performance.\nFor BMCase-2, for variable UE mobility, the collected data for training can be mixed and the generalization performance with mixed UE speeds is acceptable.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.3.2-1: AI/ML model complexity/computation complexity \nused in the evaluations for AI/ML in beam management",
                                    "table number": 10,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 6.3.2.5-1: Summary of the evaluation results for BM-Case2 \nwhen Set B=Set A for DL Tx beam prediction",
                                    "table number": 11,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 6.3.2.5-2: Summary of the evaluation results for BM-Case2 \nwhen Set B=Set A for beam pair prediction",
                                    "table number": 12,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "6.4\tPositioning accuracy enhancements",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.4.1\tEvaluation assumptions, methodology and KPIs",
                            "text_content": "For AI/ML-based positioning evaluation, RAN1 does not attempt to define any common AI/ML model as a baseline. In the evaluation, some results use UE measurement information as model input, other results use gNB measurement information as model input, and they are not distinguished for summarizing the results.\nFor AI/ML based positioning, the following methods are evaluated.\n(1)\tDirect AI/ML positioning, see an example illustrated in Figure 6.4.1-1.\n(2)\tAssisted AI/ML positioning.\n(a)\tAssisted AI/ML positioning with multi-TRP construction, see an example illustrated in Figure 6.4.1-2.\n(b)\tAssisted positioning with single-TRP construction and one model for N TRPs, see an example illustrated in Figure 6.4.1-3.\n(c)\tAssisted positioning with single-TRP construction and N models for N TRPs, see an example illustrated in Figure 6.4.1-4.\n\nThe figure depicts a direct AI/ML positioning system, showcasing a 3D representation of the positioning process. It illustrates the use of AI algorithms to determine the position of a target, with a focus on the direct approach. The system includes a 3D model of the environment, a 2D map of the target's location, and AI algorithms to analyze the data and make accurate predictions. The figure highlights the importance of AI in positioning systems, emphasizing its role in improving accuracy and efficiency.\nFigure 6.4.1-1: Direct AI/ML positioning\n\nThe figure depicts a multi-TRP construction, which is a method used in assisted positioning to improve the accuracy of location determination. The construction consists of multiple TRPs (Transmission Repeaters) that are interconnected to provide redundancy and enhance the signal strength. This method is particularly useful in scenarios where the signal strength is not sufficient to determine the exact location of the receiver.\nFigure 6.4.1-2: Assisted positioning with multi-TRP construction\n\nThe figure depicts a simplified model of assisted positioning with single-TRP construction, illustrating the concept of using a single transmitter (TRP) to provide assistance to a receiver. The figure includes a single transmitter (TRP) and a receiver (R), with the TRP transmitting a signal to the receiver. The figure also shows the receiver receiving the signal from the TRP. The model is based on the concept of using a single transmitter to provide assistance to a receiver, which is a simplified version of the more complex assisted positioning systems.\nFigure 6.4.1-3: Assisted positioning with single-TRP construction, and one model for N TRPs.\n\nThe figure depicts a simplified representation of assisted positioning with single-TRP construction and N models for N TRPs. It illustrates the process of constructing a single-TRP system, which is a key component in assisted positioning systems. The figure includes N models for N TRPs, which are used to calculate the position of each TRP in the system. The figure also shows the construction of a single-TRP system, with each TRP represented by a single line. The N models are used to calculate the position of each TRP, and the figure shows the resulting positions of the TRPs in the system.\nFigure 6.4.1-4: Assisted positioning with single-TRP construction, and N models for N TRPs.\nKPIs:\n-\tFor all scenarios and use cases, the main KPI is the CDF percentiles of horizonal accuracy\n-\tThe CDF percentiles to analyse are: 90% (baseline) and {50%, 67%, 80%} (optional)\n-\tVertical accuracy can be optionally reported\n-\tTarget positioning requirements for horizonal accuracy and vertical accuracy are not defined for AI/ML-based positioning evaluation\n-\tModel complexity, e.g., number of model parameters, and computational complexity, e.g., FLOPs\n-\tModel complexity is reported via the metric of \"number of model parameters\". Note: if complex value is used in modelling process, the number of the model parameters is doubled, which is also applicable for other AIs of AI/ML.\n- \tCompanies describe how their computational complexity values are obtained. It is out of 3GPP scope to consider computational complexity values that have platform-dependency and/or use implementation (hardware and software) optimization solutions.\n-\tFor AI/ML assisted positioning, an intermediate performance metric of model output\nModel generalization:\nTo investigate the model generalization capability, at least the following aspect(s) are considered for the evaluation for AI/ML based positioning:\n-\tDifferent drops: Training dataset from drops {A0, A1,â¦, AN-1}, test dataset from unseen drop(s) (i.e., different drop(s) than any in {A0, A1,â¦, AN-1}). Here Nâ¥1.\n-\tClutter parameters, e.g., training dataset from one clutter parameter (e.g., {40%, 2m, 2m}), test dataset from a different clutter parameter (e.g., {60%, 6m, 2m});\n-\tNetwork synchronization error, e.g., training dataset without network synchronization error, test dataset with network synchronization error;\n-\tUE/gNB RX and TX timing error;\n-\tThe baseline non-AI/ML method may enable the Rel-17 enhancement features (e.g., UE Rx TEG, UE RxTx TEG).\n-\tInF scenarios, e.g., training dataset from one InF scenario (e.g., InF-DH), test dataset from a different InF scenario (e.g., InF-HH)\n-\tThe following issues related to measurements on the positioning accuracy of the AI/ML model. The simulation assumptions reflecting these issues are up to companies.\n-\tSNR mismatch (i.e., SNR when training data are collected is different from SNR when model inference is performed).\n-\tTime varying changes (e.g., mobility of clutter objects in the environment)\n-\tChannel estimation error\nFor both direct AI/ML approach and AI/ML assisted approach, for a given AI/ML model design (e.g., input, output, single-TRP vs multi-TRP), identify the generalization aspects where model fine-tuning/mixed training dataset/model switching is necessary.\nEvaluation assumptions:\nThe IIoT indoor factory (InF) scenario is a prioritized scenario for evaluation of AI/ML based positioning. Specifically, InF-DH sub-scenario is prioritized for FR1 and FR2.\nReuse the common scenario parameters defined in Table 6-1 of TR 38.857. For evaluation of InF-DH scenario, the parameters are modified from TR 38.857 Table 6.1-1 as shown in Table 6.4.1-1. The parameters in the table are applicable to InF-DH at least. If other InF sub-scenario is evaluated in addition to InF-DH, some parameters in Table 6-5 may be updated. If an InF scenario different from InF-DH is evaluated for the model generalization capability, the selected parameters (e.g., clutter parameters) are compliant with TR 38.901 Table 7.2-4 (Evaluation parameters for InF). Note: In TR 38.857 Table 6.1-1 (Parameters common to InF scenarios), InF-SH scenario uses the clutter parameter {20%, 2m, 10m} which is compliant with TR 38.901.\nTable 6.4.1-1: Parameters common to InF scenario (Modified from TR 38.857 Table 6.1-1) for AI/ML based positioning evaluations\n\nFor the evaluation of AI/ML based positioning, the study of model input due to different number of TRPs include the following approaches. Proponents of each approach are to provide analysis for model performance, signalling overhead (including training data collection and model inference), model complexity and computational complexity.\n-\tApproach 1: Model input size stays constant as NTRP=18. The number of TRPs (NâTRP) that provide measurements to model input varies. When NâTRP < NTRP, the remaining (NTRP - NâTRP) TRPs do not provide measurements to model input, i.e., measurement value is set such that the (NTRP  NâTRP) TRPs do not affect model output.\n-\tApproach 1-A. The set of TRPs (NâTRP) that provide measurements is fixed.\n-\tApproach 1-B. The set of TRPs (NâTRP) that provide measurements can change dynamically.\n-\tNote: for Approach 1, one model is provided to cover the entire evaluation area.\n-\tApproach 2: The TRP dimension of model input is equal to the number of TRPs (NâTRP) that provide measurements as model input. When NâTRP < NTRP, the remaining (NTRP - NâTRP) TRPs are ignored by the given model.\n-\tApproach 2-A. The set of active TRPs (NâTRP) that provide measurements is fixed.\n-\tFor both Approach 1-A and 2-A: one model can be provided to cover the entire evaluation area, which is equivalent to deploying NâTRP TRPs in the evaluation area for positioning if ignoring the potential inference from the remaining (18 - NâTRP) TRPs.\n-\tApproach 2-B: The set of active TRPs (NâTRP) that provide measurements can change dynamically.\n-\tFor Approach 2-B, one model is developed to handle various patterns of active TRPs.\n-\tFor Approach 2, if Nmodel (Nmodel >1) models are provided to cover the entire evaluation area, the total model complexity is the summation of the Nmodel models.\nIn the evaluation of AI/ML based positioning, if NâTRP<18, the set of NâTRP TRPs that provide measurements to model input of an AI/ML model are reported using the TRP indices shown in Figure 6.4.1-5.\nThe figure depicts a TRP layout and its indices for evaluating AI/ML-based positioning systems. The layout includes various components such as the TRP, TRP-1, TRP-2, TRP-3, TRP-4, TRP-5, TRP-6, TRP-7, TRP-8, TRP-9, TRP-10, TRP-11, TRP-12, TRP-13, TRP-14, TRP-15, TRP-16, TRP-17, TRP-18, TRP-19, TRP-20, TRP-21, TRP-22, TRP-23, TRP-24, TRP-25, TRP-26, TRP-27, TRP-28, TRP-29, TRP-30, TRP-31, TRP-32, TRP-33, TRP-34, TRP-35, TRP-36, TRP-37, TRP-38, TRP-39,\nFigure 6.4.1-5: TRP layout and their indices for the evaluation of AI/ML based positioning.\nFor the evaluation of AI/ML based positioning method, the measurement size and signalling overhead for the model input are reported.\nImpact from implementation imperfections is to be studied. Further, how AI/ML positioning accuracy is affected by user density/size of the training dataset is to be also studied. Note: details of user density/size of training dataset to be reported in the evaluation.\nModel input, model output:\nFor the model input used in evaluations of AI/ML based positioning, if time-domain channel impulse response (CIR) or power delay profile (PDP) is used as model input in the evaluation, companies report the input dimension NTRP * Nport * Nt, where NTRP is the number of TRPs, Nport is the number of transmit/receive antenna port pairs, Nt is the number of consecutive time domain samples. If Nât (Nât < Nt) samples with the strongest power are selected as model input, with remaining (Nt â Nât) time domain samples set to zero, then companies report value Nât in addition to Nt. It is also assumed that timing info for the Nât samples need to be provided as model input.\nFor evaluation of AI/ML based positioning, when time domain samples are used as model input and sub-sampling is applied, the selection of N't measurements is based on the strongest power, unless explicitly stated otherwise. When sub-sampling is applied the N't measurement are not necessarily consecutive in time.\n-\tTraining dataset and test dataset use the same measurement selection method (e.g., strongest power) unless explicitly stated otherwise.\n-\tOther selection methodologies for N't measurements are also evaluated, and are not precluded.\nFor evaluations, companies used the following values for sampling period:\n-\t16 Sources used the following sampling period:\n-\tSampling period = 1/(Nf Ãâf). For FR1, sampling period = 1/(4096Ã30)=8.14 (ns), where Nf =4096 according to 38.211, and âf =30 kHz is the subcarrier spacing.\n-\t1 Source used: sampling period = 4.069 ns\nIf the model input is the CIR, then each input value of the CIR is a complex number, i.e., it contains two real values, either {real, imaginary} or {magnitude, phase}. If the model input is the PDP, then each input value of the PDP is a real value. Optionally companies can use delay profile (DP) as a type of information for model input. DP is a degenerated version of PDP, where the path power is not provided.\nNote: CIR and PDP may have different dimensions. Companies to provide details on their assumption on how PDP is constructed and how (if applicable) it is mapped to Nt samples.\nFor evaluation of AI/ML based positioning, when timing information is included in model input (e.g., in CIR/PDP/DP), training dataset and test dataset use the same timing format (i.e., both are absolute time, or both are relative time) unless explicitly stated otherwise.\nFor evaluation of AI/ML based positioning with multipath measurement for model input,\n-\tFor a given set of parameters (N'TRP, Nt, N't, Nport)\n-\tCIR has the largest measurement size, where CIR is composed of a list of measurements where each measurement contains the information of: (a) delay, (b) power and (c) phase.\n-\tPDP has smaller measurement size than CIR, where PDP is composed of a list of measurements where each measurement contains the information of: (a) delay and (b) power.\n-\tDP has the smallest measurement size, where DP is composed of a list of measurements where each measurement contains the information of: (a) delay.\n-\tFor each model input type (CIR, PDP, DP)\n-\tThe measurement size increases (approximately) linearly as N'TRP increases, where N'TRP is the number of active TRPs that provide measurements for the positioning.\n-\tThe measurement size increases (approximately) linearly as Nport increases, where Nport is the number of transmit/receive antenna port pairs that provide measurements for the positioning.\n-\tIf N't (N't < Nt) measurements are selected as model input, measurement size for model input increases (approximately) linearly with N't;\n-\tFor model input type CIR and PDP, if the full set of Nt measurements in time domain is used as model input, measurement size for model input increases (approximately) linearly with Nt;\n-\tNote: if DP is used as model input, DP does not use full set of Nt measurements in time domain (i.e., N't < Nt always).\n-\tNote: for Case 2b and 3b, measurement size of model input has impact to signalling overhead for model inference, data collection, and monitoring.\n-\tNote: There are trade-offs between measurement size / signalling overhead and positioning accuracy when using different sets of parameters (N'TRP, Nt, N't, Nport).\nFor both the direct AI/ML positioning and AI/ML assisted positioning, the model input is studied, considering the trade-off among model performance, model complexity and computational complexity:\n-\tThe type of information to use as model input. The candidates include at least: time-domain CIR, PDP.\n-\tThe dimension of model input in terms of NTRP, Nt, and Ntâ.\n-\tNote: For the direct AI/ML positioning, model input size has impact to signalling overhead for model inference\nAt least for model inference of AI/ML assisted positioning, evaluate and report the AI/ML model output, including:\na)\tthe type of information (e.g., ToA, RSTD, AoD, AoA, LOS/NLOS indicator) to use as model output,\nb)\tsoft information vs hard information,\nc)\twhether the model output can reuse existing measurement report (e.g., NRPPa, LPP).\nLabels:\nThe performance impact from availability of the ground-truth labels (i.e., some training data may not have ground-truth labels) is to be studied. The learning algorithm (e.g., supervised learning, semi-supervised learning) is to be reported by participating companies and, when providing evaluation results, data labelling details need to be described, including:\n-\tMeaning of the label (e.g., UE coordinates; binary identifier of LOS/NLOS; ToA)\n-\tPercentage of training data without label, if incomplete labelling is considered in the evaluation\n-\tImperfection of the ground-truth labels, if any\nWhether, and if so how, an entity can be used to obtain ground-truth label and/or other training data is to be studied.\nFor direct AI/ML positioning, the impact of labelling error to positioning accuracy is studied considering:\n-\tThe ground-truth label error in each dimension of x-axis and y-axis can be modelled as a truncated Gaussian distribution with zero mean and standard deviation of L meters, with truncation of the distribution to the [-2*L, 2*L] range. Value L is up to sources.\nFor AI/ML assisted positioning with TOA as model output, study the impact of labelling error to TOA accuracy and/or positioning accuracy.\n-\tThe ground-truth label error of TOA is calculated based on location error. The location error in each dimension of x-axis and y-axis can be modelled as a truncated Gaussian distribution with zero mean and standard deviation of L meters, with truncation of the distribution to the [-2*L, 2*L] range.\n-\tValue L is up to sources.\n-\tOther models of labelling error are not precluded\n-\tOther timing information, e.g., RSTD, as model output is not precluded.\nFor AI/ML assisted positioning with LOS/NLOS indicator as model output, study the impact of labelling error to LOS/NLOS indicator accuracy and/or positioning accuracy.\n-\tThe ground-truth label error of LOS/NLOS indicator can be modelled as m% LOS label error and n% NLOS label error. Value m and n are up to sources.\n- \tm%=FN/NLOS is false negative rate of the training data label, where FN (False Negative) is the number of actual LOS links which are incorrectly labelled as NLOS, and NLOS is the total number of actual LOS links;\n-\tn%=FP/NNLOS is the false positive rate of the training data label, FP (False Positive) is the number of actual NLOS links which are incorrectly labelled as LOS, and NNLOS is the total number of actual NLOS links\n-\tCompanies consider at least hard-value LOS/NLOS indicator as model output.\nTraining dataset:\nSynthetic dataset generated according to the statistical channel models in TR 38.901 is used for model training, validation, and testing. The dataset is generated by a system level simulator based on 3GPP simulation methodology.\nAs a starting point, the training, validation and testing dataset are from the same large-scale and small-scale propagation parameters setting. Subsequent evaluations can study the performance when the training dataset and testing dataset are from different settings.\nDetails of the training dataset generation are to be reported, including:\n-\tThe size of training dataset, e.g., the total number of UEs in the evaluation area for generating training dataset;\n-\tThe distribution of UE location for generating the training dataset may be one of the following:\n-\tOption 1: grid distribution, i.e., one training data is collected at the center of one small square grid, where, for example, the width of the square grid can be 0.25/0.5/1.0 m.\n-\tOption 2: uniform distribution, i.e., the UE location is randomly and uniformly distributed in the evaluation area.\nSub-use case specific:\nFor AI/ML-assisted positioning, companies report which construction is applied in their evaluation:\na)\tSingle-TRP construction: the input of the ML model is the channel measurement between the target UE and a single TRP, and the output of the ML model is for the same pair of UE and TRP.\nb)\tMulti-TRP construction: the input of the ML model contains N sets of channel measurements between the target UE and N (N>1) TRPs, and the output of the ML model contains N sets of values, one for each of the N TRPs.\nNotes:\tFor a measurement (e.g., RSTD) which is a relative value between a given TRP and a reference TRP, the TRP in \"single-TRP\" and \"multi-TRP\" refers to the given TRP only. For single-TRP construction, companies report whether they consider same model for all TRPs or N different models for TRPs.\nWhen single-TRP construction is used for the AI/ML model, companies report at least the AI/ML complexity (Model complexity, Computation complexity) for N TRPs, which are used to determine the position of a target UE considering the various constructions in Table 6.4.1-2 below.\nTable 6.4.1-2: Model complexity and computational complexity to support N TRPs for a target UE\nNote:\tThe reported model complexity above is intended for inference and may not be directly applicable to complexity of other LCM aspects\nFor evaluation of AI/ML assisted positioning, the following intermediate performance metrics are used:\n-\tLOS classification accuracy, if the model output includes LOS/NLOS indicator of hard values, where the LOS/NLOS indicator is generated for a link between UE and TRP;\n-\tTiming estimation accuracy (expressed in meters), if the model output includes timing estimation (e.g., ToA, RSTD).\n-\tAngle estimation accuracy (in degrees), if the model output includes angle estimation (e.g., AoA, AoD).\n-\tCompanies provide info on how LOS classification accuracy and timing/angle estimation accuracy are estimated, if the ML output is a soft value that represents a probability distribution (e.g., probability of LOS, probability of timing, probability of angle, mean and variance of timing/angle, etc.)\nModel monitoring:\nFor AI/ML assisted approach, the performance of model monitoring metrics is studied at least where the metrics are obtained from inference accuracy of model output (i.e., label-based model monitoring methods). Further, the performance of label-free model monitoring methods, which do not require ground-truth label (or its approximation) for model monitoring, is to be studied.\nFor direct AI/ML positioning, the performance of model monitoring methods is studied, including:\n-\tLabel based methods, where ground-truth label (or its approximation) is provided for monitoring the accuracy of model output.\n-\tLabel-free methods, where model monitoring does not require ground-truth label (or its approximation).\nModel Fine-tuning:\nFor evaluation of the potential performance benefits of model finetuning, training dataset setting (e.g., training dataset size necessary for performing model finetuning) and horizontal positioning accuracy (in meters) before and after model finetuning, are to be reported.\nFor both direct and AI/ML assisted positioning methods, investigate at least the impact of the amount of fine-tuning data on the positioning accuracy of the fine-tuned model. The fine-tuning data is the training dataset from the target deployment scenario.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.4.1-1: Parameters common to InF scenario (Modified from TR 38.857 Table 6.1-1) for AI/ML based positioning evaluations",
                                    "table number": 13,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 6.4.1-2: Model complexity and computational complexity to support N TRPs for a target UE",
                                    "table number": 14,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "6.4.2\tPerformance results",
                            "text_content": "POS_Table 1 through POS_Table 5 in attached Spreadsheets for Positioning accuracy enhancements evaluations present the performance results for:\n-\tPOS_Table 1. Evaluation results for supervised learning without generalization considerations (i.e., same setting for training and testing).\n-\tPOS_Table 2. Evaluation results for supervised learning with generalization considerations (i.e., different setting for training and testing).\n-\tPOS_Table 3. Evaluation results for fine-tuning to handle various generalization aspects\n-\tPOS_Table 4. Evaluation results for supervised learning with label error\n-\tPOS_Table 5. Evaluation results for semi-supervised learning\nObservations:\nDirect AI/ML positioning can significantly improve the positioning accuracy compared to existing RAT-dependent positioning methods when the generalization aspects are not considered.\nFor InF-DH with clutter parameter setting {60%, 6m, 2m}, evaluation results indicate that the direct AI/ML positioning can achieve horizontal positioning accuracy of <1m at CDF=90%, as compared to >15m for conventional positioning methods.\nBased on evaluation results of 3 sources, direct AI/ML positioning and AI/ML assisted positioning can achieve comparable performance when simulation assumptions and parameters (e.g., clutter parameter, model input type, model input size, training dataset size, model complexity) are held the same, Edirect = (0.57~1.14)  Eassisted, where\n-\tEassisted (meters) is the horizontal positioning accuracy at CDF=90% of AI/ML assisted positioning with multi-TRP construction with timing information as model output,\n-\tEdirect (meters) is the horizontal positioning accuracy at CDF=90% of direct AI/ML positioning\nAI/ML assisted positioning can significantly improve the positioning accuracy compared to existing RAT-dependent positioning methods when the generalization aspects are not considered.\n-\tFor InF-DH with clutter parameter setting {40%, 2m, 2m}, evaluation results indicate that the AI/ML assisted positioning can achieve horizontal positioning accuracy of <0.4m at CDF=90%, as compared to >9m for conventional positioning method.\n-\tFor InF-DH with clutter parameter setting {60%, 6m, 2m}, evaluation results indicate that the AI/ML assisted positioning can achieve horizontal positioning accuracy of <1m at CDF=90%, as compared to >15m for conventional positioning method.\nModel monitoring\nFor AI/ML assisted positioning, evaluation results have been provided by sources for label-based model monitoring methods. With TOA and/or LOS/NLOS indicator as model output, the estimated ground-truth label (i.e., TOA and/or LOS/NLOS indicator) is provided by the location estimation from the associated conventional positioning method. The associated conventional positioning method refers to the method which utilizes the AI/ML model output to determine target UE location.\nFor both direct AI/ML and AI/ML assisted positioning, evaluation results have been provided by sources to demonstrate the feasibility of label-free model monitoring methods.\nModel complexity and computational complexity\nFor AI/ML based positioning method, companies have submitted evaluation results to show that for their evaluated cases, for a given companyâs model design, a lower complexity (model complexity and computational complexity) model can still achieve acceptable positioning accuracy (e.g., <1m), albeit degraded, when compared to a model with higher AI/ML complexity.\nIn Figure 6.4.2-1 below, the model inference complexity reported by companies for the positioning use case is shown, including (a) on the x-axis: model complexity in number of real parameters (millions) and (b) on the y-axis: computational complexity in FLOPs (millions). Figure 6.4.2-1 shows the range of complexity for the following schemes: (1) direct positioning; (2) assisted positioning with multi-TRP; (3) assisted positioning with single-TRP and one-model for N TRPs; and (4) assisted positioning with single-TRP and N models for N TRPs. For details of the complexity values corresponding to Figure 6.4.2-1, please see POS_Table 1.\nFor the three schemes of AI/ML assisted positioning, the complexity is calculated according to Table 6.4.1-2. Both model complexity and computational complexity values are as reported by participating companies. There is no effort to align the procedure across companies on how the complexity values are obtained. In addition, optimizing AI/ML complexity (i.e., model complexity and computational complexity) is out of scope of the study item.\nThe figure depicts a model complexity and computational complexity matrix for four AI/ML-based positioning schemes, illustrating the trade-offs between accuracy, computational resources, and computational complexity.\nFigure 6.4.2-1: Model complexity and computational complexity for four schemes of AI/ML based positioning.\nObservations:\nFor data collection of training dataset for AI/ML based positioning, for a given deployment scenario (e.g., InF-scenario, clutter parameter, drop) and with uniform UE distribution, the required sample density (e.g., #samples/m2) for achieving a given positioning accuracy target varies with AI/ML design choices including:\n-\tdifferent positioning approach (direct AI/ML, AI/ML-assisted),\n-\tdifferent type of model input,\n-\tthe size of model input,\n-\tAI/ML complexity (model complexity and computational complexity).\nFor AI/ML based positioning, the positioning accuracy is affected by the training dataset size for a given UE distribution area (or equivalently, sample density in #samples/m2), when the UE is distributed uniformly in training data collection.\n-\tThere exists a tradeoff between the training dataset size and the achievable positioning accuracy. The larger the training dataset size (i.e., higher sample density), the smaller the positioning error (in meters), until a saturation point is reached where additional training data does not bring further improvement to the positioning accuracy.\n-\tNote: here a sample refers to the training data collected of one UE at one location. Sample density is equivalent to the density of UEs with data collected in the training dataset.\nEvaluation results demonstrate that the performance of AI/ML positioning with the evaluation area as the convex hull of the horizontal BS deployment shows better performance than that with the whole hall area as evaluation area. This is due to: (a) convex hull case has higher sample density if using the same training dataset size, since convex hull has smaller UE distribution area; (b) for whole hall area, the UEs located outside the convex hull have diminished access to TRPs.\n-\tFor convex hull: UE distribution area = 100x40 m;\n-\tFor whole hall area: UE distribution area = 120x60 m\nObservations:\nDirect AI/ML positioning\nEvaluation of the following generalization aspects show that the positioning accuracy of direct AI/ML positioning deteriorates when the AI/ML model is trained with dataset of one deployment scenario, while tested with dataset of a different deployment scenario.\n-\tThe generalization aspects include:\n-\tDifferent drops\n-\tDifferent clutter parameters\n-\tDifferent InF scenarios\n-\tNetwork synchronization error\n-\tCompanies have provided evaluation results which show that the positioning accuracy on the test dataset can be improved by better training dataset construction and/or model fine-tuning/re-training.\n-\tBetter training dataset construction: The training dataset is composed of data from multiple deployment scenarios, which include data from the same deployment scenario as the test dataset.\n-\tModel fine-tuning/re-training: the model is re-trained/fine-tuned with a dataset from the same deployment scenario as the test dataset.\nNote: Ideal model training and switching may provide the upper bound of achievable performance when the AI/ML model needs to handle different deployment scenarios.\nFor direct AI/ML positioning, based on evaluation results of timing error in the range of 0-50 ns, when the model is trained by a dataset with UE/gNB RX and TX timing error t1 (ns) and tested in a deployment scenario with UE/gNB RX and TX timing error t2 (ns), for a given t1,\n-\tFor a case evaluated by a given source, the positioning accuracy of cases with t2 smaller than t1 is better than the cases with t2 equal to t1. For example,\n-\tFor the case of (t1, t2)=(50ns, 30ns), evaluation results show the positioning error of (t1, t2)=(50ns, 30ns) is 0.82~0.86 times that of (t1, t2)=(50ns, 50ns).\n-\tFor the case of (t1, t2)=(50ns, 0ns), evaluation results show the positioning error of (t1, t2)=(50ns, 0ns) is 0.80~0.82 times that of (t1, t2)=(50ns, 50ns).\n-\tFor a case evaluated by a given source, the positioning accuracy of cases with t2 greater than t1 is worse than the cases with t2 equal to t1. The larger the difference between t1 and t2, the more the degradation. For example,\n-\tFor the case of (t1, t2)=(0ns, 10ns), evaluation results show the positioning error of (t1, t2)=(0ns, 10ns) is 1.25~18.7 times that of (t1, t2)=(0ns, 0ns).\n-\tFor the case of (t1, t2)=(0ns, 50ns), evaluation results show the positioning error of (t1, t2)=(0ns, 50ns) is 3.5~18.3 times that of (t1, t2)=(0ns, 0ns).\nNote: Here the positioning error is the horizonal positioning error (meters) at CDF=90%.\nFor direct AI/ML positioning, based on evaluation results of network synchronization error in the range of 0-50 ns, when the model is trained by a dataset with network synchronization error t1 (ns) and tested in a deployment scenario with network synchronization error t2 (ns), for a given t1,\n-\tFor a case evaluated by a given source, the positioning accuracy of cases with t2 smaller than t1 is better than the cases with t2 equal to t1. For example,\n-\tFor the case of (t1, t2)=(50ns, 10ns), evaluation results show the positioning error of (t1, t2)=(50ns, 10ns) is 0.52~0.83 times that of (t1, t2)=(50ns, 50ns).\n-\tFor the case of (t1, t2)=(50ns, 0ns), evaluation results show the positioning error of (t1, t2)=(50ns, 0ns) is 0.50~0.82 times that of (t1, t2)=(50ns, 50ns).\n-\tFor a case evaluated by a given source, the positioning accuracy of cases with t2 greater than t1 is worse than the cases with t2 equal to t1. The larger the difference between t1 and t2, the more the degradation. For example,\n-\tFor the case of (t1, t2)=(0ns, 10ns), evaluation results show the positioning error of (0ns, 10ns) is 1.17~9.5 times that of (0ns, 0ns).\n-\tFor the case of (t1, t2)=(0ns, 50ns), evaluation results show the positioning error of (0ns, 50ns) is 10~40 times that of (0ns, 0ns).\nNote: here the positioning error is the horizonal positioning error (meters) at CDF=90%.\nAI/ML assisted positioning\nFor AI/ML assisted positioning with timing information (e.g., ToA) as model output, evaluation of the following generalization aspects show that:\n-\tthe positioning accuracy deteriorates when the AI/ML model is trained with dataset of one deployment scenario, while tested with dataset of a different deployment scenario.\n-\tDifferent drops\n-\tDifferent clutter parameters\n-\tDifferent InF scenarios\n-\tthe positioning accuracy may or may not deteriorate when the AI/ML model is trained with dataset of one deployment scenario, while tested with dataset of a different deployment scenario.\n-\tNetwork synchronization error\n-\tUE/gNB RX and TX timing error\n-\tSNR mismatch\n-\tChannel estimation error\nFor AI/ML assisted positioning, evaluation results demonstrate that for the generalization aspects of:\n-\tDifferent drops\n-\tDifferent clutter parameters\n-\tDifferent InF scenarios\n-\tNetwork synchronization error\n-\tUE/gNB RX and TX timing error\n-\tSNR mismatch\n-\tChannel estimation error\nif the positioning accuracy would deteriorate when the AI/ML model is trained with dataset of one deployment scenario and tested with dataset of a different deployment scenario, the positioning accuracy on the test dataset can be improved by better training dataset construction and/or model fine-tuning/re-training.\n-\tBetter training dataset construction: The training dataset is composed of data from multiple deployment scenarios, which include data from the same deployment scenario as the test dataset.\n-\tModel fine-tuning/re-training: the model is re-trained/fine-tuned with a dataset from the same deployment scenario as the test dataset.\nNote: Ideal model training and switching may provide the upper bound of achievable performance when the AI/ML model needs to handle different deployment scenarios.\nFor AI/ML assisted positioning with timing information (e.g., ToA) as model output, based on evaluation results of network synchronization error in the range of 0-50 ns, when the model is trained by a dataset with network synchronization error t1 (ns) and tested in a deployment scenario with network synchronization error t2 (ns), for a given t1,\n-\tFor a case evaluated by a given source, the positioning accuracy of cases with t2 smaller than t1 is better than the cases with t2 equal to t1. For example,\n-\tFor the case of (t1, t2)=(50ns, 20~25ns), evaluation results show the positioning error of (t1, t2)=(50ns, 20~25ns) is 0.64~0.85 times that of (t1, t2)=(50ns, 50ns).\n-\tFor the case of (t1, t2)=(50ns, 0ns), evaluation results show the positioning error of (t1, t2)=(50ns, 0ns) is 0.50~0.80 times that of (t1, t2)=(50ns, 50ns).\n-\tFor a case evaluated by a given source, the positioning accuracy of cases with t2 greater than t1 is worse than the cases with t2 equal to t1. The larger the difference between t1 and t2, the more the degradation. For example,\n-\tFor the case of (t1, t2)=(0ns, 10ns), evaluation results show the positioning error of (0ns, 10ns) is 1.16~4.40 times that of (0ns, 0ns).\n-\tFor the case of (t1, t2)=(0ns, 20~25ns), evaluation results show the positioning error of (0ns, 50ns) is 2.19~10.11 times that of (0ns, 0ns).\n-\tFor the case of (t1, t2)=(0ns, 50ns), evaluation results show the positioning error of (0ns, 50ns) is 9.68~31.95 times that of (0ns, 0ns).\nNote: Here the positioning error is the horizonal positioning error (meters) at CDF=90%.\nFor AI/ML assisted positioning with timing information (e.g., ToA) as model output, based on evaluation results of timing error in the range of 0-50 ns, when the model is trained by a dataset with UE/gNB RX and TX timing error t1 (ns) and tested in a deployment scenario with UE/gNB RX and TX timing error t2 (ns), for a given t1,\n-\tFor a case evaluated by a given source, the positioning accuracy of cases with t2 smaller than t1 is better than the cases with t2 equal to t1. For example,\n-\tFor the case of (t1, t2)=(50ns, 20~25ns), evaluation results submitted to RAN1#113 show the positioning error of (t1, t2)=(50ns, 20~25ns) is 0.75~1.00 times that of (t1, t2)=(50ns, 50ns).\n-\tFor the case of (t1, t2)=(50ns, 0ns), evaluation results submitted to RAN1#113 show the positioning error of (t1, t2)=(50ns, 0ns) is 0.76~0.99 times that of (t1, t2)=(50ns, 50ns).\n-\tFor a case evaluated by a given source, the positioning accuracy of cases with t2 greater than t1 is worse than the cases with t2 equal to t1. The larger the difference between t1 and t2, the more the degradation. For example,\n-\tFor the case of (t1, t2)=(0ns, 10ns), evaluation results submitted to RAN1#113 show the positioning error of (t1, t2)=(0ns, 10ns) is 1.34~5.43 times that of (t1, t2)=(0ns, 0ns).\n-\tFor the case of (t1, t2)=(0ns, 20~25ns), evaluation results submitted to RAN1#113 show the positioning error of (t1, t2)=(0ns, 20~25ns) is 5.66~13.0 times that of (t1, t2)=(0ns, 0ns).\n-\tFor the case of (t1, t2)=(0ns, 50ns), evaluation results submitted to RAN1#113 show the positioning error of (t1, t2)=(0ns, 50ns) is 10.62~51.52 times that of (t1, t2)=(0ns, 0ns).\nNote: Here the positioning error is the horizonal positioning error (meters) at CDF=90%.\nBoth direct AI/ML positioning and AI/ML assisted positioning\nFor both direct AI/ML and AI/ML assisted positioning, evaluation results submitted show that with CIR model input for a trained model,\n-\tFor two SNR/SINR values S1 (dB) and S2 (dB), S1 â¥ S2 + 15 dB, positioning error of a model trained with data of S1 (dB) and tested with data of S2 (dB) is more than 5.75 times that of the model trained and tested with data of S1 (dB).\n-\tFor two SNR/SINR values S1 (dB) and S2 (dB), S1 â¤ S2Â âÂ 10 dB, the generalization performance of a model trained with data of S1 (dB) and tested with data of S2 (dB) is better than the performance of a model trained with data of S2 (dB) and tested with data of S1 (dB). Positioning error of a model trained with data of S2 (dB) and tested with data of S1 (dB) is more than 2.97 times that of the model trained with data of S1 (dB) and tested with data of S2 (dB).\nNote: Here the positioning error is the horizonal positioning error (meters) at CDF=90%.\nIt is concluded that no dedicated evaluation is needed for the positioning accuracy performance of model switching.\nObservations:\nDirect AI/ML positioning\nFor direct AI/ML positioning and different drops, evaluation has been performed where the AI/ML model is (a) previously trained for drop A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for drop B with a dataset of sample density x%  N (#samples/m2), (c) then tested under drop B and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t6 sources when fine-tuning dataset size is x% = 1.3%~2.5% of full training dataset size, the positioning error is E = (3.15~10.89)  E0,B;\n-\t6 sources when fine-tuning dataset size is x% = 4.0%~5.0% of full training dataset size, the positioning error is E = (2.20~8.82)  E0,B;\n-\t6 sources when fine-tuning dataset size is x% = 6.3%~10.0% of full training dataset size, the positioning error is E = (1.99~7.21)  E0,B;\n-\t6 sources when fine-tuning dataset size is x% = 12.0%~25.0% of full training dataset size, the positioning error is E = (1.58~5.13)  E0,B; 1 source the positioning error is E = (10.46)  E0,B;\n-\t3 sources when fine-tuning dataset size is x% = 34.0%~50.0% of full training dataset size, the positioning error is E = (1.22~2.70)  E0,B; 1 source the positioning error is E = (8.88)  E0,B;\n-\t2 sources when fine-tuning dataset size is x% = 100.0% of full training dataset size, the positioning error is E = (1.00~1.19)  E0,B;\nHere E0,B (meters) is the full training accuracy at CDF=90% for drop B.\nFor direct AI/ML positioning and different drops, evaluation has been performed where the AI/ML model is (a) previously trained for drop A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for drop B with a dataset of sample density x%  N (#samples/m2), (c) then tested under drop A and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t3 sources when fine-tuning dataset size is x% = 2.5%~5.0% of full training dataset size, the positioning error is E = (3.00~5.76)  E0,A;\n-\t3 sources when fine-tuning dataset size is x% = 10.0%~25.0% of full training dataset size, the positioning error is E = (3.35~5.96)  E0,A;\n-\t3 sources when fine-tuning dataset size is x% = 50.0%~100.0% of full training dataset size, the positioning error is  E = (4.50~7.71)  E0,A;\nHere E0,A  (meters) is the full training accuracy at CDF=90% for drop A.\nFor direct AI/ML positioning and different clutter parameters, evaluation has been performed where the AI/ML model is (a) previously trained for clutter parameter A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for clutter parameter B with a dataset of sample density x%  N (#samples/m2), (c) then tested under clutter parameter B and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t8 sources when fine-tuning dataset size is x% = 1.3%~2.5% of full training dataset size, the positioning error is E = ( 1.8~10.18)  E0,B;\n-\t11 sources when fine-tuning dataset size is x% = 4.0%~8.0% of full training dataset size, the positioning error is E = (1.77~7.05)  E0,B;\n-\t9 sources when fine-tuning dataset size is x% = 10.0%~17.0% of full training dataset size, the positioning error is E = (1.50~5.34)  E0,B; 1 source the positioning error is E = (14.65)  E0,B;\n-\t5 sources when fine-tuning dataset size is x% = 20.0%~34.0% of full training dataset size, the positioning error is E = (1.01~1.75)  E0,B; 1 source the positioning error is E = (12.23)  E0,B;\n-\t5 sources when fine-tuning dataset size is x% = 50.0% of full training dataset size, the positioning error is E = (1.09~1.25)  E0,B;\n-\t4 sources when fine-tuning dataset size is x% = 95%~100.0% of full training dataset size, the positioning error is E = (0.82~1.84)  E0,B;\nHere E0,B (meters) is the full training accuracy at CDF=90% for clutter parameter B.\nFor direct AI/ML positioning and different clutter parameters, evaluation has been performed where the AI/ML model is (a) previously trained for clutter parameter A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for clutter parameter B with a dataset of sample density x%  N (#samples/m2), (c) then tested under clutter parameter A and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t6 sources when fine-tuning dataset size is x% = 2.5% of full training dataset size, the positioning error is E = (2.24~22.11)  E0,A;\n-\t7 sources when fine-tuning dataset size is x% = (5.0%~5.6%) of full training dataset size, the positioning error is E = (2.02~19.49)  E0,A;\n-\t6 sources when fine-tuning dataset size is x% = (10.0%~25.0%) of full training dataset size, the positioning error is E = (1.40~18.65)  E0,A;\n-\t5 sources when fine-tuning dataset size is x% = 50.0% of full training dataset size, the positioning error is E = (1.20~10.72)  E0,A;\n-\t3 sources when fine-tuning dataset size is x% = 95.0%~100.0% of full training dataset size, the positioning error is E = (2.08~12.58)  E0,A;\nHere E0,A (meters) is the full training accuracy at CDF=90% for clutter parameter A.\nFor direct AI/ML positioning and different network synchronization error, evaluation has been performed where the AI/ML model is (a) previously trained for network synchronization error = A (ns) with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for network synchronization error = B (ns) with a dataset of sample density x%  N (#samples/m2), (c) then tested under network synchronization error = B (ns) and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t5 sources when fine-tuning dataset size is x% = (1.3%~2.5%) of full training dataset size, the positioning error is E = (0.98~5.21)  E0,B;\n-\t6 sources when fine-tuning dataset size is x% = (4.0%~8.0%) of full training dataset size, the positioning error is E = (0.84~10.70)  E0,B;\n-\t6 sources when fine-tuning dataset size is x% = (10.0%~25.0%) of full training dataset size, the positioning error is E = (0.80~10.38)  E0,B;\n-\t1 source when fine-tuning dataset size is x% = (50.0%~100.0%) of full training dataset size, the positioning error is E = (0.81~1.1)  E0,B;\nHere E0,B (meters) is the full training accuracy at CDF=90% for network synchronization error = B (ns).\nFor direct AI/ML positioning and different network synchronization error, evaluation has been performed where the AI/ML model is (a) previously trained for network synchronization error = 0 ns with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for network synchronization error = 50 ns with a dataset of sample density x%  N (#samples/m2), (c) then tested under network synchronization error = 0 ns and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t2 sources when fine-tuning dataset size is x% = (2.5%~10.0%) of full training dataset size, the positioning error is E = (5.08~23.44)  E0,A;\n-\t1 source when fine-tuning dataset size is x% = (25.0%~100.0%) of full training dataset size, the positioning error is E = (2.28~3.92)  E0,A;\nHere E0,A (meters) is the full training accuracy at CDF=90% for network synchronization error = 0 ns.\nFor direct AI/ML positioning and different UE timing error, evaluation has been performed where the AI/ML model is (a) previously trained without UE timing error with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning with UE timing error with a dataset of sample density x%  N (#samples/m2), (c) then tested with UE timing error and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t2 sources when fine-tuning dataset size is x% = 1.3%~20.0% of full training dataset size, the positioning error is E = (0.51~2.53)  E0,B;\nHere E0,B (meters) is the full training accuracy at CDF=90% for the case with UE timing error.\nFor direct AI/ML positioning and different InF scenarios, evaluation has been performed where the AI/ML model is (a) previously trained for InF scenario A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for InF scenario B with a dataset of sample density x%  N (#samples/m2), (c) then tested under InF scenario B and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t5 sources when fine-tuning dataset size is x% = (2.0%~5.6%) of full training dataset size, the positioning error is E = (0.5~16.67)  E0,B;\n-\t5 sources when fine-tuning dataset size is x% = (8.0%~15.0%) of full training dataset size, the positioning error is E =  (0.4~12.6)  E0,B;\n-\t2 sources when fine-tuning dataset size is x% = 25.0% of full training dataset size, the positioning error is E = (1.60~1.67)  E0,B;\n-\t2 sources when fine-tuning dataset size is x% = (50.0%~100.0%) of full training dataset size, the positioning error is E = (0.92~1.41)  E0,B;\nHere E0,B (meters) is the full training accuracy at CDF=90% for InF scenario B.\nFor direct AI/ML positioning and different InF scenarios, evaluation has been performed where the AI/ML model is (a) previously trained for InF scenario A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for InF scenario B with a dataset of sample density x%  N (#samples/m2), (c) then tested under InF scenario A and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t3 sources when fine-tuning dataset size is x% = (2.5%~10.0%) of full training dataset size, the positioning error is E = (2.28~30.2)  E0,A;\n-\t2 sources when fine-tuning dataset size is x% = (25.0%~100.0%) of full training dataset size, the positioning error is E = (1.7~9.24)  E0,A;\nHere E0,A (meters) is the full training accuracy at CDF=90% for InF scenario A.\nFor direct AI/ML positioning and different SNR value (dB), evaluation has been performed where the AI/ML model is (a) previously trained for SNR value A (dB) with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for SNR value B (dB) with a dataset of sample density x%  N (#samples/m2), (c) then tested under SNR value B (dB) and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t1 source when fine-tuning dataset size is x% = (5.6%~11.1%) of full training dataset size, the positioning error is E = (1.60~1.90)  E0,B;\nHere E0,B (meters) is the full training accuracy at CDF=90% for SNR value B (dB).\nFor direct AI/ML positioning and different time varying assumptions, evaluation has been performed where the AI/ML model is (a) previously trained for the scenario without time varying change with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for the scenario with time varying change with a dataset of sample density x%  N (#samples/m2), (c) then tested under the scenario with time varying change and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t1 source when fine-tuning dataset size is x% = (3.7%~22.0%) of full training dataset size, the positioning error is E = (1.68~3.49)  E0,B;\nHere E0,B (meters) is the full training accuracy at CDF=90% for the scenario with time varying change.\nFor direct AI/ML positioning and different channel estimation error, evaluation has been performed where the AI/ML model is (a) previously trained for channel estimation error = 20 dB with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for channel estimation error = 0 dB with a dataset of sample density x%  N (#samples/m2), (c) then tested under channel estimation error = 0 dB and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t1 source when fine-tuning dataset size is x% = (2.5%~25.0%) of full training dataset size, the positioning error is E = (1.50~2.79)  E0,B;\n-\t1 source when fine-tuning dataset size is x% = (50.0%~100.0%) of full training dataset size, the positioning error is E = (0.96~1.17)  E0,B;\nHere E0,B (meters) is the full training accuracy at CDF=90% for channel estimation error = 0 dB.\nFor direct AI/ML positioning and different channel estimation error, evaluation has been performed where the AI/ML model is (a) previously trained for channel estimation error = 20 dB with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for channel estimation error = 0 dB with a dataset of sample density x%  N (#samples/m2), (c) then tested under channel estimation error = 20 dB and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t1 source when fine-tuning dataset size is x% = (2.5%~25.0%) of full training dataset size, the positioning error is E = (4.22~5.95)  E0,A;\n-\t1 source when fine-tuning dataset size is x% = (50.0%~100.0%) of full training dataset size, the positioning error is E = (3.08~3.94)  E0,A;\nHere E0,A (meters) is the full training accuracy at CDF=90% for channel estimation error = 20 dB.\nAI/ML assisted positioning\nFor AI/ML assisted positioning with timing information as model output and for different drops, evaluation has been performed where the AI/ML model is (a) previously trained for drop A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for drop B with a dataset of sample density x%  N (#samples/m2), (c) then tested under drop B and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t2 sources when fine-tuning dataset size is x% = (2.0%~10.0%) of full training dataset size, the positioning error is E = (1.27~7.68)  E0,B;\n-\t2 sources when fine-tuning dataset size is x% = (12.0%~34.0%) of full training dataset size, the positioning error is E = (5.59~12.88)  E0,B;\nHere E0,B  (meters) is the full training accuracy at CDF=90% for drop B.\nFor AI/ML assisted positioning with timing information as model output and for different clutter parameters, evaluation has been performed where the AI/ML model is (a) previously trained for clutter parameter A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for clutter parameter B with a dataset of sample density x%  N (#samples/m2), (c) then tested under clutter parameter B and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t5 sources when fine-tuning dataset size is x% = (2.0%~2.5%) of full training dataset size, the positioning error is E = (1.47~5.88)  E0,B;\n-\t6 sources when fine-tuning dataset size is x% = (4.0%~5.0%) of full training dataset size, the positioning error is E = (1.39~4.42)  E0,B;\n-\t7 sources when fine-tuning dataset size is x% = (8.0%~12.0%) of full training dataset size, the positioning error is E = (1.34~3.93)  E0,B;\n-\t3 sources when fine-tuning dataset size is x% = 25.0% of full training dataset size, the positioning error is E = (1.33~1.91)  E0,B;\n-\t3 sources when fine-tuning dataset size is x% = 50.0% of full training dataset size, the positioning error is E = (1.15~1.33)  E0,B;\n-\t2 sources when fine-tuning dataset size is x% = 100.0% of full training dataset size, the positioning error is E = (0.89~1.15)  E0,B;\nHere E0,B (meters) is the full training accuracy at CDF=90% for clutter parameter B.\nFor AI/ML assisted positioning with timing information as model output and for different clutter parameters, evaluation has been performed where the AI/ML model is (a) previously trained for clutter parameter A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for clutter parameter B with a dataset of sample density x%  N (#samples/m2), (c) then tested under clutter parameter A and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t4 sources when fine-tuning dataset size is x% = (2.5%~5.0%) of full training dataset size, the positioning error is E = (1.47~12.94)  E0,A;\n-\t5 sources when fine-tuning dataset size is x% = 10.0% of full training dataset size, the positioning error is E = (1.32~11.52)  E0,A;\n-\t3 sources when fine-tuning dataset size is x% = 25.0% of full training dataset size, the positioning error is E = (1.22~7.65)  E0,A;\n-\t3 sources when fine-tuning dataset size is x% = 50.0% of full training dataset size, the positioning error is E = (1.2~5.86)  E0,A;\n-\t2 sources when fine-tuning dataset size is x% = 100.0% of full training dataset size, the positioning error is E = (2.64~4.66)  E0,A;\nHere E0,A (meters) is the full training accuracy at CDF=90% for the clutter parameter A.\nFor AI/ML assisted positioning and different network synchronization error, evaluation has been performed where the AI/ML model is (a) previously trained for network synchronization error A (ns) with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for network synchronization error B (ns) with a dataset of sample density x%  N (#samples/m2), (c) then tested under network synchronization error B (ns) and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t5 sources when fine-tuning dataset size is x% = 2.0%~5.0% of full training dataset size, the positioning error is E = (1.28~5.44)  E0,B;\n-\t5 sources when fine-tuning dataset size is x% = 8.0%~25.0% of full training dataset size, the positioning error is E = (1.10~4.07)  E0,B;\n-\t1 source when fine-tuning dataset size is x% = 50.0%~100.0% of full training dataset size, the positioning error is E = (1.01~1.47)  E0,B;\nHere E0,B (meters) is the full training accuracy at CDF=90% for network synchronization error B (ns).\nFor AI/ML assisted positioning and different network synchronization error,\n-\tevaluation has been performed where the AI/ML model is (a) previously trained for network synchronization error = 0 ns with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for network synchronization error = 50 ns with a dataset of sample density x%  N (#samples/m2), (c) then tested under network synchronization error = 0 ns and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that, denoting E0,A (meters) as the full training accuracy at CDF=90% for network synchronization error = 0 ns,\n-\t2 sources when fine-tuning dataset size is x% = (2.5%~100.0%) of full training dataset size, the positioning error is E = (3.71~5.97)  E0,A;\n-\tevaluation has been performed where the AI/ML model is (a) previously trained for network synchronization error = 50 ns with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for network synchronization error = 0 ns with a dataset of sample density x%  N (#samples/m2), (c) then tested under network synchronization error = 50 ns and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that, denoting E0,A (meters) as the full training accuracy at CDF=90% for network synchronization error = 50 ns,\n-\t1 source when fine-tuning dataset size is x% = (2.5%~100.0%) of full training dataset size, the positioning error is E = (1.15~2.23)  E0,A;\nFor AI/ML assisted positioning and different InF scenarios, evaluation has been performed where the AI/ML model is (a) previously trained for InF scenario A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for InF scenario B with a dataset of sample density x%  N (#samples/m2), (c) then tested under InF scenario B and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t3 sources when fine-tuning dataset size is x% = (2.0%~12.0%) of full training dataset size, the positioning error is E = (1.20~6.0)  E0,B;\n-\t1 source when fine-tuning dataset size is x% = 25.0%~50.0% of full training dataset size, the positioning error is E = (2.55~2.91)  E0,B;\nHere E0,B  (meters) is the full training accuracy at CDF=90% for InF scenario B.\nFor AI/ML assisted positioning and different InF scenarios, evaluation has been performed where the AI/ML model is (a) previously trained for InF-DH{60%,6m,2m} with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for InF-SH{20%,2m,10m} with a dataset of sample density x%  N (#samples/m2), (c) then tested under InF-DH{60%,6m,2m} and the horizontal accuracy at CDF=90% is E meters. Evaluation results show that,\n-\t1 source when fine-tuning dataset size is x% = 2.5%-50.0% of full training dataset size, the positioning error is E = (2.53~3.44)  E0,A;\nHere E0,A (meters) is the full training accuracy at CDF=90% for InF-DH{60%,6m,2m}.\nFor AI/ML assisted positioning with LOS/NLOS indicator as model output and for different clutter parameters, evaluation has been performed where the AI/ML model is (a) previously trained for clutter parameter A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for clutter parameter B with a dataset of sample density x%  N (#samples/m2), (c) then tested under clutter parameter B and the LOS/NLOS indication accuracy is E (using F1-score). Evaluation results show that,\n-\t1 source when fine-tuning dataset size is x% = 10.0% of full training dataset size, the accuracy (using F1-score) of LOS/NLOS indicator is E = (0.56~0.974)  E0,B;\nHere E0,B  is the full training accuracy (using F1-score) for the clutter parameter B.\nFor AI/ML assisted positioning with LOS/NLOS indicator as model output and for different clutter parameters, evaluation has been performed where the AI/ML model is (a) previously trained for clutter parameter A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for clutter parameter B with a dataset of sample density x%  N (#samples/m2), (c) then tested under clutter parameter A and the LOS/NLOS indication accuracy is E (using F1-score). Evaluation results show that,\n-\t1 source when fine-tuning dataset size is x% = 10.0% of full training dataset size, the accuracy (using F1-score) of LOS/NLOS indicator is E = (0.09~0.24)  E0,A;\nHere E0,A is the full training accuracy (using F1-score) for the clutter parameter A.\nBoth direct AI/ML positioning and AI/ML assisted positioning\nAs a summary of the observations above, for both direct AI/ML positioning and AI/ML assisted positioning, evaluation results show that:\n-\tFine-tuning/re-training a previous model with dataset of the new deployment scenario improves the model performance for the new deployment scenario. For details on the amount of improvement, see the observations listed above.\n-\tAfter fine-tuning/re-training a previous model with dataset of the new deployment scenario, the performance of the updated model degrades for the previous deployment scenario (e.g., previous clutter parameter setting) that the previous model was trained for.\n-\tExamples of the deployment scenario include: different drops, different clutter parameter, different InF scenarios\nFor both direct AI/ML positioning and AI/ML assisted positioning,\n-\tif the new deployment scenario is significantly different from the previous deployment scenario the model was trained for (e.g., different drops, different clutter parameter, different InF scenarios), fine-tuning a previous model requires similarly large training dataset size as training the model from scratch, in order to achieve the similar performance for the new deployment scenario.\n-\tIf the new deployment scenario is NOT significantly different from the previous deployment scenario the model was trained for (e.g., 2ns difference in network synchronization error between the previous and the new deployment scenario), fine-tuning a previous model requires a small (e.g., x%=10%) training dataset size as compared to training the model from scratch, in order to achieve the similar performance for the new deployment scenario.\nObservations:\nDirect AI/ML positioning\nFor the evaluation of direct AI/ML positioning, with Nt consecutive time domain samples used as model input, evaluation results show that when CIR, PDP, or DP is used as model input, using different Nt while holding other parameters the same,\n-\tReducing Nt from 256 to 128 does not appreciably degrade the positioning accuracy, while the measurement size and signalling overhead shrink to (approximately) 1/2 that of Nt=256.\n-\tPositioning error of Nt=128 is 0.81 ~ 1.19 times the positioning error of Nt=256;\n-\tReducing Nt from 256 to 64~32 may degrade the positioning accuracy, while the measurement size and signalling overhead shrink to (approximately) 1/4 ~1/8 that of Nt=256, respectively.\n-\tPositioning error of Nt=64 is 0.88 ~ 3.00 times the positioning error of Nt=256;\n-\tPositioning error of Nt=32 is 1.05 ~ 4.29 times the positioning error of Nt=256;\n-\tNote: the variation in the positioning accuracy depends on each company's simulation assumption (e.g., AI/ML complexity).\nFor direct AI/ML positioning, the evaluation of positioning accuracy at model inference is affected by the type of model input and AI/ML complexity. For a given AI/ML model design, there is a tradeoff between model input, AI/ML complexity (model complexity and computational complexity), and positioning accuracy. Evaluation results show that if changing model input type while holding other parameters (e.g., Nt, N't, Nport, N'TRP) the same,\n-\tWhen comparing PDP and CIR as model input,\n-\t9 sources showed evaluation results where the positioning error of PDP as model input is 1.06 ~ 1.62 times the positioning error of CIR as model input.\n-\t5 sources showed evaluation results where the positioning error of PDP as model input is 0.61 ~ 0.96 times the positioning error of CIR as model input.\n-\tWhen comparing DP and CIR as model input,\n-\t4 sources showed evaluation results where the positioning error of DP as model input is 1.18 ~ 1.96 times the positioning error of CIR as model input.\n-\t2 sources showed evaluation results where the positioning error of DP as model input is 0.79~0.92 times the positioning error of CIR as model input.\n-\tNote: For one of the sources (R1-2306112), the difference in relative performance is due to the complexity of the AI/ML model.\n-\tNote: For another source (R1-2307920), the difference in relative performance is due to the parameter settings.\n-\tNote: the variation in the positioning accuracy depends on each company's simulation assumption (e.g., AI/ML complexity).\nFor the evaluation of direct AI/ML positioning, when N't time domain samples with the strongest power are selected as model input, evaluation results show that:\n-\tFor model input of CIR or PDP and Nt=256, using different N't while holding other parameters constant,\n-\tReducing N't from 256 to 64 does not appreciably degrade the positioning accuracy, while the measurement size and signalling overhead shrink to (approximately) 1/4  that of Nt=N't=256.\n-\tPositioning error of N't=128 is 1.02 ~ 1.07 times the positioning error of Nt=N't=256;\n-\tPositioning error of N't=64 is 1.02 ~ 1.21 times the positioning error of Nt=N't=256;\n-\tReducing N't from 256 to 32~16 degrade the positioning accuracy, while the measurement size and signalling overhead shrink to (approximately) 1/8 ~ 1/16 that of Nt=N't=256.\n-\tPositioning error of N't=32 is 1.14 ~ 2.03 times the positioning error of Nt=N't=256;\n-\tPositioning error of N't=16 is 1.12 ~ 2.54 times the positioning error of Nt=N't=256;\n-\tReducing N't from 256 to 9~8 degrade the positioning accuracy, while the measurement size and signalling overhead shrink to (approximately) 1/32 that of Nt=N't=256.\n-\tPositioning error of N't=9~8 is 1.42 ~ 3.29 times the positioning error of Nt=N't=256;\n-\tFor model input of DP and Nt=256, using different N't while holding other parameters constant,\n-\tOne source (R1-2304339) showed that reducing N't from 64 to 32 does not degrade the positioning accuracy while the measurement size and signalling overhead shrink by (approximately) 1/2.\n-\tPositioning error of N't=32 is 1.03 times the positioning error of N't=64.\n-\tNote: the evaluation results based on the other model input (e.g., multiple path) can be added in next meeting\nBased on evaluation results by 8 sources, for TRP reduction of direct AI/ML positioning, approaches supporting dynamic TRP pattern can achieve the horizontal positioning accuracy Edynamic = (0.80~2.15)  Efixed (meters), when other design parameters are held the same, where:\n-\tEdynamic (meters) is the horizontal positioning accuracy at CDF=90% for approaches supporting dynamic TRP pattern (i.e., Approach 1-B and 2-B);\n-\tEfixed (meters) is the horizontal positioning accuracy at CDF=90% for approaches supporting fixed TRP pattern (i.e., Approach 1-A and 2-A);\nBased on evaluation results by 8 sources, for TRP reduction of direct AI/ML positioning, Approach 1-A and 2-A achieve similar performance. The horizontal positioning accuracy E2A = (0.87~1.32)  E1A (meters), when other design parameters are held the same, where:\n-\tE1A (meters) is the horizontal positioning accuracy at CDF=90% for Approach 1-A;\n-\tE2A (meters) is the horizontal positioning accuracy at CDF=90% for Approach 2-A;\nBased on evaluation results by 11 sources, for TRP reduction of direct AI/ML positioning, the positioning accuracy degrades as the number of active TRPs are reduced from 18 TRPs to 3 TRPs. The degradation increases as the number of active TRPs decreases.\n-\tWhen the number of active TRP is reduced from NTP = 18 to N'TP = 12~8, the average horizontal positioning accuracy E is in the range of E = (1.48~1.95)  E18TRP;\n-\tWhen the number of active TRP is reduced from NTP = 18 to N'TP = 6~5, the average horizontal positioning accuracy E is in the range of E = (2.35~3.04)  E18TRP;\n-\tWhen the number of active TRP is reduced from NTP = 18 to N'TP = 4~3, the average horizontal positioning accuracy E is in the range of E = (2.13~5.11)  E18TRP;\nHere E (meters) is the horizontal positioning accuracy at CDF=90% with N'TP active TRPs, E18TRP (meters) is the horizontal positioning accuracy at CDF=90% with NTP = 18 active TRPs.\nNote: some results from 2 sources show E > 11  E18TRP for N'TP= 9 and 6 when using Approach 2-B.\nAI/ML assisted positioning\nFor AI/ML assisted positioning, the positioning accuracy at model inference is affected by the type of model input.  Evaluation results show that if changing model input type while holding other parameters (e.g., Nt, N't, Nport, N'TRP) the same,\n-\tThe positioning error of PDP as model input is 1.17 ~ 1.63 times the positioning error of CIR as model input.\n-\tThe positioning error of DP as model input is 1.33 ~ 2.01 times the positioning error of CIR as model input.\nFor AI/ML assisted positioning, with Nt consecutive time domain samples used as model input, evaluation results show that when CIR or PDP are used as model input, using different Nt while holding other parameters the same,\n-\tReducing Nt from 256 to 128 does not appreciably degrade the positioning accuracy, while the measurement size and signalling overhead shrink to (approximately) 1/2 that of Nt=256.\n-\tPositioning error of Nt=128 is 1.00 ~ 1.42 times the positioning error of Nt=256;\n-\tReducing Nt from 256 to 64~32 may degrade the positioning accuracy, while the measurement size and signalling overhead shrink to (approximately) 1/4 ~1/8 that of Nt=256, respectively.\n-\tPositioning error of Nt=64 is 1.09 ~ 3.02 times the positioning error of Nt=256;\n-\tPositioning error of Nt=32 is 2.43 ~ 5.10 times the positioning error of Nt=256;\nFor AI/ML assisted positioning, when N't time domain samples with the strongest power are selected as model input, evaluation results show that for model input of CIR or PDP and Nt=256, using different N't while holding other parameters the same,\n-\tReducing N't from 256 to 64 does not appreciably degrade the positioning accuracy, while the measurement size and signalling overhead shrink to (approximately) 1/4  that of Nt=N't=256.\n-\tPositioning error of N't=128 is 1.00 ~ 1.33 times the positioning error of Nt=N't=256;\n-\tPositioning error of N't=64 is 0.98 ~ 1.23 times the positioning error of Nt=N't=256;\n-\tReducing N't from 256 to 32~16 may degrade the positioning accuracy, while the measurement size and signalling overhead shrink to (approximately) 1/8 ~ 1/16 that of Nt=N't=256.\n-\tPositioning error of N't=32 is 1.15 ~ 1.69 times the positioning error of Nt=N't=256;\n-\tPositioning error of N't=16 is 1.04 ~ 2.67 times the positioning error of Nt=N't=256;\n-\tReducing N't from 256 to 9 degrade the positioning accuracy, while the measurement size and signalling overhead shrink to (approximately) 1/32 that of Nt=N't=256.\n-\tPositioning error of N't=9 is 1.66 ~ 4.40 times the positioning error of Nt=N't=256;\nBased on evaluation results by 2 sources, for TRP reduction of AI/ML assisted positioning with multi-TRP construction, approaches supporting dynamic TRP pattern can achieve the horizontal positioning accuracy Edynamic = (1.03~1.74)  Efixed (meters), when other design parameters are held the same, where:\n-\tEdynamic (meters) is the horizontal positioning accuracy at CDF=90% for approaches supporting dynamic TRP pattern (i.e., Approach 1-B and 2-B);\n-\tEfixed (meters) is the horizontal positioning accuracy at CDF=90% for approaches supporting fixed TRP pattern (i.e., Approach 1-A and 2-A);\nNote: evaluation results of 1 source show Edynamic = (5.66~8.12)  Efixed when the number of active TRP is reduced from NTP =18 to N'TP =9 or 4.\nBased on evaluation results by 2 sources, for TRP reduction of AI/ML assisted positioning, Approach 1-A and 2-A achieve similar performance. The horizontal positioning accuracy E2A = (1~1.47)  E1A (meters), when other design parameters are held the same, where:\n-\tE1A (meters) is the horizontal positioning accuracy at CDF=90% for Approach 1-A;\n-\tE2A (meters) is the horizontal positioning accuracy at CDF=90% for Approach 2-A;\nBased on evaluation results by 4 sources, for TRP reduction of AI/ML assisted positioning, the positioning accuracy degrades as the number of active TRPs are reduced from 18 TRPs to 3 TRPs. The degradation increases as the number of active TRPs decreases.\n-\tWhen the number of active TRP is reduced from NTP =18 to N'TP =9, the average horizontal positioning accuracy is E = 2.01  E18TRP;\n-\tWhen the number of active TRP is reduced from NTP =18 to N'TP = 6, the average horizontal positioning accuracy is E = 3.04  E18TRP;\n-\tWhen the number of active TRP is reduced from NTP =18 to N'TP = 3~4, the average horizontal positioning accuracy is E = (5.01~6.53)  E18TRP;\nHere E (meters) is the horizontal positioning accuracy at CDF=90% with N'TP active TRPs, E18TRP (meters) is the horizontal positioning accuracy at CDF=90% with NTP =18 active TRPs.\nNote: some results from 1 source show E > 7.54  E18TRP for N'TP=9 and E > 42.76  E18TRP for N'TP=6 when using Approach 1-B/2-B.\nBoth direct AI/ML positioning and AI/ML assisted positioning\nEvaluation of TRP reduction for both direct AI/ML positioning and AI/ML assisted positioning shows that: identification of the active TRPs is beneficial for Approach 2-B. Otherwise, the model suffers from poor performance in terms of positioning accuracy.\nFor example, evaluation results from 4 sources show that the horizontal positioning accuracy is greater than 10 m if TRP identification is not included as model input.\nObservations:\nDirect AI/ML positioning\nFor direct AI/ML positioning, for L in the range of 0.25m to 5m, the positioning error increases approximately in proportion to L, where L (in meters) is the standard deviation of truncated Gaussian Distribution of the ground-truth label error.\nEvaluation shows that direct AI/ML positioning is robust to certain label error based on evaluation results of L in the range of (0, 5) meter. The exact range of label error that can be tolerated depends on the positioning accuracy requirement, where tighter positioning accuracy requirement demands smaller label error.\nAI/ML assisted positioning\nIn evaluation of AI/ML assisted positioning with timing information (e.g., TOA) as model output, for L in the range of 0.25m to 5m, the timing (e.g., TOA) estimation error and positioning error increases approximately in proportion to L, where L (in meters) is the standard deviation of truncated Gaussian distribution of the ground-truth label error.\nEvaluations show that AI/ML assisted positioning with timing information (e.g., ToA) as model output is robust to certain label error based on evaluation results of L in the range of (0, 5) meter. The exact range of label error that can be tolerated depends on the positioning accuracy requirement, where tighter positioning accuracy requirement demands smaller label error.\nBased on evaluation results from 3 sources, for AI/ML assisted positioning where the model output includes the LOS/NLOS indicator, when the model is trained with dataset containing random LOS/NLOS label error, the models have no or minor degradation for LOS/NLOS identification accuracy up to at least m%=20% and at least n%=20%. When the training dataset has up to m%=20% and n%=20%, evaluation results show that the LOS/NLOS identification accuracy is PlablErr = PnoLablErr â d (percentage), where d is in the range of (1.2%~3.1%).\n-\tPnoLablErr (percentage) is the LOS/NLOS identification accuracy when m%=0% and n%=0%;\nOther\nFor AI/ML based positioning, evaluation results show that semi-supervised learning is helpful for improving the positioning accuracy when the same amount of ideal labelled data is used for supervised learning, and the number of ideal labelled data is limited.\nRegarding ground-truth label generation for AI/ML based positioning, multiple sources submitted evaluation results on the impact of ground-truth label for training obtained by existing NR RAT-dependent positioning methods. Feasibility and performance benefit of utilizing ground-truth label for training estimated by existing NR RAT-dependent positioning methods are observed.\n-\tSource 1 evaluated in InF-DH {40%, 2, 2} and showed that AI/ML model can be trained with noisy labels along with the corresponding quality estimated by the legacy positioning methods, to improve positioning performance from 3.73m@90% (5k ideal label) to 1.72m @90% (5k ideal label + 20k noisy label). It also showed that the performance benefit compared to semi-supervised training of 2.78m @90% (5k ideal label + 20k unlabeled data). Note that training data weighting is used with label quality indicator.\n-\tSource 2 evaluated in InF-DH {60%, 6, 2} and showed that the performance of direct AI/ML positioning with 1k clean labelled samples improves from 13.76m to 8.72m when considering additional 350 samples that are labelled using NR-RAT positioning method. Note that the label error is up to 3.5m.\n- \tSource 3 evaluated in both InF-DH {60%, 6, 2} and InF-DH {40%, 2, 2} and showed performance loss when compared to all ideal label case. For example it showed in InF-DH {40%, 2, 2} the accuracy degrades from 0.39m @90% (100% ideal label) to 2.10m @90% (50% ideal label and 50% label obtained by existing DL-TDOA scheme). Note that noisy label is treated the same as ideal label in training.\nFor the use case of positioning accuracy enhancement, extensive evaluations have been carried out. Both direct AI/ML positioning and AI/ML assited positioning are evaluated using one-sided model. The following areas are investigated.\n-\tPerformance evaluation without generalization consideration, where the AI/ML model is trained and tested with dataset of the same deployment scenario.\no\tAI/ML vs RAT-dependent positioning methods. For the basic performance without generalization consideration, AI/ML based positioning can significantly improve the positioning accuracy compared to existing RAT-dependent positioning methods. For example, in InF-DH with clutter parameter setting {60%, 6m, 2m}, AI/ML based positioning can achieve horizontal positioning accuracy of <1m at CDF=90%, as compared to >15m for conventional positioning method.\no\tImpact of training data sample density (i.e., training dataset size for a given evaluation area). Evaluation with uniform UE distribution shows that, the larger the training dataset size (i.e., higher sample density), the smaller the positioning error (in meters), until a saturation point is reached where additional training data does not bring further improvement to the positioning accuracy.\n-\tAI/ML complexity. For a given companyâs model design, in terms of model inference complexity (model complexity and computational complexity), a lower complexity model can still achieve acceptable positioning accuracy (e.g., <1m), albeit degraded, when compared to a higher complexity model.\n-\tGeneralization study. Evaluations are carried out to investigate various generalization aspects, where the AI/ML model is trained with dataset of one deployment scenario, while tested with dataset of a different deployment scenario. The generalization aspects include: different drops; different clutter parameters; different InF scenarios; network synchronization error; UE/gNB RX and TX timing error; SNR mismatch; channel estimation error; time varying changes.\nMethods are evaluated which have been shown to be able to handle generalization issues, including:\no\tBetter training dataset construction (i.e., mixed dataset), where the training dataset is composed of data from multiple deployment scenarios, which include data from the same deployment scenario as the test dataset.\no\tFine-tuning/re-training, where the model is re-trained/fine-tuned with a dataset from the same deployment scenario as the test dataset. The impact of the amount of fine-tuning data on the positioning accuracy of the fine-tuned model is evaluated for the various generalization aspects. Evaluation results are obtained for two experiments:\nï§\tThe AI/ML model is (a) previously trained for scenario A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for scenario B with a dataset of sample density x% Ã N (#samples/m2), (c) then tested under scenario B. The horizontal positioning accuracy at CDF=90% is E meters.\nï§\tThe AI/ML model is (a) previously trained for scenario A with a dataset of sample density N (#samples/m2), (b) followed by fine-tuning for scenario B with a dataset of sample density x% Ã N (#samples/m2), (c) then tested under scenario A. The horizontal positioning accuracy at CDF=90% is E meters.\n-\tModel input size reduction. Evaluations are carried out to examine various ways to change the model input size and its impact on positioning accuracy:\no\tDifferent measurement type, for example, CIR, PDP, DP.\no\tDifferent number of consecutive time domain samples, Nt.\no\tDifferent number of non-zero samples N't selected from the Nt consecutive time domain samples (N't < Nt).\no\tDifferent number of active TRPs, N'TRP.\nThe model input size for various measurement type (CIR, PDP, DP) and dimensions (N'TRP, Nt, N't, Nport) is analyzed. Evaluation results show that, model input of different measurement type and dimensions can have different reporting overhead and positioning accuracy.\n-\tFixed TRP pattern vs dynamic TRP pattern. Evaluation results show that, approaches supporting dynamic TRP pattern may be able to achieve comparable horizontal positioning accuracy as approaches supporting fixed TRP pattern, when other design parameters are held the same.\n-\tModel output of AI/ML assisted positioning. For AI/ML assisted positioning, evaluations are carried out where the model output includes timing information and/or LOS/NLOS indicator, in the format of hard- or soft- value.\n-\tNon-ideal label in the training dataset. Evaluations are carried out to show the impact of:\no\tLabel error, where the label in the training dataset is degraded from ground-truth label by an error.\nï§\tFor direct AI/ML positioning and AI/ML assisted positioning with timing information as model output, location error in each dimension of x-axis and y-axis is modelled as a truncated Gaussian distribution.\nï§\tFor AI/ML assisted positioning where the model output includes the LOS/NLOS indicator, random LOS/NLOS label error is applied.\no\tAbsent label, where some data samples in the training dataset do not have associated labels. Semi-supervised learning is evaluated for this case.\n-\tModel monitoring. Preliminary evaluation of model monitoring methods are provided by individual companies. The following methods are shown to be feasible:\no\tLabel based methods, where ground-truth label (or its approximation) is provided for monitoring the accuracy of model output.\no\tLabel-free methods, where model monitoring does not require ground-truth label (or its approximation).\nBased on RAN1 evaluations of AI/ML based positioning,\n-\tIt is beneficial to support both direct AI/ML and AI/ML assisted positioning approaches since they can significantly improve the positioning accuracy compared to existing RAT-dependent positioning methods in the evaluated indoor factory scenarios.\n-\tBoth UE-side model and NW-side model can significantly improve the positioning accuracy compared to existing RAT-dependent positioning methods.\n-\tIt is desired to apply methods to handle generalization aspects.\n-\tIt is desired to consider training data collection requirements.\n-\tIf AI/ML based positioning is considered for normative work, it is desired to further investigate model input design aspects: the model input type (e.g., CIR, PDP, DP), dimension (e.g., parameters N'TRP, Nt, N't, Nport) and related format (e.g., for the timing information: absolute time or relative time) considering the trade-off of positioning accuracy, signalling overhead, and AI/ML complexity.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "7\tPotential specification impact assessment",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "7.1\tPhysical layer aspects",
                    "description": "",
                    "summary": "",
                    "text_content": "In this clause, aspects related to, e.g., the potential specification of the AI Model lifecycle management, and dataset construction for training, validation and test for the selected use cases are considered.\nIn addition, use case and collaboration level specific specification impact is documented, such as new signalling, means for training and validation data assistance, assistance information, measurement, and feedback.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "7.1.1\tCommon framework",
                            "text_content": "Items considered for studying the necessity, feasibility, potential specification impact:\nPerformance monitoring\nThe following metrics/methods for AI/ML model monitoring in lifecycle management per use case are considered:\n-\tMonitoring based on inference accuracy, including metrics related to intermediate KPIs\n-\tMonitoring based on system performance, including metrics related to system peformance KPIs\n-\tOther monitoring solutions, at least the following 2 options.\n-\tMonitoring based on data distribution\n-\tInput-based: e.g., Monitoring the validity of the AI/ML input, e.g., out-of-distribution detection, drift detection of input data, or SNR, delay spread, etc.\n-\tOutput-based: e.g., drift detection of output data\n-\tMonitoring based on applicable condition\nNote:\tMonitoring metric calculation may be done at NW or UE\nMethods to assess/monitor the applicability and expected performance of an inactive model/functionality, including the following examples for the purpose of activation/selection/switching of UE-side models/UE-part of two-sided models /functionalities (if applicable):\n-\tAssessment/Monitoring based on the additional conditions associated with the model/functionality\n-\tAssessment/Monitoring based on input/output data distribution\n-\tAssessment/Monitoring using the inactive model/functionality for monitoring purpose and measuring the inference accuracy\n-\tAssessment/Monitoring based on past knowledge of the performance of the same model/functionality (e.g., based on other UEs)\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.1.2\tCSI feedback enhancement",
                            "text_content": "Items considered for studying the necessity, feasibility, potential specification impact:\nIn CSI compression using two-sided model use case:\nPerformance monitoring:\n-\tModel performance monitoring related assistance signalling and procedure.\n-\tMetrics/methods including:\n-\tIntermediate KPIs (e.g., SGCS)\n-\tEventual KPIs (e.g., Throughput, hypothetical BLER, BLER, NACK/ACK).\n-\tLegacy CSI based monitoring: schemes using additional legacy CSI reporting\n-\tOther monitoring solutions, at least including the following option:\n-\tInput or Output data based monitoring: such as data drift between training dataset and observed dataset and out-of-distribution detection\n-\tNW-side performance monitoring:  NW monitors the performance and make decisions of model/functionality activation/ deactivation/updating/switching. Impact to enable performance monitoring using an existing CSI feedback scheme as the reference, including the association between AI/ML scheme and existing CSI feedback scheme for monitoring, are considered. Note: The metric for monitoring and comparison includes intermediate KPI and eventual KPI.\n-\tUE-side performance monitoring: UE monitors the performance and reports to Network, NW makes decisions of model/functionality activation/deactivation/updating/switching. Impact on triggering and means for reporting the monitoring metrics, including periodic/semi-persistent and aperiodic reporting, and other reporting initiated from UE, are not precluded.\nIntermediate KPI based model monitoring:\nThe following intermediate KPI-based model monitoring options were proposed by companies:\n-\tNW-side monitoring based on the target CSI with realistic channel estimation associated to the CSI report, reported by the UE or obtained from the UE-side.\n-\tUE-side monitoring based on the output of the CSI reconstruction model, subject to the aligned format, associated to the CSI report, indicated by the NW or obtained from the network side.\n-\tNetwork may configure a threshold criterion to facilitate UE to perform model monitoring.\n-\tUE-side monitoring based on the output of the CSI reconstruction model at the UE-side\n-\tNote: CSI reconstruction model at the UE-side can be the same or different comparing to the actual CSI reconstruction model used at the NW-side. Network may configure a threshold criterion to facilitate UE to perform model monitoring.\nFallback mode:\n-\tPotential specification impact for supporting co-existence and fallback mechanisms between AI/ML-based CSI feedback mode and legacy non-AI/ML-based CSI feedback mode\nNW/UE alignment:\n-\tAlignment of the quantization/dequantization method and the feedback message size between Network and UE, including the following:\n-\tFor vector quantization scheme, the format and size of the VQ codebook, and the size and segmentation method of the CSI generation model output\n-\tFor scalar quantization scheme, uniform and non-uniform quantization with format, e.g., quantization granularity, consisting of distribution of bits assigned to each float.\n-\tQuantization alignment for CSI feedback between CSI generation part at the UE and CSI reconstruction part at the NW is needed, e.g.,\n-\tthrough model pairing process,\n-\talignment based on standardized quantization scheme.\n-\tAdditional methods not precluded.\nModel input/output:\n-\tOutput-CSI-UE and input-CSI-NW at least for Precoding matrix\n-\tOption 1a: The precoding matrix in spatial-frequency domain\n-\tOption 1b: The precoding matrix represented using angular-delay domain projection\n-\twhether Option 2: Explicit channel matrix (i.e., full Tx * Rx MIMO channel) is also studied depends on the performance evaluations:\n-\tOption 2a: raw channel is in spatial-frequency domain\n-\tOption 2b: raw channel is in angular-delay domain\n-\tCSI part 1 includes at least CQI for first codeword, RI, and information representing the part 2 size. CSI part 2 includes at least the content of CSI generation part output. Other CSI report formats are not precluded.\nUE side data collection:\n-\tEnhancement of CSI-RS configuration to enable higher accuracy measurement.\n-\tAssistance information for UE data collection for categorizing the data in forms of ID for the purpose of differentiating characteristics of data due to specific configuration, scenarios, site etc.\n-\tThe provision of assistance information needs to consider feasibility of disclosing proprietary information to the other side.\n-\tSignalling for triggering the data collection\nNW side data collection:\n-\tEnhancement of SRS and/or CSI-RS measurement and/or CSI reporting to enable higher accuracy measurement.\n-\tContents of the ground-truth CSI including:\n-\tData sample type, e.g., precoding matrix, channel matrix etc.\n-\tData sample format: scaler quantization and/or codebook-based quantization (e.g., e-type II like).\n-\tAssistance information (e.g., time stamps, and/or cell ID, Assistance information for Network data collection for categorizing the data in forms of ID for the purpose of differentiating characteristics of data due to specific configuration, scenarios, site etc., and data quality indicator)\n-\tLatency requirement for data collection\n-\tSignalling for triggering the data collection\n-\tGround-truth CSI report for NW side data collection for model performance monitoring, including:\n-\tScalar quantization for ground-truth CSI\n-\tCodebook-based quantization for ground-truth CSI\n-\tRRC signalling and/or L1 signalling procedure to enable fast identification of AI/ML model performance\nAperiodic/semi-persistent or periodic ground-truth CSI report\n-\tGround-truth CSI format for model training, including scalar or codebook-based quantization for ground-truth CSI. The number of layers for which the ground-truth data is collected, and whether UE or NW determine the number of layers for ground-truth CSI data collection, are considered.\nIn CSI compression using two-sided model use case with training collaboration Type 3, for sequential training, at least the following aspects have been identified for dataset delivery from RAN1 perspective, including:\n-\tDataset and/or other information delivery from UE side to NW side, which can be used at least for CSI reconstruction model training\n-\tDataset and/or other information delivery from NW side to UE side, which can be used at least for CSI generation model training\n-\tPotential dataset delivery methods including offline delivery, and over the air delivery\n-\tData sample format/type\n-\tQuantization/de-quantization related information\nCSI configuration and report:\n-\tNW configuration to determine CSI payload size, e.g., possible CSI payload size, possible rank restriction and/or other related configuration.\n-\tHow UE determines/reports the actual CSI payload size and/or other CSI related information within constraints configured by the network.\n-\tRelevant UCI format considering the legacy CSI reporting principle with CSI Part 1 and Part 2 as a starting point, where Part 1 has a network configured fixed size and Part 2 size is dynamic, determined by information in Part 1.\nFor CQI determination in CSI report, if CQI in CSI report is configured.\n-\tOption 1: CQI is NOT calculated based on the output of CSI reconstruction part from the realistic channel estimation, including\n-\tOption 1a: CQI is calculated based on target CSI with realistic channel measurement\n-\tOption 1b: CQI is calculated based on target CSI with realistic channel measurement and potential adjustment\n-\tOption 1c: CQI is calculated based on legacy codebook\n-\tOption 2: CQI is calculated based on the output of CSI reconstruction part from the realistic channel estimation, including\n-\tOption 2a: CQI is calculated based on CSI reconstruction output, if CSI reconstruction model is available at the UE and UE can perform reconstruction model inference with potential adjustment\n-\tNote: CSI reconstruction part at the UE can be different comparing to the actual CSI reconstruction part used at the NW.\n-\tOption 2b: CQI is calculated using two stage approach, UE derive CQI using precoded CSI-RS transmitted with a reconstructed precoder.\n-\tNotes: feasibility of different options should be evaluated. Gap analyses between the UE side CQI calculation results and the NW side results, as well as the impact on the scheduling performance should be evaluated. Complexity of CQI calculation needs to be evaluated, including the computing complexity and potential RS/signalling overhead.\nFeasibility and methods to support the legacy CSI reporting principles:\n-\tThe priority rule regarding CSI collision handling and CSI omission\n-\tCodebook subset restriction\n-\tInput-CSI-NW/output-CSI-UE considered in angular-delay domain, beam restriction can be based on legacy SD basis vector-based input CSI in angular domain.\n-\tCSI processing Unit\nPotential specification enhancement on:\n-\tCSI-RS configurations (not including CSI-RS pattern design enhancements)\n-\tCSI configuration\n-\tFor network to indicate CSI reporting related information, e.g., gNB indication to the UE of one or more of following:\n-\tInformation indicating CSI payload size\n-\tInformation indicating quantization method/granularity\n-\tRank restriction\n-\tOther payload related aspects\n-\tCSI reporting configurations\n-\tFor UE determination/reporting of the actual CSI payload size, UE reports related information as configured by the NW\n-\tCSI report UCI mapping/priority/omission\n-\tCSI processing procedures\nIn CSI compression using two-sided model use case, feasibility and procedure to align the information that enables the UE to select a CSI generation model(s) compatible with the CSI reconstruction model(s) used by the gNB is studied. At least the following options have been proposed by companies to define the pairing information used to enable the UE to select a CSI generation model(s) that is compatible with the CSI reconstruction model(s) used by the gNB:\n-\tOption 1: The pairing information is in the forms of the CSI reconstruction model ID that NW will use.\n-\tOption 2: The pairing information is in the forms of the CSI generation model ID that the UE will use.\n-\tOption 3: The pairing information is in the forms of the paired CSI generation model and CSI reconstruction model ID.\n-\tOption 4: The pairing information is in the forms of by the dataset ID during type 3 sequential training.\n-\tOption 5: The pairing information is in the forms of a training session ID to a prior training session (e.g., API) between NW and UE.\n-\tOption 6: The pairing information is up to UE/NW offline co-engineering alignment, transparent to 3GPP specification.\n-\tNote: the disclosure of the vendor information during the model pairing procedure and model identification procedure should be considered.\n-\tNote: If each UE side model is compatible with all NW side model, the information is not needed for the UE.\n-\tNote: Above does not imply there is a need for a central entity for defining/storing/maintaining the IDs.\nIn CSI prediction using UE-sided model use case:\nData collection:\nIn CSI prediction using UE sided model use case, at least the following aspects have been proposed by companies on data collection, including:\n-\tSignalling and procedures for the data collection\n-\tData collection indicated by NW\n-\tRequested from UE for data collection\n-\tCSI-RS configuration\n-\tAssistance information for categorizing the data, if needed\n-\tThe provision of assistance information needs to consider feasibility of disclosing proprietary information to the other side.\nPerformance monitoring:\nFor CSI prediction using UE side model use case, at least the following aspects have been proposed by companies on performance monitoring for functionality-based LCM:\n-\tType 1:\n-\tUE calculates the performance metric(s)\n-\tUE reports performance monitoring output that facilitates functionality fallback decision at the network\n-\tPerformance monitoring output details can be further defined\n-\tNW may configure threshold criterion to facilitate UE side performance monitoring (if needed).\n-\tNW makes decision(s) of functionality fallback operation (fallback mechanism to legacy CSI reporting).\n-\tType 2:\n-\tUE reports predicted CSI and/or the corresponding ground-truth\n-\tNW calculates the performance metrics.\n-\tNW makes decision(s) of functionality fallback operation (fallback mechanism to legacy CSI reporting).\n-\tType 3:\n-\tUE calculates the performance metric(s)\n-\tUE reports performance metric(s) to the NW\n-\tNW makes decision(s) of functionality fallback operation (fallback mechanism to legacy CSI reporting).\n-\tFunctionality selection/activation/deactivation/switching as defined for other UE side use cases can be reused, if applicable.\n-\tConfiguration and procedure for performance monitoring\n-\tCSI-RS configuration for performance monitoring\n-\tPerformance metric including at least intermediate KPI (e.g., NMSE or SGCS)\n-\tUE report, including periodic/semi-persistent/aperiodic reporting, and event driven report\n-\tNote: down selection is not precluded.\n-\tNote: UE may make decision within the same functionality on model selection, activation, deactivation, switching operation transparent to the NW.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.1.3\tBeam management",
                            "text_content": "Items considered for studying the necessity, feasibility, potential specification impact:\nPerformance monitoring:\nFor the performance monitoring of BM-Case1 and BM-Case2:\n-\tPerformance metric(s) with the following alternatives:\n-\tAlt.1: Beam prediction accuracy related KPIs, e.g., Top-K/1 beam prediction accuracy\n-\tAlt.2: Link quality related KPIs, e.g., throughput, L1-RSRP, L1-SINR, hypothetical BLER\n-\tAlt.3: Performance metric based on input/output data distribution of AI/ML\n-\tAlt.4: The L1-RSRP difference evaluated by comparing measured RSRP and predicted RSRP\n-\tBenchmark/reference for the performance comparison, including:\n-\tAlt.1: The best beam(s) obtained by measuring beams of a set indicated by gNB (e.g., Beams from Set A)\n-\tAlt.4: Measurements of the predicted best beam(s)Â corresponding toÂ model output (e.g., Comparison between actual L1-RSRP and predicted RSRP of predicted Top-1/K Beams)\n-\tSignalling/configuration/measurement/report for model monitoring, e.g., signalling aspects related to assistance information (if supported), Reference signals\nFor BM-Case1 and BM-Case2 with a UE-side AI/ML model:\n-\tType 1 performance monitoring:\n-\tConfiguration/Signalling from gNB to UE for measurement and/or reporting\n-\tUE may have different operations\n-\tOption 1 (NW-side performance monitoring): UE sends reporting to NW (e.g., for the calculation of performance metric at NW)\n-\tOption 2 (UE-assisted performance monitoring): UE calculates performance metric(s), either reports it to NW or reports an event to NW based on the performance metric(s)\n-\tIndication from NW for UE to do LCM operations\n-\tNote: At least the performance and reporting overhead of model monitoring mechanism should be considered\n-\tType 2 performance monitoring:\n-\tIndication/request/report from UE to gNB for performance monitoring\n-\tNote: The indication/request/report may be not needed in some case(s)\n-\tConfiguration/Signalling from gNB to UE for performance monitoring measurement and/or reporting\n-\tIf it is for UE side model monitoring, UE makes decision(s) of model selection/activation/ deactivation/switching/fallback operation\n-\tMechanism that facilitates the UE to detect whether the functionality/model is suitable or no longer suitable\nFor BM-Case1 and BM-Case2 with a NW-side AI/ML model\n-\tBeam measurement and report for model monitoring\n-\tUE reporting of beam measurement(s) based on a set of beams indicated by gNB.\n-\tSignalling, e.g., RRC-based, L1-based.\n-\tNote: This may or may not have specification impact.\n-\tNW monitors the performance metric(s) and makes decision(s) of model selection/activation/ deactivation/switching/ fallback operation\n-\tNote: Performance and UE complexity, power consumption should be considered.\nTable 7.2.3-1 summarizes applicability of various alternatives for performance metric(s) of AI/ML model monitoring for BM-Case1 and BM-Case2.\nTable 7.2.3-1: Alternatives for Performance metric(s) of AI/ML model monitoring \nfor BM-Case 1 and BM-Case 2\nNote1:\tThe above analysis shall not give an indication about whether/which metric is supported or specified.\nNote2:\tMonitoring performance of the above alternatives are not addressed in the table.\nData collection:\nAt UE side for UE-side AI/ML model:\n-\tUE reporting to NW supported/preferred configurations of DL RS transmission.\n-\tTrigger/initiating data collection considering:\n-\tOption 1: data collection initiated/triggered by configuration from NW.\n-\tOption 2: request from UE for data collection.\n-\tSignalling/configuration/measurement/report for data collection, e.g., signalling aspects related to assistance information (if supported), Reference signals, content/type of the collected data, configuration related to Set A and/or Set B, information on association/mapping of Set A and Set B\n-\tAssistance information from Network to UE for UE data collection for categorizing the data for the purpose of differentiating characteristics of the data (if supported). The assistance information should preserve privacy/proprietary information.\nAt NW side for NW-side AI/ML model:\n-\tMechanism related to the reporting.\n-\tAdditional information for content of the reporting.\n-\tReporting overhead reduction.\n-\tSignalling/configuration/measurement/report for data collection, e.g., signalling aspects related to assistance information (if supported), Reference signals.\nRegarding data collection for NW-side AI/ML model regarding the contents of collected data:\n-\tOpt.1: M1 L1-RSRPs (corresponding to M1 beams) with the indication of beams (beam pairs) based on the measurement corresponding to a beam set, where M1 can be larger than 4, if applicable.\n-\tOpt.2: M2 L1-RSRPs (corresponding to M2 beams) based on the measurement corresponding to a beam set, where M2 can be larger than 4, if applicable.\n-\tOpt.3: M3 beam (beam pair) indices based on the measurement corresponding to a beam set, where M3 can be larger than 4, if applicable.\n-\tNote: Overhead, UE complexity and power consumption are to be considered for the above options.\nRegarding data collection for NW-side AI/ML model of BM-Case1 and BM-Case2, the following approaches have been identified for overhead reduction:\n-\tthe omission/selection of collected data\n-\tthe compression of collected data\n-\tNote1: For the different purposes of data collection, the overhead reduction mechanisms and corresponding specification impacts may be different.\n-\tNote2: Support of any mechanism(s) (if necessary) for each LCM purpose and the potential spec impact (if any) are separate discussions\n-\tNote 3: UE complexity and power consumption should be considered\nRegarding data collection for NW-side AI/ML model of BM-Case1 and BM-Case2, the following reporting signalling for beam-specific aspects maybe applicable:\n-\tL1 signalling to report the collected data\n-\tHigher-layer signalling to report the collected data\n-\tAt least not applicable to AI/ML model inference\n-\tNote1: Higher layer signalling design is up to RAN2\n-\tNote2: Whether each signalling applicable to each LCM purpose is a separate discussion\n-\tNote3: The legacy signalling principle (e.g. RSRP reporting for L1) can be re-used\nModel Inference related:\nIn order to facilitate the AI/ML model inference:\n-\tEnhanced or new configurations/UE reporting/UE measurement, e.g., enhanced or new beam measurement and/or beam reporting\n-\tEnhanced or new signalling for measurement configuration/triggering\n-\tSignalling of assistance information (if applicable)\nFor BM-Case1 and BM-Case2 with a UE-side AI/ML model:\n-\tIndication of the associated Set A from network to UE, e.g., association/mapping of beams within Set A and beams within Set B if applicable\n-\tBeam indication from network for UE reception, which may or may not have additional specification impact (e.g., legacy mechanism may be reused), particularly:\n-\thow to perform beam indication of beams in Set A not in Set B.  Note: also applicable to NW-side AI/ML model. Note: At least for BM-Case1 with a UE-side AI/ML model, the legacy TCI state mechanism can be used to perform beam indication of beams\n-\tNote: For DL beam pair prediction, there is no consensus to support the reporting of the predicted Rx beam(s) (e.g., Rx beam ID, Rx beam angle information, etc) from the UE to the network.\n-\tPredicted L1-RSRP(s) corresponding to the DL Tx beam(s) or beam pair(s)\n-\tWhether/how to differentiate predicted L1-RSRP and measured L1-RSRP\n-\tConfidence/probability information related to the output of AI/ML model inference (e.g., predicted beams)\nFor BM-Case1 and BM-Case2 with a NW-side AI/ML model:\n-\tL1 beam reporting enhancement for AI/ML model inference:\n-\tUE to report the measurement results of more than 4 beams in one reporting instance\n-\tOther L1 reporting enhancements can be considered\nFor BM-Case1 with a UE-side AI/ML model:\n-\tL1 signalling to report the following information of AI/ML model inference to NW:\n-\tThe beam(s) that is based on the output of AI/ML model inference.\nFor BM-Case2 with a UE-side AI/ML model:\n-\tL1 signalling to report the following information of AI/ML model inference to NW:\n-\tThe beam(s) of N future time instance(s) that is based on the output of AI/ML model inference.\n-\tInformation about the timestamp corresponding the reported beam(s).\nFor BM-Case 2:\n-\tReporting information about measurements of multiple past time instances in one reporting instance. Notes: Only applicable to NW-side AI/ML model. The potential performance gains of measurement reporting should be justified by considering UCI payload overhead.\nAssistance information:\nRegarding the explicit assistance information from UE to network for NW-side AI/ML model, RAN1 has no consensus to support the following information\n-\tUE location\n-\tUE moving direction\n-\tUE Rx beam shape/direction\nRegarding the explicit assistance information from network to UE for UE-side AI/ML model, RAN1 has no consensus to support the following information\n-\tNW-side beam shape information\n-\tE.g., 3dB beamwidth, beam boresight directions, beam shape, Tx beam angle, etc.\n-\tNote: Other information (e.g., relative information) of Tx beam(s) preserving sensitive proprietary information is a separate discussion\n-\te.g., some information following the same principle of Rel-17 positioning agreement\nFor BM-Case1 and BM-Case2 with a UE-side AI/ML model, consistency / association of Set B beams and Set A beams across training and inference is beneficial from performance perspective.\nNote: Whether specification impact is needed is a separate discussion.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 7.2.3-1: Alternatives for Performance metric(s) of AI/ML model monitoring \nfor BM-Case 1 and BM-Case 2",
                                    "table number": 15,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "7.1.4\tPositioning accuracy enhancements",
                            "text_content": "Items considered for studying the necessity, feasibility, potential specification impact:\nAI/ML functionality and model identification:\n-\tValidity conditions, e.g., applicable area/[zone/]scenario/environment and time interval, etc.\n-\tModel capability, e.g., positioning accuracy quality and model inference latency.\n-\tConditions and requirements, e.g., required assistance signalling and/or reference signals configurations, dataset information.\n-\tNote: the above-mentioned examples and terms \"validity conditions\", \"model capability\", and \"Conditions and requirements\" can be referred to the conditions and additional conditions discussed in the context of the model identification and functionality identification in clause 4.2.\nTraining data generation for AI/ML based positioning:\n-\tThe following options of entity and mechanisms to generate ground-truth label are identified:\n-\tUE with estimated/known location generates ground-truth label and corresponding label quality indicator\n-\tBased on non-NR and/or NR RAT-dependent and/or NR RAT-independent positioning methods\n-\tAt least for UE-based positioning with UE-side model (Case 1) and UE-assisted positioning with UE-side model (Case 2a)\n-\tNetwork entity generates ground-truth label and corresponding label quality indicator\n-\tBased on non-NR and/or NR RAT-dependent and/or NR RAT-independent positioning methods\n-\tAt least for UE-assisted/LMF-based positioning with LMF-side model (Case 2b),  NG-RAN node assisted positioning with gNB-side model (Case 3a) and NG-RAN node assisted positioning with LMF-side model (Case 3b)\n-\tAt least PRU is identified to generate ground-truth label for UE-based positioning with UE-side model (Case 1) and UE-assisted positioning with UE-side model (Case 2a)\n-\tAt least LMF with known PRU location is identified to generate ground-truth label for UE-assisted/LMF-based positioning with LMF-side model (Case 2b) and NG-RAN node assisted positioning with LMF-side model (Case 3b)\n-\tAt least network entity with known PRU location is identified to generate ground-truth label for NG-RAN node assisted positioning with gNB-side model (Case 3a)\n-\tNote: User data privacy needs to be preserved\n-\tThe following options of entity to generate other training data (at least measurement corresponding to model input) are identified:\n-\tFor UE-based with UE-side model (Case 1) and UE-assisted positioning with UE-side (Case 2a) or LMF-side model (Case 2b)\n-\tPRU\n-\tUE\n-\tFor NG-RAN node assisted positioning with Network-side model (Case 3a and Case 3b)\n-\tTRP\n-\tNote: Transfer of training data from the entity generating training data to a different entity is not precluded and associated potential specification impact is to be considered\nData collection for AI/ML based positioning:\nRegarding data collection for AI/ML based positioning, at least the following information of data with potential specification impact are identified.\n-\tGround-truth label\n-\tReport from the label data generation entity\n-\tMeasurement (corresponding to model input)\n-\tReport from the measurement data generation entity\n-\tQuality indicator\n-\tFor and/or associated with ground-truth label and/or measurement\n-\tReport from the label and/or the measurement data generation entity and/or as request from a different (e.g., data collection, etc.) entity\n-\tRS configuration(s)\n-\tAt least for deriving measurement\n-\tRequest from data generation entity (UE/PRU/TRP) to LMF and/or as LMF assistance signalling to UE/PRU/TRP\n-\tNote 1: There may not be any enhancements on top of existing RS configuration(s) or any new RS configuration(s) for positioning measurement\n-\tTime stamp\n-\tAt least for and/or associated with collected data\n-\tSeparate time stamp for measurement and ground-truth label, when measurement and ground-truth label are generated by different entities\n-\tReport From data generation entity together with collected data and/or as LMF assistance signalling\n-\tNote 2: There may not be any enhancements on top of time stamp in existing positioning measurement report or any new time stamp report for positioning measurement\n-\tNote 3: Whether and how the above information can be applied to different aspects of AI/ML LCM (e.g., training, updating, monitoring, etc.) can be discussed\n-\tNote 4: Transfer of data from the entity generating data to a different entity is not precluded from RAN1 perspective\n-\tNote 5: If any specification impact is identified, the impact may be different between positioning use cases (Case 1/2a/2b/3a/3b).\n-\tNote 6: The necessity of other information (e.g., scenario identifier. LOS/NLOS condition, timing error, etc.) for data collection can be discussed\n-\tDetails of request/report of label and/or other training data, and to enable delivering the collected label and/or other training data to the training entity when the training entity is not the same entity to obtain label and/or other training data\n-\tAssistance signalling indicating reference signal configuration(s) to derive label and/or other training data\n-\tRequest/report of training data: Ground-truth label; Measurement corresponding to model input; Associated information of ground-truth label and/or measurement corresponding to model input\n-\tAssistance signalling and procedure to facilitate generating training data: Reference signal (e.g., PRS/SRS) configuration(s) and configuration identifier; Assistance information, e.g., between LMF and UE/PRU, for label calculation/generation, and label validity/quality condition, etc.\n-\tNote: whether such assistance signalling and procedure can be applied to other aspect(s) of AI/ML model LCM can also be discussed\n-\tNotes: Study may consider different entity to generate training data as well as different types of training data when applicable. Study considers both of the following cases when applicable: when the training entity is the same entity to generate training data, and when the training entity is not the same entity to generate training data\nModel monitoring:\n- \tAssistance signalling and procedure at least for UE-side model\n-\tReport/feedback and procedure at least for Network-side model\n-\tNote: study is applicable to both of the following cases:\n-\tModel inference and model monitoring at the same entity\n-\tEntity to perform the model monitoring is not the same entity for model inference\n-\tData for computing monitoring metric:\n-\tIf monitoring based on model output: e.g., estimated UE location corresponding to model output for direct AI/ML positioning, estimated intermediate parameter(s) corresponding to model output for AI/ML assisted positioning, ground-truth label corresponding to model inference output for both direct and AI/ML assisted positioning\n-\tIf monitoring based on model input: e.g., measurement corresponding to model inference input.\n-\tAssistance signalling from LMF to UE/PRU/gNB for UE/gNB-side model monitoring.\n-\tAssistance signalling from UE/PRU for NW-side model monitoring.\n-\tIf certain type of data is necessary for computing monitoring metric:\n-\tHow an entity can be used to provide the given type of data for calculating monitoring metric: companies requested to report their assumption of the entity (or entities) used to provide the given type of data for calculating monitoring metric for each case\n-\tPotential signalling for provisioning of the given type of data for calculating associated monitoring metric\n-\tPotential assistance signalling and procedure to facilitate an entity providing data for calculating monitoring metric\n-\tPotential UE-network interaction: e.g., model monitoring decision indication between UE and network\n-\tEntity to derive monitoring metric\n-\tUE at least for Case 1 and 2a (with UE-side model)\n-\tgNB at least for Case 3a (with gNB-side model)\n-\tLMF at least for Case 2b and 3b (with LMF-side model)\n-\tFor AI/ML based positioning, LMF for Case 2a (with UE-side model) and Case 3a (with gNB-side model) is identified as the entity to derive the monitoring metric at least when monitoring is based on provided ground-truth label (or its approximation).\n-\tIf model monitoring does not require ground-truth label (or its approximation).\n-\tStatistics of measurement(s) compared to the statistics associated with the training data. Note: the measurement(s) may or may not be the same as model input.\n-\tExamples used in contributions: norm of model input, mean, min/max of some statistics related to measurement and/or model input, median or data temporal/spatial distribution\n-\tStatistics of model output compared to the statistics associated with the training data and/or its own previous inference output\n-\tExamples used in contributions: mean, standard deviation, variance, etc. of some statistics related to model output\n-\tFor monitoring UE-side and gNB-side model for AI/ML based positioning:\n-\tSignalling from LMF to facilitate the monitoring entity to derive the monitoring metric (if needed)\n-\tSignalling from monitoring entity to request measurement(s) (if needed)\n-\tSignalling for potential request/report of monitoring metric (if needed)\n-\tNote: there may not be any specification impact\n-\tFor monitoring LMF-side model for AI/ML based positioning\n-\tSignalling from LMF to request measurement(s) (if needed)\n-\tAssistance signalling and procedure, e.g., RS configuration(s) for measurement, measurement statistics as compared to the model input statistics of the training data, etc.\n-\tReport of the calculated metric and/or model monitoring decision\n-\tIf model monitoring requires and is provided ground-truth label (or its approximation)\n-\tMonitoring metric: statistics of the difference between model output and provided ground-truth label.\n-\tExamples used in contributions: mean, standard deviation, instantaneous value, threshold of ground-truth label (or its approximation)\n-\tFor monitoring UE-side and gNB-side model for AI/ML based positioning:\n-\tSignalling from monitoring entity to request ground-truth label (if needed)\n-\tSignalling from monitoring entity to request model output (if needed)\n-\tSignalling for potential request/report of monitoring metric (if needed)\n-\tFor monitoring LMF-side model for AI/ML based positioning\n-\tSignalling from LMF to request measurement(s) (if needed)\n-\tProvisioning of ground-truth label and associated label quality.\n-\tAssistance signalling and procedure, e.g., from LMF to UE/gNB indicating ground-truth label and/or measurement, etc.\n-\tReport of the calculated metric and/or model monitoring decision\n-\tNote: No extensive evaluation results on model monitoring metric comparison have been carried out\n-\tNote: There is no consensus during SI on whether monitoring metric will have spec impact\nModel Inference related:\n-\tFor direct AI/ML positioning (Case 2b and 3b), type of measurement(s) as model inference input considering performance impact and associated signalling overhead\n-\tPotential new measurement: CIR/PDP\n-\tExisting measurement: e.g., RSRP/RSRPP/RSTD\n-\tNote: Details of potential new measurement and/or potential enhancement to existing measurement is to be studied.\n-\tFor AI/ML assisted positioning with UE-assisted (Case 2a) and NG-RAN node assisted positioning (Case 3a):\n-\tMeasurement report to carry model output to LMF\n-\tNew measurement report: e.g., ToA, path phase\n-\tExisting measurement report: e.g., RSTD, LOS/NLOS indicator, RSRPP\n-\tEnhancement of existing measurement report: e.g., soft information/high resolution of RSTD\n-\tAt least the following types of model inference output are identified as candidates providing performance benefits:\n-\tTiming estimation\n-\tNote: the report to LMF is derived based on and maybe different from the model inference output\n-\tLOS/NLOS indicator\n-\tAssistance signalling and procedure to facilitate model inference for both UE-side and Network-side model\n-\tRS configurations\nThe specification impact related to the following items is assessed:\n-\tTypes of measurement as model inference input\n-\tnew measurement\n-\texisting measurement\n-\tUE is assumed to perform measurement as model inference input for Case 1, Case 2a and Case 2b; TRP is assumed to perform measurement as model inference input for Case 3a and Case 3b\n-\tReport of measurements as model inference input to LMF for LMF-side model (Case 2b and Case 3b)\n-\tFor AI/ML assisted positioning, new measurement report and/or potential enhancement of existing measurement report as model output to LMF for UE-assisted (Case 2a) and NG-RAN node assisted positioning (Case 3a)\n-\tAssistance signalling and procedure to facilitate model inference for both UE-side and Network-side model\n-\tNew and/or enhancement to existing assistance signalling\n-\tNote: Whether such assistance signalling and procedure can be applied to other aspect(s) of AI/ML model LCM can also be discussed\nFor direct AI/ML positioning with LMF-side model (Case 2b and 3b), the following types of measurement report are identified if beneficial and necessary (e.g., tradeoff positioning accuracy requirement and signalling overhead),\n-\tTake into account that existing Rel-16/17 measurement and/or expected Rel-18 measurement report may contain timing, power and phase information of the channel response\n-\tmeasurement report, which contains timing, power and phase information of the channel response\n-\tAt least for Case 3b\n-\tMeasurement report, which contains timing and power information of the channel response\n-\tMeasurement report, which contains timing information of the channel response\n-\tNote: Combinations of multiple measurement reports and/or post processing of the measurement reports are not precluded\nFor direct AI/ML positioning with LMF-side model (Case 2b and 3b), the following types of measurement report with potential specification impact have been studied for AI/ML based positioning accuracy enhancement\n-\tMeasurement report, which contains timing, power and phase information of the channel response\n-\tIf support, potential specification impact including new measurement report or enhancement to existing measurement report\n-\tE.g., truncation, [feature extraction,] alignment of sample/path determination\n-\tMeasurement report, which contains timing and power information of the channel response\n-\tIf support, potential specification impact including new measurement report or enhancement to existing measurement report\n-\tE.g., truncation, [feature extraction,] alignment of sample/path determination\n-\tMeasurement report, which contains timing information of the channel response\n-\tIf support, potential specification impact including enhancement to existing measurement report\n-\tE.g., alignment of sample/path determination\nLCM:\n-\tFor AI/ML based positioning accuracy enhancement, at least for Case 1 and Case 2a (model is at UE-side)\n-\twhich aspects should be specified as conditions of a Feature/FG available for functionality-based LCM.\n-\twhich aspects should be considered as additional conditions, and how to include them into model description information during model identification for model ID-based LCM.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "7.2\tProtocol aspects",
                    "description": "",
                    "summary": "",
                    "text_content": "In this clause, considering the use cases and as per RAN1 input, aspects related to life cycle management signalling procedures, model identification, data collection, model transfer/delivery, UE capability reporting, and applicability-related reporting are studied.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "7.2.1\tCommon framework",
                            "text_content": "As per the functional framework in Figure 4.4-1, in this clause the signalling procedures for different scenarios for model-ID-based management and/or functionality-based management are exemplified. The procedures can at least be considered for UE-side models. From clause 4.2, these can include scenarios for which the management decision is taken by the network or by the UE. For network-side decision, this can be either network-initiated, or UE-initiated and requested to the network. While for UE-side decision, this can be either event-triggered as configured by the network and where the UEâs decision is reported to the network, or UE-autonomous, with or without UEâs decision being reported to the network.\nNote:\tThe mapping of these scenarios to specific use cases can be left to RAN1.\nNote:\tThe scenarios discussed below shall not imply support for all potential functionality and/or model Management Instructions (e.g., (de)activation, selection, switching, fallback, etc.) for every use case.\nNote:\tIn the figures below, Management Request/Management Instruction/Management Decision Report may include details about the model/functionality selection, (de)activation, switching or fallback.\n-\tDecision by the network\no\tNetwork-initiated\nThe figure depicts a network decision tree with two branches, one for network-initiated AI/ML management and another for network-initiated AI/ML management. The AI/ML management branch includes decision points for network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/ML, network-initiated AI/\nFigure 7.2.1.1-1: Network decision, network-initiated AI/ML management\nThe case where the LCM decision is taken and initiated by the network is depicted in Figure 7.2.1.1-1.\nNote:\tThe Management Instruction may be a result of model/functionality performance monitoring at the network.\nNote:\tThe Management Instruction may include information about the model or functionality.\no\tUE-initiated and requested to the network\nThe figure depicts a network decision scenario where an UE initiates AI/ML management, illustrating the process of selecting an AI/ML solution for network optimization. The figure includes a network decision tree, which outlines the steps involved in selecting an AI/ML solution, and a decision matrix, which shows the trade-offs between different AI/ML solutions. The figure also includes a visual representation of the AI/ML decision tree, which helps to understand the decision-making process.\nFigure 7.2.1.1-2: Network decision, UE-initiated AI/ML management\nThe case where the LCM decision is taken by the network but where the request is initiated by the UE is depicted in Figure 7.2.1.1-2.\nNote:\tThe Management Request may be a result of model/functionality monitoring at the UE.\nNote:\tIn response to the Management Request, the network may send a Management Instruction to the UE.\nNote:\tThe Management Request may include information about the model or functionality.\nNote:\tThe network may accept or reject the Management Request from the UE.\nNote:\tThe Management Request may include information related to model/functionality performance metrics.\nNote:\tThe Management Instruction may include information about the model or functionality.\n-\tDecision by the UE\no\tEvent-triggered as configured by the network, UEâs decision is reported to the network\n\nThe figure depicts a network scenario where an UE (User Equipment) makes a decision based on the event-triggered configuration of the network. The network is configured to trigger an event when a specific event occurs, such as a user's action or a change in the network state. This configuration allows the network to react to user behavior and ensure a smooth user experience.\nFigure 7.2.1.1-3: UE decision, event-triggered as configured by the network\nThe case where the LCM decision is taken by the UE according to prior network configuration is depicted in Figure 7.2.1.1-3.\nNote:\tUse case-specific events/conditions may be configured by the network for event-triggered AI/ML management at the UE.\nNote:\tUE may send a Management Decision Report to the network following event-triggered AI/ML management at the UE.\nNote: The Management Decision Report may include information about the model or functionality.\no\tUE-autonomous, UEâs decision is reported to the network\nThe figure depicts a 5G network scenario where an UE (User Equipment) is autonomous and makes decisions independently. The network is divided into two main layers: the core network and the distributed network. The core network consists of a base station (gNB) and a set of distributed nodes, while the distributed network consists of a set of optical line terminals (OLTs) and a set of distributed nodes. The figure illustrates the network architecture, highlighting the importance of redundancy paths and the use of SDN principles for network management.\nFigure 7.2.1.1-4: UE autonomous, decision reported to the network\nThe case where the LCM decision can autonomously be taken by the UE is depicted in Figure 7.2.1.1-4.\nNote:\tThe UE may be configured to send a Management Decision Report to the network upon performing a model/functionality Management Decision.\no\tUE-autonomous, UEâs decision is not reported to the network\nFor the case where the LCM decision can autonomously be taken by the UE and where the decision is not reported to the network, the AI/ML management is transparent from a network perspective.\nAccording to the functional framework in Figure 4.4-1, a model ID can be used within functions and for different data/information/instruction flows to identify an AI/ML model. For example, a model ID could eventually be associated to the selection/(de)activation/switching of a model or linked to the \"Model Transfer/Delivery\" information.\nRAN2 assumes that a model ID can be globally unique, e.g., allowing for proper model validation and model testing procedures.\nNote:\tHow to ensure model ID uniqueness is out of RAN2 scope.\nNote:\tDetails of model training, validation and testing are out of RAN2 scope.\nAdditionally, to manage or control AI/ML models, some meta information about the models may be needed.\nNote:\tDetails on the relationship between model IDs and meta information for purposes of model control and management can be addressed during a normative phase.\nData collection plays a crucial role in enabling the different use cases. Therefore, it is important to define the best approaches for collecting data to support UE-side and network-side model inference, monitoring, and training.\nTable 7.3.1.2-1 lists existing data collection mechanisms available in current RAN specifications for the UE to report measurements to another entity acting as termination point for this data. As highlighted in clause 4.2, the analysis/selection of the data collection frameworks should focus on the RRC CONNECTED state for both data generation and reporting. As such, the Table can provide useful insights into existing methods with respect to various categories identified as relevant for data collection method selection.\nTable 7.3.1.2-1. Existing data collection methods identified.\n\nA set of general data collection principles is expected to be considered for network-side model training. These include:\n-\tUE to support data logging,\n-\tUE to report the collected data periodically, event-based, and on-demand,\n-\tThe UE memory, processing power, energy consumption, signalling overhead should be considered.\nNote:\tThe above principles can be revised depending on RAN1 requirements.\nFurthermore, and regarding the use cases in this study, the following is considered.\nFor CSI and beam management use cases, the training of network-side models can consider both gNB and OAM-centric data collection mechanisms. The gNB-centric data collection implies that the gNB can configure the UE to initiate/terminate the data collection procedure. The potential impact of L3 signalling for the reporting of collected data should be assessed.\nOn the other hand, OAM-centric data collection implies that the OAM provides the configuration (via the gNB) needed for the UE to initiate/terminate the data collection procedure. MDT framework can be considered to achieve this. The potential impact on MDT for RRC_CONNECTED state should be assessed.\nFor positioning use cases, when considering LMF-side inference, it is assumed that the LPP protocol should be applied to the data collected by UE and terminated at LMF, while the NRPPa protocol should be applied to the data collected by gNB and terminated at LMF. While for LMF-side performance monitoring, it is assumed that the LPP protocol should be applied to the data collected by UE and terminated at LMF, while the NRPPa protocol should be applied to the data collected by gNB and terminated at LMF.\nNote:\tFor gNB- and OAM-centric data collection, there may be a need to consult with RAN3 and SA5 whether/how OAM is to be involved.\nNote:\tFor possible impacts due to positioning use cases, there may be a need to consult with RAN3 whether/how NRPPa is to be involved.\nThe following proposals were discussed in RAN2:\n1.\tUE collects and directly transfers training data to the Over-The-Top (OTT) server;\n1a)\tOTT (TRansparent)\n1b)\tOTT (non-TRansparent)\n2.\tUE collects training data and transfers it to Core Network. Core Network transfers the training data to the OTT server.\n3.\tUE collects training data and transfers it to OAM. OAM transfers the needed data to the OTT server.\nRAN2 did not study or analyse these proposals and did not agree to requirements or recommendations.\nWhether there is a need to consider standardised solutions for transferring/delivering AI/ML model(s) is unclear from the outcome of the present study. Nonetheless, to support AI/ML model transfer/delivery, the following solutions are considered:\n-\tSolution 1a: gNB can transfer/deliver AI/ML model(s) to UE via RRC signalling.\n-\tSolution 2a: Core Network (except LMF) can transfer/deliver AI/ML model(s) to UE via NAS signalling.\n-\tSolution 3a: LMF can transfer/deliver AI/ML model(s) to UE via LPP signalling.\n-\tSolution 1b: gNB can transfer/deliver AI/ML model(s) to UE via UP data.\n-\tSolution 2b: Core Network (except LMF) can transfer/deliver AI/ML model(s) to UE via User Plane (UP) data.\n-\tSolution 3b: LMF can transfer/deliver AI/ML model(s) to UE via UP data.\n-\tSolution 4a: OTT server can transfer/deliver AI/ML model(s) to UE (e.g., transparent to 3GPP).\n-\tSolution 4b: OAM can transfer/deliver AI/ML model(s) to UE.\nNote:\tThe relationships between model transfer/delivery solutions and use cases can be derived from what is captured in clauses 7.3.2, 7.3.3, and 7.3.4.\nThe following areas are considered to evaluate the different model transfer/delivery solutions:\n-\tA1: Large, no upper limit model/model parameter size,\n-\tA2: Model transfer/delivery continuity (i.e., resume transmission of model (segments) across gNBs),\n-\tA3: Network controllability on model transfer/delivery (e.g., management decision at gNB),\n-\tA4: Model transfer/delivery QoS (for DRB) (including latency, etc.) and priority (for SRB).\nFor every model transfer/delivery solution, each of the above areas is analysed, focusing on the current status and gaps, and the potential impacts on RAN specification. The analysis is shown in the Tables below.\nTable 7.3.1.4-1 Analysis of current status and gaps, and potential \nRAN specification impact for Solution 1a\n\nTable 7.3.1.4-2 Analysis of current status and gaps, and potential \nRAN specification impact for Solutions 2a and 3a\nNote: \tNAS and LMF upper limits and potential impacts to NAS and LPP specifications have not been studied and feasibility on filling gaps is unknown.\nTable 7.3.1.4-3 Analysis of current status and gaps, and potential \nRAN specification impact for Solutions 1b\n\nTable 7.3.1.4-4 Analysis of current status and gaps, and potential \nRAN specification impact for Solutions 2b and 3b\n\nTable 7.3.1.4-5 Analysis of current status and gaps, and potential \nRAN specification impact for Solutions 4a\n\nTable 7.3.1.4-6 Analysis of current status and gaps, and potential \nRAN specification impact for Solutions 4b\nNote: \tFor Solution 4b, RAN2 discussed the following two solutions but did not study or analyse their feasibility:\n- OAM can transfer/deliver AI/ML models to UE via OAMâRANâUE, where Control Plane (CP) signalling is used for RANâUE.\n- OAM can transfer/deliver AI/ML models to UE via OAMâUE, e.g., via IP tunnel.\nA reactive and a proactive approach for initiating a model transfer/delivery can be considered in a normative phase. For the reactive approach, an AI/ML model is transferred/delivered (i.e., downloaded) to the UE when needed. This could typically happen due to changes in scenarios, configurations, sites, etc. While for the proactive model transfer/delivery approach, an AI/ML model is pre-download to the UE, and a model switch can typically be performed due to changes in scenarios, configurations, sites, etc.\nThe legacy UE capability framework serves as the baseline to report UEâs supported AI/ML-enabled Feature/FG. Therefore, for CSI and beam management use cases, this information is indicated in UE AS capability in RRC (e.g., UECapabilityEnquiry/UECapabilityInformation). While for positioning use cases, it is indicated by the positioning capability as defined in LPP.\nFurther discussions concerning UE capability details (e.g., granularity of Feature/FG, content, structure of the related UE capabilities, etc.) can be carried during a normative phase.\nAI/ML models for a given use case may be tailored towards and applicable to specific scenarios, locations, configuration, deployments, among other factors. In this regard, it is acknowledged that AI/ML models may undergo updates, such as model changes, as an inherent part of their development. Therefore, to ensure efficient network control and management, especially associated to what concerns the UE-side, UEs might have the ability to indicate relevant information about their supported AI/ML models and concerning AI/ML functionalities to the network. This can allow the network to perform decisions regarding, e.g., the (de)activation, or switching of AI/ML functionalities and AI/ML models.\nThe previously mentioned information could in principle be understood as \"applicability-related information\" in which the UE could, for example, report to the network conditions under which a model/functionality is applicable/suitable, or whether model(s)/functionality(es) are (non)applicable under the current context. Note, however, that the existing UE capability reporting framework cannot be used for such purposes.\nNote: \tHow and whether there is a need to enable UEs to report applicability-related information can be further discussed and defined in a normative phase. Mechanisms such as UE Assistance Information can eventually be used as example.\nTwo UE reporting types are identified to convey this additional information:\n-\t\"reactive\" reporting, and\n-\t\"proactive\" reporting.\nA reactive reporting would involve the UE to provide information to the network upon receiving an action from it.\nWhile a proactive reporting would involve the UE to provide information to the network without necessarily receiving an action from it. For example, the UE might proactively inform the RAN of updates/changes to its supported model(s) or functionality(es).\nNote: \tWhether necessary signalling from network is needed for proactive UE reporting can be discussed in a normative phase.\nNote: \tWhether there is a need for the network to report to the UE applicability-related information of AI/ML models and/or AI/ML functionalities can be discussed in a normative phase.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 7.3.1.2-1. Existing data collection methods identified.",
                                    "table number": 16,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 7.3.1.4-1 Analysis of current status and gaps, and potential \nRAN specification impact for Solution 1a",
                                    "table number": 17,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 7.3.1.4-2 Analysis of current status and gaps, and potential \nRAN specification impact for Solutions 2a and 3a",
                                    "table number": 18,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 7.3.1.4-3 Analysis of current status and gaps, and potential \nRAN specification impact for Solutions 1b",
                                    "table number": 19,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 7.3.1.4-4 Analysis of current status and gaps, and potential \nRAN specification impact for Solutions 2b and 3b",
                                    "table number": 20,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 7.3.1.4-5 Analysis of current status and gaps, and potential \nRAN specification impact for Solutions 4a",
                                    "table number": 21,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 7.3.1.4-6 Analysis of current status and gaps, and potential \nRAN specification impact for Solutions 4b",
                                    "table number": 22,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "7.2.2\tCSI feedback enhancement",
                            "text_content": "The following set of objectives have been identified for the two-sided CSI compression use case. Firstly, to ensure that the UE part and network part of the models are configured and applied according to their applicable scenarios and configuration. Secondly, to ensure that models match properly, ensuring that the CSI generation part used at the UE corresponds to the CSI reconstruction part employed at the gNB. Thirdly, to allow for seamless operation, requiring the simultaneous (de)activation and switching of the two-sided model.\nRegarding the last point above, for the two-sided model CSI compression use cases, the selection, (de)activation, switching, and fallback of AI/ML models or AI/ML functionalities can be initiated by either the UE or the gNB. For which it is important to distinguish the various cases and understand their applicability to UE-side versus network-side models.\nFor data collection, model transfer/delivery, and function-to-entity mapping analysis, various scenarios unfold for both the two-sided CSI compression use case, as well as for the UE-side CSI prediction use case, when the data generation and termination entities differ. For instance, for:\n-\tModel Training:\no\tFor the two-sided CSI compression use case, training data can be generated by either the UE or the gNB, depending on specific requirements, while the termination point for training data may include the gNB, OAM, Over-The-Top (OTT) server or UE.\nï§\tNote: RAN2 identified the case in which Core Network may be used for model training. However, no study was conducted since this is beyond the scope of this Working Group.\no\tFor the UE-side CSI prediction use case, training data can be generated by the UE, while the termination point for training data may include the UE or a UE-side OTT server.\nï§\tNote: RAN2 identified the cases in which OAM or Core Network may be used for UE-side model training. However, no study was conducted since this is beyond the scope of this Working Group.\nï§\tNote: RAN2 identified the case in which gNB may be used for UE-side model training. However, no conclusion was reached, as this depends on the RAN1 progress.\n-\tInference:\no\tFor the two-side CSI compression use case:\nï§\tFor network part of two-sided model inference, the UE can generate the necessary input data while the termination point for this input data lies within the gNB, where the inference process is performed.\nï§\tFor UE part of two-sided model inference, input data is internally available at UE, where the inference process is performed.\no\tFor the UE-side CSI prediction use case:\nï§\tFor UE-side model inference, input data is internally available at UE, where the inference process is performed.\n-\tManagement:\no\tFor the two-sided CSI compression use case, the model/functionality control (e.g., selection, (de)activation, switching, fallback, etc.) is performed by the gNB.\nï§\tNote: RAN2 identified the case in which the control is performed by the UE. However, no conclusion was reached, as this depends on the RAN1 progress.\no\tFor the UE-side CSI prediction use case:\nï§\tThe model/functionality control (e.g., selection, (de)activation, switching, fallback, etc.) may be performed by the UE when the monitoring resides within the UE.\nï§\tThe model/functionality control (e.g., selection, (de)activation, switching, fallback, etc.) may be performed by the gNB when the monitoring resides within the gNB or UE.\no\tMonitoring:\nï§\tThe UE monitors the performance of its UE-side model.\nï§\tFor monitoring at the network side of UE-side model, the UE can generate, if needed, calculated performance metrics or data required for performance metric calculation, while the termination point for these is the gNB.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.2.3\tBeam management",
                            "text_content": "For beam management, the selection, (de)activation, switching, and fallback of models or functionalities can also be initiated by either the UE or the gNB. For which it is important to distinguish the various cases and understand their applicability to UE-side versus network-side models.\nFor data collection, model transfer/delivery, and function-to-entity mapping analysis, various scenarios unfold when the data generation and termination entities differ. For instance, for:\n-\tModel Training:\no\tFor UE-side models, training data can be generated by the UE, while the termination point for training data may include the UE or a UE-side OTT server.\nï§\tNote: RAN2 identified the cases in which OAM or Core Network may be used for UE-side model training. However, no study was conducted since this is beyond the scope of this Working Group.\nï§\tNote: RAN2 identified the case in which gNB may be used for UE-side model training. However, no conclusion was reached, as this depends on the RAN1 progress.\no\tFor gNB-side models, training data can be generated by the gNB or UE, while the termination point for training data may include the gNB, or OAM.\nï§\tNote: RAN2 identified the case in which OTT server and Core Network may be used for gNB-side model training. However, no study was conducted since this is beyond the scope of this Working Group.\n-\tInference:\no\tFor UE-side model inference, input data is internally available at UE, where the inference process is performed.\no\tFor network-side model inference, the UE can generate the necessary input data while the termination point for this input data lies within the gNB, where the inference process is performed.\n-\tManagement:\no\tFor UE-side model, the model/functionality control (e.g., selection, (de)activation, switching, fallback, etc.) may be performed by the UE when the monitoring resides within the UE.\no\tFor UE-side model, the model/functionality control (e.g., selection, (de)activation, switching, fallback, etc.) may be performed by the gNB when the monitoring resides within the gNB or UE.\no\tMonitoring:\nï§\tThe UE monitors the performance of its UE-side model.\nï§\tFor monitoring at the network side of UE-side model, the UE can generate, if needed, calculated performance metrics or data required for performance metric calculation, while the termination point for these is the gNB.\nï§\tFor network-side model, the monitoring resides within the gNB.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.2.4\tPositioning accuracy enhancements",
                            "text_content": "For the positioning use cases, the selection, (de)activation, switching, and fallback of models or functionalities can be initiated by either the UE, the gNB, or the LMF. For which it is important to distinguish the various cases and understand their applicability to UE-side versus network-side models.\nFor data collection, model transfer/delivery, and function-to-entity mapping analysis, various scenarios unfold when the data generation and termination entities differ. For instance, for:\n-\tModel Training:\no\tFor UE-side models, training data can be generated by the UE, while the termination point for training data may include the UE or a UE-side OTT server.\nï§\tNote: RAN2 identified the cases in which OAM or Core Network may be used for UE-side model training. However, no study was conducted since this is beyond the scope of this Working Group.\nï§\tNote: RAN2 identified the case in which LMF may be used for UE-side model training. However, no conclusion was reached, as this depends on the RAN1 progress.\no\tFor gNB-side model, training data can be generated by the gNB, while the termination point for training data may include the gNB, or OAM.\nï§\tNote: RAN2 identified the case in which LMF may be used for gNB-side model training. However, no conclusion was reached, as this depends on the RAN1 progress.\no\tFor LMF-side model, the LMF is the termination point for training data.\n-\tInference:\no\tFor UE-side model inference, input data is internally available at UE, where the inference process is performed.\no\tFor gNB-side model inference, input data is internally available at gNB. For this case, the UE can also generate the necessary input data while the termination point for this input data lies within the gNB where the inference process is performed.\no\tFor LMF-side model inference, the UE or gNB can generate the necessary input data while the termination point for this input data lies within the LMF where the inference process is performed.\n-\tManagement:\no\tFor UE-side model, the model/functionality control (e.g., selection, (de)activation, switching, fallback, etc.) may be performed by the UE when the monitoring resides within the UE.\no\tFor gNB-side model, the model/functionality control (e.g., selection, (de)activation, switching, fallback, etc.) is performed by the gNB.\no\tThe model/functionality control (e.g., selection, (de)activation, switching, fallback, etc.) may be performed by the LMF when the monitoring resides within the LMF or UE.\no\tMonitoring:\nï§\tThe UE monitors the performance of its UE-side model.\nï§\tFor monitoring at the gNB side, and if needed, calculated performance metrics or data required for performance metric calculation, can at least be generated by the gNB.\nï§\tFor monitoring at the LMF side, the gNB or UE can generate, if needed, calculated performance metrics or data required for performance metric calculation, while the termination points for these metrics is the LMF.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "7.3\tInteroperability and testability aspects",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "7.3.1\tIntroduction",
                            "text_content": "In this section, the study of requirements and testing frameworks to validate AI/ML based performance enhancements and ensuring that UE and gNB with AI/ML meet or exceed the existing minimum requirements, if applicable, are documented.\nThe need and implications for AI/ML processing capabilities definition is considered.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.3.2\tCommon framework",
                            "text_content": "The general requirements and testing frameworks for AI/ML based performance enhancements mainly focus on\n-\thow to define requirements and tests for inference\n-\tevaluate feasibility and necessity of requirements/tests for LCM\n-\trequirements for data collection (in particular for training) could/need be defined\nRequirements/tests for training will not be studied unless training procedures are defined. The design of test should ensure performance is guaranteed and avoid that a UE can pass the test but perform poorly in the field.\nFor testing goals, Option 1 and/or Option 2 below will be selected depending on the test\n-\tOption 1: The testing goal is to verify whether a specific AI/ML model (if model identification is possible)/functionality can be conducted in a proper way.\n-\tFFS how to define the specific AI/ML model (e.g., a model captured in RAN4 spec as baseline)\n-\tFFS how to define that the model is properly conducted (e.g., by defining AI/ML dedicated performance/core requirements associated with model outputs)\n-\tOption 2: The testing goal is to verify whether the minimum performance gain of AI/ML model (if model identification is possible) /functionality/feature can be achieved for a static scenario/configuration.\n-\tFFS how to define a static scenario/configuration (e.g., by defining a related testing dataset based on channel models in TR 38.901)\n-\tFFS whether and how to define non-static specific scenarios/configurations\nFor the definition of AI/ML requirements, the following cases related to legacy performance should be considered\n-\tFor the cases with the existing legacy performance\n-\tTake the legacy performance as baseline for existing use cases/procedures/functionalities /measurements that are to be enhanced by AI/ML based methods\n-\tFurther study may be needed on what is baseline performance in conditions different to the requirement condition but within the expected range of operation.\n-\tNew or enhanced performance requirements/tests could be considered for existing use cases/procedures/functionalities/measurements that are to be enhanced by AI/ML based methods\n-\tFor the cases without the existing legacy performance\n-\tNew performance requirements/tests could be considered for the use cases/procedures/functionalities/measurements that are carried out or are to be enhanced by AI/ML based methods\nThe following procedure can be considered for defining core requirements:\n-\tPerformance monitoring procedure, including performance evaluation and decision-making procedure for AI/ML functionalities/models\n-\tFunctionality/Model management procedure, including functionality/model selection/activation/deactivation, and functionality/model switching/fallback/transfer/delivery/update\n-\tLatency/interruption requirement for above procedures\nThe following LCM related requirements can be considered:\n-\tModel/Functionality select/switch/activate/deactivate/fallback\n-\tModel/Functionality monitoring\n-\t On whether requirements for data collection (in particular for training) could/need be defined:\n-\tData collection requirements would only be defined if data collection procedure is defined in 3GPP specifications.\n-\tOn requirements for model transfer/update:\n-\tRequirements would only be defined if model transfer/update would be defined in 3GPP specifications.\nThe legacy framework for RRC/MAC-CE/DCI based core requirements (e.g., define delay requirements based on multiple delay components) can be used as the baseline for LCM procedures if the LCM related requirements are agreed to be introduced. If new procedures which legacy framework is not applicable to are introduced, additional core requirement framework can be discussed.\nLCM related tests should consider how the framework can address the possibility of updates/activation/deactivation /switching to the functionalities/models after the deployment of the devices in the field.\nReference block diagrams provide test modules/functionalities of TE/DUT and testing framework for different use cases. Both reference block diagrams for 1-sided model and 2-sided model are studied.\nFigure 7.3.2.3-1 provides the reference block diagram for 1-sided model. LCM in the figure includes functionality and/or model ID based LCM. The link between TE and DUT are physical and not logical. The logical link will depend on the functionality being tested. The scope of the figure includes both performance and potentially LCM tests. Offline training is assumed and some blocks may not be used in some of the tests. LCM may not be tested depending on the purpose of the test.\n\nFigure 7.3.2.3-1: Reference block diagram for 1-sided model\nFigure 7.3.2.3-2 provides the reference block diagram for 2-sided model. LCM in the figure includes functionality and/or model ID based LCM. The link between TE and DUT are physical and not logical. The logical link will depend on the functionality being tested. The scope of the figure includes both performance and potentially LCM tests. Offline training is assumed and some blocks may not be used in some of the tests. LCM may not be tested depending on the purpose of the test.\n\nFigure 7.3.2.3-2: Reference block diagram for 2-sided model\n(NOTE: At the current stage the framework in this session applies to CSI compression case.)\nIn order to determine the test encoder/decoder, the following issues are considered:\n-\tCommon assumptions for proposals of the test decoder / encoder (and the paired encoder/ decoder) for tester\n-\tThe need for and potential definition and derivation procedure of intermediate KPI for decoder evaluation and selection\n-\tData collection/generation for decoder evaluation, and the common assumptions/environment needed for data collection/generation\n-\tHow to minimize the impact of possible variations/differences in the test decoder/ test encoder design/implementation on UE/ gNB performance verification\n-\tThe impact of test decoder/ encoder for testing complexity to UE/gNB performance verification, and the advantage/disadvantage analysis of high/low complexity decoders.\nThe test decoder/encoder design should take into account complexity limitations based on e.g., feasibility of TE implementation and complexity levels considered feasible by network vendors/UE vendors for decoder/encoder deployment.\nThe choice of test decoder/encoder should aim as much as possible to avoid limiting the implementation choices, including e.g. complexity, back-bone model etc, of UE/gNB encoders/decoders operating in the field (this principle may not be fully achievable in practice).\nSpecification on the test may include some high-level parameters for the test decoder/encoder (e.g. parameters related to processing complexity, model structure, etc).\nFollowing the above principles, the considered options of test decoder are listed below\n-\tOption 1: DUT provides the decoder\n-\tOption 2: Infra vendor provides the decoder\n-\tOption 3: Full decoder specification in standard\n-\tOption 4: TE vendor provides the decoder\nOption 3 target is that a single decoder defined in the specifications for at least a single test for any DUTs.\nFor option 4, the following aspects should be considered\n-\tTE vendor should be able to develop the decoder based on the specifications\n-\tTest repeatability should be ensured (variation among TE vendor implementations should be bound)\n-\tOther vendors should also be able to develop such a decoder and which can deliver similar performance\n-\tInteroperability should be ensured based on the parameters that need to be specified\n-\tParameters that need to be specified are FFS\n-\tCandidate parameters/conditions that may be considered for defining test decoder include\n-\tTraining data set for TE decoder training\n-\tModel structure (Activation function is included in the model structure)\n-\tPerformance parameters for the TE decoder (e.g. cosine similarity, loss function, etc)\n-\tMaximum FLOPs allowed for the test decoder\n-\tMaximum number/size of model parameters\n-\tCompression ratio of decoder (output size/input size)\n-\tQuantization level\n-\tOther parameters are not precluded and to be further discussed.\n-\tNote: Feasibility of definition of parameters needs further investigated.\nOption 4 target is that a single decoder implemented by each TE vendor will be enough for at least a single test for any DUTs. TE vendor should be able to implement the test decoder for Option 4 without any involvement from another party. If this is found infeasible, another option in which TE vendors need to collaborate with DUT/infra vendors to implement the decoder could be considered.\nFurther clarifications and analysis of the four options of test decoder are included in Table 7.3.2.3-1. It is assumed that for Option 4 the TE vendors can implement the decoder just based on the specifications (no other party involved). The table would need to be revised if collaboration between TE vendor and DUT/infra vendor is needed.\nTable 7.3.2.3-1: Comparison of the four options of test decoder\n\nThe feasibility of any of the testing options has not concluded and more study is required. Other testing options are not precluded and different options might be required after RAN4 performs additional studies.\nDifferent generating methods of test dataset can be used for different tests. The following candidate methods are to be considered:\n-\tDataset based on TR 38.901, e.g. UMa channel, UMi channel, CDL channel, \"legacy approach\", etc.\n-\t\"Legacy approach\" refers legacy test in which a channel model is used\n-\tField dataset (data collected directly from field measurements)\n-\tTE generates dataset for test based on assumptions/parameters defined by RAN4 (e.g. by defining some rules/function to generate data)\n-\tOther methods are not precluded\nSome conditions and/or accuracy requirements for the training dataset or training data generation could only be introduced if the training procedure is defined in 3GPP specifications.\nThe necessity and feasibility of defining requirements or test to verify the generalization of AI/ML is studied.\nThe goals of generalization test are to verify whether the minimum level of performance of AI/ML functionality/model can be achieved/maintain under the identified scenarios and/or configurations, while the performance wonât be significantly degraded in other scenarios and/or configurations. The following aspects should be considered for generalization/scalability related testing:\n-\tdetails about the scenarios and/or configurations for test and the corresponding AI/ML models/functionality\n-\twhat the minimum level performance for each identified scenario and/or configuration is\n-\twhat the significant degradation for other scenarios and/or configurations is\nIt should also be considered that generalization and/or scalability related requirements for different scenarios/ configurations can be implicitly handled in the test case definition.\nAs for the handling of generalization tests, the following option is considered as baseline:\nSignalling based LCM procedures and performance monitoring are considered in dedicated test cases and are excluded in tests verifying generalization. RAN4 may define multiple tests with different conditions. In each of the test, TE configures the same specified UE configuration, and therefore the same specified UE configuration is tested under different conditions to verify its generalizability. (environment differs in each test but not changing dynamically during the test)\n-\tSpecified UE configuration includes functionality and/or model ID if defined\nThe practical processing capability and implementation complexity for device under test should be assumed when specifying RAN4 requirements.\n-\tThe UE capability may be needed to handle different complexity for one side and two-side models.\n-\tThe complexity of UE should also be studied when making assumption on gNB side model, and vice versa.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 7.3.2.3-1: Comparison of the four options of test decoder",
                                    "table number": 23,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "7.3.3\tCSI feedback enhancement",
                            "text_content": "Both time domain CSI prediction and spatial-frequency domain CSI compression are considered.\nPMI reporting framework (follow PMI vs. random PMI test, use of Î³ as criteria, etc.) is taken as starting point for CSI related tests. Other metrics/framework is not precluded.\nFor metrics for CSI requirements/tests, the following test metrics are identified:\n-\tOption 1: Throughput/relative throughput\n-\tOption 2: SGCS, NMSE\n-\tOption 3: CSI prediction accuracy\nOption 1 should be used as baseline. For option 3, further discuss is needed on the feasibility to define the CSI prediction accuracy in WI. For metrics for CSI monitoring, further discussion is needed in WI.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.3.4\tBeam management",
                            "text_content": "Both spatial-domain DL beam prediction and temporal DL beam prediction are considered.\nFor metrics for beam management requirements/tests, the following test metrics are identified and could be considered\n-\tOption 1: RSRP accuracy\n-\tOption 2: Beam prediction accuracy\n-\tTop-1 (%) : the percentage of \"the Top-1 strongest beam is Top-1 predicted beam\"\n-\tTop-K/1 (%) : the percentage of \"the Top-1 strongest beam is one of the Top-K predicted beams\"\n-\tTop-1/K (%) : the percentage of \"the Top-1 predicted beam is one of the Top-K strongest beams\"\n-\tOption 3: The successful rate for the correct prediction which is considered as maximum RSRP among top-K predicted beams is larger than the RSRP of the strongest beam â x dB,\n-\tRelated measurement accuracy can be considered to determine x\n-\tOption 4: combinations of above options\nThe overhead/latency reduction should be considered for the requirements as the side condition.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.3.5\tPositioning accuracy enhancements",
                            "text_content": "Both direct AI/ML positioning and AI/ML assisted positioning are considered.\nFor metrics for positioning requirements/tests, the candidate options include\n-\tOption 1: positioning accuracy: Ground truth vs. reported\n-\tonly option available for direct positioning\n-\tOption 2: CIR/PDP, channel estimation accuracy\n-\tOption 3: ToA, RSTD and RSRP, and RSRPP\n-\tOption 4: others (e.g., intermediate KPIs, LoS/NLoS)/combinations of the above\nThe feasibility and testability of different options should be further justified in WI.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "8\tConclusions",
            "description": "The following aspects have been studied for the general framework of AI/ML over air interface for one-sided models and two-sided models:\n-\tVarious Network-UE Collaboration Levels\n-\tFunctionality-based LCM and model-ID-based LCM\n-\tFunctionality/model selection, activation, deactivation, switching, fallback\n-\tFunctionality identification and model identification\n-\tData collection\n-\tPerformance monitoring\n-\tVarious model identification Types and their use cases\n-\tReporting of applicable functionalities/models\n-\tMethod(s) to ensure consistency between training and inference regarding NW-side additional conditions (if identified) for inference at UE\n-\tModel delivery/transfer and analysis of various model delivery/transfer Cases\nThe above studied aspects for General Framework can be considered for developing/specifying AI/ML use cases and common framework (if needed for some aspects) across AI/ML use cases.\nCSI feedback enhancement:\nCSI compression sub use case:\nThe performance benefit and potential specification impact were studied for AI/ML based CSI compression sub use case.\nEvaluation has been performed to assess AI/ML based CSI compression from various aspects, including performance gain over non-AI/ML benchmark, model input/output type, CSI feedback quantization methods, ground-truth CSI format, monitoring, generalization, training collaboration types, etc. Some aspects were studied but not fully investigated, including the options of CQI/RI calculation, the options of rank>1 solution.\nPerformance gain over baseline and computational complexity in FLOPs are summarized in clause 6.2.2.8.\nPotential specification impact on NW side/UE side data collection, dataset delivery, quantization alignment between CSI generation part at the UE and CSI reconstruction part at the NW, CSI report configuration, CSI report format, pairing information/procedure and monitoring approach were investigated but not all aspects were identified.\nThe pros and cons are analysed for each training collaboration types, and each training collaboration type has its own benefits and limitations in different aspects. The study has investigated the feasibility of the studied training collaboration types and necessity of corresponding potential RAN1 specification impact. However, not all aspects have been concluded.\nBoth NW side and UE side performance monitoring were studied, some but not all aspects were concluded.\nFrom RAN1 perspective, there is no consensus on the recommendation of CSI compression for normative work.\nAt least the following aspects are the reasons for the lack of RAN1 consensus on the recommendation of CSI compression for normative work:\n-\tTrade-off between performance and complexity/overhead.\n-\tIssues related to inter-vendor training collaboration.\nOther aspects that require further study/conclusion are captured in the summary above.\nCSI prediction sub use case:\nThe performance and potential specification impact were studied for AI/ML based UE side CSI prediction sub use case.\nEvaluations have been performed to assess AI/ML based CSI prediction from various aspects, including performance compared to baseline, model input/output type, generalization over UE speed, etc. Some aspects are studied but lack observations, including scalability over various configurations and generalization over other scenarios and approach of fine tuning. Performance monitoring accuracy has not been evaluated.\nPerformance compared with baseline is summarized in clause 6.2.2.8.\nPotential specification impact on data collection and performance monitoring are discussed in clause 7.2.2. Limited specification aspects were considered.\nFrom RAN1 perspective, there is no consensus on the recommendation of CSI prediction for normative work.\nThe reason for the lack of RAN1 consensus on the recommendation of CSI prediction for normative work:\n-\tLack of results on the performance gain over non-AI/ML based approach and associated complexity.\nOther aspects that require further study/conclusion are captured in the summary above.\nBeam management:\nThis study focuses on evaluation of potential benefits of AI/ML-based beam management and analysis of potential enhancements to enable AI/ML for beam management.\nDuring the study, BM-Case1 (Spatial-domain downlink beam prediction) and BM-Case2 (Temporal-domain downlink beam prediction), as described in clause 5.2, are selected as the representative sub use cases.\nEvaluation scenarios and KPIs are described in clause 6.3.1, and the detailed evaluation results from different sources and the key observations are captured in clause 6.3.2.  Evaluation results have shown that it is beneficial to enable AI/ML for beam management in the considered evaluation scenarios.\nThe necessity, feasibility, benefit and potential specification impacts of potential enhancements to enable AI/ML for beam management were studied from different aspects, and the outputs are captured in clause 7.\nFor AI-based beam management, from RAN1 perspective, at least the following are recommended for normative work:\n-\tBoth BM-Case1 and BM-Case2:\n-\tBM-Case1: Spatial-domain DL Tx beam prediction for Set A of beams based on measurement results of Set B of beams\n-\tBM-Case2: Temporal DL Tx beam prediction for Set A of beams based on the historic measurement results of Set B of beams\n-\tDL Tx beam prediction for both UE-sided model and NW-sided model\n-\tNecessary signalling/mechanism(s) to facilitate data collection, model inference, and performance monitoring for both UE-sided model and NW-sided model\n-\tSignalling/mechanism(s) to facilitate necessary LCM operations via 3GPP signalling for UE-sided model\nPositioning accuracy enhancements:\nThis study focused on the analysis of potential enhancements necessary to enable AI/ML for positioning accuracy enhancements with NR RAT-dependent positioning methods.\nEvaluation scenarios and KPIs were identified for system level analysis of AI/ML enabled RAT-dependent positioning techniques as described in clause 6.4.\nDirect AI/ML positioning and AI/ML assisted positioning were identified and selected as the representative sub-use cases. Evaluation results have shown that in considered evaluation scenarios (i.e., InF-DH, and other InF scenarios), both direct AI/ML positioning and AI/ML assisted can significantly improve the positioning accuracy compared to existing RAT-dependent positioning methods. Various aspects of AI/ML for positioning accuracy enhancement were investigated and evaluated as described in clause 6.4 that provides summary of evaluation results from different sources.\nBased on the conducted analysis, it is recommended to proceed with normative work for AI/ML based positioning.\nThe necessity, feasibility and potential enhancements to facilitate the support of AI/ML for positioning accuracy enhancements with NR RAT-dependent positioning methods were studied and the outcome are outlined in clause 7.\nIt is recommended to specify necessary measurement, signalling and procedure to facilitate training, inference, monitoring and/or other LCM operations for both direct AI/ML positioning and AI/ML assisted positioning, specifically:\n-\tspecify necessary signalling of data collection; investigate the necessity of other information for supporting data collection, and if needed, specify during normative work\n-\tinvestigate on the necessity and signalling details of measurement enhancements, and if needed, specify during normative work\n-\tinvestigate on the necessity and signalling details of monitoring method(s), and if needed, specify during normative work\nA variety of enhancements for measurements (e.g., based on extensions to current positioning measurements or with new measurements) were also identified as potentially beneficial (e.g., trade-off positioning accuracy requirement and signalling overhead) and are recommended to be investigated further and if needed, specified during normative work.\n\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 24,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        }
    ]
}