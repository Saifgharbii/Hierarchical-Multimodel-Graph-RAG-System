{
    "document_name": "22874-i20.docx",
    "content": [
        {
            "title": "Foreword",
            "description": "This Technical Report has been produced by the 3rd Generation Partnership Project (3GPP).\nThe contents of the present document are subject to continuing work within the TSG and may change following formal TSG approval. Should the TSG modify the contents of the present document, it will be re-released by the TSG with an identifying change of release date and an increase in version number as follows:\nVersion x.y.z\nwhere:\nx\tthe first digit:\n1\tpresented to TSG for information;\n2\tpresented to TSG for approval;\n3\tor greater indicates TSG approved document under change control.\ny\tthe second digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, etc.\nz\tthe third digit is incremented when editorial only changes have been incorporated in the document.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "Introduction",
            "description": "This document covers use cases and potential requirements for 5G system support of Artificial Intelligence (AI)/Machine Learning (ML) model distribution and transfer (download, upload, updates, etc.). The TR on AI/ML services includes three aspects: AI/ML operation splitting between AI/ML endpoints, AI/ML model/data distribution and sharing over 5G system, distributed/Federated Learning over 5G system.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "1\tScope",
            "description": "This report captures the study of the use cases and the potential performance requirements for 5G system support of Artificial Intelligence (AI)/Machine Learning (ML) model distribution and transfer (download, upload, updates, etc.), and identifies traffic characteristics of AI/ML model distribution, transfer and training for various applications, e.g. video/speech recognition, robot control, automotive, other verticals.\nThe aspects addressed include:\nAI/ML operation splitting between AI/ML endpoints;\nAI/ML model/data distribution and sharing over 5G system;\nDistributed/Federated Learning over 5G system.\nStudy of the AI/ML models themselves are not in the scope of the TR.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "2\tReferences",
            "description": "The following documents contain provisions which, through reference in this text, constitute provisions of the present document.\n-\tReferences are either specific (identified by date of publication, edition number, version number, etc.) or non-specific.\n-\tFor a specific reference, subsequent revisions do not apply.\n-\tFor a non-specific reference, the latest version applies. In the case of a reference to a 3GPP document (including a GSM document), a non-specific reference implicitly refers to the latest version of that document in the same Release as the present document.\n[1]\t3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".\n[2]\t3GPP TR 22.891, Feasibility Study on New Services and Markets Technology Enablers\n[3]\t3GPP TR 22.863, Feasibility study on new services and markets technology enablers for enhanced mobile broadband\n[4]\t3GPP TS 22.261, Service requirements for the 5G system\n[5]\t3GPP TS 22.104, Service requirements for cyber-physical control applications in vertical domains\n[6]\t3GPP TS 23.273, 5G System (5GS) Location Services (LCS); Stage 2\n[7]\tA. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet classification with deep convolutional neural networks”, in Proc. NIPS, 2012, pp. 1097–1105.\n[8]\tK. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” 2014, arXiv:1409.1556. [Online]. Available: https://arxiv.org/abs/1409.1556\n[9]\tC. Szegedy, et al., “Going deeper with convolutions”, in Proc. CVPR, 2015, pp. 1-9.\n[10]\tZhi Zhou, Xu Chen, , , , , “Edge intelligence: Paving the last mile of artificial intelligence with edge computing”, Proceeding of the IEEE, 2019, Volume 107, Issue 8.\n[11]\t, “Deep learning with edge computing: A review”, Proceeding of the IEEE, 2019, Volume 107, Issue 8.\n[12]\tI. Stoica et al., “A Berkeley view of systems challenges for AI”, 2017, arXiv:1712.05855. [Online]. Available: https://arxiv.org/abs/1712.05855\n[13]\tY. Kang et al., “Neurosurgeon: Collaborative intelligence between the cloud and mobile edge”, ACM SIGPLAN Notices, vol. 52, no. 4, pp. 615–629, 2017.\n[14]\tE. Li, Z. Zhou, and X. Chen, “Edge intelligence: On-demand deep learning model co-inference with device-edge synergy”, in Proc. Workshop Mobile Edge Commun. (MECOMM), 2018, pp. 31–36.\n[15]\t3GPP TR 38.913, Study on Scenarios and Requirements for Next Generation Access Technologies (Release 15)\n[16]\tB. Kehoe, S. Patil, P. Abbeel, and K. Goldberg, “A survey of research on cloud robotics and automation,” IEEE Transactions on automation science and engineering, vol. 12, no. 2, pp. 398–409, 2015.\n[17]\tHuaijiang Zhu, Manali Sharma, Kai Pfeiffer, Marco Mezzavilla, Jia Shen, Sundeep Rangan, and Ludovic Righetti, “Enabling Remote Whole-body Control with 5G Edge Computing”, to appear, in Proc. 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems. Available at: https://arxiv.org/pdf/2008.08243.pdf\n[18]\tK. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. IEEE CVPR, Jun. 2016, pp. 770-778.\n[19]\tA. G. Howard et al., “MobileNets: Efficient convolutional neural networks for mobile vision applications,” 2017, arXiv:1704.04861. [Online]. Available: https://arxiv.org/abs/1704.04861\n[20]\tB. Taylor, V. S.Marco, W. Wolff, Y. Elkhatib, and Z. Wang, “Adaptive deep learning model selection on embedded systems,” in Proc. ACM LCTES, 2018, pp. 31–43.\n[21]\tG. Shu, W. Liu, X. Zheng, and J. Li, “IF-CNN: Image-aware inference framework for CNN with the collaboration of mobile devices and cloud”, IEEE Access, vol. 6, pp. 621–633, 2018.\n[22]\tD. Stamoulis et al., “Designing adaptive neural networks for energy-constrained image classification”, in Proc. ACM ICCAD, 2018, Art. no. 23.\n[23]\tSergey Ioffe and Christian Szegedy. “Batch normalization: Accelerating deep network training by reducing internal covariate shift”, In ICML., 2015.\n[24]\tC.-J. Wu et al., “Machine learning at facebook: Understanding inference at the edge,” in Proc. IEEE Int. Symp. High Perform. Comput. Archit. (HPCA), Feb. 2019, pp. 331–344.\n[25]\tVivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, Joel S. Emer, “Efficient processing of deep neural networks: A tutorial and survey”, Proceeding of the IEEE, 2017, Volume 105, Issue 12.\n[26]\tY. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, no. 7553, pp. 436-444, May 2015.\n[27]\t“”, March 12, 2019, Posted by Johan Schalkwyk, https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html\n[28]\t, etc., “”,\n[29]\t3GPP TS 22.243: \"Speech recognition framework for automated voice services; Stage 1\".\n[30]\tH. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. Y. Arcas, “Communication-efficient learning of deep networks from decentralized data”, Proc. of the International Confe rence on Artificial Intelligence and Statistics, Apr. 20 17. [Online]. Available:\n[31] \t“Federated Learning”, https://justmachinelearning.com/2019/03/10/federated-learning/\n[32]\tT. Nishio and R. Yonetani, “Client selection for federated learning with heterogeneous resources in mobile edge”, 2018, arXiv:1804.08333. [Online]. Available: https://arxiv.org/abs/1804.08333\n[33]\tE. Park et al., “Big/little deep neural network for ultra low power inference”, in Proc. 10th Int. Conf. Hardw./Softw. Codesign Syst. Synth., 2015, pp. 124–132.\n[34]\t; ; ; ; , “”, In proc.\n[35]\tSong Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and William J Dally. “EIE: efficient inference engine on compressed deep neural network”, In 43rd International Symposium on Computer Architecture, IEEE Press, 243–254.\n[36]\tV. Sze, “Efficient Computing for Deep Learning, AI and Robotics,” Dept EECS, MIT, Available online at\n[37]\tV. Sze, Y. Chen, “Efficient Processing of Deep Neural Networks: A Tutorial and Survey” Proc. of IEEE, 2017, Available online at:\n[38]\tStanford University, CS231n – Lecture 5-7: CNN, Training NNs, Available at YouTube.com\n[39]\tS. Han, J. Pool, J. Tran, and W, J. Dally, \"Learning both weights and connections for efficient neural networks\", NIPS, May 2015\n[40]\tP. A. Merolla, et al., “A million spikingneuron integrated circuit with a scalable communication network and interface”,Science, vol. 345, no. 6197, pp. 668–673, Aug. 2014.\n[41]\tR. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, “Natural language processing (almost) from scratch,” J. Mach. Learn. Res., vol. 12 pp. 2493–2537, Aug. 2011.\n[42]\tT. N. Sainath, A.-R. Mohamed, B. Kingsbury, and B. Ramabhadran, “Deep convolutionalneural networks for LVCSR”, in Proc. ICASSP, 2013, pp. 8614–8618.\n[43]\tL. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement learning: A survey”, J. Artif. Intell. Res., vol. 4, no. 1, pp. 237–285, Jan. 1996.\n[44]\t3 AI Trends for Enterprise Computing. [Online]. Available: https://www.gartner.com/smarterwithgartner/3-ai-trends-for-enterprise-computing/\n[45]\t; ; ; ; , “”, In proc.\n[46]\t3GPP TS 22.186, Enhancement of 3GPP support for V2X scenarios; Stage 1 (Release 16) v16.2.0\n[47]\t“Develop Smaller Speech Recognition Models with NVIDIA’s NeMo Framework”,\n[48]\t3GPP TS 23.501, System architecture for the 5G System (5GS)\n[49]\t3GPP TS 23.502, Procedures for the 5G System (5GS)\n[50]\tS. Ren, K. He, R. Girshick, J. Sun, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”\n[51]\tJ. Redmon, A. Farhadi, “YOLOv3: An Incremental Improvement”\n[52]\tW. Sun, Z. Chen, “Learned Image Downscaling for Upscaling using Content Adaptive Resampler”\n[53]\tC. Ledig et al., “Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network”\n[54]\tA. G. Howard et al., “MobileNets: Efficient convolutional neural networks for mobile vision applications,” 2017, arXiv:1704.04861. [Available online: https://arxiv.org/abs/1704.04861]\n[55]\tSSD-ResNet34 - https://github.com/IntelAI/models/tree/master/benchmarks/object_detection/tensorflow/ssd-resnet34\n[56]\tMLCommons Mobile Inference Benchmark v0.7 - https://mlcommons.org/en/inference-mobile-07/\n[57]\tMASK C-RNN - https://arxiv.org/abs/1703.06870\n[58]\n[59]\t3GPP TR 28.809, Study on enhancement of management data analytics\n[60]\tMingzhe Chen, \"A Joint Learning and Communications Framework for Federated Learning over Wireless Networks\", Oct 2020\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "3\tDefinitions, symbols and abbreviations",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "3.1\tDefinitions",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the terms and definitions given in 3GPP TR 21.905 [1] and the following apply. A term defined in the present document takes precedence over the definition of the same term, if any, in 3GPP TR 21.905 [1].\ncommunication service availability: percentage value of the amount of time the end-to-end communication service is delivered according to an agreed QoS, divided by the amount of time the system is expected to deliver the end-to-end service according to the specification in a specific area.\nNOTE 1:\tThis definition was taken from TS 22.261 [4].\nEnd-to-End Latency: the time that takes to transfer a given piece of information from a source to a destination, measured at the communication interface, from the moment it is transmitted by the source to the moment it is successfully received at the destination.\nNOTE 2: The end point in \"end-to-end\" is assumed to be the communication service interface.\nNOTE 3:\tThis definition was taken from TS 22.261 [4].\nreliability: in the context of network layer packet transmissions, percentage value of the amount of sent network layer packets successfully delivered to a given system entity within the time constraint required by the targeted service, divided by the total number of sent network layer packets.\nNOTE 4:\tThis definition was taken from TS 22.261 [4].\nuser experienced data rate: the minimum data rate required to achieve a sufficient quality experience, with the exception of scenario for broadcast like services where the given value is the maximum that is needed.\nNOTE 5:\tThis definition was taken from TS 22.261 [4].\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.2\tAbbreviations",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the abbreviations given in 3GPP TR 21.905 [1] and the following apply. An abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in 3GPP TR 21.905 [1].\nAI\tArtificial Intelligence\nCNN\tConvolution Neural Network\nDNN\tDeep Neural Network\nFL\tFederated Learning\nGPU\tGraphics Processing Units\nIDC \tInternet Data Center\nML\tMachine Learning\n\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "4\tOverview",
            "description": "Artificial Intelligence (AI)/Machine Learning (ML) is being used in a range of application domains across industry sectors. In mobile communications systems, mobile devices (e.g. smartphones, automotive, robots) are increasingly replacing conventional algorithms (e.g. speech recognition, image recognition, video processing) with AI/ML models to enable applications. The 5G system can at least support three types of AI/ML operations:\nAI/ML operation splitting between AI/ML endpoints;\nAI/ML model/data distribution and sharing over 5G system;\nDistributed/Federated Learning over 5G system.\nThe scheme of split AI/ML inference can be depicted as in Figure 4-1. The AI/ML operation/model is split into multiple parts according to the current task and environment. The intention is to offload the computation-intensive, energy-intensive parts to network endpoints, whereas leave the privacy-sensitive and delay-sensitive parts at the end device. The device executes the operation/model up to a specific part/layer and then sends the intermediate data to the network endpoint. The network endpoint executes the remaining parts/layers and feeds the inference results back to the device.\nFigure 4-1 illustrates an example of split AI/ML inference, showcasing the process of dividing machine learning tasks between a central processing unit (CPU) and a graphics processing unit (GPU). The CPU is responsible for the initial data preprocessing and feature extraction, while the GPU accelerates the model training and prediction stages. This parallel processing approach enhances the overall efficiency and speed of the AI/ML system.\nFigure 4-1. Example of split AI/ML inference\nThe scheme of AI/ML model distribution can be depicted as in Figure 4-2. Multi-functional mobile terminals might need to switch the AI/ML model in response to task and environment variations. The condition of adaptive model selection is that the models to be selected are available for the mobile device. However, given the fact that the AI/ML models are becoming increasingly diverse, and with the limited storage resource in a UE, it can be determined to not pre-load all candidate AI/ML models on-board. Online model distribution (i.e. new model downloading) is needed, in which an AI/ML model can be distributed from a NW endpoint to the devices when they need it to adapt to the changed AI/ML tasks and environments. For this purpose, the model performance at the UE needs to be monitored constantly.\nFigure 4-2 illustrates the process of an AI/ML model being downloaded over a 5G system. The figure depicts the model being transmitted from a cloud server to a mobile device, with the 5G network facilitating the high-speed data transfer. The figure highlights the role of the 5G core network, which manages the data traffic and ensures seamless connectivity. It also shows the importance of edge computing, where some processing is done at the network edge, reducing latency and improving efficiency. The figure emphasizes the integration of AI/ML with 5G technology, enabling advanced applications and services.\nFigure 4-2. AI/ML model downloading over 5G system\nThe scheme of Federated Learning (FL) can be depicted as in Figure 4-3. The cloud server trains a global model by aggregating local models partially-trained by each end devices. Within each training iteration, a UE performs the training based on the model downloaded from the AI server using the local training data. Then the UE reports the interim training results to the cloud server via 5G UL channels. The server aggregates the interim training results from the UEs and updates the global model. The updated global model is then distributed back to the UEs and the UEs can perform the training for the next iteration.\nFigure 4-3 illustrates the process of federated learning over a 5G system, showcasing the collaboration between edge devices and a central server. The figure depicts the data flow, where local models are trained on edge devices and then aggregated by the server for global model updates. Key components include the 5G base stations, edge devices, and the central server. The figure emphasizes the role of 5G technology in enabling efficient and secure machine learning applications in resource-constrained environments.\nFigure 4-3. Federated Learning over 5G system\n\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "5\tSplit AI/ML operation between AI/ML endpoints",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "5.1\tSplit AI/ML image recognition",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.1.1\tDescription",
                            "text_content": "The AI/ML-based mobile applications are increasingly computation-intensive, memory-consuming and power-consuming. Meanwhile end devices usually have stringent energy consumption, compute and memory limitations for running a complete offline AI/ML inference on-board. Many AI/ML applications, e.g. image recognition, currently intent to offload the inference processing from mobile devices to internet datacenters (IDC). For example, photos shot by a smartphone are often processed in a cloud AI/ML server before shown to the user who shot them. However, the cloud-based AI/ML inference tasks need to take into account the computation pressure at IDCs, required data rate/latency and privacy protection requirement.\nImage and video are the biggest data on today’s Internet. Videos account for over 70% of daily Internet traffic [4]. Convolutional Neural Network (CNN) models have be widely used for image/video recognition tasks on mobile devices, e.g. image classification, image segmentation, object localization and detection, face authentication, action recognition, enhanced photography, VR/AR, video games. Meanwhile, CNN model inference requires an intensive computation and storage resource. For example, AlexNet [7], VGG-16 [8] and GoogleNet [9] require 724M, 15.5G and 1.43G MACs (multiply-add computation) respectively for a typical image classification task.\nMany references [10-14] have shown that AI/ML inference for image processing with device-network synergy can alleviate the pressure of computation, memory footprint, storage, power and required data rate on devices, reduce end-to-end latency and energy consumption, and improve the end-to-end accuracy, efficiency and privacy when compared to the local execution approach on either side. The scheme of split AI/ML image recognition can be depicted in Figure 5.1.1-1. The CNN is split into two parts according to the current image recognition task and environment. The intention is to offload the computation-intensive, energy-intensive parts to network server, whereas leave the privacy-sensitive and delay- sensitive parts at the end device. The device executes the inference up to a specific CNN layer and sends the intermediate data to the network server. The network server runs through the remaining CNN layers. While the model is developed or invocated, the split AI/ML operation is based on the legacy model.\nDue to the characteristics of some algorithms in the model training phase, a model has a certain degree of robustness[xx-xy]. Therefore, if there are errors in the intermediate data transmission, the model has a certain tolerance and can still guarantee the accuracy of the inference results. Since the inference result needs to be forwarded to the UE, the reliability of the inference result transmission needs to be guaranteed.\nFigure 5.1.1-1 presents an example of split AI/ML image recognition, illustrating the process of image classification through a neural network. The figure showcases two distinct input images, with the top half displaying a sample input image and the bottom half displaying the corresponding output. The output image highlights the predicted class label, demonstrating the model's ability to accurately categorize the input image. The figure emphasizes the effectiveness of AI/ML algorithms in identifying patterns and making predictions, which is crucial for various applications such as object detection, image segmentation, and natural language processing.\nFigure 5.1.1-1. Example of split AI/ML image recognition\nThe split AI/ML image recognition algorithms can be analyzed based on the computation and data characteristics of the layers in the CNN. As shown in figure 5.1.1-2 and 5.1.1-3 (based on figures adopted from [13]), the intermediate data size transferred from one CNN layer to the next depends on the location of the split point. Hence, the required UL data rate is related to the model split point and the frame rate for the image recognition, as also observed by [13-14]. For example, assuming images from a video stream with 30 frames per second (FPS) need to be classified, the required UL data rate for different split points ranges from 4.8 to 65 Mbit/s (listed in Table 5.1.1-1). The result is based on the 227×227 input images. In case of images with a higher resolution, higher data rates would be required.\nFigure 5.1.1-2 illustrates the layer-level computation and communication resource evaluation for an AlexNet model, showcasing the distribution of computational and communication resources across different layers. The figure highlights the trade-off between computational complexity and communication cost, with the goal of optimizing the model's performance. Key components include the input layer, convolutional layers, pooling layers, fully connected layers, and the output layer. The figure also presents the resource allocation for different layers, emphasizing the importance of efficient resource management in deep learning models.\nFigure 5.1.1-2. Layer-level computation/communication resource evaluation for an AlexNet model\nTable 5.1.1-1: Required UL data rate for different split points of AlexNet model for video recognition @30FPS\n\nVGG-16 is another widely-used CNN model for image recognition. Still assuming images from a video stream with 30 FPS need to be classified, the required UL data rate for different split points ranges from 24 to 720 Mbit/s (listed in Table 5.1.1-2).\nFigure 5.1.1-3 illustrates the layer-level computation/communication resource evaluation for a VGG-16 model, showcasing the distribution of resources across different layers and the impact on model performance. The figure presents a heatmap-like visualization, with darker shades indicating higher resource utilization. It highlights the trade-off between computation and communication resources, emphasizing the need for efficient allocation to optimize model efficiency while minimizing latency and energy consumption.\nFigure 5.1.1-3. Layer-level computation/communication resource evaluation for a VGG-16 model\nTable 5.1.1-2: Required user experienced UL data rate for different split points of VGG-16 model @30FPS\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.1.1-1: Required UL data rate for different split points of AlexNet model for video recognition @30FPS",
                                    "table number": 1,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 5.1.1-2: Required user experienced UL data rate for different split points of VGG-16 model @30FPS",
                                    "table number": 2,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "5.1.2\tPre-conditions",
                            "text_content": "The involved AI/ML endpoints (e.g. UE, AI/ML cloud/edge server) run applications providing the capability of AI/ML model inference for image recognition, and support the split AI/ML image recognition operation.\nThe 5G system has the ability to provide 5G network related information to the AI/ML server.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.1.3\tService Flows",
                            "text_content": "The AI/ML based image recognition application is requested by the user to start recognizing the image/video shot by the UE.\nUnder the determined split mode and split point, the AI/ML based image recognition application in an involved AI/ML endpoint executes the allocated part of AI/ML model, and sends the intermediate data to the next endpoint in the AI/ML pipeline.\nAfter all the involved AI/ML endpoints finish the co-inference, the image recognition results are fed to the user using the results.\nThe AI/ML based image recognition applications in the endpoints perform the split image recognition until the image recognition task is terminated.\nRedo Step 3) and 4) for split mode/point re-selection/switching if needed to adapt to the changing conditions.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.1.4\tPost-conditions",
                            "text_content": "The objects in the input images or videos are recognized and the recognition accuracy and latency need to be guaranteed.\nThe image recognition task can be completed under the available computation and energy resource of the UE. And the consumed the computation, communication and energy resources over the AI/ML endpoints are optimized.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.1.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "This use case mainly requires high data rate together with low latency. The high data rate requirements to 5G system are listed in Clause 7.1 and 7.6 of TS22.261 [4]. As in Table 7.1-1 of [4], 300Mbps DL experienced data rate and 50Mbps UL experienced data rate are required in dense urban scenario, and 1Gbps DL experienced data rate and 500Mbps UL experienced data rate are required in indoor hotspot scenario. As in Table 7.6.1-1 of [4], cloud/edge/split rendering- related data transmission requires up to 0.1Gbps data rate with [5-10]ms latency countrywide.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.1.6\tPotential New Requirements needed to support the use case",
                            "text_content": "The “image recognition latency” can be defined as the latency from the image is captured to the recognition results of the image are output to the user application, which was not specially addressed in [4][15]. Following the principle of analyzing the latency and data rate requirements of split image recognition introduced in Section 5.1.1, the image recognition latency is related to the user application the recognition is used for.\nComputer vision and image recognition have been widely used for many important mobile applications such as object recognition, photo enhancements, intelligent video surveillance, mobile AR, remote-controlled automotive, industrial control and robotics. The image recognition is usually a step of the processing pipeline of the application. And the recognition latency is a part of the end-to-end latency, as depicted in Figure 5.1.6-1.\nFigure 5.1.6-1 illustrates the image recognition latency as a component of the end-to-end latency in a communication system. The figure depicts the process of image recognition, starting from the camera capturing the image, passing through the ISP (Image Signal Processor), and finally reaching the recognition stage. The recognition stage involves the processing of the image by an AI model, which then sends the result back to the camera. The figure highlights the importance of minimizing latency in image recognition, as it directly impacts the overall performance of the communication system. The components shown in the figure include the camera, ISP, AI model, and the communication link between them. The figure emphasizes the need for efficient processing and low latency in order to achieve real-time recognition and communication.\nFigure 5.1.6-1. Image recognition latency is a part of end-to-end latency\nFor example, if the image recognition results are just used for the object recognition e.g. unknown object recognition for smartphone user or criminal searching in database for intelligent security, it is acceptable that the image recognition is finished in seconds. If the image recognition result is used as an input to another time-sensitive application, e.g. AR display/gaming, remote-controlled automotive, industrial control and robotics, a much more stringent latency will be required. Based on the end-to-end latency requirements of the applications, the image recognition latency requirement can be derived, as listed in Table 5.1.6-1.\nThe potential KPI requirements needed to support the use case include:\n[P.R.5.1-001] The 5G system shall support intermediate data uploading for split image recognition with a maximum latency as given in Table 5.1.6.1-1.\n[P.R.5.1-002] The 5G system shall support intermediate data uploading for split image recognition with a user experienced UL data rate as given in Table 5.1.6.1-1.\n[P.R.5.1-003] The 5G system shall support intermediate data uploading for split image recognition with communication service availability not lower than 99.999 %.\n[P.R.5.1-004] The 5G system shall support inference results downloading for split image recognition with reliability not lower than 99.999 % and intermediate data uploading for split image recognition with reliability not lower than 99.9%.\nNOTE:\tAbove requirements apply to intermediate data with a maximum size of 0.27MB (for AlexNet model) or 1.5 MB (for VGG-16 model).\nTable 5.1.6.1-1: Image recognition latency and UL data rate for intermediate data uploading\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.1.6.1-1: Image recognition latency and UL data rate for intermediate data uploading",
                                    "table number": 3,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "5.2\tEnhanced media recognition: Deep Learning Based Vision Applications",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.2.1\tDescription",
                            "text_content": "A tourist is wandering around a city and discovering the attractions and sights of the city. The user sees a beautiful object and she decides to shoot a video of the object. The application uses deep learning algorithms to process the video and identify the object of interest and provide historical information about it to the user. Furthermore, the application uses deep learning to reconstruct a 3D model of the object of interest by using the captured 2D video.\nAs an example, we investigate Feature Pyramid Network (FPN)-based object detection approaches. These networks are usually composed of a backbone FPN and a head that performs task-specific inference. The FPN processes the input images at different scales to allow for the detection of small-scale and large-scale features. The head may for instance segment the objects, infer a bounding box for the objects, or classify the objects.\nThe FPN backbone constitutes the most complex portion of the network and lends itself to be offloaded to the edge/cloud. The backbone is a common part to a wide range of networks that can perform different tasks. The produced feature maps can then be sent back to the UE for task-specific inference.\nA breakdown of the network architecture is shown in the following figure:\nFigure 5.2.1-1 presents an example of a multi-task network, illustrating the integration of various communication technologies. The figure showcases a network architecture that includes both wired and wireless components, such as Ethernet switches, Wi-Fi access points, and cellular base stations. The layout emphasizes the seamless interaction between these elements, highlighting the network's ability to support diverse applications and services simultaneously. The figure also highlights the importance of efficient resource allocation and coordination among different network entities to ensure optimal performance and reliability.\nFigure 5.2.1-1. Example Multi-Task Network\nAs shown in figure 5.2.1-1, a classical CNN architecture is used as the core. The FPN is used to extract features at different scales of the image, making it scale-invariant. The prediction tasks constitute the head of the network. By plugging in different network heads, different AI tasks can be performed.  This makes the network a Multi-Task Network (MTN). For example, a Region Proposal Network can be appended to detect and frame objects in the input sequence by outputting bounding boxes. Other Task-specific Heads can be appended to detect humans and poses, classify objects, track objects, etc.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.2.2\tPre-conditions",
                            "text_content": "The user wants to receive instantaneous information and reconstruction to enhance their experience. The user’s device is battery operated.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.2.3\tService Flows",
                            "text_content": "User opens their camera app and starts shooting a video\nApplication pre-processes the video to prepare it for inference\nNOTE:\tPre-processing is assumed to be not very intensive in this use case, e.g. downscaling the video, adjusting the frame rate, extracting simple features.\nThe application streams the extracted features and/or the video to the edge/cloud for processing.\nThe network performs the split-inference (e.g. only running the backbone) and streams the results back to the client\nThe application runs task-specific inference to solve the specific task of interest (e.g. object detection, tracking, …)\nThe application uses the inferred labels and classes to enhance the user’s view\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.2.4\tPost-conditions",
                            "text_content": "The user gets enhanced information extracted from the video about the object of interest that the user was shooting a video of.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.2.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "None.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.2.6\tPotential New Requirements needed to support the use case",
                            "text_content": "The potential KPI requirements to support the use case are as given in Table 5.2.6.1-1, based on two examples of object detection models/algorithms:\n- uplink streaming latency not higher than [100-200ms] and a user experienced UL data rate of [100-1500] kbit/s;\n- downlink streaming latency not higher than [100-500ms] and a user experienced DL data rate of [32-150] Mbit/s;\nTable 5.2.6.1-1: Recognition: latency breakdown and user experienced UL/DL data rates\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.2.6.1-1: Recognition: latency breakdown and user experienced UL/DL data rates",
                                    "table number": 4,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "5.3\tMedia quality enhancement: Video streaming upgrade",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.3.1\tDescription",
                            "text_content": "A user is playing a VR game on the cloud using their VR headset. The game is being rendered on the cloud and streamed down to the UE. The user wants to enjoy an immersive gaming experience which requires very high quality video, e.g. 8K per eye at 120 frames per second. The cloud game server can only produce 4K video data due to hardware, load, and networking restrictions. AI is used to upscale the 4K content into 16K content for a better user experience.\nThe following figure shows an example of such a network:\nFigure 5.3.1-1 presents an example of a Deep Neural Network (DNN)-based down/up-scaler used in telecommunication systems. The figure illustrates the architecture of the DNN, which includes input and output layers, as well as multiple hidden layers with various neurons. These layers are designed to learn and adapt to the complex patterns in the input data, such as varying signal strengths and noise levels. The figure also shows the use of pooling layers to reduce the dimensionality of the data, and dropout layers to prevent overfitting during the training process. The output of the DNN is then passed through a post-processing stage, which may include filtering and normalization steps to ensure the final output meets the required quality standards for transmission. Overall, the DNN-based down/up-scaler is a powerful tool for improving the quality and efficiency of telecommunication signals, particularly in scenarios where the signal strength is weak or the data rate is limited.\nFigure 5.3.1-1. Example DNN-based Down/Up-scaler\nThe Low Resolution video is streamed to the UE, which will process the video to infer the high resolution version. The down-sampling and up-sampling parts of the network are matched to produce best results. Any updates to the down-sampling part of network would require updates on the UE-side to the up-sampling part of the network. In addition to the LR version of the video, model weights and topology updates may need to be sent to the UE.\nThe pre-trained models are optimized to the type of content (e.g. sport, cartoons, movie, …) and to the scale factor. As the user switches to a different piece of content, the device will check if the corresponding pre-trained model weights are already downloaded in the cache. If not available, the UE will download the pre-trained model weights for the selected piece of content and based on the desired up-scaling factor. The new content is shown to the user in less than 3 seconds from the user selecting it.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.3.2\tPre-conditions",
                            "text_content": "The remote gaming server generates streams metadata together with the stream that is extracted by running AI autoencoder on the originally captured content.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.3.3\tService Flows",
                            "text_content": "Users starts a cloud VR gaming session on their HMD\nThe game is launched on the cloud server and the game can start\nThe cloud server renders and captures the content and downscales it to 4K video\nThe cloud server also runs a DNN to produce a metadata stream that will be used for upscaling\nThe UE uses a reverse DNN network to upscale the received 4K stream to 16K. The input to the DNN is the 4K video and the metadata stream.\nThe UE renders the high quality 16K view on the HMD.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.3.4\tPost-conditions",
                            "text_content": "The user enjoys a high-quality VR experience.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.3.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "None.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.3.6\tPotential New Requirements needed to support the use case",
                            "text_content": "The potential KPI requirements to support the use case are:\n[P.R.5.3-001] The 5G system shall support the delivery of pre-trained model weights and weight updates with a latency no more than 3 seconds.\nNOTE 1: \tthe model download time takes into account acceptable channel switch time, which generally should not exceed 2-3 seconds.\nThe following table provides an estimate of the model weight sizes:\nTable 5.3.6.1-1: Model Weight Sizes and Data rates\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.3.6.1-1: Model Weight Sizes and Data rates",
                                    "table number": 5,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "5.4\tSplit control for robotics",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.4.1\tDescription",
                            "text_content": "Mobile robots have been playing an increasingly important role in some scenarios, e.g. warehouse, disaster rescue and smart factories [5], thanks to their high mobility. The mobile robots need to work in an ever-changing environment, hence need to perform fast and reliable sensing, planning and controlling. If the corresponding computation is performed on board in the robot, it will require intensive computations which lead to increased requirements of computation capabilities and power consumption. However, a light-weight form factor is always a requirement to the mobile robots working in the real-world environment, which prevents the robots to be equipped with a large number of CPU/GPU units and large-capacity batteries. As the example provided in [17], an advanced commercially available quadruped robot, carries 3kg of batteries of about 650Wh energy, while the high-end GPU consumes more than 250W of power, significantly impacting battery life if such computational power was embedded on the robot.\nOffloading computations from robots to the cloud has been studied in many references [16]. Meanwhile relying on either data or code from a network to support the robot’s operation, the designers of autonomous mobile robots have to consider the scenarios where the robots must include capacity of local processing for low-latency responses during periods when network access quality is varying worse.\nThe resulting system is different from the fully remote-controlled robot system described in [5], in which the planning and controlling are carried out by cloud computing, and the robots only reports the sensing data (incl. video), and receives the control commands. Since the complete cloud computing can hardly meet the latency requirement of the ms-level feedback control loop of some types of mobile robots, e.g. legged robots, the split control of mobile robots is an agreeable solution in this case.\n[17] introduces a robot whole-body balance control split over 5G network. The AI inference for the controlling can be split between the robot and the cloud server: As shown in Figure 5.4.1-1, the part which is complex but less susceptible to delays is offloaded to the remote computation in the cloud or edge control server. The low-complexity part which contains the error feedback terms and is latency-critical can be efficiently done by the local computation in the robot. If the robot fails to receive the optimal control from “remote control part” from the cloud/edge control server due to communication delays or packet loss, it can approximate the “remote control part” using pre-computed feedback matrices received previously. And in certain duration, the approximation will still enable the robot to perform feedback control for the tasks approximately and ensure that the robot can still operate.\nFigure 5.4.1-1 illustrates the split control of a legged robot over a 5G network, showcasing the coordination between the robot's base station and remote controllers. The figure depicts the network infrastructure, including the 5G base station, remote controllers, and the robot itself. It emphasizes the real-time communication and control capabilities of the 5G network, enabling seamless operation of the legged robot. The figure highlights the importance of low latency and high bandwidth in enabling advanced robotics applications.\nFigure 5.4.1-1. Split control of legged robot over 5G network\nThe results in [17] show that, in case the robot is completely controlled by a cloud server, the robot cannot finish the walking task if the round-trip latency is larger than 3ms (from sending sensing data to receiving control commands, including processing at cloud/edge). Due to delayed control commands, the robot would fall down (as shown in Figure 5.4.1-2 (a)). However, if the split control is employed, a worse-case 25ms round-trip latency can be sustained, and the robot can still perform the walking task (as shown in Figure 5.4.1-2 (b)).\nThe given figure illustrates two scenarios of 5G remote control systems. In the first scenario (b), there is no local control at the robot, and the control is entirely centralized. In the second scenario (b), there is local control at the robot, allowing for decentralized decision-making. The figure shows the wireless communication between the 5G remote control system and the robot, highlighting the importance of real-time, high-speed data transmission for effective control. The figure also emphasizes the potential for improved efficiency and autonomy in robotic systems through the integration of 5G technology.\n5G remote control without local control   (b) 5G remote control with local control at robot\nFigure 5.4.1-2. Simulated performance of robot whole-body balance control over 5G network with 25ms round-trip latency\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.4.2\tPre-conditions",
                            "text_content": "The involved AI/ML endpoints (e.g. UE (robot), AI/ML cloud/edge server) run applications providing the capability of AI/ML model inference for robot control task, and support the split robot control operation.\nThe 5G system has the ability to provide 5G network related information to the AI/ML server.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.4.3\tService Flows",
                            "text_content": "The UE (robot) is connected with the cloud/edge control server via 5G network.\nThe split of the control operations on robot side and cloud/edge control server side for the robot control task is determined by the robot or the network.\nUnder the determined split mode and split point, the robot performs the local control computation based on the collected sensing data, and sends the sensing data to the cloud/edge control server. The cloud/edge control server performs the remote control computation, and feeds the outputs back to the robot.\nThe robot controls its motion jointly based on the outputs of local and remote control computations.\nStart with Step 3) with more control operations, until the robot control task is terminated.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.4.4\tPost-conditions",
                            "text_content": "The robot receives the control from local and remote with required accuracy and latency, so to finish the moving tasks, e.g. balance task and walking task.\nThe robot control task can be completed under the available computation and energy resource of the robot. And the consumed the computation, communication and energy resources over the AI/ML endpoints are optimized.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.4.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "This use case mainly requires high data rate together with low latency. The high data rate requirements to 5G system are listed in Clause 7.1 and 7.6 of TS22.261 [4]. As in Table 7.1-1 of [4], 300Mbps DL experienced data rate and 50Mbps UL experienced data rate are required in dense urban scenario, and 1Gbps DL experienced data rate and 500Mbps UL experienced data rate are required in indoor hotspot scenario. As in Table 7.6.1-1 of [4], cloud/edge/Split rendering-related data transmission requires up to 0.1Gbps data rate with [5-10]ms latency countrywide.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.4.6\tPotential New Requirements needed to support the use case",
                            "text_content": "If everything is done on the edge part, the robot needs to send 592B sensing data per control cycle (every millisecond) and receive 200B per control cycle from the “remote controller”, which leads to a UL data rate of 4.7Mbit/s and a DL data rate of 1.6Mbit/s.  However maximum round-trip latency is limited to 3ms, and the one-way latency for downloading “remote control part” needs to be limited to 1ms. If the splitting strategy is followed, downloading the “remote control part” from the cloud/edge control server requires downloading a 40kB data burst per control cycle (every millisecond) as more information is needed to ensure the local controller can take over in case of unexpected latencies, which leads to a user experienced DL data rate of 320Mbit/s. In that case, a maximum round-trip latency of 25ms can be tolerated between each control cycle, and the one-way latency for downloading “remote control part” can be relaxed to 12ms.\nThis implies a trade-off between DL data rate and latency: Compared with the full control at edge, the split control mode requires a higher user experienced DL data rate, but relaxes the stringent latency requirement. Different from the traditional URLLC services requiring continuous coverage of 5G network which can only be provided with FR1 spectrum, for a 5G operator with FR2 spectrum, the split control for robotics can be offloaded to the 5G mmWave network with non-continuous coverage.\nThe potential KPI requirements needed to support the use case include:\n[P.R.5.4-001] The 5G system shall support “Remote control part” downloading for split control for robotics with a maximum latency as given in Table 5.4.6.1-1 (corresponding size of “Remote control part” is also listed in the table).\n[P.R.5.4-002] The 5G system shall support “Remote control part” downloading for split control for robotics with a user experienced DL data rate as given in Table 5.4.6.1-1 (corresponding size of “Remote control part” is also listed in the table).\n[P.R.5.4-003] The 5G system shall support “Remote control part” downloading for split control for robotics with communication service availability not lower than 99.999 %.\nTable 5.4.6.1-1: Data rate and latency requirements for robotic control\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.4.6.1-1: Data rate and latency requirements for robotic control",
                                    "table number": 6,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "5.5\tSession-specific model transfer split computation operations",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.5.1\tDescription",
                            "text_content": "A UE, to achieve results for the user, employs split computation (The split computation is for offloading computation intensive task between UE and network). Computation intensive tasks (machine learning, complex computation using input data and the model, etc.) can be fully or partially offloaded. This use case considers a particular use – rendering augmented reality in a headset with modest computational resources. The decision how to split the computation task between the UE and other computation resources can depend on the conditions of the communication network and on computational resources available in the UE.\nNOTE 1: \tThe decision of how to split computation is itself out of scope of 3GPP and not discussed here.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.5.2\tPre-conditions",
                            "text_content": "Abigail has Augmented Reality glasses, a UE with limited computational power. She leaves a bus and stands at the bus stop, where, behind a large advertisement display, a gNB is installed. Abigail’s glasses get access through the access point. She seeks to augment her view of the city with directions and annotations (opening hours, local history, description of businesses, etc.) Augmenting the visual scene of the city in real time is a computationally intensive task, accomplished by a model developed through ML. the model has two candidate split points, each candidate split point has a different workload and communication requirement shown as below. This strategy for splitting computation has been installed in the UE and Application Server so that the split point can be adjusted dynamically based on change of communication performance and/or UE’s capabilities. The Application Server can be either in the MNO domain (i.e. a trusted application) or external to the MNO domain (i.e. an authorized third party application.) . This use case does not consider how the splitting strategy is determined, whether this can be done autonomously or what form the strategy has.\nTable 5.5.2-1: Workload and communication requirement for split points\n\nThe glasses have limited computational capacity, and this capacity varies over time. A means for identifying the current status (AI-ML information) of the UE is available to the network i.e. via application layer. Initially it is determined that the UE has the capability to support either candidate split point 1 or 2.\nThe network communication resources are enormous, it is determined by the augmented reality service to apply candidate split point 1 so that computation is executed mainly in the network, receiving large quantities of data provided by her glasses and this helps reduce computation in UE. The large quantity of data is transmitted via the QoS flow with guaranteed data rate (GBR) 200 Mbit/s.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.5.2-1: Workload and communication requirement for split points",
                                    "table number": 7,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "5.5.3\tService Flows",
                            "text_content": "Case-a (split point adjusted based on communication performance):\nAbigail walks away from the bus stop and the vicinity of the hot spot.\nAs Abigail stood a few meters from the gNB hotspot, The insufficient communication resources leads to the serving gNB becoming unable to keep the QoS flow with GBR 200 Mbit/s anymore Thus the policy decision point (which could be anywhere – we leave out what takes the decision and how) determines to downgrade will be executed at the time point-a the GBR from 200 Mbit/s to 30 Mbit/s and immediately notifies UE and Application server of this downgrade. The strategy for splitting computation for the AR application now must be adjusted, i.e. change to candidate split point 2, for which more computation needs to be done locally but the required bit rate for UL transmission is reduced to 24 Mbit/s .\nThe strategy and constraints for the partition of work is out of scope of this use case. (These could include e.g. partial results could be sent to the UE, which could perform sub-optimally with reduced resources, can model information be sent in a lossy / compressed form that is still useful, etc.) In any case, one of the crucial inputs to the decision of how to split the work is the current set of communication resources available.\nThe network provides current network resource information concerning the UE to network communication performance such as new QoS parameters (GBR=20Mbit/s), the condition information (time point-a) for update of a new QoS, as well as end to end performance between the UE and the computation resources (e.g. in the Service Hosting Environment). This information is made available (exposed) to the split computation ‘policy decision point’ (which could be anywhere – in the UE, the edge, the cloud, etc., this is not relevant to the use case.)\nCase-b (split point adjusted based on UE’s AI-ML information):\nOriginally candidate split point-2 is selected when UE’s capability can support a high work load.\nAbigail’s UE’s computational information is monitored at the application layer. When the communication resources are sufficient to support Candidate split point 1, if the UE’s conditions degraded sufficiently (e.g. due to depleted battery, lack of storage, reduced computation capacity) then this would be a reason to select Candidate split point 1.\nThen the split computation decision point then adjusts the split computation strategy:\nFor case-a: to avoid service interruption, the split computation decision point selects the new split point-2 before the time point-a arrives. How this is communicated or ‘enforced’ is out of scope of this use case and it is not suggested that this would be standardized.\nFor case-b: to guarantee the user experience, the split computation decision point selects the split point-1 as the UE’s status is insufficient to support a high work load anymore.\nThe UE status information is assumed to be collected via application layer.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.5.4\tPost-conditions",
                            "text_content": "Abigail has no awareness of the change of model split point and continues to enjoy acceptable performance as she ventures into the city, even if perhaps it is not as good as when she stood at the bus stop. Note that this use case doesn’t conclude as long as Abigail continues to use the service – as the UE to network communication performance can change at any time.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.5.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "From TS 22.261 v17.1.0, clause 6.6.2:\nBased on operator policy, the 5G system shall support an efficient mechanism for selection of a content caching application (e.g. minimize utilization of radio, backhaul resources and/or application resource) for delivery of the cached content to the UE.\nNOTE 1: \tThe selection of content caching relies upon knowledge of communication resources (e.g. radio, backhaul, application) for delivery of content to the UE. Thus this requirement satisfies the requirements partly.\nFrom TS 22.261 v17.1.0, clause 6.7.2:\nThe 5G system shall be able to provide the required QoS (e.g. reliability, end-to-end latency, and bandwidth) for a service and support prioritization of resources when necessary for that service.\nThe 5G system shall be able to support E2E (e.g. UE to UE) QoS for a service.\nNOTE 2:\tE2E QoS needs to consider QoS in the access networks, backhaul, core network, and network to network interconnect.\nThe 5G system shall be able to support QoS for applications in a Service Hosting Environment.\nFrom TS 22.261 v17.1.0, clause 6.8:\nBased on operator policy, the 5G system shall support a real-time, dynamic, secure and efficient means for authorized entities (e.g. users, context aware network functionality) to modify the QoS and policy framework. Such modifications may have a variable duration.\nBased on operator policy, the 5G system shall maintain a session when prioritization of that session changes in real time, provided that the new priority is above the threshold for maintaining the session.\nFrom TS 22.261 v17.1.0, clause 6.10.2:\nBased on operator policy, the 5G network shall provide suitable APIs to allow a trusted third-party application to request appropriate QoE from the network.\nBased on operator policy, the 5G network shall expose a suitable API to an authorized third-party to provide the information regarding the availability status of a geographic location that is associated with that third-party.\nBased on operator policy, the 5G network shall expose a suitable API to allow an authorized third-party to monitor the resource utilisation of the network service (radio access point and the transport network (front, backhaul)) that are associated with the third-party.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.5.6\tPotential New Requirements needed to support the use case",
                            "text_content": "[P.R.5.5-001] Based on operator policy, the 5G network shall be able to provide the means to allow an authorized third-party to monitor the resource utilisation of the network service that are associated with the third-party.\n[P.R.5.5-002]\tBased on operator policy, the 5G system shall be able to expose QoS information to an authorized 3rd party. The QoS information can include e.g. UE UL/DL bitrate, latency, reliability per location.\n[P.R.5.5-003] The 5G system shall be able to provide the means to predict and expose network condition changes (e.g. bitrate, latency, reliability) to the authorized third party.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "6\tAI/ML model/data distribution and sharing over 5G system",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "6.1\tAI/ML model distribution for image recognition",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.1.1\tDescription",
                            "text_content": "Image recognition is an area where a rich set of pre-trained AI/ML models are available. The optimum model depends on the feature of the input image/video, environment and the precision requirement. The model used for vision processing at device needs to be adaptively updated for different vision objects, backgrounds, lighting conditions, purposes (e.g. image recovery vs. classification), and even target compression rates. Although a static model can also work as default in some cases, adapting the model to different working conditions will provide improved recognition accuracy and better user experience.\nAn example was given in [20] for the motivation of selecting the optimum model for different image recognition tasks and environments. As shown in Figure 6.1.1-1, 4 typical CNN models were evaluated and compared for different image recognition tasks, i.e. MobileNet_v1_025 [19], ResNet_v1_50 (ResNet [18] with 50 layers), Inception_v2 [23], and ResNet_v2_152 (ResNet with 152 layers). This example shows that the best model depends on the type of input image and the task requirement. For a mobile device which needs to recognize diverse types of images and meet various requirements for different applications, the model needs to be adaptively switched.\nFigure 6.1.1-1 presents a decision tree model for selecting the most suitable machine learning model for various image recognition tasks and environments. The tree starts with the general case and branches out into more specific scenarios, ultimately leading to the most appropriate model for the given task. The figure illustrates the trade-offs between accuracy, computational complexity, and model complexity, emphasizing the importance of selecting the right model to achieve optimal performance.\nFigure 6.1.1-1. Example of selecting the optimum model for different image recognition tasks/environments (figure adopted from [20])\nIn case the selected model has not pre-loaded in the device, the device needs to download it from the network before the image recognition task can start. A model can be reused if it is kept in storage after the previous use. But due to the limited storage resource, the device cannot retain all models for potential use in storage. The data rate for downloading the needed models depends on the size of the model and the required downloading latency.\nAlong with the increasing performance requirements to AI/ML operations, the size of the models also keeps increasing, although model compression techniques are under improvements. The typical sizes of typical DNN models for image recognition are listed in Table 6.1.1-1. A DNN parameter can be expressed in 32 bits for a higher inference accuracy. The model size and downloading overhead can be compressed if the size of a parameter is reduced to 8 bits, by potentially sacrificing the image recognition accuracy.\nThe required model downloading latency depends on how fast the model needs to be ready at the device. It is impacted by the extent to which the oncoming application can be predicted. In the use case, we assume the device cannot predict and download the needed model in advance. In this case, the downloading of the AI/ML model needs to be finished in seconds or even in milliseconds. Different from a streamed video which can be played when a small portion is buffered, a DNN model can only be used until the whole model is completely downloaded.\nFor example, if the downloading latency is 1s, the required DL data rate ranges from 134.4 Mbit/s to 1.92Gbit/s in case of 32-bit parameters, as shown in Table 6.1.1-1. In case of 8-bit parameters, the required DL data rate can be limited to 33.6Mbit/s~1.1Gbit/s.\nA model consists of model topology and model weight factors. The topology reflects the structure of a neural network (e.g. neurons of each layer, the connections of neurons between two neighboring layers). The size of the model parameters is shown in Table 6.1.1-1. The size of the configuration file of a model (i.e. the model topology), usually does not exceed 1Mbits. When an application of the UE requests a model, the third-party server sends one model profile which may consist of two parts, i.e. model topology and model weight factors. Because the model topology and model weight factors are from the same server, probably they have the same IP tuple. The transmission error of model topology matters a lot comparing to model weight factors (the UE can hardly run the model if model topology has an error while the weight factors may have a high transmission error tolerance due to model robustness). Therefore, the transmission of data of model topology is more important and requires higher reliability.\nTable 6.1.1-1: Sizes of typical image-recognition models and required DL data rates for downloading in 1s\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.1.1-1: Sizes of typical image-recognition models and required DL data rates for downloading in 1s",
                                    "table number": 8,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "6.1.2\tPre-conditions",
                            "text_content": "The UE runs an application providing the capability of AI/ML model inference for image recognition.\nAn AI/ML server manages the AI/ML model pool, and is capable to download the requested model to the application providing AI/ML based image recognition.\nThe 5G system has the ability to provide 5G network related information to the AI/ML server.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.1.3\tService Flows",
                            "text_content": "The AI/ML based image recognition application is requested by the user to start recognizing the image/video shot by the UE.\nThe AI/ML model is downloaded from the model server to the AI/ML based image recognition application via 5G network.\nThe AI/ML based image recognition application employs the AI/ML model for inference until the image recognition task is finished.\nRedo Step 2) to 3) for AI/ML model re-selection and re-downloading if needed to adapt to the changing conditions.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.1.4\tPost-conditions",
                            "text_content": "The objects in the input images or videos are recognized by the AI/ML based image recognition application and the inference accuracy and latency need to be guaranteed.\nThe image recognition task can be completed under the available computation and energy resource of the UE.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.1.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "This use case mainly requires high data rate together with low latency. The high data rate requirements to 5G system are listed in Clause 7.1 and 7.6 of TS22.261 [4]. As in Table 7.1-1 of [4], 300Mbps DL experienced data rate and 50Mbps UL experienced data rate are required in dense urban scenario, and 1Gbps DL experienced data rate and 500Mbps UL experienced data rate are required in indoor hotspot scenario. As in Table 7.6.1-1 of [4], cloud/edge/split rendering-related data transmission requires up to 0.1Gbps data rate with [5-10]ms latency countrywide.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.1.6\tPotential New Requirements needed to support the use case",
                            "text_content": "Considering the time taken by the device to finish the image recognition task, a small portion of the recognition latency budget can be used to download the model. For one-shot object recognition at smartphone and photo enhancements at smartphone, a pre-installed sub-optimal model can be temporarily employed while downloading the optimal model. The optimal model in need should be downloaded on level of 1s, and then replaces the pre-installed model. If 8-bit parameters are used for describing the DNN, the required DL data rate ranges from 33.6Mbit/s to 1.1Gbit/s. For video recognition, the target can be updating the model in one frame duration (so to adopt the updated model for the next frame). But for an application with an always-on camera, the device can predict the needed model and start the downloading in-advance. Downloading the model within 1s is acceptable. Similarly, for other applications with an always-on camera, i.e. person identification in security surveillance system, AR display/gaming, remote driving, remote-controlled robotics, the required DL data rate ranges from 33.6Mbit/s to 1.1Gbit/s. It should be noted that the size of the model may be further reduced if more advanced model compression techniques can be adopted.\nThe potential KPI requirements needed to support the use case include:\n[P.R.6.1-001] The 5G system shall support AI/ML model downloading for image recognition with a maximum latency as given in Table 6.1.6.1-1.\n[P.R.6.1-002] The 5G system shall support AI/ML model downloading for image recognition with a user experienced DL data rate as given in Table 6.1.6.1-1.\n[P.R.6.1-003] The 5G system shall support AI/ML model downloading for image recognition with communication service availability not lower than 99.999 %.\n[P.R.6.1-004] The 5G system shall support AI/ML model downloading with a reliability for the transmission of data of model topology not lower than 99.999% and a reliability for the transmission of data of model weight factors not lower than 99.9%.\nTable 6.1.6.1-1: Image recognition model downloading latency analysis for example applications (8-bit parameters for the DNN)\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.1.6.1-1: Image recognition model downloading latency analysis for example applications (8-bit parameters for the DNN)",
                                    "table number": 9,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "6.2\tReal time media editing with on-board AI inference",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.2.1\tDescription",
                            "text_content": "Smartphone is the #1 device that people carry and use everywhere for audio and video recording. It also becomes the first device to exchange media content with friends and family, to publish on social media. High end smartphones embed more and more powerful CPU and GPU and even dedicated AI hardware accelerators. As camera and picture/video quality become a differentiator among high end smartphones, AI/ML models to enhance photo shoots locally emerge on these high-end devices. AI accelerators are expected to enable the execution of complex AI/ML models directly on end-users connected devices; not only photo enhancements but high-quality audio and video content analysis and enhancement are expected to be executed locally on smartphones. Smartphones will consequently become a device to edit media content prior to sharing over the network. With the advent of 5G, new services relying on on-demand downloads of large AI/ML models to be executed in (near) real time on end user device will emerge; depending on the service, the environment, the user’s preference, the device characteristics, etc., these DNN models will need to be adapted or updated under strict latency constraints which prevent all of them to be stored locally in advance.\nMedia content analysis combines tasks as object detection, segmentation, face recognition, people counting, human activity tracking. In table 6.2.1-1, examples of commonly used DNN models for object detection (with their respective sizes) are listed.\nTable 6.2.1-1: Sizes of typical object detection models\nMedia content edition combines tasks as audio and video quality improvement, language translation, face anonymisation. In tables 6.2.1-2 and 6.2.1-3, examples of commonly used DNN models for image super-resolution and for video super-resolution are listed.\nTable 6.2.1-2: Sizes of typical image super-resolution models\n\nTable 6.2.1-3: Sizes of typical video super-resolution models\nTwo settings are considered for the use case:\nIndependent user: a person takes a video or starts a video call on its UE in a noisy environment, with difficult lighting conditions, and automatic tagging of scene and objects are embedded in the video.\nCrowd: During a large event, like a live concert, several thousand people use their UEs to film or photograph the musician band at the same time, and request additional information on the concert like band discography, lyrics, artist facial recognition, instrument/equipment brand. In this context, UEs request the downloads of DNN models to improve the capture and recording of the concert, and to provide information requested by people attending the concert. Several DNN models can be requested by each UE to e.g. execute the following tasks: image shooting and video optimization, artist face recognition, musical instrument identification, audio improvement and lyrics generation. Given the heterogeneous fleet of UEs, thousands of DNN models required by the application/service – can be requested for download. These DNN models are adapted or updated to the UEs operating system type and version, hardware characteristics and environment.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.2.1-1: Sizes of typical object detection models",
                                    "table number": 10,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 6.2.1-2: Sizes of typical image super-resolution models",
                                    "table number": 11,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 6.2.1-3: Sizes of typical video super-resolution models",
                                    "table number": 12,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "6.2.2\tPre-conditions",
                            "text_content": "The setting for this use case is as follows. Alice is attending a crowded live concert. She is eager to get movie clips and pictures as a great souvenir of the concert, but she is worried about difficult conditions to get this great souvenir as the conditions for light and sound are very variable, with limited light for the spectators and too much light on the scene. The audio stereo is variable and not well-balanced dependent on where she is among the audience and has a very noisy background.\nAlice would like to store good quality movie clips and pictures on her private account on the internet for the future, and also post photos and videos tagged with artist name and other relevant information during the concert. As Alice is also an amateur musician, she also wants to get detailed real-time information about the structure of the song, the lyrics and the instruments.\nThe pre-conditions are:\nAlice is attending a crowded live concert.\nHer UE is registered to the 5G network.\nApplications of Alice’s smartphone can rely on fine-tuned machine learning models that are available via the network that covers the concert hall. These models are:\nAn ML model improving photo capture for this concert hall (a model specially fine-tuned for this concert place).\nAn ML model improving audio capture for this concert hall (a model specially fine-tuned for this concert place).\nAn ML model specialized in the discography and the lyrics of the band.\nAn ML model specialized in the artists face recognition.\nAn ML model specialized in music instrument identification.\nNOTE:\tThe listed models above are examples for this use-case and is not an exhaustive list.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.3\tService Flows",
                            "text_content": "1)\tShortly after the start of the concert, Alice, as most of the fans, launches the camera application on her mobile phone to film the scene and to get additional information about the band, the individual artists or the songs or instruments.\n2)\tShe points her device’s camera towards the scene.\n3)\tThe environment is very dark with strong light spots. To be fully functional and to render the best user experience, the camera application downloads ad-hoc ML models.\n4)\tThe proposed ML models are very performant in this environment but also very heavy in size.\n5)\tThe ML models can be used for the whole capture especially if environment remains stable. If the environment changes substantially, or better ML models become available, ML models can be progressively updated accordingly with respect of some operating rules (like a maximum number of ML update per second). In all cases, the camera application continues working seamlessly.\n6)\tThe audio and video streams are captured, improved in quality, processed to extract and display additional information, and either stored in real-time on the mobile phone itself or provided as a live stream.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.4\tPost-conditions",
                            "text_content": "Alice can see that even in harsh light conditions and with the noisy background the photos and videos are great, additional information is provided and all is correctly tagged as requested.\nThe post-conditions are:\nPhotos and videos are either stored on the mobile phone in the improved high quality, ready to be uploaded and shared on social media, or directly shared on social media.\nAudio recording is high quality with ambient noise reduction, improved stereo balance.\nAdditional information about band, song/lyrics, instruments, etc. are displayed on the mobile phone and stored in media recordings’ metadata.\nAlice can visualize additional information and upload the photos and videos on her social network(s) with the associated tags and information provided by the models, and also store the above on her personal media server.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "The performance requirements for high data rate and traffic density scenarios are found in TS 22.261 [4] clause 7.1. The scenario Broadband access in a crowd is relevant for the use case of very dense crowds, for example at stadiums or concerts. In addition to a very high connection density, the users can share their experience, i.e. what they see and hear. This can put a higher requirement on the uplink than the downlink.\nThis new use case has some requirements on the downlink not covered by the existing requirements, see table 6.2.5-1.\nTable 6.2.5-1. Excerpt from TS 22.261 [4] Table 7.1-1\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.2.5-1. Excerpt from TS 22.261 [4] Table 7.1-1",
                                    "table number": 13,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "6.2.6\tPotential New Requirements needed to support the use case",
                            "text_content": "Potential new requirements needed to support the use case result of the following parameters:\nAI/ML model size.\nAccuracy of the model.\nlatency constraint of the application or service.\nnumber of concurrent downloads, i.e. number of UEs requesting AI/ML model downloads within same time window.\nThe number of concurrent downloads further depends on the density of UE in the covered area and the covered area size.\nThe tables 6.2.6.1-1, 6.2.6.1-2 and 6.2.6.1-3 contain KPI for different aspects of the real-time media editing use case.\nTable 6.2.6.1-1. Typical sizes of AI/ML models for the UC\n\nFrom table 6.2.6.1-1, AI/ML models currently available to elaborate the use case have sizes that vary from 3.2 MB to 536 MB.\nAs indicated in the use case in clause 6.1, it can be noted that the size of AI/ML models can be reduced prior to transmission with dedicated model compression techniques. On the contrary, AI/ML models with more neural network layers and more complex architectures arise to solve more complex tasks and to improve accuracy. This trend is expected to continue in the coming years. Typical model sizes in the range of 3 MB to 500 MB appear to be a reasonable compromise to consider for this use case.\nIn the following, two categories are considered for AI/ML model sizes:\nAI/ML model sizes below 64 MB, which can be associated to models optimized for fast transmission,\nAI/ML model sizes below 500 MB, which can be associated to models optimized for higher accuracy.\nMaximum latency in function of application or service:\nVideocall service: end-to-end latency below 200 ms,\nVideo recording, video streaming, and object recognition applications: latency below 1 s.\nUser experienced DL data rate results from above AI/ML models’ sizes and maximum latency values are summarized in the table 6.2.6.1-2.\nTable 6.2.6.1-2. UC AI/ML model download – single UE – KPIs\n\nAs indicated above, the number of concurrent downloads is a third parameter to determine potential new requirements. This corresponds to the maximum number of UEs requesting a AI/ML model download in a same time window and same covered area/cell.\nThe case of a concert hall is an illustration of the scenario, “Broadband access in a crowd” from TS 22.261 [4]. This scenario assumes an overall user density of 500 000 UE / km2 (i.e. 0.5 UE / m2) and an activity factor of 30 %.\nIn the concert hall case, it is also assumed that only a part of the UEs intends to request AI/ML model downloads. Moreover, only a subpart will request AI/ML model download during the same time window in the same cell. The activity factor is finally estimated to 1 % (i.e. % of UE requesting an AI/ML model download within same time window).\nTypical number of UEs in a concert hall varies from ~1000 seats to ~ 5000 seats.\nBased on these figures and UE activity assumption, the number of concurrent downloads is estimated as given in table 6.2.6.1-3.\nTable 6.2.6.1-3. Estimated number of concurrent downloads\n\nFrom table 6.2.6.1-2 and table 6.2.6.1-3, requirements on the covered area are estimated as follows:\nTable 6.2.6.1-4. Estimated covered area DL data rate requirements\n\nAnother approach to estimate the number of concurrent downloads is to estimate the number of different AI/ML models requested by UEs instead of the number of UEs requesting AI/ML models. The AI/ML models can then be broadcast/multicast to multiple UEs. The number of different AI/ML models depends on the accuracy expectations of the AI/ML models, the execution environments and the hardware characteristics of end devices. When the number of UE requesting AI/ML models is very high, the number of different AI/ML models can remain smaller. This approach is well suited for very large crowd.\nThe number of concurrent downloads when transmitted in broadcast/multicast to many UEs can be estimated between 1 (i.e. all UEs request the same AI/ML model) and 50 (i.e. all UEs request different AI/ML models).\nIndependent user:\n[P.R.6.2-I-001] The 5G system shall support the download of AI/ML models with a latency below 1s and a user experienced data rate of 512 Mb/s.\nNOTE 1:\tThis requirement concerns AI/ML models having a size below 64 MB.\n[P.R.6.2-I-002] The 5G system shall support the download of AI/ML models with a latency below 1 s and a user experienced data rate of 4 Gb/s.\nNOTE 2:\tThis requirement concerns AI/ML models having a size below 500 MB.\nCrowd:\n[P.R.6.2-C-001] The 5G system shall support the parallel download of up to 50 AI/ML models with a latency below 1 s.\nNOTE 3:\tThis requirement concerns AI/ML models having a size below 64 MB.\n[P.R.6.2-C-002] The 5G system should support the functionality to broadcast/multicast a same AI/ML model to many UEs with a latency below 1 s.\nNOTE 4:\tThis requirement concerns AI/ML models having a size below 64 MB.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.2.6.1-1. Typical sizes of AI/ML models for the UC",
                                    "table number": 14,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 6.2.6.1-2. UC AI/ML model download – single UE – KPIs",
                                    "table number": 15,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 6.2.6.1-3. Estimated number of concurrent downloads",
                                    "table number": 16,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 6.2.6.1-4. Estimated covered area DL data rate requirements",
                                    "table number": 17,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "6.3\tAI/ML model distribution for speech recognition",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.3.1\tDescription",
                            "text_content": "AI/ML-based speech processing has been widely used in applications on mobile devices (e.g. smartphone, personal assistant, language translator), including automatic speech recognition (ASR), voice translation, speech synthesis. Speech recognition for dictation, search, and voice commands has become a standard feature on smartphones and wearable devices.\nService requirements to ASR have been addressed in [29]. Traditional ASR systems are based on hidden Markov model (HMM) and Gaussian mixture model (GMM). However, the HMM-GMM systems suffer a relatively high WER (Word Error Rate) with presence of environmental noise. Although some enhancements were developed including “feature enhancement” (attempts to remove the corrupting noise from the observations prior to recognition) and “model adaptation” (leaves the observations unchanged and instead updates the model parameters of the recognizer to be more representative of the observed speech), the traditional models can hardly fulfil the requirements of commercial applications. Acoustic models based on deep neural networks (DNN) have remarkable noise robustness [25-26], and have been widely used in the ASR applications in mobile devices.\nNowadays, most of ASR applications on smartphones are operating in cloud servers. The end device uploads the speech to the cloud server, and then downloads the decoded results back to the device. However, cloud-based speech recognition potentially introduces a higher latency (not only due to the 4G/5G network latency, but also the internet latency), and the reliability network connection and privacy issue need to be considered.\nAn embedded speech recognition system running on a mobile device is more reliable and can have lower latency. Currently, some ASR apps would switch from cloud-based model inference to offline model inference when uplink coverage of the mobile user turns weak, e.g. when entering a basement or an elevator. However, the ASR models for cloud servers are too complex for computation and storage resources on mobile devices. The size of a ML-based ASR model running on cloud server has rapidly increased in the recent year, from ~1GByte to ~10GByte, which cannot be run on a mobile device. Due to the restriction, only the simple ASR applications, e.g. wakeword detection, can be implemented on smart phones. Realizing more complicated ASR applications, e.g. large vocabulary continuous speech recognition (LVCSR) is still a challenging area for an offline speech recognizer.\nIn 2019, a state-of-the-art offline LVCSR recognizer for Android mobile devices was announced. The streaming end-to-end recognizer is based on the recurrent neural network transducer (RNN-T) model [27-28]. It was stated that, by employing all kinds of improvements and optimizations, the memory footprint can be dramatically reduced and the computation can be speed up. The model can be compressed to 80MB. Meanwhile the ASR model is compressed to fit the use at mobile device, the robustness to the various types of background noises has to been sacrificed. When the noise environment changes, the model needs to be re-selected, and in case the model is not kept in the device, the model needs to be downloaded from the cloud/edge server of the AI/ML model owner via 5G network.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.3.2\tPre-conditions",
                            "text_content": "The UE runs an application providing the capability of AI/ML model inference for speech recognition.\nAn AI/ML server manages the AI/ML model pool, and is capable to download the requested model to the application providing AI/ML based speech recognition.\nThe 5G system has the ability to provide 5G network related information to the AI/ML server.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.3.3\tService Flows",
                            "text_content": "The AI/ML based speech recognition application is requested by the user to start recognizing the speech recorded.\nThe AI/ML model is downloaded from the model server to the AI/ML based speech recognition application via 5G network.\nThe AI/ML based speech recognition application employs the AI/ML model for inference until the speech recognition task is finished.\nRedo Step 2) to 3) for AI/ML model re-selection and re-downloading if needed to adapt to the changing conditions.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.3.4\tPost-conditions",
                            "text_content": "The content in the input speech is recognized by the AI/ML based speech recognition application and the inference accuracy and latency need to be guaranteed.\nThe speech recognition task can be completed under the available computation and energy resource of the UE.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.3.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "This use case mainly requires high data rate together with low latency. The high data rate requirements to 5G system are listed in Clause 7.1 and 7.6 of TS22.261 [4]. As in Table 7.1-1 of [4], 300Mbps DL experienced data rate and 50Mbps UL experienced data rate are required in dense urban scenario, and 1Gbps DL experienced data rate and 500Mbps UL experienced data rate are required in indoor hotspot scenario. As in Table 7.6.1-1 of [4], cloud/edge/split rendering-related data transmission requires up to 0.1Gbps data rate with [5-10]ms latency countrywide.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.3.6\tPotential New Requirements needed to support the use case",
                            "text_content": "An ASR model can only be used until the whole model is completely downloaded. And device needs to adopt the noise-robust ASR model which adapts to the changing noise environment. In general, the device’s microphone is switched on only when the speech recognition application is triggered. The device needs to identify the noise environment and download the corresponding ASR model in a low latency (level of 1s). The sizes of some typical ASR models are listed [27][47] in Table 6.3.6-1, which are used to derive the required data rate.\nThe potential KPI requirements needed to support the use case include:\n[P.R.6.3-001] The 5G system shall support AI/ML model downloading for speech recognition with a latency not higher than 1s.\n[P.R.6.3-002] The 5G system shall support AI/ML model downloading for speech recognition with a user experienced DL data rate as given in Table 6.3.6.1-1.\n[P.R.6.3-003] The 5G system shall support AI/ML model downloading for speech recognition with communication service availability not lower than 99.999 %.\nTable 6.3.6-1: Sizes of typical speech-recognition models and user experienced DL data rates for downloading in 1s (8 bits per parameter)\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.3.6-1: Sizes of typical speech-recognition models and user experienced DL data rates for downloading in 1s (8 bits per parameter)",
                                    "table number": 18,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "6.4\tAI model management as a Service",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.4.1\tDescription",
                            "text_content": "The AI/ML models can be classified and generated from different perspectives, such as services, users, time, locations, etc. So there can be a large number of AI/ML models (incl. data for structure and weight factors, gradient) used for inference and training. Due to the limitation of storage, UE cannot always preload all AI/ML models for different works. It can be a new commercial opportunity that operators provide services to help manage and distribute the AI/ML models so that UE can acquire a proper model immediately.\nIn the future, it is expected that 3rd party companies will make use of AI models to support different kinds of services such as panorama tourist guide using Augmented Reality (AR) in a resort. However, most of 3rd party does not have the resource (server) in distributed places. Considering the large size of AI models, strict downloading time (illustrated in use case 6.1, 6.2, 6.3) and limited UE storage, it is appropriate for 3rd party companies to authorize professional companies to manage their AI models rather than doing the same thing locally by themselves.\nSince operator cloud resource has advantage to manage different AI/ML models centrally (e.g. cloud server) and locally (e.g. MEC server), such services can be exposed to 3rd party users as well. Specifically,\nThe operator has its own need to maintain a AI/ML model pool for its own business like network optimization;\nThe 3rd party would like to use some common AI/ML models already stored in operator’s cloud;\nDue to resource limitation of 3rd party, they would like to lease operator’s cloud resource to manage their AI models.\nTherefore, as shown in Fig.6.4.1-1, The AI model management includes:\nLet 3rd party invoke capabilities exposed by 5GS to upload/download/update/delete/store/monitor AI models.\nTransmitting AI models to user efficiently per situation (e.g. entering into a certain area)\nAs the 5GS may collect communication data, user experience, etc., it may perform a data analytics for user experience, which can generate analyzing results to help operator’s cloud train and improve AI models for the 3rd party.\nFigure 6.4.1-1 illustrates the operator cloud for model management, showcasing the integration of various components such as the edge cloud, core cloud, and network functions virtualization (NFV). The figure highlights the orchestration of network services and applications through a centralized management system, enabling efficient resource allocation and dynamic service provisioning. The visual representation emphasizes the scalability and flexibility of cloud-based telecommunication systems, which are crucial for meeting the growing demands of modern communication networks.\nFigure 6.4.1-1: Operator cloud for model management\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.2\tPre-conditions",
                            "text_content": "The operator cloud (e.g. Edge server) stores a variety of AI/ML models according to operator’s or 3rd party’s requirement.\nThe operator cloud (e.g. Edge server) is capable to distribute a model stored in the operator cloud to devices.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.3\tService Flows",
                            "text_content": "Company A provides panorama tourist guide using Augmented Reality (AR) technology. They provide guide services in a commercial area and a tourist resort. UE needs to download Model A and B (both are 32 bits VGGnet, 536MByte) respectively;\nTo reach the SLA, company A indicates to the operator that UE requires model A in area A and model B in area B.\nWhen UE moves to one place, The local Edge server trigger to establish a QoS acceleration and UE downloads the corresponding Model (A or B) stored in the local Edge server timely so that user can enjoy the continuous AR world without obvious interruption when model changes.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.4\tPost-conditions",
                            "text_content": "UE uses the Model for panorama tourist guide.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "None.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.6\tPotential New Requirements needed to support the use case",
                            "text_content": "[P.R.6.4-001] Subject to user consent, operator policies and the regional or national regulatory requirements, the 5G system shall be able to provide the capability to expose information (e.g. measured data rate, delay, network analytics results) to an authorized third-party application to support the training and monitoring of the AI/ML models.\n[P.R.6.4-002] The 5G system shall be able to support an authorized third-party application to distribute an AI/ML model ranging from 3.2~536MB to an third-party application running on the device in less than 1 second with a user density of up to 5000~10000/km2 in an urban area.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.5\tAI/ML based Automotive Networked Systems",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.5.1 \tDescription",
                            "text_content": "This use-case is illustrative for a specific example but is readily extensible for more advanced scenarios involving disaster scenarios, rescue operations or even autonomous vehicle operations.  The focus of this scenario is on machine-to-machine AI/ML transfer learning and inference systems that are networked together using 5G networks to provide smart automotive applications and services.\nThe scenario assumes that there exist multiple AI/ML systems available, in a high-reliability, low-latency, high-bandwidth system optimized for machine-to-machine interaction.  The proposed AI/ML systems may continuously exchange and share AI-ML model layers in a distributed and or federated network as determined by the system in response to a change of events, conditions or emergency situations, to improve some or all of the ML system prediction accuracy.  These systems may optimize the AI/ML inference latency by executing different layers on various AI/ML networked systems.\nThere are two main types of ML models and model processing considered in this use-case:\nLarge ML models updates using non-real time training – These ML models are trained and optimized with millions or billions of parameters using extensive computing resources to achieve the highest accuracy possible based on specific sets of input training data.  This training is performed over a long period of time and the resulting fully-trained ML model is the baseline model installed at initial production for the devices in the use-cases described below.  These fully trained models may be updated with externally provided AI-ML model data and may also improve themselves based on external sensor data.\nPartial transfer and exchange of AI-ML model data – This use case describes different applications where different types of ML systems are networked to exchange parts of AI-ML model data to improve prediction accuracy.  In some systems, local ML models can continuously improve their baseline model using data gathered from the existing environment and other sensor input.  These systems upload improved model data at relatively slow rates to a larger cloud-based network where further processing takes place to further refine the full ML model.  The specific method used for continuous improvement is outside the scope of this use case.\nDifferent example scenarios are described below.\nAI/ML Systems Emergency Response to Disabled Vehicle\nFigure 6.5.1-1 illustrates a vehicle that has broken down hidden by a blind curve in the road.  This breakdown is considered high-severity for the purposes of illustrating this use-case.\n\nFigure 6.5.1-1 illustrates a dangerous scenario where a vehicle breakdown occurs at a hidden location behind a blind curve, posing a significant risk to other road users. The breakdown is marked by a red circle, and the vehicle is represented by a broken line. The blind curve is depicted by a curved line, emphasizing the risk of visibility issues. The figure highlights the importance of road safety and the need for effective warning systems in such situations.\nFigure 6.5.1-1: Dangerous vehicle break-down hidden by a blind curve\nOn-board vehicle sensors detect the following: mechanical failure, speed reduction, GPS coordinates and a variety of technical parameters are used by on-board ML inferencing systems to immediately diagnose the failure and notify surrounding networks of the severity of the failure.\nDifferent non-real time AI models can use the gathered failure training data to refine prediction and prevention models updates.\nTraffic Surveillance Camera and MEC Metro Traffic Safety System\nSurveillance cameras use ML models to analyze image sensor data to detect and categorize the type of accident using ML inference with continuous AI-ML model improvement.  This processed accident information is uploaded along with the model updates to the local near-edge MEC for AI/ML system model prediction and algorithm processing to suggest the immediate next course of action.  These AI/ML systems suggest an automated warning broadcast.  The automated AI/ML systems relying on trained ML models to predict the position of the vehicle is especially dangerous due to the hidden blind curve and informs local Emergency Response Unit dispatch to send help to set up a traffic bypass earlier on the road.  The local near-edge MEC models are capable of continuous model improvement and return this model data back to the surveillance camera for further inference accuracy improvement.\nThe uploading of data, possibly pre-processed by the local AI/ML model, from the surveillance camera to the MEC may happen with low-latency depending on the severity of the accident while returned data to update the local model may happen on a much slower timeframe.\nApproaching Intelligent Vehicle Reaction\nFigure 6.5.1-2 illustrates the communication architecture of an approaching autonomous vehicle, showcasing the interaction between the vehicle, roadside units (RSUs), and cloud-based servers. The RSUs are strategically placed along the road to facilitate communication with nearby vehicles, while the cloud-based servers manage data processing and decision-making. The figure highlights the role of 5G technology in enabling high-speed, low-latency communication, ensuring safe and efficient operation of the autonomous vehicle.\nFigure 6.5.1-2: Approaching Autonomous Vehicle\nFigure 6.5.1-2 illustrates a vehicle rapidly approaching the car illustrated in Figure 1. The vehicle’s sensors are unable to detect the hazard and must rely upon other AI/ML systems for a warning.\nAn approaching autonomous vehicle receives multiple warning notifications and AI/ML systems in the vehicle use trained models predict and suggest appropriate automatic manoeuvres to move the vehicle to the far lane to avoid any danger. Sharing of data with other systems allows for constantly refining their models; this sharing of data does not have non-real requirements.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.2 \tPre-conditions",
                            "text_content": "The scenarios described above assume the following pre-conditions:\nAll existing AI/ML systems have existing pre-trained default AI-ML models that will perform predictions at a baseline accuracy and are capable of updating/re-fining the AI-ML models for continuously improving accuracy for the specific purpose.\nTable 6.5.2-1: Use-Case Pre-Conditions\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.5.2-1: Use-Case Pre-Conditions",
                                    "table number": 19,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "6.5.3 \tService Flows",
                            "text_content": "Appropriate AI/ML systems shall provide security and data protection as dictated by law and other appropriate policies.\nThe service flows described in Figure 3 are all machine-to-machine AI/ML intelligent system interactions designed for the sharing of information to with a range of demands from lowest-latency for critical situations (e.g. alarm messages) and relaxed latency for less critical problems. Shared information (possibly including an extensive set of data) can also be exchanged with relaxed latency to support training and updating of the AI/ML model.  Each level of service may be owned and operated by a different organization.\nThis use-case assumes that no more than [10%] of the total ML model size needs to be shared or to updated in a single-low latency transmission.\nFigure 6.5.3-1 illustrates various examples of AI-ML interactions, showcasing the integration of machine learning algorithms with artificial intelligence to enhance decision-making processes. The figure depicts a network architecture with interconnected nodes, each representing a different stage in the data processing pipeline. Key components include data ingestion, preprocessing, model training, and prediction stages, all of which are crucial for the successful application of AI-ML in real-world scenarios. The figure emphasizes the importance of efficient data flow and seamless communication between different components, highlighting the need for robust infrastructure and optimized algorithms to achieve optimal results.\nFigure 6.5.3-1: Examples of AI-ML interactions\n\nVehicle to other AI Systems\nExamples of AI-ML model data exchange are:\nVehicle health and inspection monitoring ML models – Local ML models sense and react to crash, fire, temperature and electrical sensor information.  These models are continuously improving and optimizing but not necessarily sharing data if the system is nominal.  However, once an emergency situation has occurred, emergency information can be sent to the appropriate emergency response services in the Smart City Core network, local cameras and road sensors. Extended information can also be shared with relaxed latency demands to support model updates/training.\nVehicle sensing for vehicle dynamics under ML model sensing and control.  Local ML models sense and react to skids, slides, acceleration and breaking.  These models are continuously improving and optimizing but not necessarily sharing data if the system is nominal.  However, once an emergency situation has occurred, emergency information is immediately broadcasted to appropriate emergency response services and other system applying related ML models to inform them. Extended information can also be shared with relaxed latency demands to support improve the systems prediction accuracy.\nTraffic Surveillance Camera to Metro Traffic Control\nCamera senses accident using local AI/ML models and sends notification to traffic control.  Traffic control may use multiple cameras to cover the same incident and, thereby, improve its inference accuracy. Extended information exchanged with low-latency demands can be used to re-train and redistribute reference AI model layers accordingly.\nExamples of AI-ML Model data exchange are:\nCamera and Road sensors detect emergency situations – Local camera and road sensor ML models are constantly improving their prediction of emergency situations.  Once an emergency situation has occurred emergency information data is shared with appropriate systems to improve the response for all emergency responders. Extended information can also be shared with relaxed latency demands to support improve the systems prediction accuracy.\nMetro Traffic Control to Metro Emergency Services Network\nAI/ML models use automation and prediction to send appropriate EMS vehicles to the accident site.\nExamples of AI-ML Model data exchange are:\nCamera, Road and Weather sensor use ML models. ML models sensors are continuously improving and optimizing prediction results.  Once an emergency situation is detected, emergency information from these systems can be shared with emergency vehicles to, e.g. improve their routing time through traffic based on the situation.\nStalled Vehicle to Manufacturer\nStalled vehicle sends data (possibly pre-processed, e.g. AI/ML model data) to automobile manufacturer to help diagnose the problem.  Manufacturer uses the information to improve product quality, reliability and performance.  Federated and/or Distributed Learning techniques can be used to improve on-board vehicle AI/ML model.\nStalled Vehicle to Metro Traffic Control (MTC)\nStalled vehicle sends emergency information to MTC to help warn others.  E.g., The specific blind curve location along with other regional data is used to improve all MTC responses. Extended information can also be shared with relaxed latency demands to improve MTC response models.\nStalled Vehicle to Repair Service\nStalled vehicle sends data (possibly pre-processed, e.g. AI/ML model data) to repair service to ensure appropriate response vehicles provide proper tools and equipment.  Repair service response use Federated and/or Distributed Learning techniques for constant model improvement.\nApproaching Vehicle communication with Metro Traffic Control\nMTC warns approaching vehicle of upcoming danger and depending on the SAE level of driving automation, the vehicle responds appropriately. Extended information can also be shared with relaxed latency demands to support improve the systems prediction accuracy.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.4 \tPost-conditions",
                            "text_content": "All systems identified in this scenario use independent, networked AI/ML Distributed or Federated Learning algorithms to aggregate information from multiple sources to improve layer of the system AI/ML model.  Independent AI/ML training systems ensure that improved models are distributed back to each end-system to improve the overall safety and robustness of the next response.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.5 \tExisting features partly or fully covering the use case functionality",
                            "text_content": "None.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.6 \tPotential New Requirements needed to support the use case",
                            "text_content": "Table 6.5.6-1 provides example ML models (sizes and DL data rates) that are considered for the use-cases above and Table 6.5.6-2 outlines latency requirements.  It is assumed that a maximum of 10% of the (full) model size is exchanged among the participating systems.\nTable 6.5.6-1: ML Networks/models and potential DL data rates\n\nTable 6.5.6-2: Potential Latency Requirements\n\nRequirements:\n[P.R.6.5-001] The 5G system shall be able to support downloading of data with a maximum size of ~2-40 MB to update the local AI/ML model with latency up to 500ms – 1s.\n[P.R.6.5-002] The 5G system shall be able to support downloading of data with a maximum size of ~2-40 MB to update the local AI/ML model with (user experienced) DL data rate of up to 100 Mbit/s.\n[P.R.6.5-003] The 5G system shall be able support downloading of data to update the local AI/ML model with communication service availability up to 99.999 %.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.5.6-1: ML Networks/models and potential DL data rates",
                                    "table number": 20,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 6.5.6-2: Potential Latency Requirements",
                                    "table number": 21,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "6.6\tShared AI/ML model monitoring",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.6.1\tDescription",
                            "text_content": "AI/ML models are trained on a training data set to accomplish an established task. The tasks may vary from image or speech recognition to forecasting to optimize, as example, handover performance (see 3GPP TR 28.809 [59]) or tuning Core Network assisted parameters (see clause 5.4.6.2, TS 23.501).\nIn each of these tasks, the provider of the shared AI/ML model may benefit from sharing a trained AI/ML model with the consumer(s) of the shared AI/ML model or may benefit from a distributed/federated AI/ML model training or a split AI/ML model training over the 5G system. Furthermore, AI/ML model monitoring is a requirement to enable online learning in the network (e.g. via Reinforcement Learning), a set of techniques more suited to promptly react to service degradation.\nNOTE:\tThe following terms are used in this use case:\nShared AI/ML model: AI/ML model that is shared among different applications, e.g., the AI/ML model is pre-trained and provisioned to different consumers, or the AI/ML model is trained using distributed/federated learning approach or by splitting the model training phase to different parts executed in different network locations.\nShared AI/ML model provider: application server that is providing or managing a \"shared AI/ML model\".\nShared AI/ML model consumer: application, e.g. running on the UE,  that is using/consuming a \"shared AI/ML model\".\nDue to changes in the scenario (i.e., in the context from which training data are collected), an AI/ML model may provide poor performances compared with the performance of the AI/ML model measured during the model testing phase. This can happen when over time the distribution of the input data for inference differs from the distribution of the training data, or if the AI/ML model is utilized in a different context. In this case, the shared AI/ML model provider should be able to promptly detect the performance degradation and react in order to avoid service degradation or disruptions. Quite often, update of a shared AI/ML model is not solely dependent on inference results of one shared AI/ML model consumer as performance degradation could be due to some other error source, e.g., model input measurement errors. To detect outdated shared AI/ML model, the shared AI/ML model provider can make use of inference results from multiple shared AI/ML model consumers and a spatial and temporal analysis is performed before triggering shared AI/ML model update.\nTherefore, the shared AI/ML model provider, once sharing an AI/ML model over 5G System with a shared AI/ML model consumer, needs to keep track of the model performances to detect possible performance degradation of the shared AI/ML model (e.g. based on inference feedback from AI/ML model consumer such as a lower confidence level).\nAlternatively, the shared AI/ ML model provider can split the AI/ ML model training with a shared AI/ML model consumer to constantly improve the performance of the shared AI/ ML model based on a local training and/or inference feedback from the shared AI/ML model consumer.  The local part of the shared AI/ML model will be trained/fine-tuned under the shared AI/ML model provider guidance. The input to this training will be data available at the shared AI/ML model consumer. The output of local model training or an inference at shared AI/ML model consumer can be provided to the shared AI/ML model provider and be used by the shared AI/ML model provider to provide further information for the 5G System to improve its operations.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.6.2\tPre-conditions",
                            "text_content": "The shared AI/ML model provider stores multiple AI/ML models along with their performances measured during the test phase.\nThe shared AI/ML model provider is capable of sharing AI/ML models with shared AI/ML model consumers leveraging on 5GS.\nThe AI/ ML model provider is capable of splitting and/or distributing the AI/ML model training with/to a shared AI/ ML model consumer leveraging on 5GS.\nThe shared AI/ML model consumer may run applications requiring the usage of AI/ML models and download them from the AI/ML model provider via the 5GS.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.6.3\tService Flows",
                            "text_content": "The shared AI/ML model provider wants to optimize performance of some process by means of shared AI/ML model performance.\nThe shared AI/ML model provider sends the trained shared AI/ML model to the shared AI/ML model consumer at the UE leveraging on the 5GS.\nThe UE receives the model and employs the model to perform local training and inference using data available on the UE.\nThe shared AI/ML model provider monitors the context scenario (e.g. the UE data which is available to the application) in which the UE is running the shared AI/ML model and the model performance.\nIf a change in the context scenario or model performance is detected, e.g. based on an inference feedback from the shared AI/ML model consumer, the shared AI/ML model provider, in order to avoid model performance degradation, shares with the AI/ML model consumer an updated version of the shared AI/ML model retrained to capture the new context with expected better performance.\nThe UE continues to run the updated model without experiencing performance degradation.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.6.4\tPost-conditions",
                            "text_content": "Following the example in the use case, the shared AI/ML model provider receives an accurate forecast regarding AI/ML model performance, and the AI/ML model consumer is using an AI/ML model with high performance.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.6.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.6.6\tPotential New Requirements needed to support the use case",
                            "text_content": "[P.R.6.6-001] The 5GS shall be able to transfer an updated AI/ML model from the shared AI/ML model provider to the shared AI/ML model consumer within [1s-1min] latency for AI/ML models of a maximal size of [100-500] MB.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.7\tPrediction of AI/ML model distribution",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.7.1\tDescription",
                            "text_content": "If the AI/ML model server was intelligent enough, it would be ideal that the AI/ML model server could predict that the User would need to use some AI/ML model and download the AI/ML model to the UE in advance.\nTake image recognition for example, the AI/ML model could be able to predict and download specific AI/ML model in advance based on the user’s location, direction of travel and users use of image recognition functionality. For the users located in a famous history museum or moving towards a famous history museum, the 5G network functions and the AI/ML model server could interact with each other and predict that the users could possibly need to use the AI/ML model designed specifically for such history museum which is suitable for the indoor environment and good at historic pictures recognition. Similarly, for the users located in a zoo or moving towards a zoo, the 5G network functions and the AI/ML model server could interact with each other and predict that the users could possibly need to use the AI/ML model designed specifically for animal recognition. In contrast, for the users located in an arboretum or moving towards an arboretum, the 5G network functions and the AI/ML model server could interact with each other and predict that the users could possibly need to use the AI/ML model designed specifically for flower or plant recognition.\nEditor's Note:\tit is FFS whether other examples than image recognition need to be studied.\nThe 5G network and the AI/ML model shall not violate the user privacy policies for the prediction or pre-download of AI/ML models.\nAccording to user privacy policies, the AI/ML model server may not to be able to get access to the UE’s location information.\nIf the AI/ML model server is allowed to access to the UE’s location information according to user privacy policies, the AI/ML model server could obtain the UE’s location information via two options:\nOption 1:\tthe AI/ML model server could obtain the UE’s location information directly from the UE, over the application layer. This is pure application layer interaction between the AI/ML based image recognition application and the AI/ML model server.\nOption 2:\tthe AI/ML model server could obtain the UE’s location information via the location service provided by the 5G network. This is already specified in TS 23.273 [6] (see further analysis in section 6.1.4).\nIf the AI/ML model server is not allowed to access to the UE’s location information according to user privacy policies, the 5G network has the UE’s location information and is not allowed to expose the UE location information to the AI/ML model server. The 5G network function could still interaction with the AI/ML model server for prediction of need of certain AI/ML model without breaking the user privacy policies. There could be lots of workable solutions. Here daftly describe two possible solutions:\nOption a):\tPrediction based on monitoring function of the 5G network. New monitoring event could to be introduced to assist the prediction by the AI/ML model server without disclosure of User location information to the AI/ML model server. Option b)\tPrediction based on NWDAF analytics function of the 5G network. Based on analytics of UE’s location, mobility, download data size and etc, the 5G network could predict that lots UE will download certain amount of data from an AI/ML model server in some location area and inform the AI/ML model that certain UE will probably download certain amount of data. The AI/ML model server could use such information to adjust its prediction.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.7.2\tPre-conditions",
                            "text_content": "The UE runs an application providing the capability of AI/ML model inference for image recognition.\nAn AI/ML server manages the AI/ML model pool, and is capable to download the requested model to the application providing AI/ML based image recognition.\nThe 5G system is able to provide 5G network related information to the AI/ML server.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.7.3a\tService Flows for Prediction based on monitoring function of the 5G network",
                            "text_content": "1)\tThe AI/ML model server provides to the 5G network function with some criteria. Each criterion could have different rules. For example, one criterion could contain a list of location areas which contains all the museums, galleries and antique shops all over the country.\n2)\tIf the 5G network functions determine the UE meets one criterion, for example, the UE is located in the list of location areas, then the 5GS network function (e.g., NEF) notify the AI/ML model server which criterion the UE meets.\n3)\tAccording the criterion that the UE meets, the AI/ML model server determines that the UE might need to use the AI/ML model designed specifically for such history museum which is suitable for the indoor environment and good at historic pictures recognition.\n4)\tThe 5G operator need to make sure that the criteria provided by the AI/ML model does not break the user privacy policies.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.7.3b\tService Flows for Prediction based on NWDAF analytics function of the 5G network",
                            "text_content": "1)\tThe NWDAF do the data collection and analytics work for the UE’s location, mobility, download data size etc.\n2)\tBased on the NWDAF analytics, the 5G network could predict that lots UE will download certain amount of data from an AI/ML model server in some location area.\n3)\tFor the AI/ML model server that has subscribed the data analytics service from 5G network, 5G network inform AI/ML model server directly or via NEF that certain UE will probably download certain amount of data.\n4)\tThe AI/ML model server could use such information to adjust its prediction.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.7.4\tPost-conditions",
                            "text_content": "The 5G network and the AI/ML model server could predict the need of download certain AI/ML model from the UE and refine the prediction during the interaction.\nThe 5G network could adjust its resource for download of data from AI/ML without expansion of capacity based on the prediction.\nThe AI/ML model server could predict the user preference and needs without knowing UE’s private information, such as location information.\nThe UE could get better user experience during the use of AI/ML model application via 5G network.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.7.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "TS23.273[6] clause 4.1:\nLocation information for one or multiple target UEs may be requested by and reported to an LCS client or an AF within or external to a PLMN, or a control plane NF within a PLMN. Location information contained in the location request and location information contained in the location response are defined in clause 5.5.\nFor location request from LCS client (neither in the UE nor in the NG-RAN) or AF external to a PLMN, privacy verification of the target UE shall be enabled to check whether it is allowed to acquire the UE location information based on UE LCS privacy profile and whether the LCS client or the AF is authorised to use the location service as defined in clause 5.4.\nNote: the AI/ML server plays the role of AF to acquire the UE location information.\nTS 23.501[48] clause4.15.3.1\nCurrently the Monitoring Events feature defined in TS 23.501 [48] is intended for monitoring of specific events in the 3GPP system and reporting such Monitoring Events via the NEF. Location Reporting is one of the supported monitoring events is specified in Table 4.15.3.1-1 of TS 23.502 [49]. However, the current monitoring event features indicates either the Current Location or the Last Known Location of a UE to an AF via NEF, this is not applicable when the AI/ML model server is not allowed to access to the UE’s location information according to user privacy policies.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.7.6\tPotential Requirements",
                            "text_content": "[P.R.6.7-001] Subject to user consent, operator policy and regulatory constraints, the 5G system shall support the provision of monitoring information or analytics information to a trusted 3rd party AI/ML server for allowing this 3rd party AI/ML server to make a prediction for a suitable AI/ML model to be downloaded to the concerned UE.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "7\tDistributed/Federated Learning over 5G system",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "7.1\tUncompressed Federated Learning for image recognition",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "7.1.1\tDescription",
                            "text_content": "Nowadays, the smartphone camera has become the most popular tool to shoot image and video, which holds many valuable vision data for image recognition model training. For many image recognition tasks, the images/videos collected by mobile devices are essential for training a global model. Federated Learning (FL) is an increasingly widely-used approach for training computer vision and image recognition models.\nIn Federated Learning mode, the cloud server trains a global model by aggregating local models partially-trained by each end devices based on the iterative model averaging [30]. As depicted in Figure 7.1-2, within each training iteration, a device performs the training based on the model downloaded from the AI server using the local training data. Then the device reports the interim training results (e.g., gradients for the DNN) to the cloud server via 5G UL channels. The server aggregates the gradients from the devices, and updates the global model. Next, the updated global model is distributed to the devices via 5G DL channels, the devices can perform the training for the next iteration.\nFigure 7.1.1-1 illustrates the process of federated learning over a 5G system, showcasing the collaboration between edge devices and a central server. The figure depicts the data flow, where local models are trained on edge devices and then aggregated by the server to create a global model. This process enhances the efficiency of machine learning algorithms by reducing the need for large-scale data transfers and improving privacy by keeping data local. The figure highlights the role of 5G technologies, such as low-latency communication and high-speed data transfer, in enabling real-time learning and decision-making in various applications.\nFigure 7.1.1-1. Federated Learning over 5G system\nAn iterative Federated Learning procedure is illustrated in Figure 7.1.1-2. In the Nth training iteration, the device performs the training based on the model downloaded from the FL training server using the images/videos collected locally. Then the device reports the Nth-iteration interim training results (e.g., gradients for the DNN) to the server via 5G UL channels. Meanwhile, the global model and training configuration for the (N+1)th iteration are sent to the device. When the server aggregates the gradients from the devices for the Nth iteration, the device performs the training for the (N+1)th iteration. The federated aggregation outputs are used to update the global model, which will be distributed to devices, together with the updated training configuration.\nFigure 7.1.1-2 illustrates the timeline of federated learning for image recognition, showcasing the process of data aggregation and model updates across multiple decentralized devices. The figure depicts the initial data collection phase, where local devices upload their data to a central server. Following this, the server performs a round of model updates, and the updated model is then sent back to the devices for the next round of data collection and aggregation. This process is repeated multiple times, with each iteration improving the overall model's accuracy. The figure emphasizes the importance of efficient communication and synchronization among devices to ensure the success of federated learning in image recognition tasks.\nFigure 7.1.1-2. Federated Learning timeline for image recognition\nIn order to fully utilizing the training resource at device and minimizing the training latency, the training pipeline shown in Figure 7.1.1-2 requires the training results report for the (N-1)th iteration and the global model/training configuration distribution for the (N+1)th iteration are finished during the device’s training process for the Nth iteration. The analysis in Section 7.1.6 will be developed based on the processing timeline. In practice, more relaxing FL timeline can also be considered with sacrificing the training convergence speed.\nThe training time should be minimized since mobile devices may only stay in an environment for a short period of time. Further, considering the limited storage at device, it may not realistic to require the training device to store a large amount of training data in the memory for a training after it moves outside the environment.\nDifferent from the decentralized training operated in cloud datacenters, Federated Learning over wireless communications systems need to be modified to adapt to the variable wireless channel conditions, unstable training resource on mobile devices and the device heterogeneity [10, 32, 34]. The Federated Learning protocol for wireless communications can be depicted in Figure 7.1.1-3 [10, 31-32].\nFor each iteration, the training devices can firstly be selected. The candidate training devices report their computation resource available for the training task to the FL server. The FL server makes the training device selection based on the reports from the devices and other conditions, e.g. the devices’ wireless channel conditions, geographic location, etc..\nHereby, besides performing federated learning task, the training devices in a communication system have their other data to transmit at uplink (e.g. for ongoing service transactions), that may be high priority and not latency-tolerant and its transmission may affect a device's ability to upload the locally trained model. Device selection must therefore account for a trade-off to upload the training results as compared to uploading other uplink data. Furthermore, skipping a device from federated learning model aggregation for one or more iterations affects the convergence of the federated learning model. Therefore, candidate training device selection over wireless links is more complex as compared to federated learning in data centers.\nAfter the training devices are selected, the FL server will send the training configurations to the selected training devices, together with global model for training. A training device starts training based on the received global model and training configuration. When finishing the local training, a device reports its interim training results (e.g., gradients for the DNN) to the FL server. In Figure 7.1.1-3, the training device selection is performed, and the training configurations are sent to the training devices at the beginning of each iteration. If the conditions (e.g. device’s computation resource, wireless channel condition, other service transactions of the training devices) are not changed, the training device re-selection and training re-configuration might not be needed for each iteration, i.e. the same group of training devices can participate the training with the same configuration for multiple iterations. Still, the selection of training devices should be alternated over time in order to achieve an independent and identically distributed sampling from all devices, i.e., give a fair chance to all devices to contribute to the aggregated model.\nFigure 7.1.1-3 illustrates a typical federated learning protocol over wireless communication systems, showcasing the collaboration between multiple edge devices and a central server. The figure depicts the data exchange process, where local models are trained on each device and periodically uploaded to the server for global aggregation. The server then sends back the updated global model to the devices for retraining. This process ensures that the learning model benefits from the collective knowledge of the devices while maintaining privacy and reducing communication overhead. Key components include the edge devices, the server, and the communication links, emphasizing the distributed nature of federated learning.\nFigure 7.1.1-3. Typical Federated Learning protocol over wireless communication systems\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.1.2\tPre-conditions",
                            "text_content": "The UE runs an application providing the capability of Federated Learning (FL) for the image recognition task.\nThe FL application on the UE is capable to report its interim training results to the FL server.\nThe FL server is capable to aggregate the interim training results from the federated UE, form the global model, and distribute the global model for training in the next iteration.\nThe 5G system has the ability to provide 5G network related information to the FL server.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.1.3\tService Flows",
                            "text_content": "The FL server selects a set of federated UEs based on different own criteria. One possibility is to use available information on UE geographic location (subject to user consent and/or operator/regulatory requirements).\nThe FL server distributes the global model to be federated UEs via 5G network.\nThe FL application in a federated UE performs the training based on the local training data set collected by the UE, and then reports the interim training results (e.g., gradients for the DNN) to the FL server via 5G network.\nThe FL server aggregates the gradients from the UEs, and updates the global model.\nRedo Step 1) to 4) for the training for the next iteration.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.1.4\tPost-conditions",
                            "text_content": "The AI/ML model for image recognition is trained and converges, and the training accuracy and latency need to be guaranteed.\nThe FL training task for image recognition can be completed under the available computation and energy resource of the federated UEs. And the consumed the computation, communication and energy resources over the federated UEs and the FL server are optimized.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.1.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "This use case mainly requires high data rate together with low latency. The high data rate requirements to 5G system are listed in Clause 7.1 and 7.6 of TS22.261 [4]. As in Table 7.1-1 of [4], 300Mbps DL experienced data rate and 50Mbps UL experienced data rate are required in dense urban scenario, and 1Gbps DL experienced data rate and 500Mbps UL experienced data rate are required in indoor hotspot scenario. As in Table 7.6.1-1 of [4], cloud/edge/split rendering-related data transmission requires up to 0.1Gbps data rate with [5-10]ms latency countrywide.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.1.6\tPotential New Requirements needed to support the use case",
                            "text_content": "As introduced in clause 7.1.1, in order to minimizing the training latency for Federated Learning for image recognition, the computation resource at device for the training task should be fully utilized, i.e. the training pipeline in figure 7.1.1-1 is desired to be maintained.\nIf considering to train a 7-bit CNN model VGG16_BN using 2242243 images as training data, table 7.1.6-1 shows the sum of gradient uploading latency, the federated aggregation latency and the global model downloading latency should be no larger than the GPU computation time at device for one iteration. For different batch sizes, the gradient uploading and the global model downloading for each iteration needs to be finished in [52~162ms], respectively.\nDifferent from the “single-UE latency” considered by previous requirement study [15], what is more essential for synchronous Federated Learning is the latency within which all federated devices can finish the gradient uploading. In other words, all training devices need to finish the gradient uploading within the latency in table 7.1.6-1, even if multiple training devices are present in a cell.\nThe size of the 8-bit VGG16_BN model is 132MByte for either the trained gradients or the global model. Hence in order to finish the gradient uploading and the global model downloading within the duration, the required UL and DL data rate are shown in table 7.1.6.1-1, which are [6.5Gbit/s to 20.3Gbit/s] respectively. And it should be noted that 132MByte is the size without compression. The size may be reduced if advanced model/gradient compression techniques can be adopted.\nIn the legacy requirements to 5G system, e.g. [4], the full coverage is always desired for all UEs. However, the AI/ML model training task may to some extent relax the requirements on continuous network coverage. When a FL server selects the training devices for a Federated Learning task, it can try to pick the UEs in a satisfactory coverage, if they can collect the training data needed. This implies that even in a non-continuous coverage of 5G mmWave, the Federated Learning task can be well carried out. This provides to 5G operators a service better exploring the use of their FR2 spectrum resource.\nThe potential KPI requirements needed to support the use case include:\n[P.R.7.1-001] The 5G system shall support a user experienced DL data rate as given in table 7.1.6.1-1, to enable uncompressed Federated Learning for image recognition.\n[P.R.7.1-002] The 5G system shall support a user experience UL data rate as given in from table 7.1.6.1-1, to enable uncompressed Federated Learning for image recognition.\nTable 7.1.6.1-1: Latency and user experienced UL/DL data rates for uncompressed Federated Learning\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 7.1.6.1-1: Latency and user experienced UL/DL data rates for uncompressed Federated Learning",
                                    "table number": 22,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "7.2\tCompressed Federated Learning for image/video processing",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "7.2.1\tDescription",
                            "text_content": "Federated learning can be used to train AI/ML models based on number of images and videos generated by cameras in mobiles devices by iteratively exchanging gradient of updating models instead of direct user images and videos. Because this method can utilize images and videos from many users, the performance of a trained AI/ML model can be significantly higher than a stand-alone case. However, the basic federated learning methods can have disadvantages by massive uplink traffics and high computational cost at a mobile device. Therefore, it is beneficial to consider a compressed federated learning (CFL) method, which allows compressed (not full) models to be transferred during a learning period.\nFigure 7.2.1-1 shows the essential procedure of CFL. CFL iteratively performs a set of the three operation stages. In order to describe the iterations in CFL, we introduce a cycle of I: each cycle begins with the 1st iteration and ends with I-th iteration, which is immediately followed by the 1st iteration of the next cycle (e.g., (I+1)st iteration). For each iteration, the three operation stages include the training UE selection, the sparse weight distribution, and the training result reporting stages. The operations of these three stages in the first iteration and the operation in the last iteration are different from the operations in the other iterations.\nEach iteration in CFL starts with the training UE selection stage, at which the CFL server selects a set of available users from the candidate users to associate with the same purpose AI/ML model. To the selected users, ready to participate in the learning process because of being an available state, the CFL server transmits the train configuration information. At the next stage, the CFL server sends the sparse global model, which could be an initial version of the AI/ML model in the first iteration. Otherwise, the sparse global model is an aggregated version based on user reporting information.\nThen, each UE trains a received model after expanding the spatial model and reports an intermediate training result to the CFL server, where the training result is comprised only of significant value weight gradients for applying a model compression. By doing so, uplink throughout requirement can be significantly reduced in comparison with the basic federated learning method without compression. In the last iteration, I-th iteration, the CFL Server sends ‘train stop message’ to UEs so that the UEs can stop sending its update any longer, and the CFL Server performs fine-tuning by pruning unnecessary nodes. Throughout these multiple iterations of a cycle (i.e., from the 1st iteration through the I-th iteration) as in the figure, the AI/ML model will be progressively enhanced based on user data in mobile networks at reduced requirements of uplink and downlink throughput.\n\n\nFigure 7.2.1-1. Compressed Federated Learning timeline for image recognition illustrates the process of distributed machine learning where multiple devices collaborate to train a model without sharing raw data. The timeline shows the iterative process of local training, model aggregation, and global model update, emphasizing the trade-off between communication cost and model accuracy. Key components include the client devices, server, and the federated learning algorithm. The figure highlights the importance of efficient communication and synchronization in distributed machine learning applications.\nFigure 7.2.1-1. Compressed Federated Learning timeline for image recognition\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.2.2\tPre-conditions",
                            "text_content": "UE can have a computational hardware and algorithm capability to train an AI/ML model such as for an image and video cognition.\nUE can send intermediate training results to a CFL server.\nA CFL server can select training devices and determine training configuration.\nA CFL server can aggregate intermediate training results and generate a sparse global model for the next learning iteration.\nA CFL server can distribute a global AI/ML mode to a set of selected users.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.2.3\tService Flows",
                            "text_content": "Step 1: The CFL server selects the training users from candidate users.\nStep 2: The CFL server sends the configuration information to the selected users.\nStep 3: The CFL server distributes the initial (or, aggregated) sparse global model to the selected users through a 5G networks.\nStep 4: Each UE expands the sparse global model and train the expanded model using its local data. Then, each UE sends only significant value weight gradients to the CFL server.\nStep 5: The CFL server aggregates the training results received from the training UEs and update a global model using the aggregated results.\nStep 6: Until the AI/ML model reaches saturated performance enhancement, the process runs repeatedly from step 1.\nStep 7: Otherwise, the CFL server performs fine-tuning for a global model compression for a global model. This process can be applied regularly so as to improve bandwidth and computation resource efficiency before the training finalization.\nFinally, the CFL server distributes the new sparse global model to all users which needs the same AI/ML model.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.2.4\tPost-conditions",
                            "text_content": "For a UE prospective, CFL can reduce uplink and downlink throughput requirements for the federated learning process. Also, the computational complexity in UEs can be significantly reduced because of enabling a compressed model.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.2.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "Latency analysis for gradient uploading and the global model downloading for image recognition\nAI/ML model training data for CFL is a new type of traffic. Consider CFL to train an 8-bit CNN VGG16 model with 224x224x3 images. Table 7.2.5-1 shows that the single GPU computation time should be larger than the addition of gradient uploading latency and global model downloading latency.\nTable 7.2.5-1: GPU computation time for different mini-batch sizes \nfor Compressed Federated Learning\n\nData rate analysis for gradient uploading and the global model downloading for image recognition\nTable 7.2.5-2 shows the required data rate for gradient uploading and global model downloading for the above 8-bit VGG16 model when CFL is applied. We calculated the required data rate based on table 1 in [39], in which the pruning size of the 8-bit VGG16 model can be reduced 13 times from the original size of 138 Mbyte. It is noteworthy that 13 times model compression gives almost no accuracy degradation for the 8-bit VGG16 model. If we assume that the minibatch size is 4, the uplink required rate is compressed trained parameter size * 8 / (GPU computation time / 2) = (138 / 13) Mbyte * 8 bits / ((105ms*20/1000) / 2) = 80.88 Mbit/s which is same to the downlink require rate.\nTable 7.2.5-2: User experienced data rate for gradient uploading and global model downloading\nUser experienced data rate = compressed trained parameter size * 8 / (GPU computation time / 2)\n\nIn the case of 8-bit CNN VGG16, CFL compared to FL can transmit up to 13 times more model information through model compression. Hence, when using the same uplink payload as FL, up to 13 times more users can be simultaneously supported. For example, as shown in Figure 7.2.5-1 which is redrawing using the associated data in [50], the accuracy performance of the FL with 15 participants, which is five times than the FL with 3 participants, can be significantly increased by 6.5%. If we consider the maximum compression efficiency of CFL with supporting 13 times more users, a further performance gain is expected.\nFigure 7.2.5-1 illustrates the performance of federated learning in terms of accuracy as the total number of users increases. The graph shows that the accuracy generally improves as the number of users grows, with some fluctuations. The shaded area around the line represents the variance in the results, indicating that the performance is consistent across different runs. The x-axis represents the total number of users, ranging from 10 to 100, while the y-axis represents the accuracy, ranging from 0.5 to 1.0. The data points are connected by a line, and the trend line suggests a positive correlation between the number of users and the accuracy of federated learning.\nFigure 7.2.5-1: Federated Learning accuracy as the total number of users \nRedrawing using the associated data in [60]\nFor the flexibility and accuracy of FL, the compression rate of CFL can be adjusted according to the user's channel conditions. For example, even in the case of a user with a bad channel, the problem of not participating in FL can be solved by increasing the compression ratio of the model.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 7.2.5-1: GPU computation time for different mini-batch sizes \nfor Compressed Federated Learning",
                                    "table number": 23,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 7.2.5-2: User experienced data rate for gradient uploading and global model downloading\nUser experienced data rate = compressed trained parameter size * 8 / (GPU computation time / 2)",
                                    "table number": 24,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "7.2.6\tPotential New Requirements needed to support the use case",
                            "text_content": "[P.R.7.2.6-001] The 5G system shall support to upload a trained gradient for each iteration of Compressed Federated Learning with a maximum latency of 1.05~3.25s.\n[P.R.7.2.6-002] The 5G system shall support downloading the global model for each iteration of Compressed Federated Learning with a maximum latency of 1.05~3.25s.\n[P.R.7.2.6-003] The 5G system shall support UL unicast transmission with 26.13~80.88Mbit/s user experienced UL data rates and a communication service availability not lower than [99.9%] for reporting the trained gradients for Compressed Federated Learning.\n[P.R.7.2.6-004] The 5G system shall support DL multicast transmission with 26.13~80.88Mbit/s user experienced DL data rates and a communication service availability not lower than [99.9%] for distributing the global model for Compressed Federated Learning.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "7.3\tData Transfer Disturbance in Multi-agent multi-device ML Operations",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "7.3.1\tDescription",
                            "text_content": "A brief story of machine learning is nothing but a computer (that has no or limited imprinted programs for a certain task) exploiting its own capability (“performance”) towards a certain task using data (“experience”). There are several criteria to classify the types of Machine Learning depending on the characteristics of the method used. This use case is intended to describe a case of multi-agent multi-device ML operations with heavy data (i.e., the data size is huge) when there is partial or total disturbance for data collection/transfer (e.g., privacy regulation or temporary technical limitation like shortage of network resources or temporary failure). As depicted in Fig. 7.3.1-1, this use case, Part I, is specifically related to a scenario that there are multiple agents and multiple collecting devices where the devices can perform ML operations, not necessarily in full but as much as they can (i.e., functional splitting is possible between a device and one or more learning agents).\nNOTE 1: \tMBL (Multiple Batch Learning) is one of examples that belong to this scenario, which is known to be better performing when the data is too big for a single agent/server to handle.\nNOTE 2: \tA learning agent is a type of AI/ML training servers that can be inside 5GS (e.g., AI/ML-based support for communication) or outside 5GS (e.g., communication support for AI/ML). Multi-agent training has multiple collaborating agents involved as depicted in Fig. 7.3.1-1 where dotted lines denote collaboration relation between two or more learning agents. The respective learning agents may have different operational situations (e.g., different technical issues, such as congestion or breakdown, and different jurisdiction-related restrictions). In this use case, it is assumed that the learning agent is typically in the cloud, interacting with UE (as a collecting agent). Learning agents A1, A2, ..., An are not an UE.\nFigure 7.3.1 illustrates the functional relationship between multiple devices (M1, M2, ... , Mk) and multiple agents (A1, A2, ... , An). In this scenario, data sharing between any pair of agents is not restricted, and the data exchanged is typically deep-processed data rather than pre-processed or raw data. The figure showcases the interconnectedness of these entities, emphasizing the seamless exchange of information and the potential for collaboration between different devices and agents.\nFigure 7.3.1-1: Functional relation between multiple devices (M1, M2, …, Mk in the form of UE) and multiple agents (A1, A2, …, An). Data sharing between any pair of agents, if exists, is not disturbed/restricted. In “sharing scenario”, the data would generally be deep-processed data as opposed to simply pre-processed or raw data.\nIn the age that privacy was not affecting the flow of data from the source to the learning agent (e.g., a computer), the expected performance is the outcome of all possible computational considerations of the data collected (e.g., refer to the green solid curve of Fig. 7.3.1-2). However, if there is a certain level of disturbance in data collection, the achievable performance toward the given task would not be as good as the one with no such disturbances (refer to the gap between the green solid curve and blue dotted curve in the same figure). Some examples of such disturbances include:\nprivacy regulations, such as EU’s General Data Protection Regulation (GDPR) or California Consumer Privacy Act (CCPA);\nlimited capability of transport-layers, such as lack of network resources (e.g., radio resources due to temporal degradation, higher noise/interference level, highly crowed situations, partial/total break-down, and so on) preventing input data from being delivered in time:\ncase 1: a portion of input data delivered in time, if any, is still useful (i.e., entropy can get increased/ improved)\ncase 2: a portion of input data delivered in time, if any, is not useful (i.e., it’s not enough to get entropy increased/ improved)\nNOTE 3: \t“input data” can be raw data, trained data, or an intermediate combination of them that is to be transferred from a UE (or a group of UEs, respectively) to one of learning agents.\nIt is commonly understood that the more data a learning model utilizes the better performance the learning model can achieve (assuming the data are reasonably independent from statistics perspectives or sufficiently correctly labelled (when supervised learning is concerned)), if not too large. However, if there is some disturbance in data collection/transfer, such as regional regulations or technical limitations (as described above), the expected performance would not be as good as the case without the disturbance (the vertical difference in figure 7.3.1-2); in addition, it is expected that the learning model would need to take more time to accumulate “experience” with the reduced feeding-rate of learning data caused by such disturbance (the horizontal difference in figure 7.3.1-2).\nFigure 7.3.1-2 illustrates the performance gap versus the experience of learning for a specific task, depicting two scenarios: one with disturbance in input data collection/transfer (represented by green, solid lines) and another without disturbance (represented by blue, dotted lines). The graph shows the relationship between the performance gap and the experience of learning, highlighting the impact of data collection/transfer disturbances on the learning process. The visual representation emphasizes the importance of data integrity and consistency in enhancing the efficiency and effectiveness of machine learning models.\nFigure. 7.3.1-2: Performance gap vs. experience of learning for a given task: (1) with disturbance of input data collection/transfer (green, solid) (2) without disturbance of input data collection/transfer (blue, dotted).\nFig. 7.3.1-3 shows an example of possible preparation action that can be taken in UE side if some predictive information can be made available when disturbance is about to happen. In the figure, the preferred deadline for input data transfer is 1 sec (t = t0+1) with the amount of useful “input data” 3 bits in which two different kinds of scheduling are given: (a) is imperfect scheduling whereas (b) is good scheduling as an example, respectively. In reality, the transfer payload type is not limited to “input data” for learning agent(s) and it can also be applied to “learning model” transfer. The transfer direction can be “uplink” (e.g., for input data transfer) or “downlink” (e.g., for model distribution/transfer). This simplified example is intended to explain the justification why new technical requirements would be needed especially when some disturbance exists (e.g., by regulatory or technical causes).\n\n\nFigure 7.3.1-3 illustrates the impact of input data transfer disturbances on a network within a 1-second deadline. The figure presents two scenarios: (a) a 3-bit input data amount represented by a trapezoid in grey, which takes 2 seconds with imperfect scheduling, and (b) the same 3-bit input data amount represented by a rectangle in grey, which is processed in 1 second with good scheduling, allowing network resources to be allocated to other users (UEs) during the specified time frame. The figure highlights the importance of efficient scheduling in ensuring timely data transfer within a network.\nFigure 7.3.1-3: Example of disturbance of input data transfer within a preferred deadline of 1 sec (t = t0+1) with the amount of useful “input data” 3 bits (as an example): (a) For the 3-bit input data amount (“trapezoid” in grey), it takes 2 sec with “imperfect scheduling” (b) For the 3-bit input data amount (“rectangle” in grey), it takes 1 sec with “good scheduling” where the network resources can be assigned to others (other UEs) during (t0+1, t0+2).\nGiven the regulatory disturbance, it is intended to minimize (or to prepare to minimize) the impact on transfer caused by technology (e.g., scheduling and/or information necessary for 3GPP entity to perform “good scheduling”). The expected requirements and service flow description are as follows.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.3.2\tPre-conditions",
                            "text_content": "There are three UEs M1, M2 and M3 (computers or learning machines in the form of a UE).\nThere are two learning agents/servers in the cloud.\nUEs M1, M2 and M3 collect data and they process the data for learning they have collected or are collecting, if available, but they don’t have to complete the processing due to limited computational capability.\nEach UE (M1, M2 and M3, respectively) has a functional splitting point negotiated with their agent(s) regarding data processing for learning.\nAgent A1 working with agent A2 for a task can share its outcome with agent A2 so that the outcome from agent A2 can jointly improve the outcome, if possible, which is possibly better than A1’s individual outcome and A2’s individual outcome.\nUE M1 and UE M2 are located in Area 1 of some jurisdiction that doesn’t restrict collecting certain type of data.\nInitial connections:\nUE M1 is connected to agent A1 (via eNB, ng-eNB, or gNB) when data connection is necessary (e.g., when needed to upload some data or when needed to download some model).\nAgent A1 provides UE M1 with an alternative agent (i.e., Agent A2) for the use of disturbance, which is (one of) participating agent(s) that agent A1 shares data. [See Description clause for typical types of data]\nUE M1 is transferring learning data to agent 1.\nUE M2 is connected to agent A1.\nAgent A1 provides UE M2 with an alternative agent (i.e., Agent A2) for the use of disturbance, which is (one of) participating agent(s) that agent A1 shares data.\nUE M2 is transferring learning data to agent 1.\nUE M3 is connected to agent A2.\nAgent A2 provides UE M3 with an alternative agent (i.e., Agent A1) for the use of disturbance, which is (one of) participating agent(s) that agent A2 shares data.\nUE M3 is transferring learning data to agent 2.\n\nFigure 7.3.2-1 illustrates the initial connections between mobile devices and agents, showcasing the establishment of communication links through various network nodes. The figure depicts the interaction between mobile devices, represented by small circles, and the network agents, denoted by larger circles. These agents facilitate the communication process, ensuring data transmission between devices. The figure emphasizes the importance of efficient network architecture in supporting seamless connectivity and data exchange between mobile devices and agents.\nFigure 7.3.2-1: Initial connections b/w mobile devices and agents\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.3.3\tService Flows",
                            "text_content": "While transferring learning data to agent A1, UEs M1 and M2 move into Area 2 of some jurisdiction that has restrictive regulations for agent A1 to collect data from UEs within a specific area (or outside a specific area).\nAs a result, UEs M1 and M2 are restricted to transfer their data to agent 1.\nWhile transferring learning data to agent A2, UE M3 moves into a different area where 5G system provides prior notification on possible traffic congestion which might disturb UE M3 from keeping transferring learning data to agent A2.\nUE M3 makes a selection of an action policy: (action 1) to defer the transfer or (action 2) to request to speed up the transfer.\nIf action 1 is selected, UE M3 will resume transferring when it becomes available (not the main focus of this use case);\nIf action 2 is selected, UE M3 will be able to (2a) more urgent/useful segment of data (send priority one over the other) and/or (2b) request more network resources to use.\nModified connections (UEs M1 and M2):\nUE M1 attempts to get connected to agent A2 based on the information that agent A1 has provided when initially connected.\nUE M1 keeps transferring the data to agent A2\nAgent A1 can share the collected data or its processed form of data, vice versa\nUE M2 is connected to agent A2\nUE M1 keeps transferring the data to agent A2\nAgent A1 can share the collected data or its processed form of data, vice versa\nUE M3 is allowed to use more network resources to speed up the transfer to agent A2 (case 2b).\nFigure 7.3-1 illustrates the modified connections between mobile devices and agents, showcasing the intricate network infrastructure that facilitates seamless communication. The figure depicts various nodes, including mobile devices, access points, and central servers, all interconnected through a complex web of lines representing data transmission paths. The visual representation emphasizes the efficiency and robustness of the network, with multiple paths ensuring high availability and low latency. The figure also highlights the role of edge computing, where some processing is done at the network's edge, reducing the need for data to be transmitted over long distances. This approach enhances the overall performance and responsiveness of the telecommunication system.\nFigure 7.3.3-1: Modified connections b/w mobile devices and agents\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.3.4\tPost-conditions",
                            "text_content": "NOTE: \tPost-conditions are described in two different aspects (communication aspect and ML operation aspect).\nTable 7.3.4-1: Post-conditions in communication aspect and ML operation aspect.\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 7.3.4-1: Post-conditions in communication aspect and ML operation aspect.",
                                    "table number": 25,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "7.3.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "Editor’s note: text to be provided.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.3.6\tPotential New Requirements needed to support the use case",
                            "text_content": "In Table 7.3.6-1, the service-level implications of AI/ML operation are summarized in the third column of Table 7.3.6-1, which are used to derive high-level potential service requirements in communication layer in order to support the AI/ML operation.\nTable 7.3.6-1: High level service requirements (in Communication and AI/ML operation aspects)\nTable 7.3.6-2 provides a summary of performance requirements for different usage scenarios. The required KPIs are dependent upon usage scenarios, especially on the task splitting points of given tasks even under the same usage scenarios. The calculation procedure can be referred to in [36-39].\nFor an example of image size 32 x 32 x 3 (32 wide, 32 high, 3 depth/colour channels), the weight is 3072; for images with more respectable size 200 x 200 x 3 = 120,000 weights; For a simple ConvNet for CIFAR-10 classification, the regular Neural Network architecture is INPUTCONVRELUPOOLFC (Input layer, convolutional layer, pooling layer and fully-connected layer).\nEXAMPLE 1: 32 x 32 x 3 image and six 5 x 5 filters produce a new image of size 28 x 28 x 6! = 564,480\nEXAMPLE 2 (Language understanding): BERT_{base} with L = 12 (layers), H = 768 (hidden size), A = 12 (heads). The number of parameters = 110M\nEXAMPLE 3 (Language understanding): BERT_{large} with L = 24 (layers), H = 1024 (hidden size), A = 16 (heads). The number of parameters = 340M\nEXAMPLE 4: [39] for 8-bit VGG16 Pruned, it can reduce the original size (VGG-16 Ref) of 138MB by a factor of (1/13), which size will be approximately 10.3MB. Thus, 10.3MB / (GPU time / 2) ≒ 196MB/sec = 1.56Gb/sec.\nNOTE 2: Compared to raw data, the latency requirement for trained data is considered more rigorous as it belongs to the category of data that is more readily usable by the related machine (e.g., by UE, or by agent).\nTable 7.3.6-2: Performance requirements (KPI vectors)\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 7.3.6-1: High level service requirements (in Communication and AI/ML operation aspects)",
                                    "table number": 26,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 7.3.6-2: Performance requirements (KPI vectors)",
                                    "table number": 27,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "7.4\tGroup Performance “Flocking” Use Case",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "7.4.1\tDescription",
                            "text_content": "A new ‘service enabler’ is introduced that allows a service provider to achieve effective performance for the entire group of devices. The term ‘Flock’ stands for a group that has performance requirements that consider the performance ‘as a team’ as opposed to the ‘total’ or results of the ‘best performers.’\nThe example of a Flock provided in this use case (an application of this service enabler) is for Synchronous Federated Learning. Please note that “Flocking” is a general service enabler. Synchronous Federated Learning is discussed here because (a) it is a concrete application of the “Flocking” service enabler, (b) the topic of this study is model and AI data set communication. To explain and motivate “Flocking” we need an example, and fortunately this application is directly within the scope of the FS_AMMT study.\nSynchronous federated learning involves a set of contributing terminals, as described in clause 7 of this TR. In a federation, a hierarchy exists that provides an effective delegation of work and information. This federation functions as if it were a single (non-federated) system to the extent that the distributed components can operate within the same expectations. For synchronous federated learning some number of the federation’s components lag, these become stragglers. Information and function availability of the whole federation suffers when the performance of individual components fall significantly behind the others as the entire group should complete the iteration.\nSynchronous federated learning works best by eliminating bias – allowing diverse users and devices to participate and bring to the learning task diversity of input data, as the users will have different attributes. It is important not to merely focus on the ‘best performing devices’ in the federation and drop the rest. It may increase the performance in terms of time to iterate the synchronous federated learning task to drop stragglers, but this will reduce the diversity of the data set and introduce bias.\nWhere group performance is defined by the weakest member (as in the slowest flying bird), we term this a “flock.” The 5GS normally considers performance objectives and QoS for individual communicating terminals. Here, the 5GS QoS objective relates to the entire set of terminals making up the federation, the “flock” of UEs.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.4.2\tPre-conditions",
                            "text_content": "A set of UEs that participated in federated learning exists. These UEs have registered with a PLMN and operate in a federation to perform federated learning tasks.\nThe federated learning service provider “Avian” organizes the work of these UEs so that repeated iterations of training will occur over time.\nIt is assumed that the UEs provide federated learning input using the same network resources (e.g. network slice) and that the policy for this network communication is distinct from the policy for other activities that the UE performs. In this way, the network can adjust the QoS policy for federated learning communication for individual UEs without any service impact except to the federated learning service.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.4.3\tService Flows",
                            "text_content": "As the performance and quality of the output of the entire set of UEs is bounded by the performance of the weakest members of the group, Avian provides the 5GS with a policy identifying the reporting interval for which different iterations should conclude. Avian also provides reports on the progress of different UEs as they proceed. The 5GS is then in a position to adjust the QoS policies of some UEs to allocate more resources for those UEs that lag, and less resources for those that are ahead of the flock. Therefore, the slowest UEs (e.g. at producing a report after an iteration of a federated learning task) achieve an improved performance. The fastest UEs (e.g. also at producing a report after a federated learning task) do not need as much network resources (higher QoS), so the 5GS can reduce the QoS guarantees for these, and thereby saves these resources. The overall result is more efficient for the Synchronous Federated Learning service and for the network operator. The resource allocated to UE is maintained for at least one iteration.\nThe 5GS can inform Avian of any additional UEs with good communication performance (e.g. due to radio resources) and/or existing UEs whose connection has degraded to a level which is no longer sufficient for FL tasks. This enables Avian to determine when to add new UEs into the flock or remove existing UEs from the flock.\nNOTE: \tWhile it is clear that the speed with which training occurs and reports are generated by UEs is only partially bounded by communication, it is assumed that the communication resources available to the UE is a significant contributor to the time it requires to complete a training iteration.\nWhen a new UE joins the federation, it will register with Avian. Avian can then notify the 5GS (by means of a standard interface) of this addition. This interface is depicted logically in Figure 7.4.3-1 below.\nFigure 7.4.3-1 illustrates the 5G Service Enabler interface, specifically focusing on an example of Synchronous Federated Learning. The figure showcases the interaction between the 5G network and various devices, highlighting the synchronization and communication processes. Key components include the 5G network, devices, and the interface that enables the exchange of data and resources. The diagram emphasizes the importance of real-time collaboration and data sharing for efficient and effective learning processes.\nFigure 7.4.3-1: 5G Service Enabler interface, Example for Synchronous Federated Learning\nSimilarly, when a UE leaves the federation, the 5GS is notified. This allows the 5GS to modify the policy to balance the QoS policy to achieve the most consistent performance across the involved UEs. During the adjustment of QoS policy, the total communication resource (e.g. total GBR of all members in the flock) can be given a maximum set of resources, (e.g. a GBR aggregate that should not exceed a maximum value).\nAs the data transmission for Federated Learning is not for regular data services such as video, voice call, webpage browsing, etc., the 5GS needs to have a charging exemption or rewarding to this kind of traffic.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.4.4\tPost-conditions",
                            "text_content": "The ‘flock’ of UEs performs consistently.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.4.5\tExisting features partly or fully covering the use case functionality",
                            "text_content": "The existing QoS features controlled by the network with reconfigurable policy provide necessary but not sufficient functionality to support the use case.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.4.6\tPotential New Requirements needed to support the use case",
                            "text_content": "[P.R.7.4-001]\tThe 5G system shall be able to support ‘aggregated performance’ for a group of UEs where the worst performing member defines the performance of the entire group. E.g. the 5G system could achieve performance for the entire group so as to avoid members achieving either significantly less or more performance than others in the group.\n[P.R.7.4-002]\tThe 5G system shall be able determine whether a required QoS for each member in a group can be maintained.\n[P.R.7.4-003] The 5G system shall be able to expose QoS information for a group of UEs to an authorized service provider.\nEditor’s Note: the requirement above is FFS.\n[P.R.7.4-004] The 5G core network shall support collection of charging information based on whether the traffic is for AI/ML services.\nEditor’s Note: it is to check whether it is already covered by existing requirement\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "8\tConsolidated potential requirements",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "9\tConclusion and recommendations",
            "description": "Regarding the Feasibility Study on traffic characteristics and performance requirements for AI/ML model transfer in 5GS, the TR analyses use cases of AMMT as follows:\nUse cases on AI/ML operation splitting between AI/ML endpoints:\nSplit AI/ML image recognition;\nEnhanced media recognition: Deep Learning Based Vision Applications;\nMedia quality enhancement: Video streaming upgrade;\nSplit control for robotics;\nSession-specific model transfer split computation operations.\nUse cases on AI/ML model/data distribution and sharing over 5G system:\nAI/ML model distribution for image recognition;\nReal time media editing with on-board AI inference;\nAI/ML model distribution for speech recognition;\nAI model management as a Service;\nAI/ML based Automotive Networked Systems;\nShared AI/ML model monitoring;\nPrediction of AI/ML model distribution.\nUse cases on Distributed/Federated Learning over 5G system:\nUncompressed Federated Learning for image recognition;\nCompressed Federated Learning for image/video processing;\nData Transfer Disturbance in Multi-agent multi-device ML Operations;\nGroup Performance “Flocking” Use Case.\nIt is recommended to proceed with normative work, and include the potential new requirements identified by this TR into a new TS. The consolidated potential requirements in Clause 8 are candidates for the normative requirements.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "A.1\tAI and ML",
                    "description": "",
                    "summary": "",
                    "text_content": "Artificial Intelligence (AI)/Machine Learning (ML) is being used in a range of application domains across industry sectors, realizing significant productivity gains. In particular, in mobile communications systems, mobile devices (e.g. smartphones, smart vehicles, UAVs, mobile robots) are increasingly replacing conventional algorithms (e.g. speech recognition, machine translation, image recognition, video processing, user behaviour prediction) with AI/ML models to enable applications like enhanced photography, intelligent personal assistants, VR/AR, video gaming, video analytics, personalized shopping recommendation, autonomous driving/navigation, smart home appliances, mobile robotics, mobile medicals, as well as mobile finance. As forecast by Gartner [44], more than 80% of enterprise IoT projects will include an AI component by 2022, up from only 10% today.\nArtificial Intelligence (AI) is the science and engineering to build intelligent machines capable of carrying out tasks as humans do, defined by John McCarthy in 1956. The categorization of AI approaches can be illustrated in figure A.1-1 [25].\nFigure A.1 illustrates the categorization of AI/ML approaches, as adapted from [25]. The figure presents a hierarchical structure, starting from foundational methods at the bottom to more advanced techniques at the top. The categories encompass various levels of automation, from rule-based systems to fully autonomous models. Each level is represented by a distinct color-coded bar, with the corresponding level description provided in the legend on the right. The figure emphasizes the progression and interplay of these methods, highlighting their role in driving the development and implementation of AI/ML solutions in different fields.\nFigure A.1-1. Categorization of AI/ML approaches (figure adopted from [25])\nWithin AI is a large subfield called machine learning (ML), which was defined in 1959 by Arthur Samuel as the field of study that gives computers the ability to learn without being explicitly programmed. Instead of the laborious and hit-or-miss approach of creating a distinct, custom program to solve each individual problem in a domain, a single ML algorithm simply needs to learn, via a process called training, to handle each new problem [25]. Many ML methodologies as exemplified by decision tree, K-means clustering, and Bayesian network have been developed to train the model to make classifications and predictions, based on the data obtained from the real world [19].\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "A.2\tDeep neural network",
                    "description": "",
                    "summary": "",
                    "text_content": "Within the ML field, there is an area that is often referred to as brain-inspired computation, which is a program aiming to emulate some aspects of how we understand the brain to operate. Since it is believed that the main computational elements a human brain are 86 billion neurons, the two subareas of brain-inspired computation are both inspired by the architecture of a neuron [25], as shown in figure A.2-1 (a).\nCompared to spiking computing approaches, e.g. [3], the more popular ML approaches are using “neural network” as the model. Neural networks (NN) take their inspiration from the notion that a neuron’s computation involves a weighted sum of the input values. But instead of simply outputting the weighted sum, a NN applies a nonlinear function to generate an output only if the inputs cross some threshold, as shown in figure A.2-1(a). Figure A.2-1(b) shows a diagrammatic picture of a computational neural network. The neurons in the input layer receive some values and propagate them to the neurons in the middle layer of the network, which is also called a “hidden layer”. The weighted sums from one or more hidden layers are ultimately propagated to the output layer, which presents the final outputs of the network [25].\nFigure (b) illustrates the network topology of a large-scale telecommunications provider, showcasing the interconnected nodes and their relationships. The figure reveals the hierarchical structure, with core switches at the center, facilitating high-speed data transmission. The presence of multiple access and aggregation points indicates a robust network infrastructure capable of handling a high volume of traffic. The use of virtualization technologies is evident, as shown by the overlay of network functions, which enhances scalability and flexibility. The figure emphasizes the provider's commitment to delivering a reliable and efficient service to its vast customer base.           Figure (b) presents a detailed network topology of a large-scale telecommunications provider, highlighting the interconnected nodes and their hierarchical relationships. The core switches are centrally located, facilitating high-speed data transmission. The presence of multiple access and aggregation points demonstrates a robust network infrastructure capable of handling a high volume of traffic. The overlay of network functions, such as virtualization technologies, enhances scalability and flexibility. This figure emphasizes the provider's dedication to delivering a reliable and efficient service to its extensive customer base.\n(b)\nFigure A.2-1. Architecture of neuron and neural network\nNeural networks having more than three layers, i.e., more than one hidden layer are called deep neural networks (DNN). In contrast to the conventional shallow-structured NN architectures, DNNs, also referred to as deep learning, made amazing breakthroughs since 2010s in many essential application areas because they can achieve human-level accuracy or even exceed human accuracy. Deep learning techniques use supervised and/or unsupervised strategies to automatically learn hierarchical representations in deep architectures for classification [26]. With a large number of hidden layers, the superior performance of DNNs comes from its ability to extract high-level features from raw sensory data after using statistical learning over a large amount of data to obtain an effective representation of an input space [25]. In recent years, thanks to the big data obtained from the real world, the rapidly increased computation capacity and continuously-evolved algorithms, DNNs have become the most popular ML models for many AI applications.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "A.3\tTraining and inference",
                    "description": "",
                    "summary": "",
                    "text_content": "Training is a process in which a AI/ML model learns to perform its given tasks, more specifically, by optimizing the value of the weights in the DNN. A DNN is trained by inputting a training set, which are often correctly-labelled training samples. Taking image classification for instance, the training set includes correctly-classified images. When training a network, the weights are usually updated using a hill-climbing optimization process called gradient descent. The gradient indicates how the weights should change in order to reduce the loss (the gap between the correct outputs and the outputs computed by the DNN based on its current weights). The training process is repeated iteratively to continuously reduce the overall loss [25]. Until the loss is below a predefined threshold, the DNN with high precision is obtained.\nThere are multiple ways to train the network for different targets. The introduced above is supervised learning which uses the labelled training samples to find the correct outputs for a task. Unsupervised learning uses the unlabelled training samples to find the structure or clusters in the data. Reinforcement learning can be used to output what action the agent should take next to maximize expected rewards. Transfer learning is to adjust the previously-trained weights (e.g. weights in a global model) using a new training set, which is used for a faster or more accurate training for a personalized model [25].\nAfter a DNN is trained, it can perform its task by computing the output of the network using the weights determined during the training process, which is referred to as inference. In the model inference process, the inputs from the real world are passed through the DNN. Then the prediction for the task is output, as shown in Figure A.3-1. For instance, the inputs can be pixels of an image, sampled amplitudes of an audio wave or the numerical representation of the state of some system or game. Correspondingly, the outputs of the network can be a probability that an image contains a particular object, the probability that an audio sequence contains a particular word or a bounding box in an image around an object or the proposed action that should be taken [25].\nFigure A.3-1 illustrates an example of AI/ML inference, showcasing the process of data input, model training, and prediction output. The figure depicts a machine learning model receiving input data, which is then processed and analyzed by various layers of the model. The output is a prediction, which is an essential component in various applications such as image recognition, natural language processing, and predictive analytics. The figure emphasizes the importance of AI/ML in transforming raw data into valuable insights and decisions.\nFigure A.3-1. Example of AI/ML inference\nThe performance of DNNs is gained at the cost of high computational complexity. Hence more efficient compute engines are often used, e.g. graphics processing units (GPU) and network processing units (NPU). Compared to the inference which only involves the feedforward process, the training often requires more computation and storage resources because it involves also the backpropagation process [10].\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "A.4\tWidely-used DNN models and algorithms",
                    "description": "",
                    "summary": "",
                    "text_content": "Many DNN models have been developed over the past two decades. Each of these models has a different “network architecture” in terms of number of layers, layer types, layer shapes (i.e., filter size, number of channels and filters), and connections between layers [25]. Figure A.4-1 presents three popular structures of DNNs: multilayer perceptrons (MLPs), convolution neural networks (CNNs), and recurrent neural networks (RNNs). Multilayer perceptrons (MLP) model is the most basic DNN, which is composed of a series of fully connected layers [41]. In a fully connected layer, all outputs are connected to all inputs, as shown in Figure A.4-1. Hence MLP requires a significant amount of storage and computation.\nFigure A.4-1 illustrates the Multi-Layer Perceptron (MLP) and Deep Neural Network (DNN) model used in a machine learning application. The figure showcases the architecture of the MLP, which includes input layers, hidden layers, and output layers, each with neurons that perform weighted sum and activation functions. The DNN model is depicted with multiple stacked layers, each processing the output of the previous layer, allowing for complex feature extraction and learning. The figure emphasizes the importance of these models in pattern recognition and prediction tasks, highlighting their ability to handle large datasets and complex relationships.\nFigure A.4-1. MLP DNN model\nAn approach to limiting the number of weights that contribute to an output is to calculate the output only using a function of a fixed-size window of inputs. An extremely popular window-based DNN model uses a convolution operation to structure the computation, hence is named as convolution neural network (CNN) [25]. A CNN is composed of multiple convolutional layers, as shown in figure A.4-2. Applying various convolutional filters, CNN models can capture the high-level representation of the input data, making it popular for image classification [7] and speech recognition [42] tasks. In recent years, the modern CNN models have dramatically improved the performance of image classification tasks (e.g., AlexNet [7], VGG network [8], GoogleNet [9], ResNet [18], MobileNet [19]), as shown in figure A.4-3 [25].\nFigure A.4-2 illustrates a Convolutional Neural Network (CNN) model, showcasing its architecture with various layers such as convolutional, pooling, and fully connected layers. The figure also includes dropout and batch normalization layers, which are used to prevent overfitting and improve the model's performance. The input data is passed through these layers, with the final layer producing the output. The model is designed to classify images in the given dataset, with the aim of achieving high accuracy and generalization.\nFigure A.4-2. CNN model\n\nFigure A.4-3 illustrates the significant enhancements in image classification achieved through the application of Convolutional Neural Network (CNN) models. The figure showcases the performance improvements made by various CNN models, as referenced from source [25]. The visual representation highlights the progression from traditional image processing techniques to advanced deep learning methodologies, emphasizing the substantial leap in accuracy and efficiency. The figure underscores the pivotal role of CNNs in modern image analysis, particularly in fields such as medical imaging, autonomous vehicles, and environmental monitoring.\nFigure A.4-3. Image classification improvements made by CNN models (Figure adopted from [25])\nRecurrent neural network (RNN) models are another type of DNNs, which use sequential data feeding. The input of RNN consists of the current input and the previous samples. Each neuron in an RNN owns an internal memory that keeps the information of the computation from the previous samples. As shown in figure A.4-4, the basic unit of RNN is called cell, and further, each cell consists of layers and a series of cells enables the sequential processing of RNN models. RNN models have been widely used in the natural language processing task on mobile devices, e.g., language modelling, machine translation, question answering, word embedding, and document classification.\nFigure A.4-4 illustrates a Recurrent Neural Network (RNN) model, specifically designed for analyzing sequential data. The figure showcases the basic structure of an RNN, including the input layer, hidden layers, and output layer. The recurrent connections within the hidden layers enable the model to process sequences of data, making it well-suited for tasks such as time series forecasting and natural language processing. The figure also highlights the importance of proper initialization and training techniques to ensure the model's performance and avoid issues like vanishing gradients.\nFigure A.4-4. RNN model\nDeep reinforcement learning (DRL) is not another DNN model. It is composed of DNNs and reinforcement learning [43]. As illustrated in figure A.4-5, the goal of DRL is to create an intelligent agent that can perform efficient policies to maximize the rewards of long-term tasks with controllable actions. The typical application of DRL is to solve various scheduling problems, such as decision problems in games, rate selection of video transmission, and so on.\nFigure A.4-5 illustrates the reinforcement learning process in a telecommunication network, focusing on the interaction between the agent and the environment. The figure showcases the agent's decision-making process, represented by the policy, and the resulting state transition, depicted by the state-action pair. The objective is to maximize the cumulative reward, which is shown as the total reward over time. Key components include the state space, action space, and the reward function, which guide the agent's learning and adaptation. The figure emphasizes the iterative nature of reinforcement learning, where the agent learns from its actions and updates its policy to improve future performance.\nFigure A.4-5. Reinforcement learning\nIn recent years, the AI/ML-based mobile applications are increasingly computation-intensive, memory-consuming and power-consuming. Meanwhile end devices usually have stringent energy consumption, compute and memory cost limitations for running a complete offline AI/ML inference onboard. Hence many AI/ML applications currently intent to offload the inference processing from mobile devices to internet datacenters (IDC). Nowadays, even photos shot by a smartphone are often processed in a cloud AI/ML server before shown to the user who shot them. However, the cloud-based AI/ML inference tasks need to take the following factors into account:\nComputation pressure at IDCs\nAs the estimates in [44], by 2021, nearly 850ZB data will be generated by end devices per year, whereas the global IDC traffic can only reach 20.6ZB. That means most of the data can only be left at network edge (i.e. devices and MEC) for AI/ML processing.\nRequired data rate and latency\nIncreasing number of AI/ML applications are requiring a high data rate meanwhile a low latency for communications between devices and the network, e.g. VR/AR, automatic driving, remote-controlled robotics. According to the estimates in [44] on device-initiated traffic, offloading all the data to cloud servers for AI/ML inference would consume excessive uplink bandwidth. This introduces challenging requirements on mobile communications system capacity, including for the 5G system.\nPrivacy protection requirement\nThe sensing/perception data supporting the inference in the cloud server often carry privacy of the end users. Different types of privacy protection problems need to be considered in case of either processing the data at the device or reporting it to the cloud/edge server. Compared to reporting it to the server, keeping the raw data at the device can reduce the pressure of privacy protection at the network side.\nHence in many cases, the split AI/ML inference over device and network are required, to enable the AI/ML applications with conflicting requirements which are computation-intensive, energy-intensive as well as privacy-sensitive and delay- sensitive. Many references [10-14] have shown that processing AI/ML inference with device-network synergy can alleviate the pressure of computation, memory footprint, storage, power and required data rate on devices, reduce end-to-end latency and energy consumption, and improve the end-to-end accuracy and efficiency when compared to the local execution approach on either side.\nThe scheme of split AI/ML inference can be depicted in Figure B.1-1. The AI/ML operation/model is split into multiple parts according to the current task and environment. The intention is to offload the computation-intensive, energy-intensive parts to network endpoints, whereas leave the privacy-sensitive and delay- sensitive parts at the end device. The device executes the operation/model up to a specific part/layer and sends the intermediate data to the network endpoint. The network endpoint executes the remaining parts/layers and feeds the inference results back to the device. It should be noted that, in the example in Figure B.1-1, the final inference result is output by network AI/ML endpoint 2. According to actual use case, the inference result can also be output by other endpoints, e.g. network AI/ML endpoint 1.\nFigure B.1-1 illustrates an example of split AI/ML inference, showcasing the process of dividing machine learning tasks between a central server and edge devices. The figure depicts the flow of data and processing steps, highlighting the role of the cloud server and edge nodes in enhancing efficiency and reducing latency. Key components include the input data, the central server for model training and inference, and the edge devices for real-time processing and decision-making. The figure emphasizes the importance of balancing computational resources and data locality to optimize performance in AI/ML applications.\nFigure B.1-1. Example of split AI/ML inference\nThe modes for split AI/ML operations between device and network are illustrated in Figure B.1-2. The modes are in general applicable for AI/ML training as well as inference. In this section, we focus on the inference processing. Mode a) and b) are traditional schemes operating the AI/ML inference wholly on one endpoint. Mode c) - g) attempt to split the AI/ML inference or even the model into multiple parts according to the current task and environment, to alleviate the pressure of computation, memory/storage, power and required data rate on both device and NW endpoints, as well as to obtain a better model inference performance on latency, accuracy and privacy protection.\nMode a): Cloud/edge-based inference\nIn this mode (as shown in Figure B.1-2 (a)), the AI/ML model inference is only carried out in a cloud or edge server. The device only reports the sensing/perception data to the server, and does not need to support AI/ML inference operations. The server returns the inference results to the device. The advantage of this mode is limiting the device complexity. One disadvantage is that the inference performance depends on communications data rate and latency between the device and the server. Real-time uploading some perception data (e.g. high-resolution video streaming) requires a stably-high data rate and some AI/ML services (e.g. remote-controlled robotics) requires a stably-low latency, which are challenging to be guaranteed in 5G system due to different network coverages. And due to the disclosure of the privacy-sensitive data to the network, corresponding privacy protection measurements are required.\nMode b): Device-based inference\nIn this mode (as shown in Figure B.1-2 (b)), the AI/ML model inference is performed locally at the mobile device. The advantage is that, during the inference process, the device does not need to communicate with the cloud/edge server. Another motivation of this mode is preserving the privacy at the data source, i.e. the device, although the privacy protection problem needs also be considered at the device side. The disadvantage is potentially imposing an excessive computation/memory/storage resource to the device. And also pointed out by [10], we cannot assume the device always keep all the potentially-needed AI/ML models onboard. In some cases, the mobile device may need to obtain the AI/ML model from the edge cloud/server, which requires a corresponding downloading data rate from the 5G system, as introduced in Section 7.\nMode c): Device-cloud/edge split inference\nIn this mode (as shown in Figure B.1-2 (c)), an AI/ML inference operation or model is firstly split into two parts between the device and the cloud/edge server according to the current system environmental factors such as communications data rate, device resource, and server workload. Then, the device will execute the AI/ML inference up to a specific part or the DNN model up to a specific layer, and send the intermediate data to the cloud/edge server. The server will execute the remaining part/layers and sends the inference results to the device. Compared to Mode a) and b), this mode is more flexible and more robust to the varying computation resource and communications condition. A key link for this mode is to properly select the optimum split point between device side and network side based on the conditions.\nMode d): Edge-cloud split inference\nThis mode (as shown in Figure B.1-2 (d)) can be regarded as an extension of Mode a). The difference is that the DNN model is executed through edge-cloud synergy, rather than executed only on either cloud or edge server. The latency-sensitive part of an AI/ML inference operation or layers of an AI/ML model can be performed at the edge server. The computation-intensive parts/layers that the edge server cannot perform can be offloaded to cloud server. The device only reports the sensing/perception data to the server, and does not need to support AI/ML inference operations. The intermediate data are sent from the edge server to the cloud server. A proper split point needs to be selected for an efficient cooperation between edge server and cloud server.\nMode e): Device-edge-cloud split inference\nThis mode (as shown in Figure B.1-2 (e)) is the combination of Mode c) and d). An AI/ML inference operation or an AI/ML model is split over the mobile device, the edge server and the cloud server. The computation-intensive parts/layers of an AI/ML operation/model can be distributed among the cloud and/or edge server. The latency-sensitive parts/layers can be performed on the device or the edge server. The privacy-sensitive data can be left at the device. The device sends the intermediate data outcome from its computation to the edge server. And the edge server sends the intermediate data outcome from its computation to the cloud server. Two split points need to be selected for an efficient cooperation between the device, the edge server and the cloud server.\nMode f): Device-device split inference\nThis mode (as shown in Figure B.1-2 (f)) provides a de-centralized split inference. An AI/ML inference operation or model can be split over different mobile devices. A group of mobile devices can perform different parts of an AI/ML operation or different DNN layers for an inference task, and exchange intermediate data between each other.  The computation load can be distributed over devices meanwhile each device preserves it private information locally.\nMode g): Device-device-cloud/edge split inference\nMode g) can be further combined with Mode c) or e). As shown in Figure B.1-2 (g), an AI/ML inference operation or model is firstly split into the device part and network part. Then the device part can be executed in a de-centralized manner, i.e. further split over different mobile devices. The intermediate data can be sent from one device to the cloud/edge server. Or multiple devices can send intermediate data to the cloud/edge server.\n\n\n(a)                              (b)                                   (c)                                           (d)\nFigure e illustrates the integration of various communication technologies within a network infrastructure, showcasing the coexistence of different protocols and their interconnections. The figure highlights the importance of seamless communication between different network layers, emphasizing the role of network management systems in maintaining network efficiency and reliability.\n\nFigure f provides a detailed view of a network's data flow, illustrating the path taken by data packets as they traverse the network. The figure emphasizes the role of routers and switches in directing traffic and ensuring data is delivered efficiently. The use of color-coded lines helps to distinguish between different data paths and their respective network components.\n\nFigure g focuses on the performance of a network under varying traffic conditions, demonstrating how network congestion can impact data delivery times. The figure highlights the importance of network optimization techniques in managing traffic and maintaining high levels of network performance. The inclusion of various network elements, such as switches, routers, and load balancers, underscores the complexity of modern network architectures and the need for sophisticated management tools.  Figure e illustrates the integration of various communication technologies within a network infrastructure, showcasing the coexistence of different protocols and their interconnections. The figure highlights the importance of seamless communication between different network layers, emphasizing the role of network management systems in maintaining network efficiency and reliability.\n\nFigure f provides a detailed view of a network's data flow, illustrating the path taken by data packets as they traverse the network. The figure emphasizes the role of routers and switches in directing traffic and ensuring data is delivered efficiently. The use of color-coded lines helps to distinguish between different data paths and their respective network components.\n\nFigure g focuses on the performance of a network under varying traffic conditions, demonstrating how network congestion can impact data delivery times. The figure highlights the importance of network optimization techniques in managing traffic and maintaining high levels of network performance. The inclusion of various network elements, such as switches, routers, and load balancers, underscores the complexity of modern network architectures and the need for sophisticated management tools.           Figure e presents the intricate integration of various communication technologies within a network infrastructure, showcasing the coexistence of different protocols and their interconnections. The figure emphasizes the importance of seamless communication between different network layers, highlighting the role of network management systems in maintaining network efficiency and reliability.\n\nFigure f offers a comprehensive view of a network's data flow, illustrating the path taken by data packets as they traverse the network. The figure underscores the role of routers and switches in directing traffic and ensuring data is delivered efficiently. The use of color-coded lines helps to distinguish between different data paths and their respective network components.\n\nFigure g focuses on the performance of a network under varying traffic conditions, demonstrating how network congestion can impact data delivery times. The figure highlights the importance of network optimization techniques in managing traffic and maintaining high levels of network performance. The inclusion of various network elements, such as switches, routers, and load balancers, underscores the complexity of modern network architectures and the need for sophisticated management tools.\n(e)                                                   (f)                                                     (g)\nFigure B.1-2. Split AI/ML inference modes over endpoints\nFor the inference tasks which requires low latency and desires the privacy-sensitive data to be preserved at the UE side, offline AI/ML inference is desired, rather than the cloud-based inference. However, an offline AI/ML model running on mobile devices must have a relatively low computation complexity and a small storage size. An approach to enabling offline DNN models on mobile devices is to compress the model to reduce its resource and computational requirements [27-28, 35, 45]. However, DNN compression will lead to loss of inference accuracy and adaptivity to various tasks and environments. A solution to this challenge is to adaptively select the model for inference from a set of trained models [10]. The model selection is motivated by the observation that the optimum model for inference depends on the input data and the precision requirement [20][45]. Multi-functional mobile terminals usually need to switch the AI/ML model in response to task and environment variations.\nThe condition of adaptive model selection is that the models to be selected have been available for the mobile device. However, given the fact that the DNN models are becoming increasingly diverse, and with the limited storage resource in a UE, it is unfeasible to pre-load all candidate AI/ML models on-board. Online model distribution (i.e. new model downloading) or online transfer learning (i.e. partial model updating) is needed. As illustrated in Figure C.1-1, an AI/ML model can be distributed from a NW endpoint to the devices when they need it to adapt to the changed AI/ML tasks and environments.\nFigure C.1-1 illustrates the process of an AI/ML model being downloaded over a 5G system. The figure depicts the model as a data packet traveling through the network, starting from the cloud server and ending at the edge device. The 5G network is shown as a series of base stations and antennas, which facilitate the high-speed, low-latency communication necessary for real-time data processing. The figure also highlights the role of the core network, which manages the data traffic and ensures seamless connectivity. The use of AI/ML in this context is crucial for optimizing network performance and providing personalized services to users.\nFigure C.1-1. AI/ML model downloading over 5G system\nThe model to be distributed can be determined in two ways: Requested by a device, or controlled by a network server. The condition of the first mechanism is that the device can make the model selection/re-selection decision based on the understanding to the oncoming AI/ML task, environment and the list of the models available at the network server. As shown in Figure C.1-2, a model selector on the device is trained to select the best DNN for different input data.\nThe model selector is trained to determine the optimum DNN model for a new, unseen input using a set of automatically tuned features of the DNN model input, and taking into consideration the precision constraint and the characteristics of the input.\nFigure C.1-2 illustrates the process of selecting and downloading an AI/ML model, showcasing various stages such as model selection, model training, and model deployment. The figure highlights the importance of choosing the right model for a specific task, with considerations for accuracy, computational resources, and real-world constraints. It also emphasizes the role of cloud-based platforms in facilitating the efficient transfer of large-scale models, ensuring seamless integration with existing systems.\nFigure C.1-2. AI/ML model selection and downloading\nThe data rate for downloading the needed models depends on the following factors:\nSize of the model\nThis depends on different AI/ML applications. Along with the increasing performance requirements to AI/ML operations, the sizes of the models also keep increasing, although model compression techniques are under improvements.\nRequired downloading latency\nThis depends on how fast the model needs to be ready at the device. It is impacted by the extent to which the oncoming application can be predicted. Considering the unpredictability of user behaviour and typical waiting time a user can tolerate, the downloading of the AI/ML model needs to be finished in seconds or even in milliseconds. Different from a streamed video which can be played when a small portion is buffered, a DNN model can only be used until the whole model is completely downloaded.\nIn should be noted that the network-based and split AI/ML inference often requires a high and constant uplink data rate for continuously offloading sensing/intermediate data to the cloud/edge server. On the contrary, AI/ML model distribution mainly requires a high downlink data rate in a burst. This makes model distribution more suitable to the downlink-dominant (e.g. employing a high DL-to-UL ratio) mobile communications systems or systems with an unstable coverage. Of course the condition is that the mobile device’s computation resource can afford the on-board execution of the AI/ML model. If the computation load is beyond the device’s capability, the network-based or split inference has to be adopted.\nWith continuously improving capability of cameras and sensors on mobile devices, valuable training data, which are essential for AI/ML model training, are increasingly generated on the devices. For many AI/ML tasks, the fragmented data collected by mobile devices are essential for training a global model. In the traditional approaches, the training data gathered by mobile devices are centralized to the cloud datacenter for a centralized training.\nHowever, an AI/ML model training often requires a large data set and significant computational resources for multiple weight-update iterations. Nowadays, most of the AI/ML model training tasks are performed in the power cloud datacenters since the resource consumption of the training phase significantly overweights the inference phase. In many cases, training a DNN model still takes several hours to multiple days. However, cloud-based training means that the enormous amount of training data should be shipped from devices to the cloud, incurring prohibitive communication overhead as well as the data privacy pressure at the network side [10]. Similar to the split AI/ML inference introduced in Annex B, AI/ML model training tasks can also work in a cloud-device coordination manner. Distributed Learning and Federated Learning are examples in this manner.\nIn Distributed Learning mode, as shown in Figure D.1-1, each computing node trains its own DNN model locally with local data, which preserves private information locally. To obtain the global DNN model by sharing local training improvement, nodes in the network will communicate with each other to exchange the local model updates. In this mode, the global DNN model can be trained without the intervention of the cloud datacenter [10].\nFigure D.1-1 illustrates the distributed learning process in a neural network, showcasing the interaction between neurons and their respective learning rates. The figure depicts a layered architecture with input neurons, hidden layers, and output neurons, emphasizing the parallel processing capabilities of the network. The learning rate adjustment mechanisms are highlighted, showing how the network fine-tunes its parameters to optimize performance.\nFigure D.1-1. Distributed Learning\nIn Federated Learning (FL) mode, the cloud server trains a global model by aggregating local models partially-trained by each end devices. The most agreeable Federated Learning algorithm so far is based on the iterative model averaging [30]. As depicted in Figure D.1-2, within each training iteration, a UE performs the training based on the model downloaded from the AI server using the local training data. Then the UE reports the interim training results (e.g., gradients for the DNN) to the cloud server via 5G UL channels. The server aggregates the gradients from the UEs, and updates the global model. Next, the updated global model is distributed to the UEs via 5G DL channels. Then the UEs can perform the training for the next iteration.\nFigure D.1-2 illustrates the process of federated learning over a 5G system, showcasing the collaboration between multiple edge devices and a central server. The figure depicts the data flow, where local models are trained on edge devices and then aggregated by the server to create a global model. This process ensures privacy preservation while enhancing the efficiency of machine learning applications in a 5G network. Key components include the edge devices, the 5G core network, and the server, which work together to enable distributed learning and improve the overall performance of the system.\nFigure D.1-2. Federated Learning over 5G system\nThe performance requirements for Distributed/Federated Learning are listed below. The requirements to 5G communication links (e.g. data rate, latency, reliability) can be derived from the following requirements.\nTraining loss:\nTraining loss is the gap between the correct outputs and the outputs computed by the DNN model which indicates how well the trained DNN model fits the training data. Aim of training task is to minimize the training loss. Training loss is mainly affected by the quality of the training data and the efficiency of the training methods, i.e. whether the meaning of training data can be fully and properly explored. For Federated Learning, only when the valuable local training data can be fully learned in the duration of the iteration and the local training updates can be correctly reported to the cloud server within the target duration, the training loss can be minimized.\nThis implies that the requirements to the devices joining in the training process on the achievable UL data rate, latency and reliability for reporting the trained updates, and the achievable UL data rate, latency and reliability for distributing the model for training in next iteration. And to minimize the training loss with device heterogeneity (in computation and communication performance), training device selection and training configuration are needed before the training is performed in an iteration [31, 48] (will be introduced later in this section). The QoS of the relevant controlling messages, e.g. for training request, training resource reporting, training device selection, training configuration, and resource allocation for the training updates reporting, also needs to be guaranteed.\nTraining latency:\nTraining latency is one of the most fundamental performance metrics of AI/ML model training task since it directly influences when the trained model is available for use. Nowadays, cloud-based training often takes several hours to multiple days. The latency of the Distributed/Federated Learning process would take even a longer time if the computation latency or the communication latency is not minimized.\nThe latency of the Distributed/Federated Learning process is determined by the convergence rate (e.g. number of iterations before the training process converges to a consensus) and the latency of each iteration which consists of computation latency and communication latency. The computation latency depends on the computation/memory resource available on training devices. The computation latency depends on the DL data rate available for model distribution and UL data rate available for trained model updating. The latency of the whole training process is determined by the larger one between the computation latency and the communication latency. Hence the latencies of the computation and communication links need to be cooperatively minimized. If the communication latency cannot match to the computation latency, the communication link will become the bottleneck and prolong the whole training process.\nFor synchronous Federated Learning, in each iteration, the training latency is determined by the last device that reports its training update because the federated aggregation can be finished when all needed training updates are correctly gathered. That means the device heterogeneity (in computation and communication performance) will also highly impact the overall training latency. Rather than requiring the UL transmission latency of a specific device, the overall latency required for all training devices to upload the training updates (device-group latency) needs to be defined. And the QoS of the controlling messages for minimizing the device-group latency, e.g. for training request, training resource reporting, training device selection, training configuration, and resource allocation for the training updates reporting, also needs to be guaranteed.\nEnergy efficiency:\nFor Distributed/Federated Learning, both the computation and communication processes consume considerable energy. The Federated Learning architecture and protocol should also consider the power constraints on the training devices and the energy efficiency on device as well as the network side.\nPrivacy:\nWhen training the DNN model by using the data originated at a massive of end devices, the raw data or intermediate data should be transferred out of the end devices. Compared to reporting it to the cloud/edge server, preserving privacy at the end devices can reduce the pressure of privacy protection at network side. For example, Federated Learning is an agreeable approach to avoid uploading the raw data from device to network, as a cloud-based training requires.\n\n\n",
                    "tables": [
                        {
                            "description": "",
                            "table number": 32,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        }
    ]
}