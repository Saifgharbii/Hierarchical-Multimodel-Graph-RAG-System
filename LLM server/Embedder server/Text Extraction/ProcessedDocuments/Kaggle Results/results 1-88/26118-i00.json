{
    "document_name": "26118-i00.docx",
    "content": [
        {
            "title": "Foreword",
            "description": "This Technical Specification has been produced by the 3rd Generation Partnership Project (3GPP).\nThe contents of the present document are subject to continuing work within the TSG and may change following formal TSG approval. Should the TSG modify the contents of the present document, it will be re-released by the TSG with an identifying change of release date and an increase in version number as follows:\nVersion x.y.z\nwhere:\nx\tthe first digit:\n1\tpresented to TSG for information;\n2\tpresented to TSG for approval;\n3\tor greater indicates TSG approved document under change control.\ny\tthe second digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, etc.\nz\tthe third digit is incremented when editorial only changes have been incorporated in the document.\nIn the present document, modal verbs have the following meanings:\nshall\t\tindicates a mandatory requirement to do something\nshall not\tindicates an interdiction (prohibition) to do something\nThe constructions \"shall\" and \"shall not\" are confined to the context of normative provisions, and do not appear in Technical Reports.\nThe constructions \"must\" and \"must not\" are not used as substitutes for \"shall\" and \"shall not\". Their use is avoided insofar as possible, and they are not used in a normative context except in a direct citation from an external, referenced, non-3GPP document, or so as to maintain continuity of style when extending or modifying the provisions of such a referenced document.\nshould\t\tindicates a recommendation to do something\nshould not\tindicates a recommendation not to do something\nmay\t\tindicates permission to do something\nneed not\tindicates permission not to do something\nThe construction \"may not\" is ambiguous and is not used in normative elements. The unambiguous constructions \"might not\" or \"shall not\" are used instead, depending upon the meaning intended.\ncan\t\tindicates that something is possible\ncannot\t\tindicates that something is impossible\nThe constructions \"can\" and \"cannot\" are not substitutes for \"may\" and \"need not\".\nwill\t\tindicates that something is certain or expected to happen as a result of action taken by an agency the behaviour of which is outside the scope of the present document\nwill not\t\tindicates that something is certain or expected not to happen as a result of action taken by an agency the behaviour of which is outside the scope of the present document\nmight\tindicates a likelihood that something will happen as a result of action taken by some agency the behaviour of which is outside the scope of the present document\nmight not\tindicates a likelihood that something will not happen as a result of action taken by some agency the behaviour of which is outside the scope of the present document\nIn addition:\nis\t(or any other verb in the indicative mood) indicates a statement of fact\nis not\t(or any other negative verb in the indicative mood) indicates a statement of fact\nThe constructions \"is\" and \"is not\" do not indicate requirements.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "Introduction",
            "description": "The present document provides technologies for interoperable Virtual Reality services with focus on streaming and consumption.\nVirtual Reality (VR) is the ability to be virtually present in a space created by the rendering of natural and/or synthetic image and sound correlated by the movements of the immersed user allowing interacting with that world.\nSuitable media formats for providing immersive experiences are specified to enable Virtual Reality Services in the context of 3GPP bearer and user services.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "1\tScope",
            "description": "The present document defines interoperable formats for Virtual Reality for streaming services. Specifically, the present document defines operation points, media profiles and presentation profiles for Virtual Reality. The present document builds on the findings and conclusions in TR 26.918 [2].\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "2\tReferences",
            "description": "The following documents contain provisions which, through reference in this text, constitute provisions of the present document.\n-\tReferences are either specific (identified by date of publication, edition number, version number, etc.) or non-specific.\n-\tFor a specific reference, subsequent revisions do not apply.\n-\tFor a non-specific reference, the latest version applies. In the case of a reference to a 3GPP document (including a GSM document), a non-specific reference implicitly refers to the latest version of that document in the same Release as the present document.\n[1]\t3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".\n[2]\t3GPP TR 26.918: \"Virtual Reality (VR) media services over 3GPP\".\n[3]\tRecommendation ITU-R BT.709-6 (06/2015): \"Parameter values for the HDTV standards for production and international programme exchange\".\n[4]\tRecommendation ITU-R BT.2020-2 (10/2015): \"Parameter values for ultra-high definition television systems for production and international programme exchange\".\n[5]\tRecommendation ITU-T H.264 (04/2017): \"Advanced video coding for generic audiovisual services\" | ISO/IEC 14496-10:2014: \"Information technology – Coding of audio-visual objects – Part 10: Advanced Video Coding\".\n[6]\tRecommendation ITU-T H.265 (02/2018): \"High efficiency video coding\" | ISO/IEC 23008-2:2018: \"High Efficiency Coding and Media Delivery in Heterogeneous Environments – Part 2: High Efficiency Video Coding\".\n[7]\tvoid.\n[8]\t3GPP TS 26.247: \"Transparent end-to-end Packet-switched Streaming Service (PSS); Progressive Download and Dynamic Adaptive Streaming over HTTP (3GP-DASH)\".\n[9]\tISO/IEC 14496-15: \"Information technology - Coding of audio-visual objects - Part 15: Carriage of network abstraction layer (NAL) unit structured video in ISO base media file format\".\n[10]\tISO/IEC 23001-8: \"Information technology -- MPEG systems technologies -- Part 8: Coding-independent code points\".\n[11]\tRecommendation ITU-R BT.2100-1: \"Image parameter values for high dynamic range television for use in production and international programme exchange\".\n[12]\t3GPP TS 26.116: \"Television (TV) over 3GPP services; Video profiles\".\n[13]\tISO/IEC 23090-2: \"Coded representation of immersive media -- Part 2: Omnidirectional media format\".\n[14]\tISO/IEC DIS 23091-2: \"Information technology -- Coding-independent code points -- Part 2: Video\".\n[15]\t3GPP TS 26.260: \"Objective test methodologies for the evaluation of immersive audio systems\".\n[16]\t3GPP TS 26.259: \"Subjective test methodologies for the evaluation of immersive audio systems\".\n[17]\tISO/IEC 14496-12: \"Information technology -- Coding of audio-visual objects -- Part 12: ISO base media file format\".\n[18]\tISO/IEC 23009-1: \"Information technology -- Dynamic adaptive streaming over HTTP (DASH) -- Part 1: Media presentation description and segment formats\".\n[19]\tISO/IEC 23008-3:2015: \"Information technology -- High efficiency coding and media delivery in heterogeneous environments - Part 3: 3D audio\", ISO/IEC 23008-3:2015/Amd2:2016: \"MPEG-H 3D Audio File Format Support \", ISO/IEC 23008-3:2015/Amd 3:2017: \"MPEG-H 3D Audio Phase 2\", ISO/IEC 23008-3:2015/Amd 5: \"Audio metadata enhancements\".\n[20]\tIETF RFC 6381: \"The 'Codecs' and 'Profiles' Parameters for \"Bucket\" Media Types\", R. Gellens, D. Singer, P. Frojdh, August 2011.\n[21]\tAES69-2015: \"AES standard for file exchange - Spatial acoustic data file format\", 2015.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "3\tDefinitions, symbols and abbreviations",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "3.1\tDefinitions",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the terms and definitions given in TR 21.905 [1] and the following apply. A term defined in the present document takes precedence over the definition of the same term, if any, in TR 21.905 [1].\nbitstream: a bitstream that conforms to a video encoding format and certain Operation Point.\nfield of view: the extent of visible area expressed with vertical and horizontal angles, in degrees in the 3GPP 3DOF reference system.\noperation point: a collection of discrete combinations of different content formats including spatial and temporal resolutions, colour mapping, transfer functions, rendering metadata and the encoding format.\npose: position derived by the head tracking sensor expressed by (azimuth; elevation; tilt angle).\nreceiver: a receiver that can decode and render any bitstream that is conforming to a certain Operation Point.\nviewport: the part of the 3DOF content to render based on the pose and the field of view.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.2\tSymbols",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the following symbols apply:\nα\tyaw of the 3GPP 3DOF coordinate system\nβ\tpitch of the 3GPP 3DOF coordinate system\nγ\troll of the 3GPP 3DOF coordinate system\nϕ\tazimuth of the 3GPP 3DOF coordinate system\nθ\televation of the 3GPP 3DOF coordinate system\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.3\tAbbreviations",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the abbreviations given in TR 21.905 [1] and the following apply. An abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in TR 21.905 [1].\n3DOF\t3 Degrees of freedom\nACN\tAmbisonics Channel Number\nAPI\tApplication Programming Interface\nAVC\tAdvanced Video Coding\nBMFF\tBase Media File Format\nBRIR\tBinaural Room Impulse Response\nCMP\tCube-Map Projection\nCIBR\tCommon Informative Binaural Renderer\nDASH\tDynamic Adaptive Streaming over HTTP\nDRC\tDynamic Range Control\nEOTF\tElectro-Optical Transfer Function\nERP\tEquiRectangular Projection\nESD\tEquivalent Spatial Domain\nFFT\tFast Fourier Transform\nFIR\tFinite Impulse Response\nFOA\tFirst Order Ambisonics\nFOV\tField Of View\nGPU\tGraphics Processing Unit\nHDR\tHigh Dynamic Range\nHDTV\tHigh Definition TeleVision\nHEVC\tHigh Efficiency Video Coding\nHMD\tHead Mounted Display\nHOA\tHigh Order Ambisonics\nHRD\tHypothetical Reference Decoder\nHRIR\tHead-Related Impulse Responses\nHRTF\tHead-Related Transfer Function\nHTTP\tHyperText Transfer Protocol\nIFFT\tInverse FFT\nIRFFT\tInverse RFFT\nMAE\tMPEG-H Audio Metadata information\nMCC\tMetrics Collection and Computation\nMHAS\tMPEG-H Audio Stream\nMIME\tMultipurpose Internet Mail Extensions\nMPD\tMedia Presentation Description\nMPEG\tMoving Pictures Experts Group\nNAL\tNetwork Abstraction Layer\nOMAF\tOmnidirectional MediA Format\nPCM\tPulse Code Modulation\nRAP\tRandom Access Point\nRFFT\tReal FFT\nRWP\tRegion-Wise Packing\nSDR\tStandard Dynamic Range\nSEI\tSupplemental Enhancement Information\nSN3D\tSchmidt semi-normalisation\nSOFA\tSpatially Oriented Format for Acoustics\nSPS\tSequence Parameter Set\nSRQR\tSpherical Region-wise Quality Ranking\nVCL\tVideo Coding Layer\nVST\tVirtual Studio Technology\nVUI\tVideo Usability Information\nVR\tVirtual Reality\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "4\tArchitectures and Interfaces for Virtual Reality",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "4.1\tDefinitions and Reference Systems",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.1.1\tOverview",
                            "text_content": "Virtual reality is a rendered version of a delivered visual and audio scene. The rendering is designed to mimic the visual and audio sensory stimuli of the real world as naturally as possible to an observer or user as they move within the limits defined by the application.\nVirtual reality usually, but not necessarily, assumes a user to wear a head mounted display (HMD), to completely replace the user's field of view with a simulated visual component, and to wear headphones, to provide the user with the accompanying audio as shown in Figure 4.1-1.\nFigure 4-1 presents a reference system that illustrates the components and interactions within a telecommunication network. The system is composed of various entities, including the central office (CO), local exchange (LE), and the customer premises equipment (CPE). The CO is responsible for managing the network resources, while the LE connects the CO to the local area network (LAN). The CPE interfaces with the CO and the LAN, allowing for data transmission and reception.\n\nThe figure also depicts the role of the access network, which connects the CPE to the CO. This network is typically a fiber-optic or copper-based system, and it plays a crucial role in ensuring that data is transmitted and received efficiently. The figure also highlights the importance of the network interface device (NID), which is responsible for managing the network traffic and ensuring that data is transmitted and received in a timely and efficient manner.\n\nOverall, Figure 4-1 provides a comprehensive overview of the components and interactions within a telecommunication network, highlighting the importance of each entity and their role in ensuring that data is transmitted and received efficiently and reliably.\nFigure 4.1-1: Reference System\nSome form of head and motion tracking of the user in VR is usually also necessary to allow the simulated visual and audio components to be updated in order to ensure that, from the user's perspective, items and sound sources remain consistent with the user's movements. Sensors typically are able to track the user's pose in the reference system. Additional means to interact with the virtual reality simulation may be provided but are not strictly necessary.\nVR users are expected to be able to look around from a single observation point in 3D space defined by either a producer or the position of one or multiple capturing devices. When VR media including video and audio is consumed with a head-mounted display or a smartphone, only the area of the spherical video that corresponds to the user's viewport is rendered, as if the user were in the spot where the video and audio were captured.\nThis ability to look around and listen from a centre point in 3D space is defined as 3 degrees of freedom (3DOF). According to the figure 4.1-1:\n-\ttilting side to side on the X-axis is referred to as Rolling, also expressed as γ\n-\ttilting forward and backward on the Y-axis is referred to as Pitching, also expressed as β\n-\tturning left and right on the Z-axis is referred to as Yawing, also expressed as α\nIt is worth noting that this centre point is not necessarily static - it may be moving. Users or producers may also select from a few different observational points, but each observation point in 3D space only permits the user 3 degrees of freedom. For a full 3DOF VR experience, such video content may be combined with simultaneously captured audio, binaurally rendered with an appropriate Binaural Room Impulse Response (BRIR). The third relevant aspect is the interactivity: Only if the content is presented to the user in such a way that the movements are instantaneously reflected in the rendering, then the user will perceive a full immersive experience. For details on immersive rendering latencies, refer to TR 26.918 [2].\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.1.2\t3GPP 3DOF Coordinate System",
                            "text_content": "The coordinate system is specified for defining the sphere coordinates azimuth () and elevation () for identifying a location of a point on the unit sphere, as well as the rotation angles yaw (α), pitch (β), and roll (γ). The origin of the coordinate system is usually the same as the centre point of a device or rig used for audio or video acquisition as well as the position of the user's head in the 3D space in which the audio or video are rendered. Figure 4.1-2 specifies principal axes for the coordinate system. The X axis is equal to back-to-front axis, Y axis is equal to side-to-side (or lateral) axis, and Z axis is equal to vertical (or up) axis. These axis map to the reference system in Figure 4.1-1.\nFigure 4 presents a two-dimensional coordinate system, where the x-axis represents the horizontal dimension and the y-axis represents the vertical dimension. The system is divided into four quadrants, each representing a different region of the plane. The origin of the coordinate system is located at the bottom-left corner, with the positive x-axis pointing to the right and the positive y-axis pointing upwards. This coordinate system is commonly used in various fields such as physics, engineering, and computer graphics to represent and analyze geometric shapes and spatial relationships.\nFigure 4.1-2: Coordinate system\nSignals defined in the present document are represented in a spherical coordinate space in angular coordinates (ϕ,θ) for use in omnidirectional video and 3D audio. The viewing and listing perspective are from the origin sensing/looking/hearing outward toward the inside of the sphere. Even though a spherical coordinate is generally represented by using radius, elevation, and azimuth, it assumes that a unit sphere is used for capturing and rendering of VR media. Thus, a location of a point on the unit sphere is identified by using the sphere coordinates azimuth () and elevation (). The spherical coordinates are defined so that ϕ is the azimuth and θ is the elevation. As depicted in Figure 4.1-2, the coordinate axes are also used for defining the rotation angles yaw (α), pitch (β), and roll (γ). The angles increase clockwise when looking from the origin towards the positive end of an axis. The value ranges of azimuth, yaw, and roll are all −180.0, inclusive, to 180.0, exclusive, degrees. The value range of elevation and pitch are both −90.0 to 90.0, inclusive, degrees.\nDepending on the applications or implementations, not all angles may be necessary or available in the signal. The 360 video may have a restricted coverage as shown in Figure 4.1-3. When the video signal does not cover the full sphere, the coverage information is described by using following parameters:\n-\tcentre azimuth: specifies the azimuth value of the centre point of sphere region covered by the signal.\n-\tcentre elevation: specifies the elevation value of the centre of sphere region.\n-\tazimuth range: specifies the azimuth range through the centre point of the sphere region.\n-\televation range: specifies the elevation range through the centre point of the sphere region.\n-\ttilt angle: indicates the amount of tilt of a sphere region, measured as the amount of rotation of the sphere region along the axis originating from the origin passing through the centre point of the sphere region, where the angle value increases clockwise when looking from the origin towards the positive end of the axis.\nFigure 4 illustrates a restricted coverage scenario within a sphere region, as depicted by the cropped output picture. The coverage area is centered around omni_projection_{yaw | pitch | roll}_center, which represents the center of the coverage region. The figure showcases the limitations of omnidirectional projection techniques in certain environments, highlighting the need for more targeted and efficient methods in certain applications.\nFigure 4.1-3: Restricted coverage of the sphere region covered by the cropped output picture with omni_projection_{yaw | pitch | roll}_center the center of the coverage region\nFor video, such a centre point may exist for each eye, referred to as stereo signal, and the video consists of three colour components, typically expressed by the luminance (Y) and two chrominance components (U and V).\nThe coordinate systems for all media types are assumed to be aligned in 3GPP 3DOF coordinate system. Within this coordinate system, the pose is expressed by a triple of azimuth, elevation, and tilt angle characterizing the head position of a user consuming the audio-visual content. The pose is generally dynamic, and the information may be provided through sensors in a frequently sampled version.\nThe field of view (FoV) of a rendering device is static and defined in two dimensions, the horizontal and vertical FoV, each in units of degrees in the angular coordinates (ϕ,θ). The pose together with the field of view of the device enables the system to generate the user viewport, i.e., the presented part of the content at a specific point in time.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.1.3\tVideo Signal Representation",
                            "text_content": "Commonly used video encoders cannot directly encode spherical videos, but only 2D textures. However, there is a significant benefit to reuse conventional 2D video encoders. Based on this, Figure 4.1-4 provides the basic video signal representation in the context of omnidirectional video in the context of the present document. By pre-processing, the spherical video is mapped to a 2D texture. The 2D texture is encoded with a regular 2D video encoder and the VR rendering metadata (i.e. the data describing the mapping from the spherical coordinate to the 2D texture) is encoded and provided along with the video bitstream, such that at the receiving end the inverse process can be applied to reconstruct the spherical video.\nFigure 4 illustrates the representation of a video signal in a digital format, showcasing the process of encoding and compression. The figure depicts the signal flow from the camera, through various stages of processing, including quantization and entropy coding, ultimately resulting in a compact representation suitable for transmission and storage. The visual elements emphasize the efficiency of the video coding process, which is crucial for maintaining quality while reducing bandwidth consumption.\nFigure 4.1-4: Video Signal Representation\nMapping of a spherical picture to a 2D texture signal is illustrated in Figure 4.1-5. The most commonly used mapping from spherical to 2D is the equirectangular projection (ERP) mapping. The mapping is bijective, i.e. it may be expressed in both directions.\nFigure 4 illustrates the process of mapping spherical data to 2D representations, showcasing various methods such as spherical harmonics, spherical Gaussian, and spherical Fourier series. The figure demonstrates how these techniques decompose spherical data into a grid-like structure, enabling efficient computation and visualization in 2D space. The visual elements include concentric circles representing the sphere, with corresponding 2D plots showing the transformation of data across different methods.\nFigure 4.1-5: Examples of Spherical to 2D mappings\nFollowing the definitions in clause 4.1.2, the mapping of the colour samples of 2D texture images onto a spherical coordinate space in angular coordinates (ϕ,θ) for use in omnidirectional video applications for which the viewing perspective is from the origin looking outward toward the inside of the sphere. The spherical coordinates are defined so that ϕ is the azimuth and θ is the elevation.\nAssume a 2D texture with pictureWidth and pictureHeight, being the width and height, respectively, of a monoscopic projected luma picture, in luma samples and the center point of a sample location (i,j) along the horizontal and vertical axes, respectively, then for the equirectangular projection the sphere coordinates (,) for the luma sample location, in degrees, are given by the following equations:\n= ( 0.5 − i ÷ pictureWidth ) * 360\n = ( 0.5 − j ÷ pictureHeight ) * 180\nWhereas ERP is commonly used for production formats, other mappings may be applied, especially for distribution. The present document also introduces cubemap projection (CMP) for distribution in clause 5. In addition to regular projection, other pre-processing may be applied to the spherical video when mapped into 2D textures. Examples include region-wise packing, stereo frame packing or rotation. The present document defines different pre- and post-processing schemes in the context of video rendering schemes.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.1.4\tAudio Signal Representation",
                            "text_content": "Audio for VR can be produced using three different formats. These are broadly known as channels-, objects- and scene-based audio formats. Audio for VR can use any one of these formats or a hybrid of these (where all three formats are used to represent the spherical soundfield). The audio signal representation model is shown in Figure 4.1-6.\nThe present document expects that an audio encoding system is capable to produce suitable audio bitstreams that represent a well-defined audio signal in the reference system as defined in clause 4.1.1. The coding and carriage of the VR Audio Rendering Metadata is expected to be defined by the VR Audio Encoding system. The VR Audio Receiving system is expected to be able to use the VR Audio Bitstream to recover audio signals and VR Audio Rendering metadata. Both signals, audio signals and metadata, are well-defined by the media profile, such that different audio rendering systems may be used to render the audio based on the decoder audio signals, VR audio rendering metadata and the user position.\nIn the present document, all media profiles are defined such that for each media profile at least one Audio Rendering System is defined as a reference renderer and additional Audio Rendering systems may be defined. The audio rendering system is described based on well-defined output of the VR Audio decoding system.\nFigure 4 presents a detailed representation of an audio signal, illustrating its transformation from a source through various stages of processing. The signal begins as a waveform, which is then converted into a series of discrete samples. These samples are further processed, including filtering and amplification, before being encoded into a digital format. The digital signal is then compressed and transmitted over a communication channel, where it undergoes further processing and decoding at the receiver end. The figure emphasizes the importance of each stage in the audio signal's journey, highlighting the various techniques used to ensure quality and efficiency in audio communication.\nFigure 4.1-6: Audio Signal Representation\nFor more details on audio rendering, refer to clause 4.5.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.2\tEnd-to-end Architecture",
                    "description": "",
                    "summary": "",
                    "text_content": "The architecture introduced in this clause addresses service scenarios for the distribution of VR content in file-based download and DASH-based streaming services.\nFigure 4.2-1 considers a functional architecture for such scenarios. VR Content is acquired and the content is pre-processed such that all media components are mapped to the 3GPP 3DOF coordinate system and are temporarily synchronized. Such pre-processing may include video stitching, rotation or other translations. The 3GPP VR Headend is responsible for generating content that can be consumed by receivers conforming to the present document. Typically, 3D Audio and spherical video signals are properly encoded. Especially for video, the processing follows the two step approach of mapping, projecting and pre-processing to 2D texture and then encoding with regular 2D video codecs. After media encoding, the content is made available to file format encapsulation engine as elementary streams. The encapsulated streams are referred to as 3GPP VR Tracks, i.e. they are spatially mapped to the same timing system for synchronized playback. For file based distribution a complete file for delivery is generated by multiplexing the 3GPP VR tracks into a single file. For DASH based delivery, the content is mapped to DASH segments and proper Adaptation Sets are generated, including the necessary MPD signaling. The Adaptation Sets are included in a VR Media Presentation, documented in a DASH MPD. Content may be made available such that it is optimized for a specific viewpoint, so the same content may be encoded in an ensemble of multiple viewport-optimized versions.\nThe content is delivered through file based delivery of DASH based delivery, potentially using 3GPP services such as DASH in PSS or DASH-over-MBMS.\nAt the receiving end, a VR application is assumed that communicates with the different functional blocks in the receivers' 3GPP VR service platform, namely, the DASH client or the download client, the file processing units for each media profile, the media decoding units, the rendering environment and the pose generator. The reverse operations of the VR Headend are performed. The operation is expected to be dynamic, especially taking into account updated pose information in the different stages of the receiver. The pose information is essential in the rendering units, but may also be used in the download or DASH client for delivery and decoding optimizations. For more details on the client reference architecture, refer to clause 4.3.\nFigure 4.2-1 illustrates the architecture for VR streaming services, showcasing the various components involved in delivering an immersive virtual reality experience. The figure depicts a centralized server, responsible for processing and rendering the VR content, connected to multiple edge servers dispersed across the network. These edge servers are strategically placed to reduce latency and ensure smooth streaming. The figure also highlights the importance of content delivery networks (CDNs) in caching frequently accessed VR files, thereby improving the overall performance and user experience.\nFigure 4.2-1: architecture for VR streaming services\nBased on the architecture in Figure 4.2-1, the following components are relevant for 3GPP VR Streaming Services:\n-\tConsistent source formats that can be distributed by a 3GPP VR Headend:\n-\tFor audio that can be used by a 3D audio encoding profile according the present document.\n-\tFor video that can be used by a spherical video encoding profile according to the present document.\n-\tMapping formats from a 3-dimensional representation to a 2D representation in order to use regular video encoding engines\n-\tEncapsulation of the media format tracks to ISO file format together, adding sufficient information on to decode and render the VR content. The necessary metadata may be on codec level, file format level, or both.\n-\tDelivery of the formats through file download, DASH delivery and DASH-over-MBMS delivery.\n-\tStatic and dynamic capabilities and environmental data, including decoding and rendering capabilities, as well as dynamic pose information.\n-\tMedia decoders that support the decoding of the formats delivered to the receiver.\n-\tInformation for audio and video rendering to present the VR Presentation on the VR device.\nBased on the considerations above, to support the use case of VR Streaming, the following functions are defined in the present document:\n-\tConsistent content contribution formats for audio and video for 360/3D AV applications including their metadata. This aspect should be considered informative, but example formats are provided to enable explaining the workflow.\n-\tEfficient encoding of 360 video content. In the present document, this encoding is split in two steps, namely a pre-processing and projection mapping from 360 video to 2D texture and a regular video encoding.\n-\tEfficient encoding of 3D audio including channels, objects and scene-based audio.\n-\tEncapsulation of VR media into a file format for download delivery.\n-\tThe relevant enablers for DASH delivery of VR experiences.\n-\tThe necessary capabilities for static and dynamic consumption of the encoded and delivered experiences in the Internet media type and the DASH MPD.\n-\tA reference client architecture that provides the signalling and processing steps for download delivery as well as DASH delivery as well as the interfaces between the 3GPP VR service platform, a VR application (e.g. pose information), and the VR rendering system (displays, GPU, loudspeakers).\n-\tDecoding requirements for the defined 360 video formats.\n-\tDecoding requirements for the defined 3D audio formats.\n-\tRendering requirements or recommendations for the above formats, for both separate and integrated decoding/rendering.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.3\tClient Reference Architecture",
                    "description": "",
                    "summary": "",
                    "text_content": "This clause provides more details of a client reference architecture for VR streaming applications and describes their components and interfaces.\nFigure 4.3-1 and Figure 4.3-2 show a high-level structure of the client reference architecture for VR DASH streaming and VR local playback, respectively, which consist of five functional components:\n-\tVR Application: The VR application controls the rendering depending on a user viewport or display capabilities. The application may communicate with other functional components, e.g., the access engine, the file decoder. The access engine or file decoder may parse some abstracted control information to the VR application and the application makes the decision on which adaptation sets or preselections to select or which tracks to choose taking into account platform or user information as well as the dynamic pose information.\n-\tAccess Engine: The access engine connects through a 3GPP bearer and provides a conforming VR presentation to the receiver. The access engine fetches the Media Presentation Description (MPD), constructs and issues requests and receives Segments or parts of Segments. In the case of local playback, the 3GPP VR Track is accessed from the local storage. The access engine may interface with the VR application function to dynamically change the delivery session. The access engine provides a conforming 3GPP VR track to the file decoder.\n-\tFile Decoder: The file decoder processes the 3GPP VR Track to generate signals that can be processed by the renderer. The file decoder typically includes at least of two sub-modules; the file parser and the media decoder. The file parser processes the file or segments, extracts elementary streams, and parses the metadata, if present. The processing may be supported by dynamic information provided by the VR application, for example which tracks to choose based on static and dynamic configurations. The media decoder decodes media streams of the selected tracks into the decoded signals. The file decoder outputs the decoded signals and metadata which is used for rendering. The file decoder is the primary focus of the present document.\n-\tVR Renderer: The VR Renderer uses the decoded signals and rendering metadata and provides a viewport presentation taking into account the viewport and possible other information. With the pose, a user viewport is determined by determining horizontal/vertical field of view of the screen of a head-mounted display or any other display device to render the appropriate part of decoded video or audio signals. The renderer is addressed in individual media profiles. For video, textures from decoded signals are projected to the sphere with rendering metadata received from the file decoder. During the texture-to-sphere mapping, a sample of the decoded signal is remapped to a position on the sphere. Likewise, the decoded audio signals are represented in the reference system domain. The appropriate part of video and audio signals for a current pose is generated by synchronizing and spatially aligning the rendered video and audio.\n-\tSensor: The sensor extracts the current pose according to the user's movement and provides it to the renderer for viewport generation. The current pose may for example be determined by the head tracking and possibly also eye tracking functionalities. The current pose may also be used by the VR application to control the access engine on which adaptation sets or preselections to select (for the streaming case), or to control the file decoder on which tracks to choose for decoding (for the local playback case).\nThe main objective of the present document is to enable the file decoder to generate decoded signals and the rendering metadata from a conforming 3GPP VR Track by generating a bitstream that conforms to a 3GPP Operation Point. Both, a 3GPP VR Track as well as a bitstream conforming to an Operation Point are a well-defined conformance points for a VR File decoder and a Media Decoder. Both enable to represent the contained media in the VR reference system (spatially and temporally).\nNOTE 1:\t3GPP VR Track represents media in container formats according to the ISO/IEC 14496-12 [17] ISO Base Media File Format and may consist of one or more ISO BMFF tracks following the requirements of this specification.\nFigure 4 illustrates the client reference architecture for VR DASH streaming applications, showcasing the components involved in delivering high-quality virtual reality experiences. The figure depicts the client device, the content delivery network (CDN), and the cloud-based server infrastructure. It emphasizes the role of adaptive bitrate streaming (DASH) in optimizing video quality based on the user's network conditions. The architecture leverages edge computing for faster content delivery and improved user experience.\nFigure 4.3-1: Client Reference Architecture for VR DASH Streaming Applications\n\nFigure 4 illustrates the client reference architecture for VR local playback, showcasing the components involved in the process. The figure depicts the client device, which is the primary source of the VR content, connected to a local server. This server is responsible for processing and rendering the VR content, ensuring a smooth and immersive experience for the user. Additionally, the figure shows the connection between the client device and the cloud-based VR platform, which serves as a secondary source of content and enables remote collaboration and content distribution. The architecture emphasizes the importance of efficient communication between the client device, local server, and cloud platform to deliver high-quality VR experiences.\nFigure 4.3-2: Client Reference Architecture for VR Local Playback\nNOTE 2:\tThe dashed arrows indicate optional interfaces between components in the Figure 4.3-2. Viewport information should optionally be input to the access engine and file decoder.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.4\tRendering Schemes, Operation Points and Media Profiles",
                    "description": "",
                    "summary": "",
                    "text_content": "The present document provides several interoperability points that may be referred external specifications. These are:\n-\tMedia profiles: providing DASH, file format and elementary stream constraints for a single media type.\n-\tOperation Points: a collection of discrete combinations of different content formats including spatial and temporal resolutions, colour mapping, transfer functions, rendering metadata and the encoding format.\n-\tBitstream: A video bitstream that conforms to a video encoding format and certain Operation Point including VR Rendering Metadata.\n-\tRendering Scheme: post-decoder processing of decoder output signals together with rendering metadata.\nNote that this applies to both media types, audio and video. For audio, the 3GPP VR Rendering Scheme interoperability point serves as an informative output of the File Decoder. The 3GPP VR viewport interoperability point serves as the output of the entire file decoding process.\nBoth features provide clear requirements for interoperability for receiver. Figure 4.4-1 provides an overview on this.\nFigure 4.4-1 illustrates the interoperability aspects for 3GPP Virtual Reality (VR) Profiles, showcasing the various stages of VR content creation and delivery. The figure depicts the interaction between the VR content creator, VR content distributor, and VR content consumer. The VR content creator is responsible for generating the VR content, while the VR content distributor handles the distribution of the content to the VR content consumer. The VR content consumer then interacts with the VR content, utilizing the VR device to experience the content. The figure also highlights the role of the network in facilitating the smooth delivery of VR content, with the network providing the necessary bandwidth and latency for a seamless VR experience.\nFigure 4.4-1: Interoperability aspects for 3GPP VR Profiles\nMedia profile for timed media is defined as requirements and constraints for a set of one or more 3GPP VR tracks of a single media type. The conformance of a set of one or more 3GPP VR tracks to a media profile is specified as a combination of:\n-\tSpecification of which sample entry type(s) are allowed, and which constraints and extensions are required in addition to those imposed by the sample entry type(s).\n-\tConstraints on the samples of the tracks, typically expressed as constraints on the elementary stream contained within the samples of the tracks.\nThe elementary stream constraints of a media profile may be indicated by a requirement to comply with a certain profile and level of the media coding specification, possibly including additional constraints and extensions, such as a requirement of the presence of certain information for rendering and presentation.\nEach media profile specified in the present document includes a file decoding process such that all file decoders that conform to the video media profile will produce:\n-\tFor video: numerically identical cropped decoded pictures when invoking the file decoding process associated with that video media profile for a set of 3GPP VR tracks conforming to the video media profile. A bitstream that conforms to the elementary stream constraints specified for the video media profile is reconstructed as an intermediate product of the file decoding process. Output of the file decoding process consists of all of the following:\n-\ta list of decoded pictures with associated presentation times;\n-\tfor projected omnidirectional video VR rendering metadata.\n-\tfor audio: a set of audio signals when invoking the file decoding process associated with that audio media profile for a VR Track conforming to the audio media profile. A bitstream that conforms to the elementary stream constraints specified for the audio media profile is reconstructed as an intermediate product of the file decoding process. Output of the file decoding process consists of all of the following:\n-\ta sequence of audio samples with associated presentation times;\n-\taudio VR rendering metadata.\nA file decoder conforms to the file decoding process requirements of the present document when it complies with both of the following:\n-\tfor video:\n-\tThe file decoder includes a conforming decoder that produces numerically identical cropped decoded pictures to those produced by the file decoding process specified for the video media profile in clause 5 (with the correct output order or output timing, as specified in the video coding specification of the video media profile, respectively).\n-\tThe file decoder outputs rendering metadata that is equivalent to that produced by the file decoding process specified for the video media profile in clause 5 (with the correct association of the rendering metadata to particular cropped decoded pictures, as specified in the present document).\n-\tfor audio:\n-\tThe file decoder includes a conforming decoder that produces a sequence of audio samples with associated presentation times as defined in clause 6.\n-\tThe file decoder outputs Audio rendering metadata that is equivalent to that produced by the file decoding process specified for the audio media profile in clause 6 (with the correct association of the rendering metadata to particular audio samples).\nA player claiming conformance to a media profile shall include a file decoder complying with the file decoding process of that video media profile as specified above. While the player operation, with the exception of the file decoding process, is not specified normatively in the present document, specifications of a media profile may include an informative clause on expectations of a player operation, for example including recommendations for rendering.\nIn addition to the interoperability on track level, also a DASH level interoperability for each media profile is defined. This interoperability includes the signalling and content generation, such that by dynamic switching based on network constraints or sensor input a conforming 3GPP VR Track for this media profile may be obtained.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.5\tAudio Rendering",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.5.1\tAudio Renderer Definitions",
                            "text_content": "a)\tThe purpose of the Reference Renderer is to provide a documented audio rendering solution in 3GPP for its corresponding media profile. A Reference Renderer:\na)\tIs specified along with the media profile.\nb)\tSupports binaural and loudspeaker based rendering.\nc)\tHas a standardized implementable description documented either in 3GPP or in an external SDO.\nd)\tSupports diegetic and non-diegetic content.\ne)\tHas a Motion to Sound latency characterized according to the method defined in TS 26.260 [15].\nf)\tHas a Loudness characterized according to the method defined in TS 26.260 [15].\ng)\tProvides a suitable subjective quality level characterized by the Rendering Test (see clause 4.5.1.6).\nh)\tProvides an interface to specify the set of HRTFs used for binaural rendering.\nNOTE:\tThe Reference Renderer could be an external renderer following the properties defined above.\nThe CIBR is a binaural renderer defined for the purposes of the Renderer Test in TS 26.259 [16]. The CIBR:\na)\tSupports binaural rendering.\nb)\tSupports diegetic and non-diegetic content.\nc)\tHas a Motion to Sound latency characterized according to the method defined in TS 26.260 [15].\nd)\tHas a Loudness characterized according to the method defined in TS 26.260 [15].\ne)\tIs intended to provide a quality comparison point for the Reference Renderer (see clause 4.5.1.1) and any External Renderer (see clause 4.5.1.3).\nThe CIBR consists of four components. The first three are currently available as VST audio plugins:\n1)\tThe \"ESD to HOA\" component, which receives a set of audio input signals in an Equivalent Spatial Domain (ESD) representation and converts them into a set of audio output signals in the HOA domain (ACN/SN3D format). The ESD representation corresponds to the immersive audio content rendered at a set of pre-determined virtual loudspeaker locations (Fliege Points) The ESD to HOA conversion is accomplished using the \"AmbiX Decoder\" plugin (https://github.com/kronihias/ambix) with the appropriate conversion matrices specified in TS 26.260 [15] clause 4.1 (Fliege Points).\n2)\tThe \"Sound Field rotation\" component, which performs rotation of the soundfield in the HOA domain. The sound field rotation is accomplished with the \"AmbiX Soundfield Rotator\" plugin (https://github.com/kronihias/ambix) using a connected headtracking device.\n3)\tThe \"HOA to Binaural\" component, which performs the binaural rendering of the HOA signals. The HOA to binaural conversion is accomplished with the \"Google Resonance Monitoring\" plugin (https://github.com/resonance-audio/resonance-audio-daw-tools), which supports up to 3rd order HOA.\n4)\tA \"Diegetic/Non-Diegetic content mixer\". The non-diegetic signals are directly mixed at the headphone output.\nNote that the CIBR may introduce spatial and or timbral quality changes to the rendered objects and channel based-audio signals (ESD loudspeaker inputs).\nFigure 4 presents a block diagram of the Common Informative Binaural Renderer (CIBR), illustrating its key components and their interactions. The diagram showcases the auditory periphery, which includes the pinnae, cochlea, and auditory nerve, as well as the central auditory processing area. The auditory periphery is connected to the central auditory processing area through the auditory nerve, which is represented by a dashed line. The central auditory processing area is further connected to the auditory cortex, which is depicted as a shaded area. The diagram also includes a dashed line representing the auditory pathway, which connects the auditory periphery to the central auditory processing area. The overall layout of the diagram provides a clear and concise representation of the CIBR's structure and function.\nFigure 4.5-1: Block diagram of Common Informative Binaural Renderer\nThe primary purpose of the External Renderer is to enable alternatives to the Reference Renderer. There may be several External Renderers for a given media profile.\nAn External Renderer:\na)\tSupports binaural and/or loudspeaker based rendering.\nb)\tCan be the Reference Renderer associated to the Audio Media Profile.\nc)\tDoes not require a standardized implementable description.\nd)\tExposes an External Renderer API (see clause 4.5.1.5) and/or the Common Renderer API (see clause 4.5.1.4) for connecting it to an audio decoder.\ne)\tSupports diegetic and non-diegetic content.\nf)\tHas a Motion to Sound latency documented according to the method defined in TS 26.260 [15].\ng)\tHas a Loudness documented according to the method defined in TS 26.260 [15].\nh)\tProvides a suitable subjective quality level characterized according to the Rendering Test (see clause 4.5.1.6) with additional comparison with the Reference Renderer.\ni)\tProvides an interface to specify the set of HRTFs used for binaural rendering if applicable.\nThe purpose of the Common Renderer API is to enable the use of an External Renderer that can support all VRStream media profiles. The Common Renderer API:\na)\tIs normative.\nb)\tHas a standardized implementable description in 3GPP technical specifications or by reference.\nThe purpose of the External Renderer API is to enable the use of an External Renderer. The External Renderer API:\na)\tHas a standardized implementable description in 3GPP technical specifications or by reference.\nb)\tProvides the necessary information to connect a VRStream media profile with an External Renderer.\nThe purpose of the Rendering Test is to characterize the Quality of Experience (QoE) when using the Reference Renderer or External Renderer.\nThe Rendering Test:\na)\tIs defined in TS 26.259 [16] clause 6.\nb)\tCharacterizes media profile performance with Reference Renderer or External Renderer.\nc)\tAssesses performance for multiple audio quality attributes and overall quality.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "5\tVideo",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "5.1\tVideo Operation Points",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.1.1\tDefinition of Operation Point",
                            "text_content": "For the purpose to define interfaces to a conforming video decoder, video operation points are defined. In this case the following definitions hold:\n-\tOperation Point: A collection of discrete combinations of different content formats including spatial and temporal resolutions, colour mapping, transfer functions, VR specific rendering metadata, etc. and the encoding format.\n-\tReceiver: A receiver that can decode and render any bitstream that is conforming to a certain Operation Point.\n-\tBitstream: A video bitstream that conforms to a video encoding format and certain Operation Point including VR rendering metadata.\nFigure 5.1 -1 illustrates the various video operation points within a telecommunication system, highlighting the key components involved in the process. The figure showcases the input sources, such as cameras and sensors, which capture and transmit video data. These signals are then processed and compressed at the encoding points, ensuring efficient transmission over the network. The figure also depicts the distribution points, where the compressed video is distributed to various end-users, such as televisions and mobile devices. The figure emphasizes the importance of each operation point in the video transmission process, highlighting the need for efficient and reliable communication between these components to deliver high-quality video content to the end-users.\nFigure 5.1-1: Video Operation Points\nThis clause focuses on the interoperability point to a media decoder as indicated in Figure 5.1-1. This clause does not deal with the access engine and file parser which addresses aspects how the video bitstream is delivered.\nIn all video operation points, the VR Presentation can be rendered using a single media decoder which provides decoded signals and rendering metadata by decoding relevant SEI messages.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.1.2\tParameters of Visual Operation Point",
                            "text_content": "This clause defines the potential parameters of Visual Operation Points. This includes the video decoder profile and levels with additional restrictions, conventional video signal parameters and VR rendering metadata. The requirements are defined from the perspective of the video decoder and renderer.\nParameters for a Visual Operation Point include:\n-\tCodec, Profile and level requirements\n-\tRestrictions of regular video parameters, typically expressed in the Video Usability information\n-\tUsage and restrictions of VR rendering metadata\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.1.3\tOperation Point Summary",
                            "text_content": "The present document defines several operation points for different target applications and scenarios. In particular, two legacy operation points are defined that use existing video codecs H.264/AVC and H.265/HEVC to enable distribution of up to 4K full 360 mono video signals up to 60 Hz by using simple equirectangular projection.\nIn addition, one operation for each codec is defined that enables enhanced features, in particular stereo video, up to 8K mono, higher frame rates and HDR.\nFurthermore, one additional operation point is defined that uses H.265/HEVC to enable distribution of up to 8K full 360 mono video signals up to 60 Hz and with HDR using equirectangular projection.\nTable 5.1-1 summarizes the Operation Points, the detailed definitions are defined in the remainder of clause 5.1 where 3k refers to 2880 × 1440 pixels, 4k to 4096 × 2048 pixels, 6k to 6144 × 3072 pixels and 8k to 8192 × 4096 pixels (expressed in luminance pixel width × luminance pixel height).\nNote: The Table only provides an informative high-level summary and is not considered to be complete. The specification text in the remainder of clause 5.1 refines the table and takes precedence over any information documented in the table.\nRestrictions on source formats such as resolution and frame rates, content generation and encoding guidelines are provided in Annex A.\nTable 5.1-1: High-level Summary of Operation Points\n\nVR Rendering metadata in the Operation Points is carried in SEI messages. Receivers are expected to be able to process the VR metadata carried in SEI messages. However, the same VR metadata may be duplicated on system-level. In this case, the Receiver may rely on the system level processing to extract the relevant VR Rendering metadata rather than extracting this from the SEI message.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.1-1: High-level Summary of Operation Points",
                                    "table number": 3,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "5.1.4\tBasic H.264/AVC",
                            "text_content": "This operation point targets simple deployments and legacy receivers at basic quality. A full 360-degree video signal with equirectangular projection following the 3GPP reference system may be provided to the decoding and rendering system for immediate decoding and rendering. Note that this operation point enables to distribute 4k video at regular frame rates and 3k video at higher frame rates.\nRestricted coverage is supported as well, but only in a basic and backward-compatible fashion.\nA Bitstream conforming to the 3GPP VR Basic H.264/AVC Operation point shall conform to the requirements in the remainder of clause 5.1.4.\nA receiver conforming to the 3GPP VR Basic H.264/AVC Operation point shall support decoding and rendering a Bitstream conforming to the 3GPP VR Basic H.264/AVC Operation point. Detailed receiver requirements are provided in the remainder of clause 5.1.4.\nA Bitstream conforming to the 3GPP VR Basic H.264/AVC Operation point shall conform to H.264/AVC Progressive High Profile Level 5.1 [5] for H.264/AVC with the following additional restrictions and requirements:\n-\tthe maximum VCL Bit Rate is constrained to be 120 Mbps with cpbBrVclFactor and cpbBrNalFactor being fixed to be 1250 and 1500, respectively.\n-\tthe bitstream does not contain more than 10 slices per picture.\nNote:\tHigh Profile for H.264/AVC excludes Flexible macro-block order, Arbitrary slice ordering, Redundant slices, Data partition.\nHence, for a Bitstream conforming to the 3GPP VR Basic H.264/AVC Operation point, the following applies:\n-\tThe profile_idc shall be set to 100 indicating the High profile.\n-\tThe constrain_set0_flag, constrain_set1_flag, constrain_set2_flag and constrain_set3_flag shall all be set to 0.\n-\tThe value of level_idc shall not be greater than 51 (corresponding to the level 5.1) and should indicate the lowest level to which the Bitstream conforms.\nPicture aspect ratio 2:1 should be used for the encoded picture.\nThe spatial resolution of the original format in equirectangular projection (ERP) should be one of the following (expressed in luminance pixel width × luminance pixel height):\n-\t4096 × 2048, 3840 × 1920, 3072 × 1536, 2880 × 1440, 2048 × 1024.\nThe spatial resolution of the distribution format should be one of the following (expressed in luminance pixel width × luminance pixel height):\n-\t3840 × 1920, 2880 × 1440, 1920 × 960, 1440 × 720, 960 × 480.\n-\t4096 × 2048, 3072 × 1536, 2048 × 1024, 1536 × 768, 1024 × 512.\nNOTE:\tDistribution formats do not exceed the native resolution of the Operation Point, but they may be subsampled in order to optimize distribution or adapt to the viewing conditions.\nA Receiver conforming to the 3GPP VR Basic H.264/AVC Operation Point shall be capable of decoding and rendering Bitstreams that contain spatial resolutions as above.\nA Bitstream conforming to the 3GPP VR Basic H.264/AVC Operation Point shall use Recommendation ITU-R BT.709 [3] colorimetry. Hence, in the VUI, the colour parameter information shall be present, i.e.\n-\tvideo_signal_type_present_flag value and colour_description_present_flag value shall be set to 1.\n-\tThe colour_primaries value, the transfer_characteristics value and the matrix_coefficients value in the Video Usability Information shall all be set to 1.\nA Receiver conforming to the 3GPP VR Basic H.264/AVC Operation Point shall be capable of decoding and rendering Bitstreams that use Recommendation ITU-R BT.709 [3] colorimetry according to the bitstream requirements documented above.\nA Bitstream conforming to the 3GPP VR Basic H.264/AVC Operation Point shall have one of the following frame rates: 24; 25; 30; 24/1001; 30/1001; 50; 60; 60/1001 Hz.\nThe profile and level constraints of H.264/AVC Progressive High Profile Level 5.1 require careful balance of the permitted frame rates and spatial resolutions. Table 5.1-2 provides the permitted combinations of spatial resolutions and frame rates.\nTable 5.1-2: Permitted combinations of spatial resolutions and frame rates\n\nIn the VUI, the timing information may be present:\n-\tIf the timing information is present, i.e. the value of timing_info_present_flag is set to 1, then the values of num_units_in_tick and time_scale shall be set according to the frame rates allowed above. The timing information present in the video Bitstream should be consistent with the timing information signalled at the system level.\n-\tThe frame rate shall not change between two RAPs. fixed_frame_rate_flag value shall be set to 1.\nA Receiver conforming to the 3GPP VR Basic H.264/AVC Operation Point shall be capable of decoding and rendering Bitstreams that use frame rates according to the bitstream requirements documented above.\nFor H.264/AVC random access point (RAP) definition refer to TS 26.116 [12], clause 4.4.1.1.\nRAPs shall be present in the Bitstream at least once every 5 seconds. It is recommended that RAPs occur in the video Bitstream on average at least every 2 seconds. The time interval between successive RAPs is measured as the difference between their respective decoding time values.\nThe following restrictions apply to the active Sequence Parameter Set (SPS):\n-\tgaps_in_frame_num_value_allowed_flag value shall be set to 0.\n-\tThe Video Usability Information shall be present in the active Sequence Parameter Set. The vui_parameter_present_flag shall be set to 1.\n-\tThe source video format shall be progressive. frame_mbs_only_flag shall be set to 1 for every picture of the Bitstream.\nIn addition to the previous constraints on the VUI on colour information in clause 5.1.4.4 and on frame rates in clause 5.1.4.5, this clause contains further requirements.\nThe aspect ratio information shall be present, i.e.\n-\tThe aspect_ratio_present_flag value shall be set to 1.\n-\tThe aspect_ratio_idc value shall be set to 1 indicating a square pixel format.\nThere are no requirements on output timing conformance for H.264/AVC decoding (Annex C of [5]). The Hypothetical Reference Decoder (HRD) parameters, if present, should be ignored by the Receiver.\nThis operation point uses equirectangular projection, such the video is automatically rendered in the 3GPP reference system. This is enabled by using the MPEG metadata on equirectangular projection.\nA Bitstream conforming to the 3GPP VR Basic H.264/AVC Operation Point shall include the equirectangular projection SEI message (payloadType equal to 150) at every RAP. The erp_guard_band_flag shall be set to 0.\nA Receiver conforming to the 3GPP VR Basic H.264/AVC Operation Point shall be able to process the information contained on equirectangular projection SEI message (payloadType equal to 150) with erp_guard_band_flag shall be set to 0.\nThis operation point permits the decoding and rendering of restricted coverage video signals in a rudimentary way. In this case it is expected that pixels that are projected to a non-covered region are included in the full image, but are visually differentiated from the covered region, for example using black, grey or white colour.\nApplication or system-based signalling may support signalling the coverage region.\nFor a Bitstream conforming to the 3GPP VR Basic H.264/AVC Operation Point:\n-\tthe equirectangular projection SEI message (payloadType equal to 150) with erp_guard_band_flag not set to 0 shall not be present,\n-\tthe sphere rotation SEI message (payloadType equal to 154) shall not be present,\n-\tthe region-wise packing SEI message (payloadType equal to 155) shall not be present,\n-\tthe frame-packing arrangement SEI message (payloadType equal to 45) shall not be present.\nReceivers conforming to the 3GPP VR Basic H.264/AVC Operation Point shall support decoding and displaying 3GPP VR Basic H.264/AVC Operation Point Bitstreams.\nReceivers conforming to the 3GPP VR Basic H.264/AVC Operation Point shall support all Receiver requirements in clause 5.1.4.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.1-2: Permitted combinations of spatial resolutions and frame rates",
                                    "table number": 4,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "5.1.5\tMain H.265/HEVC",
                            "text_content": "This operation targets enhanced 360 video decoding and rendering of H.265/HEVC video for VR applications. Among others, this operation point supports among others rendering of:\n-\t4K mono video at up to 60 Hz frame rates\n-\t3K stereoscopic video at up to 60 Hz frame rates\n-\tHigher than 4K resolutions for restricted coverage\n-\tRendering of certain viewports in higher quality than others beyond 4K\n-\textended colour space and SDR transfer characteristics\nA Bitstream conforming to the 3GPP VR Main H.265/HEVC Operation point shall conform to the requirements in the remainder of clause 5.1.5.\nA Receiver conforming to the 3GPP VR Main H.265/HEVC Operation point shall support decoding and rendering a Bitstream conforming to the 3GPP VR Main H.265/HEVC Operation point. Detailed receiver requirements are provided in the remainder of clause 5.1.5.\nA Bitstream conforming to the 3GPP VR Main H.265/HEVC Operation point shall conform to H.265/HEVC Main-10 Profile Main Tier Profile Level 5.1 [6].\nHence, for a Bitstream conforming to the 3GPP VR Main H.265/HEVC Operation point shall comply with the following restrictions:\n-\tThe general_profile_idc shall be set to 2 indicating the Main10 profile.\n-\tThe general_tier_flag shall be set to 0 indicating the Main tier.\n-\tThe value of level_idc shall not be greater than 153 (corresponding to the Level 5.1) and should indicate the lowest level to which the Bitstream conforms.\nBitstreams conforming to the 3GPP VR Main H.265/HEVC Operation point shall be encoded with either 8 or 10 bit precision:\n-\tbit_depth_luma_minus8 = 0 or 2 (8 or 10 bits respectively)\n-\tbit_depth_chroma_minus8 = bit_depth_luma_minus8\nReceivers conforming to the 3GPP VR Main H.265/HEVC Operation Point shall support 8 bit and10 bit precision.\nDue to the options provided in this operation point, additional original format may be considered that can then be decoded and rendered by a Receiver conforming to this operation point. Recommended original formats beyond those specified in clause 5.1.4.3 for equirectangular projection (ERP) are:\n-\tMono formats: 6144 × 3072, 5880 × 2880\n-\tStereo formats with resolution for each eye: 3840 × 1920, 2880 × 1440, 2048 × 1024\nIf original signals are beyond the maximum permitted resolution of the video codec, then the region-wise packing needs to be applied to generate suitable distribution formats.\nThe distribution formats are more flexible as additional VR metadata as defined in the remainder of clause 5.1.5 may be used. However, for the distribution formats, all requirements of H.265/HEVC Main-10 Profile Main Tier Profile Level 5.1 [6] shall apply to the decoded texture signal.\nAccording to H.265/HEVC Main-10 Profile Main Tier Profile Level 5.1 [6], the maximum luminance width and height does not exceed 8,444 pixels. In addition to the H.265/HEVC Main-10 Profile Main Tier Profile Level 5.1 [6] constraints, a Bitstream conforming to the 3GPP VR Main H.265/HEVC Operation point, the decoded texture signal shall in addition:\n-\tnot exceed the luminance width of 8192 pixels, and\n-\tnot exceed the luminance height of 8192 pixels.\nA Receiver conforming to the 3GPP VR Main H.265/HEVC Operation Point shall be capable of decoding and rendering Bitstreams with a decoded texture signal of maximum luminance width of 8192 pixels a, maximum luminance height of 8192 pixels and the overall profile/level constraints.\nA Bitstream conforming to the 3GPP VR Main H.265/HEVC Operation Point shall use either Recommendation ITU-R BT.709 [3] colorimetry or Recommendation ITU-R BT.2020 [4] colorimetry in non-constant luminance for standard dynamic range (SDR).\nSpecifically, in the VUI, the colour parameter information shall be present, i.e.:\n-\tvideo_signal_type_present_flag value and colour_description_present_flag value shall be set to 1.\n-\tIf BT.709 [3] is used, it shall be signalled by setting colour_primaries to the value 1, transfer_characteristics to the value 1 and matrix_coeffs to the value 1.\n-\tIf BT.2020 [4] and SDR is used,\n- \tit shall be signalled by setting colour_primaries to the value 9, transfer_characteristics to the value 14 and matrix_coeffs to the value 9;\n-\tthe chroma_loc_info_present_flag should be equal to 1, and if set, the chroma_sample_loc_type_top_field and chroma_sample_loc_type_bottom_field shall both be equal to 2.\nA Receiver conforming to the 3GPP VR Main H.265/HEVC Operation Point shall be capable of decoding and rendering according to any of the two above configurations.\nA Bitstream conforming to the 3GPP VR Main H.265/HEVC Operation Point shall have one of the following frame rates: 24; 25; 30; 24/1001; 30/1001; 50; 60; 60/1001 Hz.\nSelected combinations of frame rates with other source parameters are provided in Annex A.2.2.2.\nIn the VUI, the timing information may be present:\n-\tIf the timing information is present, i.e. the value of vui_timing_info_present_flag is set to 1, then the values of vui_num_units_in_tick and vui_time_scale shall be set according to the frame rates allowed in this clause. The timing information present in the video Bitstream should be consistent with the timing information signalled at the system level.\n-\tThe frame rate shall not change between two RAPs. fixed_frame_rate_flag value, if present, shall be set to 1.\nThere are no requirements on output timing conformance for H.265/HEVC decoding (Annex C of [6]). The Hypothetical Reference Decoder (HRD) parameters, if present, should be ignored by the Receiver.\nA Receiver conforming to the 3GPP VR Main H.265/HEVC Operation Point shall be capable of decoding and rendering Bitstreams that use frame rates according to the bitstream requirements documented above.\nFor H.265/HEVC random access point (RAP) definition refer to TS 26.116 [12], clause 4.5.1.2.1.\nRAPs shall be present in the Bitstream at least once every 5 seconds. It is recommended that RAPs occur in the video Bitstream on average at least every 2 seconds. The time interval between successive RAPs is measured as the difference between their respective decoding time values.\nIf viewport adaptation is offered, then RAPs should occur even more frequently to enable transitioning across these viewport-optimized bitstreams.\nReceivers conforming to the 3GPP VR Main H.265/HEVC Operation Point should ignore the content of all Video Parameter Sets (VPS) NAL units as defined in Recommendation ITU-T H.265 / ISO/IEC 23008-2 [6].\nThe following restrictions apply to the active Sequence Parameter Set (SPS):\n-\tThe Video Usability Information (VUI) shall be present in the active Sequence Parameter Set. The vui_parameters_present_flag shall be set to 1.\n-\tThe chroma sub-sampling shall be 4:2:0, chroma_format_idc value shall be set to 1.\n-\tThe source video format shall be progressive, i.e.:\n-\tThe general_progressive_source_flag shall be set to 1,\n-\tThe general_interlaced_source_flag shall be set to 0,\n-\tThe general_frame_only_constraint_flag shall be set to 1.\nReceivers conforming to the 3GPP VR Main H.265/HEVC Operation Point shall support Bitstreams with the restrictions on the SPS defined above.\nIn addition to the previous constraints on the VUI on colour information in clauses 5.1.5.5 and 5.1.5.6, this clause contains further requirements.\nThe aspect ratio information shall be present, i.e.:\n-\tThe aspect_ratio_present_flag value shall be set to 1.\n-\tThe aspect_ratio_idc value shall be set to 1 indicating a square pixel format.\nThere are no requirements on output timing conformance for H.265/HEVC decoding (Annex C of [6]). The Hypothetical Reference Decoder (HRD) parameters, if present, should be ignored by the Receiver.\nThis operation point permits using either equirectangular projection following the MPEG metadata specifications, such the video is automatically rendered in the 3GPP reference system.\nA Bitstream conforming to the 3GPP VR Main H.265/HEVC Operation Point shall include at every RAP the equirectangular projection SEI message (payloadType equal to 150) with the erp_guard_band_flag set to 0.\nThis operation point permits to distribute content with less than 360 degree coverage in an encoding optimized manner by the use of region-wise packing.\nIt is recommended that the number of pixels that are projected to non-covered regions are minimized in the decoded texture signal. If this is applied and not the full 360 video is encoded, the region-wise packing SEI message (payloadType equal to 155) shall be included in the bitstream to signal the encoded regions of the 360 video. If present, it shall be present in a H.265/HEVC RAP.\nApplication or system-based signalling may support signalling the exact coverage region in the spherical coordinates.\nThis operation point permits the use of region-wise packing, for example to optimize the spatial resolution of specific viewports. For some example usage and settings, refer to Annex A.2.\nA Bitstream conforming to the 3GPP VR Main H.265/HEVC Operation Point may include the region-wise packing SEI message (payloadType equal to 155). If present, it shall be present in a H.265/HEVC RAP.\nA Receiver conforming to the 3GPP VR Main H.265/HEVC Operation Point shall be able to process the region-wise packing SEI message (payloadType equal to 155).\nA Bitstream conforming to the 3GPP VR Main H.265/HEVC Operation Point may include the frame packing arrange SEI message (payloadType equal to 45). If present, then the following settings shall apply:\n-\tThe SEI message is present in a H.265/HEVC RAP.\n-\tThe value of frame_packing_arrangement_cancel_flag is equal to 0.\n-\tThe value of frame_packing_arrangement_type is equal to 4.\n-\tThe value of quincunx_sampling_flag is equal to 0.\n-\tThe value of spatial_flipping_flag is equal to 0.\n-\tThe value of field_views_flag is equal to 0.\n-\tThe value of frame0_grid_position_x is equal to 0.\n-\tThe value of frame0_grid_position_y is equal to 0.\n-\tThe value of frame1_grid_position_x is equal to 0.\n-\tThe value of frame1_grid_position_y is equal to 0.\nA Receiver conforming to the 3GPP VR Main H.265/HEVC Operation Point shall process the frame packing arrangement SEI (payloadType equal to 45) with settings restrictions as above. If processing is supported, then the Receiver shall render the viewport indicated by the message.\nFor a Bitstream conforming to the 3GPP VR Main H.265/HEVC Operation Point:\n-\tthe sphere rotation SEI message (payloadType equal to 154) shall not be present.\n-\tany frame-packing arrangement SEI message (payloadType equal to 45) that does not conform to an SEI message defined in clause 5.1.5.13 shall not be present.\nReceivers conforming to the 3GPP VR Main H.265/HEVC Operation Point shall support decoding and displaying 3GPP VR Main H.265/HEVC Operation Point Bitstreams.\nReceivers conforming to the 3GPP VR Main H.265/HEVC Operation Point shall support all Receiver requirements in clause 5.1.5. Specifically, receivers conforming to the 3GPP VR Main H.265/HEVC Operation Point shall support decoding and rendering Bitstreams that include the following VR rendering metadata:\n-\tthe region-wise packing SEI message (for details see clauses 5.1.5.11 and 5.1.5.12)\n-\tthe equirectangular projection SEI message (for details see clause 5.1.5.10)\n-\tthe frame-packing arrangement SEI message (for details see clause 5.1.5.13)\n-\tany combinations of those\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.1.6\tFlexible H.265/HEVC",
                            "text_content": "This operation targets enhanced 360 video decoding and rendering of H.265/HEVC video for VR applications. Among others, this operation point supports rendering of:\n-\t4K mono video at up to 120 Hz frame rates\n-\t3K stereoscopic video at up to 60 Hz frame rates\n-\tHigher than 4K resolutions for restricted coverage\n-\tRendering of certain viewports in higher quality than others beyond 4K\n-\tERP and CMP projection\n-\tSDR and HDR transfer characteristics\nA Bitstream conforming to the 3GPP VR Flexible H.265/HEVC Operation point shall conform to the requirements in the remainder of clause 5.1.6.\nA Receiver conforming to the 3GPP VR Flexible H.265/HEVC Operation point shall support decoding and rendering a Bitstream conforming to the 3GPP VR Flexible H.265/HEVC Operation point. Detailed receiver requirements are provided in the remainder of clause 5.1.6.\nA Bitstream conforming to the 3GPP VR Flexible H.265/HEVC Operation point shall conform to H.265/HEVC Main-10 Profile Main Tier Profile Level 5.1 [6].\nHence, for a Bitstream conforming to the 3GPP VR Flexible H.265/HEVC Operation point shall comply with the following restrictions:\n-\tThe general_profile_idc shall be set to 2 indicating the Main10 profile.\n-\tThe general_tier_flag shall be set to 0 indicating the Main tier.\n-\tThe value of level_idc shall not be greater than 153 (corresponding to the Level 5.1) and should indicate the lowest level to which the Bitstream conforms.\nBitstreams conforming to the 3GPP VR Flexible H.265/HEVC Operation point shall be encoded with either 8 or 10 bit precision:\n-\tbit_depth_luma_minus8 = 0 or 2 (8 or 10 bits respectively)\n-\tbit_depth_chroma_minus8 = bit_depth_luma_minus8\nReceivers conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall support 8 bit and10 bit precision.\nDue to the options provided in this operation point, additional original format may be considered that can then be decoded and rendered by a Receiver conforming to this operation point. Recommended original formats beyond those specified in clause 5.1.5.4 for equirectangular projection (ERP) are:\n-\tMono formats: 8192 × 4096\nThis operation point permits the distribution of ERP signals directly as well as the conversion of ERP signals to cube-map (CMP) projection. A conversion operation is provided in Annex A.2.3. Typical original cubemap format, either generated by conversion or provided by the content provider, that are suitable for this operation point are listed as follows:\n-\tMono Formats: 6144x4096, 4608x3072, 4320x2880, 3072x2048, 2880x1920, 2304x1536, 2160x1440\n-\tStereo Formats with resolution for each eye: 4320x2880, 3072x2048, 2880x1920, 2304x1536, 2160x1440\nIf original signals are beyond the maximum permitted resolution of the video codec, then region wise packing needs to be applied to generate suitable distribution formats.\nThe distribution formats are more flexible as additional VR metadata as defined in the remainder of clause 5.1.6 may be used. However, for the distribution formats, all requirements of H.265/HEVC Main-10 Profile Main Tier Profile Level 5.1 [6] shall apply to the decoded texture signal.\nAccording to H.265/HEVC Main-10 Profile Main Tier Profile Level 5.1 [6], the maximum luminance width and height does not exceed 8,444 pixels. However, for improved interoperability, for a Bitstream conforming to the 3GPP VR Flexible H.265/HEVC Operation point, the decoded texture signal:\n-\tshall not exceed the luminance width of 8192 pixels, and\n-\tshall not exceed the luminance height of 8192 pixels.\nA Receiver conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall be capable of decoding and rendering Bitstreams with a decoded texture signal of maximum luminance width of 8192 pixels and maximum luminance height of 8192 pixels.\nA Bitstream conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall use either Recommendation ITU-R BT.709 [3] colorimetry or Recommendation ITU-R BT.2020 [4] colorimetry in non-constant luminance for standard dynamic range (SDR).\nFor Perceptual Quantization (PQ) High Dynamic Range (HDR), BT.2020 [4] colorimetry in non-constant luminance and the PQ electro-optical transfer function (EOTF) as defined in Recommendation ITU-R BT.2100 [11] are used.\nFor Hybrid Log–Gamma (HLG) High Dynamic Range (HDR), BT.2020 [4] colorimetry in non-constant luminance and the HLG opto-electronic transfer function (OETF) as defined in Recommendation ITU-R BT.2100 [11] are used.\nSpecifically, in the VUI, the colour parameter information shall be present, i.e.:\n-\tvideo_signal_type_present_flag value and colour_description_present_flag value shall be set to 1.\n-\tIf BT.709 [3] is used, it shall be signalled by setting colour_primaries to the value 1, transfer_characteristics to the value 1 and matrix_coeffs to the value 1.\n-\tIf BT.2020 [4] and SDR is used,\n-\tit shall be signalled by setting colour_primaries to the value 9, transfer_characteristics to the value 14 and matrix_coeffs to the value 9,\n-\tthe chroma_loc_info_present_flag should be equal to 1, and if set\tthe chroma_sample_loc_type_top_field and chroma_sample_loc_type_bottom_field shall both be equal to 2\n-\tIf BT.2020 [4] and ITU-R BT.2100 [11] are used in HDR,\n- \tit shall be signalled by setting colour_primaries to the value 9 and matrix_coeffs to the value 9,\n-\tthe chroma_loc_info_present_flag should be equal to 1, and if set, the chroma_sample_loc_type_top_field and chroma_sample_loc_type_bottom_field shall both be equal to 2\n-\tIf the PQ EOTF is used, transfer_characteristics shall be set to the value 16.\n-\tIf the HLG OETF is used, transfer_characteristics shall be set to the value 14. The Bitstream shall also contain the alternative_transfer_characteristics SEI message. The alternative_transfer_characteristics SEI message shall be inserted at each RAP, and its parameter preferred_transfer_characteristics shall be set to the value 18.\nNOTE 1:\tHLG is specified using the alternative_transfer_characteristics method only to ensure backwards compatibility with earlier releases at this Operation Point.\nNOTE 2:\tIf the content is provided to a receiver that is not able to process the SEI message, the receiver uses the backward-compatibility mode of HLG to present an SDR representation of the signal.\nA Receiver conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall be capable of decoding and rendering according to any of the above configurations.\nSEI messages for HDR metadata signalling may be used. The requirements and recommendations for Bitstreams and Receivers as documented in TS 26.116 [12], clause 4.5.5.7 also apply for the 3GPP VR Flexible H.265/HEVC Operation Point.\nA Bitstream conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall have one of the following frame rates: 24; 25; 30; 24/1001; 30/1001; 50; 60; 60/1001, 90, 100, 120 Hz.\nSelected combinations of frame rates with other source parameters are provided in Annex A.2.2.2.\nIn the VUI, the timing information may be present:\n-\tIf the timing information is present, i.e. the value of vui_timing_info_present_flag is set to 1, then the values of vui_num_units_in_tick and vui_time_scale shall be set according to the frame rates allowed in this clause. The timing information present in the video Bitstream should be consistent with the timing information signalled at the system level.\n-\tThe frame rate shall not change between two RAPs. fixed_frame_rate_flag value, if present, shall be set to 1.\nThere are no requirements on output timing conformance for H.265/HEVC decoding (Annex C of [6]). The Hypothetical Reference Decoder (HRD) parameters, if present, should be ignored by the Receiver.\nA Receiver conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall be capable of decoding and rendering Bitstreams that use frame rates according to the bitstream requirements documented above.\nFor H.265/HEVC random access point (RAP) definition refer to TS 26.116 [12], clause 4.5.1.2.1.\nRAPs shall be present in the Bitstream at least once every 5 seconds. It is recommended that RAPs occur in the video Bitstream on average at least every 2 seconds. The time interval between successive RAPs is measured as the difference between their respective decoding time values.\nIf viewport adaptation is offered, then RAPs should occur even more frequently to enable transitioning across these viewport-optimized bitstreams.\nReceivers conforming to the 3GPP VR Flexible H.265/HEVC Operation Point should ignore the content of all Video Parameter Sets (VPS) NAL units  as defined in Recommendation ITU-T H.265 / ISO/IEC 23008-2 [6].\nThe following restrictions apply to the active Sequence Parameter Set (SPS):\n-\tThe Video Usability Information (VUI) shall be present in the active Sequence Parameter Set. The vui_parameters_present_flag shall be set to 1.\n-\tThe chroma sub-sampling shall be 4:2:0, chroma_format_idc value shall be set to 1.\n-\tThe source video format shall be progressive, i.e.:\n-\tThe general_progressive_source_flag shall be set to 1,\n-\tThe general_interlaced_source_flag shall be set to 0,\n-\tThe general_frame_only_constraint_flag shall be set to 1.\nReceivers conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall support Bitstreams with the restrictions on the SPS defined above.\nIn addition to the previous constraints on the VUI on colour information in clauses 5.1.6.5 and 5.1.6.6, this clause contains further requirements.\nThe aspect ratio information shall be present, i.e.:\n-\tThe aspect_ratio_present_flag value shall be set to 1.\n-\tThe aspect_ratio_idc value shall be set to 1 indicating a square pixel format.\nThere are no requirements on output timing conformance for H.265/HEVC decoding (Annex C of [6]). The Hypothetical Reference Decoder (HRD) parameters, if present, should be ignored by the Receiver.\nThis operation point permits using either equirectangular projection or cubemap projection following the MPEG metadata specifications, such the video is automatically rendered in the 3GPP reference system.\nA Bitstream conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall include at every RAP either:\n-\tthe equirectangular projection SEI message (payloadType equal to 150) with the erp_guard_band_flag set to 0, or\n-\tthe cubemap projection SEI message (payloadType equal to 151).\nA Receiver conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall be able to process the equirectangular projection SEI message (payloadType equal to 150) and the cubemap projection SEI message (payloadType equal to 151).\nThis operation point permits to distribute content with less than 360 degree coverage in an encoding optimized manner by the use of region-wise packing.\nIt is recommended that the number of pixels that are projected to non-covered regions are minimized in the decoded texture signal. If this is applied and not the full 360 video is encoded, the region-wise packing SEI message (payloadType equal to 155) shall be included in the bitstream to signal the encoded regions of the 360 video. If present, it shall be present in a H.265/HEVC RAP.\nApplication or system-based signalling may support signalling the exact coverage region in the spherical coordinates.\nThis operation point permits the use of region-wise packing, for example to optimize the spatial resolution of specific viewports. For some example usage and settings, refer to Annex A.2.\nA Bitstream conforming to the 3GPP VR Flexible H.265/HEVC Operation Point may include the region-wise packing SEI message (payloadType equal to 155). If present, it shall be present in a H.265/HEVC RAP.\nA Receiver conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall be able to process the region-wise packing SEI message (payloadType equal to 155).\nA Bitstream conforming to the 3GPP VR Flexible H.265/HEVC Operation Point may include the frame packing arrange SEI message (payloadType equal to 45). If present, then the following settings shall apply:\n-\tThe SEI message is present in a H.265/HEVC RAP.\n-\tThe value of frame_packing_arrangement_cancel_flag is equal to 0.\n-\tThe value of frame_packing_arrangement_type is equal to 4.\n-\tThe value of quincunx_sampling_flag is equal to 0.\n-\tThe value of spatial_flipping_flag is equal to 0.\n-\tThe value of field_views_flag is equal to 0.\n-\tThe value of frame0_grid_position_x is equal to 0.\n-\tThe value of frame0_grid_position_y is equal to 0.\n-\tThe value of frame1_grid_position_x is equal to 0.\n-\tThe value of frame1_grid_position_y is equal to 0.\nA Receiver conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall process the frame packing arrangement SEI (payloadType equal to 45) with settings restrictions as above. If processing is supported, then the Receiver shall render the viewport indicated by the message.\nFor a Bitstream conforming to the 3GPP VR Flexible H.265/HEVC Operation Point:\n-\tthe sphere rotation SEI message (payloadType equal to 154) shall not be present.\n-\tany frame-packing arrangement SEI message (payloadType equal to 45) that does not conform to an SEI message defined in clause 5.1.6.13 shall not be present.\nReceivers conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall support decoding and displaying 3GPP VR Main H.265/HEVC Operation Point Bitstreams and 3GPP VR Flexible H.265/HEVC Operation Point Bitstreams.\nReceivers conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall support all Receiver requirements in clause 5.1.6. Specifically, receivers conforming to the 3GPP VR Flexible H.265/HEVC Operation Point shall support decoding and rendering Bitstreams that include the following display or VR rendering metadata:\n-\tthe region-wise packing SEI message (for details see clauses 5.1.6.11 and 5.1.6.12),\n-\tthe equirectangular projection SEI message (for details see clause 5.1.6.10),\n-\tthe cubemap projection SEI message (for details see clause 5.1.6.10),\n-\tthe frame-packing arrangement SEI message (for details see clause 5.1.6.13),\n- \tthe alternative_transfer_characteristics SEI message with preferred_transfer_characteristics set to the value 18 (for details see clause 5.1.6.5),\n-\tany combinations of those.\n\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.1.7\tMain 8K H.265/HEVC",
                            "text_content": "This operation targets enhanced 360 video decoding and rendering of H.265/HEVC video for VR applications. Among others, this operation point supports among others rendering of:\n-\t8K mono video at up to 60 Hz frame rates for full coverage\n-\t6K stereoscopic video at up to 60 Hz frame rates for full coverage\n-\t4K mono video at up to 120 Hz frame rates for full coverage\n-\tHigher than 8K resolutions for restricted coverage\n-\tExtended colour space, and SDR and HDR transfer characteristics\nA Bitstream conforming to the 3GPP VR Main 8K H.265/HEVC Operation point shall conform to the requirements in the remainder of clause 5.1.7.\nA Receiver conforming to the 3GPP VR Main 8K H.265/HEVC Operation point shall support decoding and rendering a Bitstream conforming to the 3GPP VR Main 8K H.265/HEVC Operation point. Detailed receiver requirements are provided in the remainder of clause 5.1.7.\nA Bitstream conforming to the 3GPP VR Main 8K H.265/HEVC Operation point\nshall conform to H.265/HEVC Main-10 Profile Main Tier Profile Level 6.1 [6], i.e. for a Bitstream conforming to the 3GPP VR Main 8K H.265/HEVC Operation,\n-\tthe general_profile_idc shall be set to 2 indicating the Main10 profile.\n-\tthe general_tier_flag shall be set to 0 indicating the Main tier.\n-\tthe value of level_idc shall not be greater than 183 (corresponding to the Level 6.1) and should indicate the lowest level to which the Bitstream conforms.\n-\tshall have general_progressive_source_flag equal to 1,\n-\tshall have general interlaced_source_flag equal to 0,\n-\tshall have general_frame_only_constraint_flag equal to 1\n-\tshall conform to following further limitations:\n-\tif frame rate 60 fps is used, then the maximum luma picture size in samples of 33,554,432 is not exceeded,\n-\tthe maximum VCL Bit Rate is constrained to be 80 Mbps with CpbVclFactor and CpbNalFactor being fixed to be 1000 and 1100, respectively.\nBitstreams conforming to the 3GPP VR Main 8K H.265/HEVC Operation point shall be encoded with 10-bit precision:\n-\tbit_depth_luma_minus8 = 2 (10 bits)\n-\tbit_depth_chroma_minus8 = bit_depth_luma_minus8\nReceivers conforming to the 3GPP VR Main 8K H.265/HEVC Operation Point shall support 10-bit precision.\nDue to the options provided in this operation point, additional original format may be considered that can then be decoded and rendered by a Receiver conforming to this operation point. Recommended original formats beyond those specified in clause 5.1.4.3 for equirectangular projection (ERP) are:\n-\tMono formats: 8192 × 4096, 7680 × 3840, 6144 × 3072, 5760 × 2880\n-\tStereo formats with resolution for each eye: 6144 × 3072, 5860 × 2880, 4096 × 2048, 3840 × 1920, 2880 × 1440, 2048 × 1024\nThe distribution formats are more flexible as additional VR metadata as defined in the remainder of clause 5.1.7 may be used. However, for the distribution formats, all requirements of H.265/HEVC Main-10 Profile Main Tier Profile Level 6.1 [5] shall apply to the decoded texture signal.\nAccording to H.265/HEVC Main-10 Profile Main Tier Profile Level 6.1 [6], the maximum luminance width and height does not exceed 16,888 pixels. In addition to the H.265/HEVC Main-10 Profile Main Tier Profile Level 6.1 [6] constraints, a Bitstream conforming to the 3GPP VR Main 8K H.265/HEVC Operation point, the decoded texture signal shall in addition:\n-\tnot exceed the luminance width of 16,384 pixels, and\n-\tnot exceed the luminance height of 16,384 pixels.\nA Receiver conforming to the 3GPP VR Main 8K H.265/HEVC Operation Point shall be capable of decoding and rendering Bitstreams with a decoded texture signal of maximum luminance width of 16,384 pixels, a maximum luminance height of 16,384 pixels and the overall profile/level constraints.\nA Bitstream conforming to the 3GPP VR Main 8K H.265/HEVC Operation Point shall use either Recommendation ITU-R BT.709 [3] colorimetry or Recommendation ITU-R BT.2020 [4] colorimetry in non-constant luminance for standard dynamic range (SDR).\nFor Perceptual Quantization (PQ) High Dynamic Range (HDR), BT.2020 [4] colorimetry in non-constant luminance and PQ electro-optical transfer function (EOTF) as defined in Recommendation ITU-R BT.2100 [11] are used.\nFor Hybrid Log–Gamma (HLG) High Dynamic Range (HDR), BT.2020 [4] colorimetry in non-constant luminance and the HLG opto-electronic transfer function (OETF) as defined in Recommendation ITU-R BT.2100 [11] are used.\nSpecifically, in the VUI, the colour parameter information shall be present, i.e.:\n-\tvideo_signal_type_present_flag value and colour_description_present_flag value shall be set to 1.\n-\tIf BT.709 [3] is used, it shall be signalled by setting colour_primaries to the value 1, transfer_characteristics to the value 1 and matrix_coeffs to the value 1.\n-\tIf BT.2020 [4] and SDR is used, it shall be signalled by setting colour_primaries to the value 9, transfer_characteristics to the value 14 and matrix_coeffs to the value 9,\n-\tthe chroma_loc_info_present_flag should be equal to 1, and if set\tthe chroma_sample_loc_type_top_field and chroma_sample_loc_type_bottom_field shall both be equal to 2.\n-\tIf BT.2020 [4] and ITU-R BT.2100 [11] are used in HDR, it shall be signalled by setting colour_primaries to the value 9 and matrix_coeffs to the value 9. The chroma_sample_loc_type_top_field shall be set to 2.\n-\tIf the PQ EOTF is used, transfer_characteristics shall be set to the value 16.\n-\tIf the HLG OETF is used, transfer_characteristics shall be set to either the value 18 or 14. In the latter case, the Bitstream shall also contain the alternative_transfer_characteristics SEI message. The alternative_transfer_characteristics SEI message shall be inserted at each RAP, and its parameter preferred_transfer_characteristics shall be set to the value 18.\n-\tthe chroma_loc_info_present_flag should be equal to 1, and if set, \tthe chroma_sample_loc_type_top_field and chroma_sample_loc_type_bottom_field shall both be equal to 2.\nA Receiver conforming to the 3GPP VR Main 8K H.265/HEVC Operation Point shall be capable of decoding and rendering according to any of the above configurations.\nSEI messages for HDR metadata signalling may be used. The requirements and recommendations for Bitstreams and Receivers as documented in TS 26.116 [12], clause 4.5.5.7 also apply for the 3GPP VR Main 8K H.265/HEVC Operation Point.\nA Bitstream conforming to the 3GPP VR Main 8K H.265/HEVC Operation Point shall have one of the following frame rates: 24; 25; 30; 24/1001; 30/1001; 50; 60; 60/1001; 90; 100; 120; 120/1001 Hz.\nSelected combinations of frame rates with other source parameters are provided in Annex A.2.2.2a.\nIn the VUI, the timing information may be present:\n-\tIf the timing information is present, i.e. the value of vui_timing_info_present_flag is set to 1, then the values of vui_num_units_in_tick and vui_time_scale shall be set according to the frame rates allowed in this clause. The timing information present in the video Bitstream should be consistent with the timing information signalled at the system level.\n-\tThe frame rate shall not change between two RAPs. fixed_frame_rate_flag value, if present, shall be set to 1.\nThere are no requirements on output timing conformance for H.265/HEVC decoding (Annex C of [6]). The Hypothetical Reference Decoder (HRD) parameters, if present, should be ignored by the Receiver.\nA Receiver conforming to the 3GPP VR Main 8K H.265/HEVC Operation Point shall be capable of decoding and rendering Bitstreams that use frame rates according to the bitstream requirements documented above.\nThe same requirements as those defined for 3GPP VR Main H.265/HEVC Operation Point in clause 5.1.5.7 apply.\nReceivers conforming to the 3GPP VR Main 8K H.265/HEVC Operation Point shall satisfy the same requirements as those defined for 3GPP VR Main H.265/HEVC Operation Point in clause 5.1.5.8.\nThe same requirements as those defined for 3GPP VR Main H.265/HEVC Operation Point in clause 5.1.5.9 apply.\nBitstreams conforming to the 3GPP VR Main 8K H.265/HEVC Operation Point shall satisfy the same requirements as those defined for 3GPP VR Main H.265/HEVC Operation Point in clause 5.1.5.10.\nThe same requirements as those defined for 3GPP VR Main H.265/HEVC Operation Point in clause 5.1.5.11 apply.\nHowever, the region-wise packing SEI message (payloadType equal to 155) shall only be used for restricted coverage signalling (in contrast to 3GPP VR Main H.265/HEVC Operation Point, where region-wise packing SEI message is used for signalling both restricted coverage and viewport-optimized content).\nWhen the video does not cover the entire sphere, for each picture, there shall be a region-wise packing SEI message present in the bitstream that applies to the picture. Furthermore, the following restrictions apply:\n-\tnum_packed_regions shall be set to 1.\n-\trwp_guard_band_flag[0] shall be equal to 0.\n-\trwp_transform_type[0] shall be equal to 0.\n-\tThe value of packed_region_width[0] shall be equal to proj_region_width[0].\n-\tThe value of packed_region_height[0] shall be equal to proj_region_height[0].\nBitstreams and receivers conforming to the 3GPP VR Main 8K H.265/HEVC Operation Point shall satisfy the same requirements as those defined for 3GPP VR Main H.265/HEVC Operation Point in clause 5.1.5.13.\nFor a Bitstream conforming to the 3GPP VR Main 8K H.265/HEVC Operation Point:\n-\tthe equirectangular projection SEI message (payloadType equal to 150) with erp_guard_band_flag not set to 0 shall not be present,\n-\tthe sphere rotation SEI message (payloadType equal to 154) shall not be present.\n-\tthe region-wise packing SEI message (payloadType equal to 155) not conforming to the restrictions stated in clause 5.1.7.11 shall not be present.\nReceivers conforming to the 3GPP VR Main 8K H.265/HEVC Operation Point shall satisfy the same requirements as those defined for 3GPP VR Main H.265/HEVC Operation Point in clause 5.1.5.15.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "5.2\tVideo Media Profiles",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.2.1\tIntroduction and Overview",
                            "text_content": "This clause defines the media profiles for video. Media profiles include specification on the following:\n-\tElementary stream constraints based on the video operation points defined in clause 5.1.\n-\tFile format encapsulation constraints and signalling including capability signalling. The defines to a 3GPP VR Track as defined above.\n-\tDASH Adaptation Set constraints and signalling including capability signalling. This defines a DASH content format profile.\nTable 5.2-1 provides an overview of the Media Profiles in defined in the remainder of clause 5.3.2.\n\nTable 5.2-1 Video Media Profiles\n\nNote:\tAdvanced Video Profile Receivers are expected to playback content conforming to the Main Video Media Profile.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.2-1 Video Media Profiles",
                                    "table number": 5,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "5.2.2\tBasic Video Media Profile",
                            "text_content": "The Basic Video Media Profile permits to download and stream elementary streams for VR content generated according to the H.264/AVC Basic Operation Point as defined in clause 5.1.4. This enables reuse of the avc1 sample entry as for example also used in the TV Video Profiles in TS 26.116 [12]. It also permits to reuse streaming the VR video content in an adaptive manner by offering multiple switchable Representations in a single Adaptation Set in a DASH MPD.\nFor content generation guidelines for this media profile refer to Annex A.2.3.\n3GP VR Tracks conforming to this media profile used in the context of the specification shall conform to ISO BMFF [17] with the following further requirements:\n-\tThe bitstream included on the track shall comply to the Bitstream requirements and recommendations for the Basic H.264/AVC Operation Point as defined in clause 5.1.4.\n-\tThe sample entry type of each sample entry of the track shall be equal to 'resv'.\n-\tThe scheme_type value of SchemeTypeBox in the RestrictedSchemeInfoBox shall be 'podv', and all instances of CompatibleSchemeTypeBox defined in ISO/IEC 23090-2 [13] in the same RestrictedSchemeInfoBox shall include at least the scheme_type value 'erpv'.\n-\tThe untransformed sample entry type shall be equal to 'avc1' as defined in ISO/IEC 14496-15 [9].\nNote:\tIf a file decoder experiences issues in the playback of the VR Track with the restricted sample 'resv', but the application is able to control the rendering according to the VR rendering metadata, then the untransformed sample entry could be used to initialize the decoding process for the file decoder.\n-\tThe Track Header Box ('tkhd') shall obey the following constraints:\n-\tThe width and height fields for a visual track shall specify the track's visual presentation size as fixed-point 16.16 values expressed in on a uniformly sampled grid (commonly called square pixels) (of the decoded texture signal)\n-\tThe Video Media Header ('vmhd') shall obey the following constraints:\n-\tThe value of the version field shall be set to '0'.\n-\tThe value of the graphicsmode field shall be set to '0'.\n-\tThe value of the opcolor field shall be set to {'0', '0', '0'}.\n-\tThe Sample Description Box ('stsd') obeys the following constraints:\n-\tA visual sample entry shall be used.\n-\tThe box shall include a NAL Structured Video Parameter Set.\n-\twidth and height field shall correspond to the cropped horizontal and vertical sample counts provided in the Sequence Parameter Set of the track.\n-\tIt shall contain a Decoder Configuration Record which signals the Profile, Level, and other parameters of the video track.\n-\tIt shall contain AVCConfigurationBox as defined in ISO/IEC 14496-15 [9] which signals the Profile, Level, Bit depth, and other parameters conforming to the bitstream constraints specified in clause 5.1.4.\n-\tThe Colour Information Box ('colr') should be present. If present, it shall signal the colour_primaries, transfer_characteristics and matrix_coeffs applicable to all the bitstreams associated with this sample entry.\n-\tThe ProjectionFormatBox with projection_type equal to 0 as defined in ISO/IEC 23090-2 [13] should be present in the sample entry applying to the sample containing the picture.\n-\tIt shall not contain the RegionWisePackingBox as defined in ISO/IEC 23090-2 [13] and StereoVideoBox.\n-\tIf the content contained in the Bitstream in the track does not cover the entire sphere, the CoverageInformationBox as defined in ISO/IEC 23090-2 [13] should be present. If present, only a single region may be signaled and the following restrictions apply:\n-\tThe coverage_shape_type shall be set to 1.\n-\tThe num_regions value shall be set to 1.\n-\tThe view_idc_presence_flag shall be set to 0.\n-\tThe default_view_idc shall be set to 0.\nIf 3GP VR Tracks conforming to the constraints of this media profile, the '3vrb' ISO brand should be set as a compatible_brand in the File Type Box ('ftyp').\nIf all Representations in an Adaptation Set conform to the requirements in clause 5.2.2.3.2 and the Adaptation Set conforms to the requirements in clause 5.2.2.3.3, then the @profiles parameter in the Adaptation Set may signal conformance to this Operation Point by using \"urn:3GPP:vrstream:mp:video:basic\".\nIf a VR Track conforming to this media profile is included in a DASH Representation, the Representation use movie fragments and therefore, the following additional requirements apply:\n-\tThe Media Header Box ('mdhd') shall obey the following constraints:\n-\tThe value of the duration field shall be set to '0'.\n-\tThe value of the duration field in the Movie Header Box ('mvhd') shall be set to a value of '0'.\n-\tThe Sample Table Box ('stbl') shall obey the following constraints:\n-\tThe entry_count field of the Sample-to-Chunk Box ('stsc') shall be set to '0'.\n-\tBoth the sample_size and sample_count fields of the Sample Size Box ('stsz') box shall be set to zero ('0'). The sample_count field of the Sample Size Box ('stz2') box shall be set to zero ('0'). The actual sample size information can be found in the Track Fragment Run Box ('trun') for the track.\nNote:\tThis is because the Movie Box ('moov') contains no media samples.\n-\tThe entry_count field of the Chunk Offset Box ('stco') shall be set to '0'.\n-\tThe Track Header Box ('tkhd') shall obey the following constraints:\n-\tThe value of the duration field shall be set to '0'.\n-\tMovie Fragment Header Boxes ('mfhd') shall contain sequence_number values that are sequentially numbered starting with the number 1 and incrementing by +1, sequenced by movie fragment storage and presentation order.\n-\tAny Segment Index Box ('sidx'), if present, shall obey the additional constraints:\n-\tThe timescale field shall have the same value as the timescale field in the Media Header Box ('mdhd') within the same track; and\n-\tThe reference_ID field shall be set to the track_ID of the ISO Media track as defined in the Track Header Box ('tkhd').\n-\tThe Segment Index shall describe the entire file and only a single Segment Index Box shall be present.\nFor all Representation in an Adaptation Set, the following shall apply:\n-\tThe identical coverage information shall be present on all Representations in one Adaptation Set.\n-\tThe frame rates of all Representations in one Adaptation Set shall be identical.\nFor a video Adaptation Set, the following constraints apply:\n-\tThe @codecs parameter shall be present on Adaptation Set level and shall signal the maximum required capability to decode any Representation in the Adaptation Set. The @codecs parameter should be signalled on the Representation level if different from the one on Adaptation Set level.\n-\tThe attributes @maxWidth and @maxHeight shall be present. They are expected be used to signal the original projected source content format. This means that they may exceed the actual largest size of any coded Representation in one Adaptation Set.\n-\tThe @width and @height shall be signalled for each Representation (possibly defaulted on Adaptation Set level) and shall match the values of the maximum width and height in the Sample Description box of the contained Representation.\n-\tThe Chroma Format may be signalled. If signalled:\n-\tAn Essential or Supplemental Descriptor shall be used to signal the value by setting the @schemeIdURI attribute to urn:mpeg:mpegB:cicp:MatrixCoefficients as defined ISO/IEC 23001-8 [10] and the @value attribute according to Table 4 of ISO/IEC 23001-8 [10]. The values shall match the values set in the VUI.\n-\tThe signalling shall be on Adaptation Set level.\n-\tThe Colour Primaries and Transfer Function may be signalled. If signalled:\n-\tAn Essential or Supplemental Descriptor shall be used to signal the value by setting the @schemeIdURI attribute to urn:mpeg:mpegB:cicp:ColourPrimaries and urn:mpeg:mpegB:cicp:TransferCharacteristics as defined ISO/IEC 23001-8 [10] and the @value attribute according to Table 4 of ISO/IEC 23001-8 [10]. The values shall match the values set in the VUI.\n-\tThe signalling shall be on Adaptation Set level only, i.e. the value shall not be different for different Representations in one Adaptation Set.\n-\tThe @frameRate should be signalled on Adaptation Set level.\n-\tRandom Access Points shall be signalled by @startsWithSAP set to 1 or 2.\n-\ta Supplemental Descriptor should be used to signal the projection by setting the @schemeIdURI attribute to urn:mpeg:mpegI:omaf:2017:pf as defined ISO/IEC 23090-2 [13] and the omaf:@projection_type attribute set to 0.\n-\tIf the CoverageInformationBox is present, a Supplemental Descriptor should be used to signal the value by setting the @schemeIdURI attribute to urn:mpeg:mpegI:omaf:2017:cc as defined ISO/IEC 23090-2 [13] and shall match the information provided in the CoverageInformationBox. Specifically,\n-\tthe cc@shape_type shall be present and be set to 1.\n-\tthe cc@view_idc_presence_flag shall not be present.\n-\texactly one cc.CoverageInfo element shall be present.\n-\tany cc.CoverageInfo attribute that is not centre_azimuth, centre_elevation,\nazimuth_range and elevation_range, shall not be present.\n-\tThe signalling shall be on Adaptation Set level only, i.e. the value shall not be different for different Representations in one Adaptation Set.\n-\tThe FramePacking element shall not be present.\n-\tThe @profiles parameters may be present to signal the constraints for the Adaptation Set.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.2.3\tMain Video Media Profile",
                            "text_content": "The Main Video Media Profile permits to download and stream elementary streams for VR content generated according to the H.265/HEVC Main Operation Point as defined in clause 5.1.5 or H.265/HEVC Main 8K Operation Point as defined in clause 5.1.7. This enables reuse of the hvc1 sample entry as for example also used in the TV Video Profiles in TS 26.116 [12]. It also permits to reuse streaming the VR video content in an adaptive manner by offering multiple switchable Representations in a single Adaptation Set in a DASH MPD. Furthermore, this profile enables that multiple Video Adaptation Sets are offered for the same content, each encoded for a preferred viewport. Multiple Viewpoints may be signaled, for example expressing different type of content or different camera positions.\nFor content generation guidelines for this media profile refer to Annex A.2.3.2.\n3GP VR Tracks conforming to this media profile used in the context of the specification shall conform to ISO BMFF [17] with the following further requirements:\n-\tThe included in the video track shall comply to the Bitstream requirements and recommendations for the Main.H.265/HEVC Operation Point as defined in clause 5.1.5 or Main.8K H.265/HEVC Operation Point as defined in clause 5.1.7 with the additional constraints\n-\tthe region-wise packing SEI message (payloadType equal to 155). if present in any H.265/HEVC RAP, shall be present in any H.265/HEVC RAP and shall be identical for all H.265/HEVC RAP.\n-\tThe sample entry type of each sample entry of the track shall be equal to 'resv'.\n-\tThe scheme_type value of SchemeTypeBox in the RestrictedSchemeInfoBox shall be 'podv', and all instances of CompatibleSchemeTypeBox defined in ISO/IEC 23090-2 [13] in the same RestrictedSchemeInfoBox shall include at least one of the scheme_type values 'erpv' and 'ercm'.\n-\tThe untransformed sample entry type shall be equal to 'hvc1' as defined in ISO/IEC 14496-15 [9].\nNote:\tIf a file decoder experiences issues in the playback of the VR Track with the restricted sample 'resv', but the application is able to control the rendering according to the VR rendering metadata, then the untransformed sample entry could be used to initialize the decoding process for the file decoder.\n-\tThe Track Header Box ('tkhd') shall obey the following constraints:\n-\tThe width and height fields for a visual track shall specify the track's visual presentation size as fixed-point 16.16 values expressed in on a uniformly sampled grid (commonly called square pixels) (of the decoded texture signal)\n-\tThe Video Media Header ('vmhd') shall obey the following constraints:\n-\tThe value of the version field shall be set to '0'.\n-\tThe value of the graphicsmode field shall be set to '0'.\n-\tThe value of the opcolor field shall be set to {'0', '0', '0'}.\n-\tThe Sample Description Box ('stsd') obeys the following constraints:\n-\tA visual sample entry shall be used.\n-\tThe box shall include at least one Sequence Parameter Set NAL unit.\n-\twidth and height field shall correspond to the cropped horizontal and vertical sample counts provided in the Sequence Parameter Set of the track.\n-\tIt shall contain a Decoder Configuration Record which signals the Profile, Level, and other parameters of the video track.\n-\tThe Colour Information Box ('colr') should be present. If present, it shall signal the colour_primaries, transfer_characteristics and matrix_coeffs applicable to all the bitstreams associated with this sample entry.\n-\tThe ProjectionFormatBox with projection_type equal to 0 as defined in ISO/IEC 23090-2 [13] shall be present in the sample entry applying to the sample containing the picture.\n-\tIf the content contained in the Bitstream in the track does not cover the entire sphere, the CoverageInformationBox as defined in ISO/IEC 23090-2 [13] should be present. If present, only a single region may be signaled and the following restrictions apply:\n-\tThe coverage_shape_type shall be set to 1, i.e. the sphere region is specified by two azimuth circles and two elevation circles.\n-\tThe num_regions value shall be set to 1.\n-\tThe view_idc_presence_flag shall be set to 0.\n-\tThe default_view_idc shall be set to 0 or 3.\n-\tIf the content contained in the Bitstream in the track includes the region-wise packing SEI message (payloadType equal to 155), then the RegionWisePackingBox as defined in ISO/IEC 23090-2 [13] shall be present. It shall signal the same information that is included in the region-wise packing SEI message(s) in the elementary stream.\n-\tIf the content contained in the Bitstream in the track does includes the frame packing arrangement SEI message (payloadType equal to 45) in the video stream, the StereoVideoBox shall be present in the sample entry applying to the sample containing the picture. When StereoVideoBox is present, it shall signal the frame packing format that is included in the frame packing arrangement SEI message(s) in the elementary stream.\nIf 3GP VR Tracks conforming to the constraints of this media profile, the '3vrm' ISO brand should be set as a compatible_brand in the File Type Box ('ftyp').\nIf all Representations in an Adaptation Set conform to the requirements in clause 5.2.3.3.2 and the Adaptation Set conforms to the requirements in clause 5.2.3.3.3, then the @profiles parameter in the Adaptation Set may signal conformance to this Operation Point by using \"urn:3GPP:vrstream:mp:video:main\".\nClause 5.2.3.3.4 defines Adaptation Set Ensembles for viewport-optimized offering.\nIf a VR Track conforming to this media profile is included in a DASH Representation, the Representation use movie fragments and therefore, the following additional requirements apply:\n-\tThe Media Header Box ('mdhd') shall obey the following constraints:\n-\tThe value of the duration field shall be set to '0'.\n-\tThe value of the duration field in the Movie Header Box ('mvhd') shall be set to a value of '0'\n-\tThe Sample Table Box ('stbl') shall obey the following constraints:\n-\tThe entry_count field of the Sample-to-Chunk Box ('stsc') shall be set to '0'.\n-\tBoth the sample_size and sample_count fields of the Sample Size Box ('stsz') box shall be set to zero ('0'). The sample_count field of the Sample Size Box ('stz2') box shall be set to zero ('0'). The actual sample size information can be found in the Track Fragment Run Box ('trun') for the track.\nNotE:\tThis is because the Movie Box ('moov') contains no media samples.\n-\tThe entry_count field of the Chunk Offset Box ('stco') shall be set to '0'.\n-\tThe Track Header Box ('tkhd') shall obey the following constraints:\n-\tThe value of the duration field shall be set to '0'.\n-\tMovie Fragment Header Boxes ('mfhd') shall contain sequence_number values that are sequentially numbered starting with the number 1 and incrementing by +1, sequenced by movie fragment storage and presentation order.\n-\tAny Segment Index Box ('sidx'), if present, shall obey the additional constraints:\n-\tThe timescale field shall have the same value as the timescale field in the Media Header Box ('mdhd') within the same track; and\n-\tThe reference_ID field shall be set to the track_ID of the ISO Media track as defined in the Track Header Box ('tkhd').\n-\tThe Segment Index shall describe the entire file and only a single Segment Index Box shall be present.\nFor all Representation in an Adaptation Set, the following shall apply:\n-\tThe identical coverage information shall be present on all Representations in one Adaptation Set, both on ISO BMFF and elementary stream level.\n-\tThe frame rates of all Representations in one Adaptation Set shall be identical.\n-\tThe identical region-wise packing information shall be present all Representations in one Adaptation Set, both on ISO BMFF and elementary stream level.\n-\tThe identical stereoscopic information shall be present all Representations in one Adaptation Set, both on ISO BMFF and elementary stream level.\nFor an Adaptation Set, the following constraints apply:\n-\tThe @codecs parameter shall be present on Adaptation Set level and shall signal the maximum required capability to decode any Representation in the Adaptation Set. The @codecs parameter should be signalled on the Representation level if different from the one on Adaptation Set level.\n-\tThe attributes @maxWidth and @maxHeight shall be present. They are expected be used to signal the used format prior to encoding. This means that they may exceed the actual largest size of any coded Representation in one Adaptation Set.\n-\tThe @width and @height shall be signalled for each Representation (possibly defaulted on Adaptation Set level) and shall match the values of the maximum width and height in the Sample Description box of the contained Representation.\n-\tThe Chroma Format may be signalled. If signalled:\n-\tAn Essential or Supplemental Descriptor shall be used to signal the value by setting the @schemeIdURI attribute to urn:mpeg:mpegB:cicp:MatrixCoefficients as defined ISO/IEC 23001-8 [10] and the @value attribute according to Table 4 of ISO/IEC 23001-8 [10]. The values shall match the values set in the VUI.\n-\tThe signalling shall be on Adaptation Set level.\n-\tThe Colour Primaries and Transfer Function may be signalled. If signalled:\n-\tAn Essential or Supplemental Descriptor shall be used to signal the value by setting the @schemeIdURI attribute to urn:mpeg:mpegB:cicp:ColourPrimaries and urn:mpeg:mpegB:cicp:TransferCharacteristics as defined ISO/IEC 23001-8 [10] and the @value attribute according to Table 4 of ISO/IEC 23001-8 [10]. The values shall match the values set in the VUI.\n-\tThe signalling shall be on Adaptation Set level only, i.e. the value shall not be different for different Representations in one Adaptation Set.\n-\tThe @frameRate shall be signalled on Adaptation Set level.\n-\tRandom Access Points shall be signalled by @startsWithSAP set to 1 or 2.\n-\tA Supplemental Descriptor should be used to signal the projection by setting the @schemeIdURI attribute to urn:mpeg:mpegI:omaf:2017:pf as defined ISO/IEC 23090-2 [13] and the omaf:@projection_type attribute set to 0.\n-\tIf the CoverageInformationBox is present then the Coverage information should be signaled on Adaptation Set. If signalled\n-\ta Supplemental Descriptor shall be used to signal the value by setting the @schemeIdURI attribute to urn:mpeg:mpegI:omaf:2017:cc as defined ISO/IEC 23090-2 [13] and shall match the information provided in the CoverageInformationBox. Specifically:\n-\tThe cc@shape_type shall be present and be set to 1.\n-\tThe cc@view_idc_presence_flag shall not be present.\n-\tExactly one cc.CoverageInfo element shall be present.\n-\tAny cc.CoverageInfo attribute that is not centre_azimuth, centre_elevation,\nazimuth_range and elevation_range, shall not be present.\n-\tThe signalling shall be on Adaptation Set level only, i.e. the value shall not be different for different Representations in one Adaptation Set.\n-\tIf the StereoVideoBox is present then the stereo information should be signaled on Adaptation Set. If signalled\n-\tA FramePacking descriptor shall be used to signal the value by setting the @schemeIdURI attribute to urn:mpeg:mpegB:cicp:VideoFramePackingType as defined ISO/IEC 23008-1 [10] and the @value attribute shall be set to 4.\n-\tThe signalling shall be on Adaptation Set level only, i.e. the value shall not be different for different Representations in one Adaptation Set.\n5.2.3.3.4.1\tIntroduction\nIf multiple Adaptation Sets are offered for the same content in order to permit seamless switching across Representations for a different Viewports, each offered in a different Adaptation Set, then this forms an Ensemble of Adaptation Sets. Note that switching across viewports is not a DASH client functionality, but it is enabled by possible access to the pose and/or viewport information by the DASH client using the 3GPP VR API as shown in Figure 4.6.\n5.2.3.3.4.2\tDefinition and Adaptation Set Signalling\nAn Ensemble is defined as by Adaptation Sets with a Viewpoint Descriptor for which the value of the @schemeIdURI is prefixed as urn:3GPP:vrstream:ve and the actual value is urn:3GPP:vrstream:ve:<id> with <id> an unsigned integer that is identical for all Adaptation Sets in one Ensemble. By using different ids, multiple ensembles may be defined, each defining a different content (for example different camera angles). The value of @value of the descriptor, if present, is either\n-\ta single unsigned integer value that is different for each Adaptation Set in the Ensemble. If this is present, then the spherical region-wise quality ranking (SRQR) descriptor as defined in ISO/IEC 23090-2 [13] for which the value of the @schemeIdURI is prefixed as urn:mpeg:mpegI:omaf:2017:srqr shall be present in the each Adaptation Set, or\n-\ta tuple of integer values, separated by a white-spaces. The semantics and order are as follows:\n-\tcentre_azimuth: Specifies the azimuth of the centre point of the sphere region in units of 2−16 degrees relative to the 3GPP coordinate system for which this Ensemble has been optimized.\n-\tcentre_elevation: Specifies the elevation of the centre point of the sphere region in units of 2−16 degrees relative to the 3GPP coordinate system for which this Ensemble has been optimized.\nthe spherical region-wise quality ranking (SRQR) descriptor as defined in ISO/IEC 23090-2 [13] for which the value of the @schemeIdURI is prefixed as urn:mpeg:mpegI:omaf:2017:srqr may additionally be present for additional information.\nIf the @value attribute is not present, then this Adaptation Set is not optimized for any Viewport. At most one adaptation set without the @value not present shall be present.\nOne Adaptation Set of one Ensemble shall be signalled as the main content. Signaling as main content shall be done by using the Role descriptor with @schemeIdUri=\"urn:mpeg:dash:role:2011\" and @value=\"main\". If for the main Ensemble an Adaptation Set is present for which the @value of the Viewpoint descriptor is not present, then this should be signalled as the main Adaptation Set.\nThe content should be offered such that within an Ensemble, if multiple Adaptation Sets with different centre points are signalled, the one is preferred which has the minimum square distance to actual Viewport center.\n5.2.3.3.4.3\tRepresentation Constraints in an Ensemble\nFor all Representations in an Ensemble, the following shall apply:\n-\tThe identical coverage information shall be present on all Representations in one Ensemble, both on ISO BMFF and elementary stream level.\n-\tThe frame rates of all Representations in one Ensemble shall be identical.\n-\tThe identical stereoscopic information shall be present all Representations in one Ensemble, both on ISO BMFF and elementary stream level.\n5.2.3.3.4.4\tAdaptation Set Constraints in an Ensemble\nFor all Adaptation Sets in an Ensemble, the following shall apply:\n-\tThe @codecs parameter shall be identical for all Adaptation Sets in one Ensemble.\n-\tThe Chroma Format shall be identical for all Adaptation Sets in one Ensemble.\n-\tThe Color Primaries and Transfer Function shall be identical for all Adaptation Sets in one Ensemble.\n-\tThe @frameRate shall be identical for all Adaptation Sets in one Ensemble.\n-\tSegments and subsegments shall be aligned, i.e. @segmentAlignment or @subSegmentAlignment shall be present and shall signal the same unsigned integer value for all Adaptation Sets in an Ensemble.\n-\tCoverage information shall be identical for all Adaptation Sets in one Ensemble.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.2.4\tAdvanced Video Media Profile",
                            "text_content": "This Profile permits to download and stream elementary streams for VR content generated according to the Flexible H.265/HEVC operation point as defined in clause 5.1.6. It also allows unconstrained use of rectangular region-wise packing and monoscopic and stereoscopic spherical video up to 360 degrees are supported. With the presence of region-wise packing, the resolution or quality of the omnidirectional video could be emphasized in certain regions, e.g., according to the user's viewing orientation. In addition, the untransformed sample entry type 'hvc2' is allowed, making it possible to use extractors and get a conforming HEVC bitstream when tile-based streaming is used.\n3GP VR Tracks conforming to this media profile used in the context of the specification shall conform to ISO BMFF [17] with the following further requirements.\nWhen a track is the only track in a file, compatible_brands containing a brand equal to '3vra' in FileTypeBox indicates that the track conforms to this media profile. When a file contains multiple tracks, compatible_brands containing a brand equal to '3vra' in FileTypeBox indicates that at least one of the tracks conforms to this media profile.\n-\tThe video track shall be indicated to conform to this media profile through one or both of FileTypeBox and TrackTypeBox.\n-\tAt least one sample entry type of each sample entry of the track shall be equal to 'resv'.\n-\tThe scheme_type value of SchemeTypeBox in the RestrictedSchemeInfoBox shall be 'podv', and of all instances of CompatibleSchemeTypeBox defined in ISO/IEC 23090-2 [13] in the same RestrictedSchemeInfoBox shall include at least one of the scheme_type values 'erpv' and 'ercm'.\n-\tThe untransformed sample entry type shall be equal to 'hvc1' or 'hvc2' as defined in ISO/IEC 14496-15 [9].\nNote:\tWhen tile-based streaming is used, each tile is stored as a VR Track with an untransformed sample entry type equal to 'hvc1' and further VR Tracks with an untransformed sample entry type equal to 'hvc2' contain extractors that combine tiles from 'hvc1' VR Tracks leading to a conforming HEVC bitstream.\n-\tWhen the untransformed sample entry type is 'hvc2', the track shall include one or more 'scal' track references.\n-\tLHEVCConfigurationBox as defined in ISO/IEC 14496-15 [9] shall not be present in VisualSampleEntry.\n-\tHEVCConfigurationBox as defined in ISO/IEC 14496-15 [9] in VisualSampleEntry shall be added such that it does not contradict to the Bitstream requirements of the Flexible H.265/HEVC operation point in clause 5.1.6.\n-\tThe track_not_intended_for_presentation_alone flag of the TrackHeaderBox may be used to indicate that a track is not intended to be presented alone.\n-\tThe Track Header Box ('tkhd') shall obey the following constraints:\n-\tThe width and height fields for a visual track shall specify the track's visual presentation size as fixed-point 16.16 values expressed in on a uniformly sampled grid (commonly called square pixels) (of the decoded texture signal)\n-\tThe Video Media Header ('vmhd') shall obey the following constraints:\n-\tThe value of the version field shall be set to '0'.\n-\tThe value of the graphicsmode field shall be set to '0'.\n-\tThe value of the opcolor field shall be set to {'0', '0', '0'}.\n-\tThe Sample Description Box ('stsd') obeys the following constraints:\n-\tA visual sample entry shall be used.\n-\tThe box shall include a NAL Structured Video Parameter Set.\n-\twidth and height field shall correspond to the cropped horizontal and vertical sample counts provided in the Sequence Parameter Set of the track.\n-\tIt shall contain a Decoder Configuration Record which signals the Profile, Level, and other parameters of the video track.\n-\tThe Colour Information Box ('colr') should be present. If present, it shall signal the colour_primaries, transfer_characteristics and matrix_coeffs applicable to all the bitstreams associated with this sample entry.\n-\tA ProjectionFormatBox as defined in ISO/IEC 23090-2 [13] shall be present in the sample entry with projection_type equal to 0 or 1.\n-\tIf the content contained in the Bitstream in the track does not cover the entire sphere, the CoverageInformationBox as defined in ISO/IEC 23090-2 [13] should be present.\n-\tIf the video content contained in the Bitstream in the track is a subset of the entire video content carried in the file and the CoverageInformationBox as defined in ISO/IEC 23090-2 [13] is present, the following restrictions apply:\n-\tIf the equirectangular projection is used then,\n-\tThe coverage_shape_type shall be set to 1, i.e. the sphere region is specified by two azimuth circles and two elevation circles.\n-\tThe num_regions value shall be set to 1.\n-\tIf the cubemap projection is used, then one of the two following options applies:\na)\tThe coverage_shape_type shall be set to 1, i.e. the sphere region is specified by two azimuth circles and two elevation circles and the num_regions value shall be set to 1, or\nb)\tThe coverage_shape_type shall be set to 0, i.e. the sphere region is specified by four great circles.\n-\tThe view_idc_presence_flag shall be set to 0.\n-\tThe default_view_idc shall be set to 0 or 3.\n-\tIf the content contained in the Bitstream in the track includes the region-wise packing SEI message (payloadType equal to 155), then the RegionWisePackingBox as defined in ISO/IEC 23090-2 [13] shall be present. It shall signal the same information that is included in the region-wise packing SEI message(s) in the elementary stream.\n-\tIf the content contained in the Bitstream in the track includes the frame packing arrangement SEI message (payloadType equal to 45) in the video stream, the StereoVideoBox shall be present in the sample entry applying to the sample containing the picture. When StereoVideoBox is present, it shall signal the frame packing format that is included in the frame packing arrangement SEI message(s) in the elementary stream.\nIf all Representations in an Adaptation Set conform to the requirements in clause 5.2.4.3.2 and the Adaptation Set conforms to the requirements in clause 5.2.4.3.3, then the @profiles parameter in the Adaptation Set may signal conformance to this Operation Point by using \"urn:3GPP:vrstream:mp:video:advanced\".\nIf a VR Track conforming to this media profile is included in a DASH Representation, the Representation use movie fragments and therefore, the following additional requirements apply:\n-\tThe value of the duration field in the Media Header Box ('mdhd') shall be set to a value of '0'.\n-\tThe value of the duration field in the Movie Header Box ('mvhd') shall be set to a value of '0'.\n-\tThe value of the duration field in the Track Header Box ('tkhd') shall be set to a value of '0'.\n-\tMovie Fragment Header Boxes ('mfhd') may contain sequence_number values that are not sequentially numbered.\n-\tAny Segment Index Box ('sidx'), if present, shall obey the additional constraints:\n-\tthe timescale field shall have the same value as the timescale field in the Media Header Box ('mdhd') within the same track;\n-\tthe reference_ID field shall be set to the track_ID of the ISO Media track as defined in the Track Header Box ('tkhd').\n-\tThe Sample Table Box ('stbl') shall obey the following constraints:\n-\tThe entry_count field of the Sample-to-Chunk Box ('stsc') shall be set to '0'.\n-\tBoth the sample_size and sample_count fields of the Sample Size Box ('stsz') box shall be set to zero ('0'). The sample_count field of the Sample Size Box ('stz2') box shall be set to zero ('0'). The actual sample size information can be found in the Track Fragment Run Box ('trun') for the track.\nNOTE 1:\tThis is because the Movie Box ('moov') contains no media samples.\n-\tThe entry_count field of the Chunk Offset Box ('stco') shall be set to '0'.\n-\tThe same projection format shall be used on all Representations in one Adaptation Set.\n-\tThe same frame packing format shall be used on all Representations in one Adaptation Set.\n-\tThe same coverage information shall be used on all Representations in one Adaptation Set.\n-\tThe same spatial resolution shall be used on all Representations in one Adaptation Set.\n-\tWhen @dependencyId is used, the values of profiles of the respective dependent and complementary Representations shall be the same.\nWhen the MPD contains a Representation with a track for which the untransformed sample entry type is equal to 'hvc2', the following applies:\n-\tEither the Representations carrying a track conforming to the media profile track constraints with the untransformed sample entry type equal to 'hvc2' shall contain @dependencyId listing all dependent Representations that carry a track conforming to the media profile track constraints with the untransformed sample entry type equal to 'hvc1' or a Preselection property descriptor shall be present and constrained as follows:\n-\tThe Main Adaptation Set shall contain a Representation carrying a track conforming to the media profile track constraints with the untransformed sample entry type equal to 'hvc2'.\n-\tThe Partial Adaptation Sets shall contain Representations each carrying a track conforming to the media profile track constraints with the untransformed sample entry type equal to 'hvc1'.\nNOTE 2:\tWhen using the Preselection property descriptor, the number of Representations for carrying tracks with the untransformed sample entry type equal to 'hvc2' is typically smaller than when using @dependencyId. However, the use of @dependencyId might be needed for encrypted video tracks.\n-\tThe Initialization Segment of the Representation that contains @dependencyId or belongs to the Main Adaptation Set is constrained as follows:\n-\tTracks conform to the media profile track constraints.\n-\tThe track corresponding to the untransformed sample entry type equal to 'hvc2' refers to the tracks indicated in the TrackReferenceBox of the Initialization Segment.\nNOTE 3:\tWhen Preselection is used, the sequence_number integer values are not required to be processed and therefore the concatenation of the Subsegments (of the different Representations of the Adaptation Sets of ﻿a Preselection) in any order results in a conforming file.\nNOTE 4:\tThe conforming Segment sequence formed on the basis of the Preselection property descriptor or by resolving @dependencyId attribute(s) as specified in ISO/IEC 23009-1 [18] and the track_ID value of the track with the untransformed sample entry type equal to 'hvc2' produces the HEVC bitstream which conforms to H.265/HEVC Flexible Operation Point.\nWhen switching or accessing Representations at each segment or subsegment is relevant, the following DASH profiles include sufficient constraints:\n-\tISO Base Media File Format Live profile: urn:mpeg:dash:profile:isoff-live:2011\n-\tISO Base Media File Format Main profile: urn:mpeg:dash:profile:isoff-main:2011\nWhen low latency considerations are relevant, the following DASH profiles provide tools to support efficient low latency services:\n-\tISO Base Media File Format On Demand profile: urn:mpeg:dash:profile:isoff-on-demand:2011\n-\tISO Base Media File Format Broadcast TV profile: urn:mpeg:dash:profile:isoff-broadcast:2015\nFor all Representation in an Adaptation Set, the following shall apply:\n-\tThe identical coverage information shall be present on all Representations in one Adaptation Set, both on ISO BMFF and elementary stream level.\n-\tThe frame rates of all Representations in one Adaptation Set shall be identical.\n-\tThe identical region-wise packing information shall be present all Representations in one Adaptation Set, both on ISO BMFF and elementary stream level.\n-\tThe identical stereoscopic information shall be present all Representations in one Adaptation Set, both on ISO BMFF and elementary stream level.\nFor an Adaptation Set, the following constraints apply:\n-\tThe @codecs parameter shall be present on Adaptation Set level and shall signal the maximum required capability to decode any Representation in the Adaptation Set. The @codecs parameter should be signalled on the Representation level if different from the one on Adaptation Set level.\n-\tThe attributes @maxWidth and @maxHeight shall be present. They are expected be used to signal the decoded texture format of the original signal. This means that they may exceed the actual largest size of any coded Representation in one Adaptation Set.\n-\tThe @width and @height shall be signalled for each Representation (possibly defaulted on Adaptation Set level) and shall match the values of the maximum width and height in the Sample Description box of the contained Representation.\n-\tThe Chroma Format may be signalled. If signalled:\n-\tAn Essential or Supplemental Descriptor shall be used to signal the value by setting the @schemeIdURI attribute to urn:mpeg:mpegB:cicp:MatrixCoefficients as defined ISO/IEC 23001-8 [10] and the @value attribute according to Table 4 of ISO/IEC 23001-8 [10]. The values shall match the values set in the VUI.\n-\tThe signalling shall be on Adaptation Set level.\n-\tThe Colour Primaries and Transfer Function may be signalled. If signalled:\n-\tAn Essential or Supplemental Descriptor shall be used to signal the value by setting the @schemeIdURI attribute to urn:mpeg:mpegB:cicp:ColourPrimaries and urn:mpeg:mpegB:cicp:TransferCharacteristics as defined ISO/IEC 23001-8 [10] and the @value attribute according to Table 4 of ISO/IEC 23001-8 [10]. The values shall match the values set in the VUI.\nThe signalling shall be on Adaptation Set level only, i.e. the value shall not be different for different Representations in one Adaptation Set.\n-\tThe @frameRate shall be signalled on Adaptation Set level.\n-\tRandom Access Points shall be signalled by @startsWithSAP set to 1 or 2.\n-\tAn Essential Descriptor shall be used to signal the projection by setting the @schemeIdURI attribute to urn:mpeg:mpegI:omaf:2017:pf as defined ISO/IEC 23090-2 [13] and the omaf:@projection_type attribute set to 0 or 1.\n-\tIf the CoverageInformationBox is present, then the Coverage information should be signaled on Adaptation Set. If signalled:\n-\tA Supplemental Descriptor shall be used to signal the value by setting the @schemeIdURI attribute to urn:mpeg:mpegI:omaf:2017:cc as defined ISO/IEC 23090-2 [13] and shall match the information provided in the CoverageInformationBox. Specifically,\n-\tthe cc@shape_type shall be present and be set to 0 or 1.\n-\tthe cc@view_idc_presence_flag shall not be present.\n-\texactly one cc.CoverageInfo element shall be present.\n-\tany cc.CoverageInfo attribute that is not centre_azimuth, centre_elevation, azimuth_range and elevation_range, shall not be present.\n-\tThe signalling shall be on Adaptation Set level only, i.e. the value shall not be different for different Representations in one Adaptation Set.\n-\tIf the StereoVideoBox is present, then the stereo information should be signaled on Adaptation Set. If signalled:\n-\ta FramePacking descriptor shall be used to signal the value by setting the @schemeIdURI attribute to urn:mpeg:mpegB:cicp:VideoFramePackingType as defined ISO/IEC 23008-1 [10] and the @value attribute shall be set to 4.\n-\tThe signalling shall be on Adaptation Set level only, i.e. the value shall not be different for different Representations in one Adaptation Set.\n-\tThe following applies for the use of @mimeType:\n-\t@mimeType of the Main Adaptation Set shall include the profiles parameter '3vra'.\n-\tWhen Preselection is used, the value of profiles of the main Adaptation Set shall be the same as the value of profiles of its partial Adaptation Sets.\n-\tWhen Preselection is used, the following applies:\n-\tThe value of @subsegmentAlignment in the Main Adaptation Set shall be an unsigned integer and equal to the value of @subsegmentAlignment of the each associated Partial Adaptation Set.\n-\tThe value of @segmentAlignment in the Main Adaptation Set shall be an unsigned integer and equal to the value of @segmentAlignment of the each associated Partial Adaptation Set.\nIf multiple Adaptation Sets are offered for the same content which have emphasized quality regions for different viewports, in order to provide signaling information for switching across Viewports, the spherical region-wise quality ranking (SRQR) descriptor as defined in ISO/IEC 23090-2 [13] for which the value of the @schemeIdURI is prefixed as urn:mpeg:mpegI:omaf:2017:srqr shall be present in the each Adaptation Set with following restrictions:\n-\tThe sphRegionQuality@view_idc_presence_flag shall be set to 0.\n-\tThe sphRegionQuality@default_view_idc shall be set to 0 or 3.\n-\tThe value of sphRegionQualityll.qualityInfo@quality_ranking shall be greater than 0.\nFor all Representations in multiple Adaptation Sets for switching accross Viewports, the following shall apply:\n-\tThe identical coverage information shall be present on all Representations, both on ISO BMFF and elementary stream level.\n-\tThe frame rates of all Representations in Adaptation Sets shall be identical.\n-\tThe identical stereoscopic information shall be present all Representations, both on ISO BMFF and elementary stream level.\nFor all Adaptation Sets with SRQR descriptors for switching across Viewports, the following shall apply:\n-\tThe @codecs parameter shall be identical for all Adaptation Sets.\n-\tThe Chroma Format shall be identical for all Adaptation Sets.\n-\tThe Colour Primaries and Transfer Function shall be identical for all Adaptation Sets.\n-\tThe @frameRate shall be identical for all Adaptation Sets.\n-\tSegments and subsegments shall be aligned, i.e. @segmentAlignment or @subSegmentAlignment shall be present and shall signal the same unsigned integer value for all Adaptation Sets.\n-\tCoverage information shall be identical for all Adaptation Sets.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "6\tAudio",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "6.1\tAudio Operation Points",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.1.1\tDefinition of Operation Point",
                            "text_content": "For the purpose to define interfaces to a conforming audio decoder, audio operation points are defined. In this case the following definitions hold:\n-\tOperation Point: A collection of discrete combinations of different content formats and VR specific rendering metadata, etc. and the encoding format.\n-\tReceiver: A receiver that can decode and render any bitstream that is conforming to a certain Operation Point.\n-\tBitstream: A audio bitstream that conforms to an audio format.\nFigure 6.1 -1 illustrates the various audio operation points within a telecommunication system, highlighting the critical components involved in signal processing and transmission. The figure showcases the microphone, which captures sound, and the audio codec, responsible for converting analog audio signals into digital format. The digital signal then travels through the network, passing through multiple operation points such as the network interface controller (NIC), the router, and the switch. These components ensure the efficient transmission of audio data over the network. The figure also highlights the importance of error correction techniques, such as Forward Error Correction (FEC), to maintain the quality of the audio signal during transmission.\nFigure 6.1-1: Audio Operation Points\nThis clause focuses on the interoperability point to a media decoder as indicated in Figure 6.1-1. This clause does not deal with the access engine and file parser which addresses aspects how the audio bitstream is delivered.\nIn all audio operation points, the VR Presentation can be rendered using a single media decoder which provides decoded PCM signals and rendering metadata to the audio renderer.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.1.2\tParameters of Audio Operation Point",
                            "text_content": "This clause defines the potential parameters of Audio Operation Points. This includes the detailed audio decoder requirements and audio rendering metadata. The requirements are defined from the perspective of the audio decoder and renderer.\nParameters for an Audio Operation Point include:\n-\tthe audio decoder that the bitstream needs to conform to,\n-\tthe mandated or permitted rendering data that is included in the audio bitstream.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.1.3\tSummary of Audio Operation Points",
                            "text_content": "Table 6.1-1 provides an informative overview of the Audio Operating Points. The detailed, normative specification for each audio operating point is subsequently provided in the referenced clause.\nTable 6.1-1: Overview of OMAF operation points for audio (informative)\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.1-1: Overview of OMAF operation points for audio (informative)",
                                    "table number": 6,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "6.1.4\t3GPP MPEG-H Audio Operation Point",
                            "text_content": "The 3GPP MPEG-H Audio Operation Point fulfills the requirements to support 3D audio and is specified in ISO/IEC 23090-2 [13], clause 10.2.2. Channels, Objects and First/Higher-Order Ambisonics (FOA/HOA) are supported, as well as combinations of those. The Operation Point is based on MPEG-H 3D Audio [19].\nA bitstream conforming to the 3GPP MPEG-H Audio Operation Point shall conform to the requirements in clause 6.1.4.2.\nA receiver conforming to the 3GPP MPEG-H Audio Operation Point shall support decoding and rendering a Bitstream conforming to the 3GPP MPEG-H Audio Operation Point. Detailed receiver requirements are provided in clause 6.1.4.3.\nThe audio stream shall comply with the MPEG-H 3D Audio Low Complexity (LC) Profile, Levels 1, 2 or 3 as defined in ISO/IEC 23008-3, clause 4.8 [19]. The values of the mpegh3daProfileLevelIndication for LC Profile Levels 1, 2 and 3 are \"0x0B\", \"0x0C\" and \"0x0D\", respectively, as specified in ISO/IEC 23008-3 [19], clause 5.3.2.\nAudio encapsulation shall be done according to ISO/IEC 23090-2 [12], clause 10.2.2.2.\nAll Low Complexity Profile and Levels restrictions specified in ISO/IEC 23008-3 [19], clause 4.8.2 shall apply. The constraints on input and output configurations are provided in Table 3 — \"Levels and their corresponding restrictions for the Low Complexity Profile\", of ISO/IEC 23008-3 [19]. This includes the following for Low Complexity Profile Level 3:\n-\tMaximum number of core coded channels (in compressed data stream): 32,\n-\tMaximum number of decoder processed core channels: 16,\n-\tMaximum number of loudspeaker output channels: 12\n-\tMaximum number of decoded objects: 16\n-\tMaximum HOA order: 6\nMPEG-H Audio sync samples contain Immediate Playout Frames (IPFs), as specified in ISO/IEC 23008-3, clause 20.2 [19] and shall follow the requirements specified in ISO/IEC 23090-2 [12], clause 10.2.2.3.1.\nA receiver supporting the 3GPP MPEG-H Audio Operation Point shall fulfill all requirements specified in the remainder of clause 6.1.4.3.\nThe receiver shall be capable of decoding MPEG-H Audio LC Profile Level 1, Level 2 and Level 3 bitstreams as specified in ISO/IEC 23008-3, clause 4.8 [19] with the following relaxations:\n-\tThe Immersive Renderer defined in ISO/IEC 23008-3 [19], clause 11 is optional.\n-\tThe carriage of generic data defined in ISO/IEC 23008-3 [19], clause 14.7 is optional and thus MHAS packets of the type PACTYP_GENDATA are optional and the decoder may ignore packets of this type.\nThe decoder shall read and process MHAS packets of the following types in accordance with ISO/IEC 23008-3 [19], clause 14:\nPACTYP_SYNC,\nPACTYP_MPEGH3DACFG,\nPACTYP_AUDIOSCENEINFO,\nPACTYP_AUDIOTRUNCATION,\nPACTYP_MPEGH3DAFRAME,\nPACTYP_USERINTERACTION,\nPACTYP_LOUDNESS_DRC,\nPACTYP_EARCON,\nPACTYP_PCMCONFIG, and\nPACTYP_PCMDATA.\n\nThe decoder may read and process MHAS packets of the following types:\nPACTYP_SYNCGAP,\nPACTYP_BUFFERINFO,\nPACTYP_MARKER and\nPACTYP_DESCRIPTOR.\n\nOther MHAS packets may be present in an MHAS elementary stream, but may be ignored.\nThe Earcon metadata shall be processed and applied as described in ISO/IEC 23008-3 [19], clause 28.\nThe audio decoder is able to start decoding a new audio stream at every random access point (RAP). As defined in clause 6.1.4.2, the sync sample (RAP) contains the configuration information (PACTYP_MPEGH3DACFG and PACTYP_AUDIOSCENEINFO) that is used to initialize the audio decoder. After initialization, the audio decoder reads encoded audio frames (PACTYP_MPEGH3DAFRAME) and decodes them.\nTo optimize startup delay at random access, the information from the MHAS PACTYP_BUFFERINFO packet should be taken into account. The input buffer should be filled at least to the state indicated in the MHAS PACTYP_BUFFERINFO packet before starting to decode audio frames.\nNOTE 1:\tIt may be necessary to feed several audio frames into the decoder before the first decoded PCM output buffer is available, as described in ISO/IEC 23008-3 [19], clause 5.5.6.3 and clause 22.\nIt is recommended that, at random access into an audio stream, the receiving device performs a 100 ms fade-in on the first PCM output buffer that it receives from the audio decoder.\nNOTE 2:\tThe MPEG-H 3D Audio Codec can output the original input samples without any inherent fade-in behavior. Thus, the receiving device needs to appropriately handle potential signal discontinuities, resulting from the original input signal, by fading in at random access into an audio stream.\nIf the decoder receives an MHAS stream that contains a configuration change, the decoder shall perform a configuration change according to ISO/IEC 23008-3 [19], clause 5.5.6. The configuration change can, for instance, be detected through the change of the MHASPacketLabel of the packet PACTYP_MPEGH3DACFG compared to the value of the MHASPacketLabel of previous MHAS packets.\nIf MHAS packets of type PACTYP_AUDIOTRUNCATION are present, they shall be used as described in ISO/IEC 23008-3 [19], clause 14.\nThe Access Unit that contains the configuration change and the last Access Unit before the configuration change may contain a truncation message (PACTYP_AUDIOTRUNCATION) as defined in ISO/IEC 23008-3 [19], clause 14. The MHAS packet of type PACTYP_AUDIOTRUNCATION enables synchronization between video and audio elementary streams at program boundaries. When used, sample-accurate splicing and reconfiguration of the audio stream are possible.\nThe receiver shall be capable of simultaneously receiving at least 3 MHAS streams. The MHAS streams can be simultaneously decoded or combined into a single stream prior to the decoder, by utilizing the field mae_bsMetaDataElementIDoffset in the Audio Scene Information as described in ISO/IEC 23008-3 [19], clause 14.6.\n6.1.4.3.6.1\tGeneral\nThe 3GPP MPEG-H Audio Operation Point builds on the MPEG-H 3D Audio codec, which includes rendering to loudspeakers, binaural rendering and also provides an interface for external rendering. Legacy binaural rendering using fixed loudspeaker setups can be supported by using loudspeaker feeds as output of the decoder.\n6.1.4.3.6.2\tRendering to Loudspeakers\nRendering to loudspeakers shall be done according to ISO/IEC 23008-3 [19] using the interface for local loudspeaker setup and rendering as defined in ISO/IEC 23008-3 [19], clause 17.3.\nNOTE:\tISO/IEC 23008-3 [19] specifies rendering to predefined loudspeaker setups as well as rendering to arbitrary setups.\n6.1.4.3.6.3\tBinaural Rendering of MPEG-H 3D Audio\n6.1.4.3.6.3.1\tGeneral\nMPEG-H 3D Audio specifies methods for binauralizing the presentation of immersive content for playback via headphones, as is needed for omnidirectional media presentations. MPEG-H 3D Audio specifies a normative interface for the user's viewing orientation and permits low-complexity, low-latency rendering of the audio scene to any user orientation.\nThe binaural rendering of MPEG-H 3D Audio shall be applied as described in ISO/IEC 23008-3 [19], clause 13 according to the Low Complexity Profile and Levels restrictions for binaural rendering specified in ISO/IEC 23008-3 [19], clause 4.8.2.2.\n6.1.4.3.6.3.2\tHead Tracking Interface\nFor binaural rendering using head tracking the useTrackingMode flag in the BinauralRendering() syntax element shall be set to 1, as described in ISO/IEC 23008-3 [19], clause 17.4. This flag defines if a tracker device is connected and the binaural rendering shall be processed in a special headtracking mode, using the scene displacement values (yaw, pitch and roll).\nThe values for the scene displacement data shall be sent using the interface for scene displacement data specified in ISO/IEC 23008-3 [19], clause 17.9. The syntax of mpegh3daSceneDisplacementData() interface provided in ISO/IEC 23008-3 [19], clause 17.9.3 shall be used.\n6.1.4.3.6.3.3\tSignaling and processing of diegetic and non-diegetic audio\nThe metadata flag fixedPosition in SignalGroupInformation() indicates if the corresponding audio signals are updated during the processing of scene-displacement angles. In case the flag is equal to one, the positions of the corresponding audio signals are not updated during the processing of scene displacement angles.\nChannel groups for which the flag gca_directHeadphone is set to \"1\" in the mpegh3da_getChannelMetadata()sytax element are routed to left and right output channel directly and are excluded from binaural rendering using scene displacement data (non-diegetic content). Non-diegetic content may have stereo or mono format. For mono, the signal is mixed to left and right headphone channel with a gain factor of 0.707.\n6.1.4.3.6.3.4\tHRIR/BRIR Interface processing\nThe interface for binaural room impulse responses (BRIRs) specified in ISO/IEC 23008-3 [19], clause 17.4 shall be used for external BRIRs and HRIRs. The HRIR/BRIR data for the binaural rendering can be fed to the decoder by using the syntax element BinauralRendering(). The number of BRIR/HRIR pairs in each BRIR/HRIR set shall correspond to the number indicated in the relevant level-dependent row in Table 9 - \"The binaural restrictions for the LC profile\" of ISO/IEC 23008-3 [19] according to the Low Complexity Profile and Levels restrictions in ISO/IEC 23008-3 [19], clause 4.8.2.2.\nThe measured BRIR positions are passed to the mpegh3daLocalSetupInformation(), as specified in ISO/IEC 23008-3 [19], clause 4.8.2.2. Thus, all renderer stages are set to the target layout that is equal to the transmitted channel configuration. As one BRIR is available per regular input channel, the Format Converter can be passed through in case regular input channel positions are used. Preferably, the BRIR measurement positions for standard target layouts 2.0, 5.1, 10.2 and 7.1.4 should be provided.\n6.1.4.3.6.4\tRendering with External Binaural Renderer\nMPEG-H 3DA provides the output interfaces for the delivery of un-rendered channels, objects, and HOA content and associated metadata as specified in clause 6.1.4.3.6.5. External binaural renderers can connect to this interface e.g. for playback of head-tracked audio via headphones. An example of such external binaural renderer that connects to the external rendering interface of MPEG-H 3DA is specified in Annex B.\n6.1.4.3.6.5\tExternal Renderer Interface\nISO/IEC 23008-3 [19], clause 17.10 specifies the output interfaces for the delivery of un-rendered channels, objects, and HOA content and associated metadata. For connecting to external renderers, a receiver shall implement the interfaces for object output, channel output and HOA output as specified in ISO/IEC 23008-3 [19], clause 17.10, including the additional specification of production metadata defined in ISO/IEC 23008-3 [19], clause 27. Any external renderer should apply the metadata provided in this interface and related audio data in the same manner as if MPEG-H internal rendering is applied:\n-\tCorrect handling of loudness-related metadata in particular with the aim of preserving intended target loudness\n-\tPreserving artistic intent, such as applying transmitted Downmix and HOA Rendering matrices correctly\n-\tRendering spatial attributes of objects appropriately (position, spatial extent, etc.)\nNOTE:\tThe external example binaural renderer in Annex B only handles a subset of the parameters to illustrate the use of the output interface. Alternative external binaural renderers are expected to apply and handle the metadata provided in this interface and related audio data in the same manner as if internal rendering is applied.\nIn this interface the PCM data of the channels and objects interfaces is provided through the decoder PCM buffer, which first contains the regular rendered PCM signals (e.g. 12 signals for a 7.1+4 setup). Subsequently  additional signals carry the PCM data of the originally transmitted channel representation. These are followed by  signals carrying the PCM data of the un-rendered output objects. Then additional signals carry the  HOA audio PCM data which number is indicated in the HOA metadata interface via the HOA order (e.g. 16 signals for HOA order 3). The HOA audio PCM data in the HOA output interface is provided in the so-called Equivalent Spatial Domain (ESD) representation. The conversion from the HOA domain into the ESD representation and vice versa is described in ISO/IEC 23008-3 [19], Annex C.5.1.\nThe metadata for channels, objects, and HOA is available once per frame and their syntax is specified in mpegh3da_getChannelMetadata(), mpegh3da_getObjectAudioAndMetadata(), and mpegh3da_getHoaMetadata() respectively. The metadata and PCM data shall be aligned for an external renderer to match each metadata element with the respective PCM frame.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.2\tAudio Media Profiles",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.2.1\tIntroduction and Overview",
                            "text_content": "This clause defines the media profiles for audio. Media profiles include specification on the following:\n-\tElementary stream constraints based on the audio operation points defined in clause 6.1.\n-\tFile format encapsulation constraints and signalling including capability signalling. The defines to a 3GPP VR Track as defined above.\n-\tDASH Adaptation Set constraints and signalling including capability signalling. This defines a DASH content format profile.\nTable 6.2-1 provides an overview of the Media Profiles in defined in the remainder of clause 6.2.\nTable 6.2-1 Audio Media Profiles\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.2-1 Audio Media Profiles",
                                    "table number": 7,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "6.2.2\tOMAF 3D Audio Baseline Media Profile",
                            "text_content": "MPEG-H 3D Audio [19] specifies coding of immersive audio material and the storage of the coded representation in an ISO BMFF track. The MPEG-H 3D Audio decoder has a constant latency, see Table 1 — \"MPEG-H 3DA functional blocks and internal processing domain\", of ISO/IEC 23008-3 [19]. With this information, content authors could synchronize audio and video portions of a media presentation, e.g. ensuring lip-synch.\nISO BMFF integration for this profile is provided following the requirements and recommendations in ISO/IEC 23090-2 [12], clause 10.2.2.3.\n3GP VR Tracks conforming to this media profile used in the context of the specification shall conform to the ISO BMFF [17] with the following further requirements:\n-\tThe audio track shall comply to the Bitstream requirements and recommendations for the Operation Point as defined in clause 6.1.4.\n-\tThe sample entry 'mhm1' shall be used for encapsulation of MHAS packets into ISO BMFF files, per ISO/IEC 23008-3 [19], clause 20.6.\n-\tAll ISO Base Media File Format constraints specified in ISO/IEC 23090-2 [12], clause 10.2.2.3 shall apply.\n-\tISO BMFF Tracks shall be encoded following the requirements in ISO/IEC 23090-2 [12], clause 10.2.2.3.1.\nA configuration change takes place in an audio stream when the content setup or the Audio Scene Information changes (e.g., when changes occur in the channel layout, the number of objects etc.), and therefore new PACTYP_MPEGH3DACFG and PACTYP_AUDIOSCENEINFO packets are required upon such occurrences. A configuration change usually happens at program boundaries, but it may also occur within a program.\nConfiguration change constraints specified in ISO/IEC 23090-2 [12], clause 10.2.2.3.2 shall apply.\nThe multi-stream-enabled MPEG-H Audio System is capable of handling Audio Programme Components delivered in several different elementary streams (e.g., the main MHAS stream containing one complete audio main, and one or more auxiliary MHAS streams, containing different languages and audio descriptions). The MPEG-H Audio Metadata information (MAE) allows the MPEG-H Audio Decoder to correctly decode several MHAS streams.\nThe sample entry 'mhm2' shall be used in cases of multi-stream delivery, i.e., the MPEG-H Audio Scene is split into two or more streams for delivery as described in ISO/IEC 23008-3 [19], clause 14.6. All constraints for file formats using the sample entry 'mhm2' specified in ISO/IEC 23090-2 [12], clause 10.2.2.3.3 shall apply.\nDASH Integration is provided following the requirements and recommendations in ISO/IEC 23090-2 [12], clause B.2.1. All constraints in ISO/IEC 23090-2 [12], clause B.2.1 shall apply.\nAn instantiation of an OMAF 3D Audio Baseline Profile in DASH should be represented as one Adaptation Set. If so the Adaptation Set should provide the following signalling according to ISO/IEC 23090-2 [12] and ISO/IEC 23008-3 [19], clause 21 as shown in Table 6.2-2.\nTable 6.2-2: MPEG-H Audio MIME parameter according to RFC 6381 and ISO/IEC 23008-3\n\nMapping of relevant MPD elements and attributes to MPEG-H Audio as well as the Preselection Element and Preselection descriptor are specified in ISO/IEC 23090-2 [12], clause B.2.1.2.\nMPEG-H 3D Audio enables seamless bitrate switching in a DASH environment with different Representations (i.e., bit streams encoded at different bitrates) of the same content, i.e., those Representations are part of the same Adaptation Set.\nIf the decoder receives a DASH Segment of another Representation of the same Adaptation Set, the decoder shall perform an adaptive switch according to ISO/IEC 23008-3 [19], clause 5.5.6.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.2-2: MPEG-H Audio MIME parameter according to RFC 6381 and ISO/IEC 23008-3",
                                    "table number": 8,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "title": "7\tMetadata",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "7.1\tPresentation without Pose Information to 2D Screens",
                    "description": "",
                    "summary": "",
                    "text_content": "In several devices, the VR sensor providing pose information may not be available or may be disabled by the user and the presentation of the VR360 presentation is on a 2D screen. In this case, the receiver needs to rely on other information to determine a proper rendering of the VR360 presentation that is to be presented at a specific media time.\nFor this purpose, a VR Media Presentation may include the recommended viewport metadata. The recommended viewport metadata may be encapsulated in a timed metadata track in either a file or a DASH representation.\nReceivers presenting on a 2D screen and not implementing a viewport sensor should implement the recommended viewport processing to process the recommended viewport metadata and to render VR video or audio accordingly.\nReceivers implementing a viewport sensor may implement the recommended viewport processing.\nIf the viewport sensor is not implemented at the receiver, or if the viewport sensor is disabled (permanently or temporarily), the receiver should process the recommended viewport metadata, if present.\nIf the viewport metadata is provided in the VR Media Presentation and if processing is supported and applied, then the Receiver shall render the viewport indicated metadata.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "8\tVR Presentation",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "8.1\tDefinition",
                    "description": "",
                    "summary": "",
                    "text_content": "A VR presentation provides an omnidirectional audio-visual experience. A 3GPP VR Presentation is a VR Presentation for which each of the VR Tracks contained in a VR Presentation are aligned to the 3GPP DOF Reference System as defined in clause 4 and are time-synchronized.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "8.2\t3GPP VR File",
                    "description": "",
                    "summary": "",
                    "text_content": "A 3GPP VR Presentation may be provided in an ISO BMFF conforming file. A 3GPP VR File is defined as a file that conforms to ISO BMFF [17] and for which at least two tracks are present whereby:\n-\tat least one track conforms to a 3GPP VR Track according to a video media profile defined in clause 5,\n-\tat least one track conforms to a 3GPP VR Track according to an audio media profile defined in clause 6.\nConformance to a 3GPP VR File may be signalled with a compatibility brand '3gvr'.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "8.3\t3GPP VR DASH Media Presentation",
                    "description": "",
                    "summary": "",
                    "text_content": "A 3GPP VR Presentation may be provided in DASH Media Presentation. A 3GPP VR DASH Media Presentation is defined as a DASH Media Presentation that conforms to 3GP DASH and for which at least two Adaptation Sets are present whereby\n-\tat least one Adaptation Set conforms to an Adaptation Set for a video media profile defined in clause 5,\n-\tat least one Adaptation Set conforms to an Adaptation Set for an audio media profile defined in clause 6.\nConformance to a 3GPP VR File may be signalled with an MPD @profiles parameter'urn:3gpp:vrstream:presentation'.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "9\tVR Metrics",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "9.1\tGeneral",
                    "description": "",
                    "summary": "",
                    "text_content": "VR metrics is a functionality where the client collects specific quality-related metrics during a session. These collected metrics can then be reported back to a network side node for further analysis. The metric functionality is based on the QoE metrics concept in 3GP-DASH [8], but further extended to also cover VR-specific metrics. A VR client supporting VR metrics shall support all metrics listed in clause 9.3, and shall handle metric configuration and reporting as specified in clauses 9.4 and 9.5.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "9.2\tVR Client Reference Architecture",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "9.2.1\tArchitecture",
                            "text_content": "The client reference architecture for VR metrics, shown below in Figure 9.2.1-1, is based on the client architecture in Figure 4.3-1. It also contains a number of observation points where specific metric-related information can be made available to the Metrics Collection and Computation (MCC) function. The MCC can use and combine information from the different observation points to calculate more complex metrics.\nNote that these observation points are only defined conceptually, and might not always directly interface to the MCC. For instance, an implementation might relay information from the actual observation points to the MCC via the VR application. It is also possible that the MCC is not separately implemented, but simply included as an integral part of the VR application.\nAlso note that in this version of this specification not all of the described observation points are necessarily used to produce VR metrics.\nFigure 9.2.1-1 presents a client reference architecture for VR metrics, illustrating the integration of various components to measure and analyze virtual reality performance. The figure showcases the central processing unit (CPU), graphics processing unit (GPU), and input/output (I/O) devices, emphasizing their role in handling VR data. Additionally, it highlights the importance of network connectivity, represented by the network interface controller (NIC), and the storage subsystem, represented by the solid and dashed lines. The figure emphasizes the synchronization of these components to ensure an optimal VR experience.\nFigure 9.2.1-1: Client reference architecture for VR metrics\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "9.2.2\tObservation Point 1",
                            "text_content": "The access engine fetches the MPD, constructs and issues segment requests for relevant adaptation sets or preselections as ordered by the VR application, and receives segments or parts of segments. It may also adapt between different representations due to changes in available bitrate. The access engine provides a conforming 3GPP VR track to the file decoder.\nThe interface from the access engine towards MCC is referred to as observation point 1 (OP1) and is defined to monitor:\n-\tA sequence of transmitted network requests, each defined by its transmission time, contents, and the TCP connection on which it is sent\n-\tFor each network response, the reception time and contents of the response header and the reception time of each byte of the response body\n-\tThe projection/orientation metadata carried in network manifest file if applicable\n-\tThe reception time and intended playout time for each received segment\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "9.2.3\tObservation Point 2",
                            "text_content": "The file decoder processes the 3GPP VR Track and typically includes a file parser and a media decoder. The file parser processes the file or segments, extracts elementary streams, and parses the metadata, if present. The processing may be supported by dynamic information provided by the VR application, for example which tracks to choose based on static and dynamic configurations. The media decoder decodes media streams of the selected tracks into the decoded signals. The file decoder outputs the decoded signals and metadata which is used for rendering.\nThe interface from the file decoder towards MCC is referred to as observation point 2 (OP2) and is defined to monitor:\n-\tMedia resolution\n-\tMedia codec\n-\tMedia frame rate\n-\tMedia projection, such as region wise packing, region wise quality ranking, content coverage\n-\tMono vs. stereo 360 video\n-\tMedia decoding time\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "9.2.4\tObservation Point 3",
                            "text_content": "The sensor extracts the current pose according to the user's head and/or eye movement and provides it to the renderer for viewport generation. The current pose may also be used by the VR application to control the access engine on which adaptation sets or preselections to fetch.\nThe interface from the sensor towards MCC is referred to as observation point 3 (OP3) and is defined to monitor:\n-\tHead pose\n-\tGaze direction\n-\tPose timestamp\n-\tDepth\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "9.2.5\tObservation Point 4",
                            "text_content": "The VR Renderer uses the decoded signals and rendering metadata, together with the pose and the knowledge of the horizontal/vertical field of view, to determine a viewport and render the appropriate part of the video and audio signals.\nThe interface from the media presentation towards MCC is referred to as observation point 4 (OP4) and is defined to monitor:\n-\tThe media type\n-\tThe media sample presentation timestamp\n-\tWall clock counter\n-\tActual presentation viewport\n-\tActual presentation time\n-\tActual playout frame rate\n-\tAudio-to-video synchronization\n-\tVideo-to-motion latency\n-\tAudio-to-motion latency\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "9.2.6\tObservation Point 5",
                            "text_content": "The VR application manages the complete device, and controls the access engine, the file decoder and the rendering based on media control information, the dynamic user pose, and the display and device capabilities.\nThe interface from the VR application towards MCC is referred to as observation point 5 (OP5) and is defined to monitor:\n-\tDisplay resolution\n-\tMax display refresh rate\n-\tField of view, horizontal and vertical\n-\tEye to screen distance\n-\tLens separation distance\n-\tOS support, e.g. OS type, OS version\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "9.3\tMetrics Definitions",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "9.3.1\tGeneral",
                            "text_content": "As the VR metrics functionality is based on the DASH QoE metrics [8], all metrics already defined in [8] are valid also for a VR client. Thus the following sub-clauses only define additional VR-related metrics.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "9.3.2\tComparable quality viewport switching latency",
                            "text_content": "The comparable quality viewport switching latency metric reports the latency and the quality-related factors when viewport movement causes quality degradations, such as when low-quality background content is briefly shown before the normal higher-quality is restored. Note that this metric is only relevant if the Advanced Video Media profile and region-wise packing is used. Also note that the metric currently does not report factors related to foveated rendering.\nThe viewport quality is represented by two factors; the quality ranking (QR) value, and the pixel resolution of one or more regions within the viewport. The resolution is defined by the orig_width and orig_height values in ISO/IEC 23090-2 [13] in SRQR (Spherical-Region Quality Ranking) or 2DQR (2-Dimensional Quality Ranking). The resolution corresponds to the monoscopic projected picture from which the packed region covering the viewport is extracted.\nIn order to determine whether two viewports have a comparable quality, if more than one quality ranking region is visible inside the viewport, the aggregated viewport quality factors are calculated as the area-weighted average for QR and the area-weighted (effective) pixel resolution, respectively.\nFor instance, if 60% of the viewport is from a region with QR=1, Res=3840 x 2160, and 40% is from a region with QR=2, Res=960 x 540, then the average QR is 0.6 x 1 + 0.4 x 2, and the effective pixel resolution is 0.6 x 3840 x 2160 + 0.4 x 960 x 540 (also see Annex D.1 for more examples).\nIf the viewport is moved so that the current viewport includes at least one new quality ranking region (i.e. a quality ranking region not included in the previous viewport), a switch event is started. The list of quality factors related to the last evaluated viewport quality before the switch are assigned to the firstViewport log entry. The start time of the switch is also set to the time of the last evaluated viewport before the switch.\nThe end time for the switch is defined as when both the weighted average QR and the effective resolution for the viewport reach values comparable to the ones before the switch. A value is comparable if it is not more than QRT% (QR threshold) or ERT% (effective resolution threshold) worse than the corresponding values before the switch. If comparable values are not achieved within N milliseconds, a timeout occurs (for instance if an adaptation to a lower bitrate occurs, and the viewport never reaches comparable quality).\nNote that smaller QR values and larger resolution values are better. For instance, QRT=5% would require a weighted average QR value equal or smaller than 105 % of the weighted average QR before the switch, but ERT=5% would require an effective resolution value equal or larger than 95% of the effective resolution before the switch.\nThe list of quality factors related to the viewport which fulfills both thresholds are assigned to the secondViewport log entry, and the latency (end time minus start time) is assigned to the latency log entry. In case of a timeout, this is indicated under the cause log entry.\nDuring the duration of the switch the worst evaluated viewport is also stored, and assigned to the worstViewport log entry. The worst viewport is defined as the viewport with the worst relative weighted average QR or relative effective resolution, as compared to the values before the switch.\nIf a new viewport switching event occurs (e.g. yet another new region becomes visible) before an ongoing switch event has ended, only the N milliseconds timeout is reset. The ongoing measurement process continues to evaluate the viewport quality until a comparable viewport quality value is achieved (or a timeout occurs).\nThe observation points needed to calculate the metrics are:\n-\tOP2 File Decoder: SRQR/2DQR information\n-\tOP3 Sensor: Gaze information\n-\tOP4 VR Renderer: Start of switch event detection (alternatively, region coverage information from SRQR/2DQR can be used as strict rendering pixel-exactness is not required)\n-\tOP5 VR Application: Field-of-view information of the device\nThe accuracy of the measured latency depends on how the client implements the viewport switching monitoring. As this might differ between clients, the client shall report the estimated accuracy.\nThe thresholds QRT, ERT, and the timeout N, can be specified during metrics configuration (see clause 9.4) as attributes within parenthesis, e.g. \"CompQualLatency(QRT=3.5,ERT=6.8,N=900)\". If a threshold or the timeout is not specified the client shall use appropriate default values.\nThe data type ViewportDataType is defined in Table 9.3.2-1 below, and identifies the direction and coverage of the viewport.\nTable 9.3.2-: ViewportDataType\n\nThe data type Viewport-Item is defined as shown in Table 9.3.2-2. Viewport-Item is an Object which identifies a viewport and quality-related factors for the region(s) covered by the viewport.\nTable 9.3.2-2: ViewportItem\n\nThe comparable quality viewport switching latency metric is specified in Table 9.3.2-1 below.\nTable 9.3.2-1: Comparable quality viewport switching latency metric\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 9.3.2-: ViewportDataType",
                                    "table number": 9,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 9.3.2-2: ViewportItem",
                                    "table number": 10,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 9.3.2-1: Comparable quality viewport switching latency metric",
                                    "table number": 11,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "9.3.3\tRendered viewports",
                            "text_content": "The rendered viewports metric reports a list of viewports that have been rendered during the media presentation.\nThe client shall evaluate the current viewport gaze every X ms and potentially add the viewport to the rendered viewport list. To enable frequent viewport evaluations without necessarily increasing the report size too much, consecutive viewports which are close to each other may be grouped into clusters, where only the average cluster viewport data is reported. Also, clusters which have too short durations may be excluded from the report.\nThe viewport clustering is controlled by an angular distance threshold D. If the center (i.e. the azimuth and the elevation) of the current viewport is closer than the distance D to the current cluster center (i.e. the average cluster azimuth and elevation), the viewport is added to the cluster. Note that the distance is only compared towards the current (i.e. last) cluster, not to any earlier clusters which might have been created.\nIf the distance to the cluster center is instead equal to or larger than D, a new cluster is started based on the current viewport, and the average old cluster data and the start time and duration for the old cluster is added to the viewport list.\nBefore reporting a viewport list, a filtering based on viewport duration shall be done. Each entry in the viewport list is first assigned an \"aggregated duration\" equal to the duration of that entry. Then, for each entry E, the other entries in the viewport list are checked. The duration for a checked entry is added to the aggregated duration for entry E, if the checked entry is both less than T ms away from E, and closer than the angular distance D from E.\nAfter all viewport entries have been evaluated and have received a final aggregated duration, all viewport entries with an aggregated duration of less than T are deleted from the viewport list (and thus not reported). Note that the aggregated duration is only used for filtering purposes, and not itself included in the viewport list reports.\nSome examples of metric calculation are shown in Annex D.2.\nThe observation points needed to calculate the metrics are:\n- \tOP3 Sensor: Gaze information\n- \tOP5 P5 VR Application: Field-of-view information of the device\nThe viewport sample interval X (in ms), the distance threshold D (in degrees), and the duration threshold T (in ms) can be specified during metrics configuration as attributes within parenthesis, e.g. \"RenderedViewports(X=50,D=15,T=1500)\". Note that if no clustering or duration filtering is wanted, the D and T thresholds can be set to 0 (e.g. specifying \"RenderedViewports(X=1000,D=0,T=0)\" will just log the viewport every 1000 ms). If no sample interval or thresholds values are specified the client shall use appropriate default values.\nThe rendered viewports metric is specified in Table 9.3.3-1.\nTable 9.3.3-1: Rendered viewports metric\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 9.3.3-1: Rendered viewports metric",
                                    "table number": 12,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "9.3.4\tVR Device information",
                            "text_content": "This metric contains information about the device, and is logged at the start of each session and whenever changed (for instance if the rendered field-of-view for the device is adjusted). If an individual metric cannot be logged, its value shall be set to 0 (zero) or to the empty string.\nThe observation point needed to report the metrics is:\n- OP5 VR Application: Device Information\nTable 9.3.4-1: Device information\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 9.3.4-1: Device information",
                                    "table number": 13,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "9.4\tMetrics Configuration and Reporting",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "9.4.1\tConfiguration",
                            "text_content": "Metrics configuration is done according to clauses 10.4 and 10.5 in DASH [8], but can also include any metrics defined in clause 9.3.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "9.4.2\tReporting",
                            "text_content": "Metrics reporting is done according to clause 10.6 in DASH [8], with the type QoeReportType extended to handle the additional VR-specific metrics according the XML schema in clause 9.4.3. In this version of the specification the element vrMetricSchemaVersion shall be set to 1.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "9.4.3\tReporting Format",
                            "text_content": "",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "",
                                    "table number": 14,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "title": "A.1\tIntroduction",
            "description": "This clause collects information that supports the generation of VR Content following the details in the present document. Video and audio related aspects are collected. For additional details and background also refer to TR 26.918 [2].\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.2\tVideo",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "A.2.1\tOverview",
                    "description": "",
                    "summary": "",
                    "text_content": "This clause collects information on that support the generation of video bitstreams that conform to operation points and media profile in the present document.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "A.2.2\tDecoded Texture Signal Constraints",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "A.2.2.1\tGeneral",
                            "text_content": "Due to the restrictions to use a single decoder, the decoded texture signals require to follow the profile and level constraints of the decoder. Generally, this requires a careful balance of the permitted frame rates, stereo modes, spatial resolutions, and usage of region wise packing for different resolutions and coverage restrictions. Details on preferred settings such as frame rates and spatial resolutions are for example discussed in TR 26.918 [2].\nThis clause provides a summary of restrictions for the different operation points defined in the present document.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "A.2.2.2\tConstraints for Main and Flexible H.265/HEVC Operation Point",
                            "text_content": "The profile and level constraints of H.265/HEVC Main-10 Profile Main Tier Profile Level 5.1 require careful balance of the permitted frame rates, stereo modes, spatial resolutions, and usage of region wise packing for different resolutions and coverage restrictions. If the decoded texture signal is beyond the Profile and Level constraints, then a careful adaptation of the signal is recommended to fulfil the constraints.\nThis clause provides a brief overview of potential signal constraints and possible adjustments.\nTable A.2-1 provides selected permitted combinations of spatial resolutions, frame rates and stereo modes assuming full coverage and no region-wise packing applied. Note that fractional frame rates are excluded for better readability. Note that the Main H.265/HEVC Operation Point only allows frame rates up to 60 Hz.\nTable A.2-1: Selected permitted combinations of spatial resolutions and frame rates\n\nTable A.2-2 provides the maximum percentage of high-resolution area that can be encoded assuming that the low-resolution area is encoded in 2k resolution covering the full 360 degree area, i.e. using 2048 × 1024 or 1920 × 960 and full coverage is provided for different frame rates. Note also that a viewport typically covers about 12-25% of a full 360 video. Note that fractional frame rates are excluded for better readability.\nTable A.2-2: Maximum Percentage of high-resolution area when assuming that the low-resolution area is encoded in 2k resolution, i.e. using 2048 × 1024 or 1920 × 960 and full coverage is provided for different frame rates\n\nTable A.2-3 provides the maximum percentage of coverage area that can be encoded assuming that the remaining pixels are not encoded for different frame rates. Note that fractional frame rates are excluded for better readability.\nTable A.2-3: Maximum Percentage of coverage area for different frame rates\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table A.2-1: Selected permitted combinations of spatial resolutions and frame rates",
                                    "table number": 15,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table A.2-2: Maximum Percentage of high-resolution area when assuming that the low-resolution area is encoded in 2k resolution, i.e. using 2048 × 1024 or 1920 × 960 and full coverage is provided for different frame rates",
                                    "table number": 16,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table A.2-3: Maximum Percentage of coverage area for different frame rates",
                                    "table number": 17,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "A.2.2.2a\tConstraints for Main 8K H.265/HEVC Operation Point",
                            "text_content": "The profile and level constraints of H.265/HEVC Main-10 Profile Main Tier Profile Level 6.1 require careful balance of the permitted frame rates, stereo modes, spatial resolutions, and usage of region wise packing for different resolutions and coverage restrictions. If the decoded texture signal is beyond the Profile and Level constraints, then a careful adaptation of the signal is recommended to fulfil the constraints.\nThis clause provides a brief overview of potential signal constraints and possible adjustments.\nTable A.2a-1 provides selected permitted combinations of spatial resolutions, frame rates and stereo modes assuming full coverage and no region-wise packing applied. Note that fractional frame rates are excluded for better readability. Note that the Main 8K H.265/HEVC Operation Point only allows frame rates up to 60 Hz.\nTable A.2a-1: Selected permitted combinations of spatial resolutions and frame rates\n\nTable A.2a-2 provides the maximum percentage of coverage area that can be encoded assuming that the remaining pixels are not encoded for different frame rates. Note that fractional frame rates are excluded for better readability.\nTable A.2a-2 Maximum Percentage of coverage area for different frame rates\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table A.2a-1: Selected permitted combinations of spatial resolutions and frame rates",
                                    "table number": 18,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table A.2a-2 Maximum Percentage of coverage area for different frame rates",
                                    "table number": 19,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "A.2.3\tConversion of ERP Signals to CMP",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "A.2.3.1\tGeneral",
                            "text_content": "The 3D XYZ coordinate system as shown in Figure A.1 can be used to describe the 3D geometry of ERP and CMP projection format representations. Starting from the center of the sphere, X axis points toward the front of the sphere, Z axis points toward the top of the sphere, and Y axis points toward the left of the sphere.\nFigure A.1 provides a comprehensive overview of the 3D XYZ coordinate system, illustrating the three primary axes (X, Y, and Z) that define spatial positions. The figure showcases how each axis is divided into equal intervals, allowing for precise measurements and calculations in three-dimensional space. The diagram emphasizes the importance of understanding this fundamental concept in various fields, such as engineering, architecture, and computer graphics, where accurate spatial representation is crucial.\nFigure A.1: 3D XYZ coordinate definition\nNote:\tThe text in this Annex is based on JVET output document (JVET-H1004: Algorithm descriptions of projection format conversion and video quality metrics in 360Lib (Version 5)) with simplifications to only two projection types which are used in the present document and further fixes regarding misalignments. C++ implementation of the concepts described by this annex is available in 360Lib Software: available at: .\nThe coordinate system is specified for defining the sphere coordinates azimuth (ϕ) and elevation (θ) for identifying a location of a point on the unit sphere. The azimuth ϕ is in the range [−π, π], and elevation θ is in the range [−π/2, π/2], where π is the ratio of a circle's circumference to its diameter. The azimuth (ϕ) is defined by the angle starting from X axis in counter-clockwise direction as shown in Figure A.1. The elevation (θ) is defined by the angle from the equator toward Z axis as shown in Figure A.1. The (X, Y, Z) coordinates on the unit sphere can be evaluated from (ϕ, θ) using following equations:\nX = cos(θ)*cos(ϕ)\nY = cos(θ)*sin(ϕ)\nZ = sin(θ)\nInversely, the longitude and latitude (ϕ, θ) can be evaluated from (X, Y, Z) coordinates using:\nϕ = tan-1(Y/X)\nθ = sin-1(Z/(sqrt(X2+Y2+Z2)))\nA 2D plane coordinate system is defined for each face in the 2D projection plane. Where Equirectangular Projection (ERP) has only one face, Cubemap Projection (CMP) has six faces. In order to generalize the 2D coordinate system, a face index is defined for each face in the 2D projection plane. Each face is mapped to a 2D plane associated with one face index.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "A.2.3.2\tEquirectangular Projection (ERP)",
                            "text_content": "Equirectangular mapping is the most commonly used mapping from spherical video to a 2D texture signal. The mapping is bijective, i.e. it may be expressed in both directions and is illustrated in Figure A.2.\nFigure A.2 illustrates the process of mapping spherical video data into a 2D texture signal, which is essential for efficient compression and transmission. The figure showcases the conversion of spherical coordinates into a 2D image, highlighting the transformation of the fisheye lens distortion. Key components include the camera, lens, and the spherical video data, which is then compressed and represented as a 2D signal for transmission. This process is crucial for enabling the widespread adoption of immersive media in VR and AR applications.\nFigure A.2: Mapping of spherical video to a 2D texture signal\nERP has only one face and the face index f for ERP is always set to 0. The sphere coordinates (ϕ, θ) for a sample location (i, j), in degrees, are given by the following equations:\nϕ = (0.5 – i/pictureWidth)*360\nθ = (0.5 – j/pictureHeight)*180\nFinally, (X, Y, Z) can be calculated from the equations given above.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "A.2.3.3\tCubemap Projection (CMP)",
                            "text_content": "Figure A.3 shows the CMP projection with 6 square faces, labelled as PX, PY, PZ, NX, NY, NZ (with \"P\" standing for \"positive\" and \"N\" standing for \"negative\"). Table A.2-4 specifies the face index values corresponding to each of the six CMP faces.\nFigure A.3 illustrates the relationship between the cube face arrangement of a projected image and the sphere coordinates, showcasing how the image is transformed from a three-dimensional space onto a two-dimensional plane. The diagram demonstrates the process of mapping the image onto the sphere, highlighting the transformation techniques used to maintain the image's integrity and perspective. Key components include the projection matrix and the transformation of the cube faces onto the sphere, with the image's edges and corners being particularly important in maintaining the visual fidelity.\nFigure A.3: Relation of the cube face arrangement of the projected picture to the sphere coordinates\n\nTable A.2-4: Face index of CMP\n\nThe 3D coordinates (X, Y, Z) are derived using following equations:\nlw = pictureWidth / 3\nlh = pictureHeight / 2\ntmpHorVal = i − Floor( i ÷ lw ) * lw\ntmpVerVal = j − Floor( j ÷ lh ) * lh\ni′ = −( 2 * tmpHorVal ÷ lw ) + 1\nj′ = −( 2 * tmpVerVal  ÷ lh ) + 1\nw = Floor( i ÷ lw )\nh = Floor( j ÷ lh )\nif( w  = =  1  &&  h  = =  0 ) { // PX: positive x front face\n\tX = 1.0\n\tY = i′\n\tZ = j′\n} else if( w  = =  1  &&  h  = =  1 ) { // NX: negative x back face\n\tX = −1.0\n\tY = −j′\n\tZ = −i′\n} else if( w  = =  2  &&  h  = =  1 ) { // PZ: positive z top face\n\tX = −i′\n\tY = −j′\n\tZ = 1.0\n} else if( w  = =  0  &&  h  = =  1 ) { // NZ: negative z bottom face\n\tX = i′\n\tY = −j′\n\tZ = −1.0\n} else if( w  = =  0  &&  h  = =  0 ) { // PY: positive y left face\n\tX = −i′\n\tY = 1.0\n\tZ = j′\n} else { // ( w  = =  2  &&  h  = =  0 ), NY: negative y right face\n\tX = i′\n\tY = −1.0\n\tZ = j′\n}\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table A.2-4: Face index of CMP",
                                    "table number": 20,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "A.2.3.4\tConversion between two projection formats",
                            "text_content": "Denote (fd,id,jd) as a point (id,jd) on face fd in the destination projection format, and (fs,is,js) as a point (is,js) on face fs in the source projection format. Denote (X,Y,Z) as the corresponding coordinates in the 3D XYZ space. The conversion process starts from each sample position (fd,id,jd) on the destination projection plane, maps it to the corresponding (X,Y,Z) in 3D coordinate system, finds the corresponding sample position (fs,is,js) on the source projection plane, and sets the sample value at (fd,id,jd) based on the sample value at (fs,is,js).\nTherefore, the projection format conversion process from ERP source format to CMP destination format is performed in the following three steps:\n1)\tMap the destination 2D sampling point (fd,id,jd) to 3D space coordinates (X,Y,Z) based on the CMP format.\n2)\tMap (X,Y,Z) from step 1 to 2D sampling point (f0,is,js) based to the ERP format.\n3)\tCalculate the sample value at (f0,is,js) by interpolating from neighboring samples at integer positions on face f0, and the interpolated sample value is placed at (fd,id,jd) in the destination projection format.\nThe above steps are repeated until all sample positions (fd,id,jd) in the destination projection format are filled. Note that (Step 1) and (Step 2) can be pre-calculated at the sequence level and stored as a lookup table, and only (Step 3) needs to be performed per sample position for each picture in order to render the sample values.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "B.1\tGeneral",
            "description": "Binaural rendering allows 3D audio content to be played back via headphones. The rendering is performed as a fast convolution of point sound source streams in the 3D space with head-related impulse responses (HRIRs) or binaural room impulse responses (BRIRs) corresponding to the direction of incidence relative to the listener. HRIRs will be provided from an external source.\nFigure B.1-1 provides a high-level overview of an external binaural renderer setup, illustrating the various components involved in the process. The figure showcases the input signals, which include a monaural signal and a binaural signal, as they pass through the setup. The monaural signal is processed through a monaural to binaural converter, which transforms the signal into a binaural format. This is then sent to the binaural renderer, which is responsible for generating the binaural output. The binaural output is then sent to the headphones, which play the audio to the user. The figure also shows the various microphones and other sensors that are used to capture the input signals. Overall, the figure provides a comprehensive overview of the external binaural renderer setup and its various components.\nFigure B.1-1: High level overview of an external binaural renderer setup.\nThe renderer has three input interfaces (see Fig. B.1-1): the audio streams and metadata from the MPEG-H decoder, a head tracking interface for scene displacement information (for listener tracking), and a head-related impulse response (HRIR) interface providing binaural impulse responses for a given direction of incidence. The metadata as described in B.3, together with the scene displacement information, is used to construct a scene model, from which the renderer can infer the proper listener-relative point source positions.\nThe audio input streams may include Channel content, Object content, HOA content. The renderer performs pre-processing steps to translate the respective content type into several point sources that are then processed for binaural rendering. Channel groups and objects that are marked a non-diegetic in the metadata are excluded from any scene displacement processing.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "B.2\tInterfaces",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "B.2.1\tInterface for Audio Data and Metadata",
                    "description": "",
                    "summary": "",
                    "text_content": "The example external binaural renderer has an interface for the input of un-rendered channels, objects, and HOA content and associated metadata. The syntax of this input interface follows the specification of the External Renderer Interface for MPEG-H 3D Audio to output un-rendered channels, objects, and HOA content and associated metadata according to clause 6.1.4.3.6.5.\nThe input PCM data of the channels and objects interfaces is provided through an input PCM buffer, which first contains  signals carry the PCM data of the channel content. These are followed by  signals carrying the PCM data of the un-rendered objects. Then additional signals carry the  HOA data which number is indicated in the HOA metadata via the HOA order (e.g. 16 signals for HOA order 3). The HOA audio data in the HOA interface is provided in the ESD representation. The conversion from the HOA domain into the equivalent spatial domain representation and vice versa is described in ISO/IEC 23008-3 [19], Annex C.5.1.\nThe metadata for channels, objects, and HOA is received via the input interface once per frame and their syntax is specified in mpegh3da_getChannelMetadata(), mpegh3da_getObjectAudioAndMetadata(), and mpegh3da_getHoaMetadata() respectively, see ISO/IEC 23008-3, clause 17.10 [19]. The metadata and PCM data will be aligned to match each metadata element with the respective PCM frame.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.2.2\tHead Tracking Interface.",
                    "description": "",
                    "summary": "",
                    "text_content": "The external binaural renderer receives scene displacement values (yaw, pitch and roll) e.g. from an external head tracking device via the head tracking interface. The syntax is specified in mpegh3daSceneDisplacementData() as defined in ISO/IEC 23008-3 [19], clause 17.9.3.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.2.3\tInterface for Head-Related Impulse Responses",
                    "description": "",
                    "summary": "",
                    "text_content": "An interface is provided to specify the set of HRIRs used for the binaural rendering. These directional FIR filters will be input using the SOFA (Spatially Oriented Format for Acoustics) files format according to AES-69 [21]. The SimpleFreeFieldHRIR convention will be used, where binaural filters are indexed by polar coordinates (azimuth φ in radians, elevation ϕ in radians, and radius r in meters) relative to the listener.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "B.3\tPreprocessing",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "B.3.1\tChannel Content",
                    "description": "",
                    "summary": "",
                    "text_content": "Channel input content is converted into a corresponding set of point sources with associated positions using the loudspeaker configuration data included in mpegh3da_getChannelMetadata() and the associated PCM data obtained via the interface specified in B.2.1\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.3.2\tObject Content",
                    "description": "",
                    "summary": "",
                    "text_content": "Object input content is converted into corresponding point sources with associated positions using the metadata included in mpegh3da_getObjectAudioAndMetadata() and the associated PCM data obtained via the interface specified in clause B.2.1\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.3.3\tHOA Content",
                    "description": "",
                    "summary": "",
                    "text_content": "As specified in clause B.2.1 HOA content is input in the ESD representation together with the metadata included in mpegh3da_getHoaMetadata(). As a pre-processing step, the ESD representation is first converted into HOA coefficients. All coefficients associated with HOA of order larger than three are discarded to limit the maximum computational complexity.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.3.4\tNon-diegetic Content",
                    "description": "",
                    "summary": "",
                    "text_content": "Channel groups for which the gca_directHeadphone flag is set in mpegh3da_getChannelMetadata() are routed to left and right output channel directly and are excluded from binaural rendering using scene displacement data (non-diegetic content). Non-diegetic content may have stereo or mono format. For mono, the signal is mixed to left and right headphone channel with a gain factor of 0.707.\nFor each channel group it has to be checked in the mpegh3da_getChannelMetadata() if the gca_fixedChannelsPosition flag is equal to 0 or 1. A channel group with an associated 'gca_fixedChannelsPosition == 1' is included in the binaural rendering but excluded from the scene displacement processing according to clause B.4, i.e. its position is not updated.\nFor each object it has to be checked in the mpegh3da_getObjectAudioAndMetadata() if the goa_fixedPosition flag is equal to 0 or 1. An object with an associated 'goa_fixedPosition == 1' is included in the binaural rendering but excluded from the scene displacement processing according to clause B.4, i.e. its position is not updated.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "B.4\tScene Displacement Processing",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "B.4.1\tGeneral",
                    "description": "",
                    "summary": "",
                    "text_content": "The position of each point source derived from the channels and objects input is represented by a 3-dimensional vector  in a Cartesian coordinate system. The scene displacement information is used to compute an updated version of the position vector  as described in clause B.4.2. The position of point sources that result from non-diegetic channel groups with an associated 'gca_fixedChannelsPosition == 1' or from non-diegetic objects with an associated 'goa_fixedPosition == 1' (see clause B.3.4) is not updated, i.e.  is equal to .\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.4.2\tApplying Scene Displacement Information",
                    "description": "",
                    "summary": "",
                    "text_content": "The vector representation of a point source is transformed to the listener-relative coordinate system by rotation based on the scene displacement values obtained via the head tracking interface. This is achieved by multiplying the position with a rotation matrix calculated from the orientation of the listener:\n\nThe determination of the rotation matrix  is defined in ISO/IEC 23008-3 [19], Annex I.\nFor HOA content, the rotation matrix  suited for rotating the spherical harmonic representation is calculated as defined in ISO/IEC 23008-3 [19], Annex I. After the rotation, the HOA coefficients are transformed back into the ESD representation. Each ESD component is then converted to the corresponding point source with its associated positional information. For the ESD components the position information is fixed, i.e.  , as the rotation due to scene displacement is performed in the spherical harmonic representation.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "B.5\tHeadphone Output Signal Computation",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "B.5.1\tGeneral",
                    "description": "",
                    "summary": "",
                    "text_content": "The overall Scene Model is represented by the collection of all point sources with updated position  obtained from the rotated channels, objects, and the ESD components as well as the non-diegetic channels and objects for which 'gca_fixedChannelsPosition == 1' or 'goa_fixedPosition == 1'. The overall number of point sources in the Scene Model is denoted with .\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.5.2\tHRIR Selection",
                    "description": "",
                    "summary": "",
                    "text_content": "The position  of each point source in the listener-relative coordinate system is used to query a best-match HRIR pair from the set of available HRIRs. For lookup, the polar coordinates of the HRIR locations are transformed into the internally used cartesian coordinates and the closest-match available HRIR for a given point source position is selected. As no interpolation between different HRIRs is performed, HRIR datasets with sufficient spatial resolution should be provided.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.5.3\tInitialization",
                    "description": "",
                    "summary": "",
                    "text_content": "The HRIR filters used for binauralization are asynchronously partitioned and transformed into the frequency domain using a Fast FourierTransform (FFT). The necessary steps for each of the  HRIR filter pairs are as follows:\n1)\tUniformly partition the length N HRIR filter pairs  into  filter partitions  of length .\n2)\tZero-pad the filter partitions to length .\n3)\tTransform all filter partitions into the frequency domain using real-to-complex FFT to obtain the  frequency domain filter pairs , where  denotes the frequency index.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.5.4\tConvolution and Crossfade",
                    "description": "",
                    "summary": "",
                    "text_content": "Each audio block of a point source of the Scene Model is convolved with its selected HRIR filter pair for the left and right ear respectively. To reduce the computational complexity, a fast frequency domain convolution technique of uniformly partitioned overlap-save processing is useful for typical FIR filter lengths for HRIRs/BRIRs. The required processing steps are described in the following.\nThe following block processing steps are performed for each of the  point sources of the Scene Model:\na)\tObtain a block of  new input samples of the point source  .\nb)\tPerform a real-to-complex FFT transforms of length  to obtain the frequency domain representation of the input .\nc)\tCompute the frequency domain headphone output signal pair  for the point source  by multiplying each HRIR frequency domain filter partition  with the associated frequency domain input block and adding the product results over all partitions.\nd)\t samples of the time domain output signal pair  are obtained from  by performing a complex-to-real IFFT.\ne)\tOnly the last  output samples represent valid output samples. The  samples before are time-aliased and are discarded.\nf)\tIn case of a HRIR filter exchange happens due to changes in the scene displacement, steps 3-5 are computed for both the current HRIR filter and the ones used in the previous block. A time-domain crossfade is performed over the B output samples obtained in step 5:\ng)\nh)\tThe crossfade envelopes are defined as\n\n\nto preserve a constant power of the resulting output signal.\nThe crossfade operation define in step 6 is only applied to point sources of the Scene Model that have been generated from channel or object content. For HOA content, the crossfade is applied between the current and the previous rotation matrices  (see B.4.2).\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.5.5\tBinaural Downmix",
                    "description": "",
                    "summary": "",
                    "text_content": "The rendered headphone output signal is computed as the sum over all binauralized point source signal pairs . In case that the metadata provided together with the audio data at the input interface (see X.3.1) includes gain values applicable to a specific channel group (gca_channelGain in mpegh3da_getChannelMetadata()) or objects (goa_objectGainFactor in mpegh3da_getObjectAudioAndMetadata()), these gain values  are applied to the corresponding binauralized point source signal  before the summation:\n\nFinally, any additional non-binauralized non-diegetic audio input ('gca_directHeadphone == 1', see B.3.4) is added time-aligned to the two downmix channels.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.5.6\tComplexity",
                    "description": "",
                    "summary": "",
                    "text_content": "The algorithmic complexity of the external binaural renderer using a fast convolution approach can be evaluated for the following computations:\n\nAdditional computations are required for scene displacement processing (see B.4).\nThe total complexity per output sample can be determined by adding the complexity estimation for convolution and downmix and dividing by the block length B. In blocks where a filter exchange is performed, items 2-4 from the convolution contribute two times to the overall complexity in addition to the time-domain crossfade multiplications and additions (filter exchange items 2 and 3). The partitioning and FFT for the filter exchange, as well as the scene displacement, can be performed independent of the input block processing.\n",
                    "tables": [
                        {
                            "description": "The algorithmic complexity of the external binaural renderer using a fast convolution approach can be evaluated for the following computations:",
                            "table number": 21,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.5.7\tMotion Latency",
                    "description": "",
                    "summary": "",
                    "text_content": "The Scene Model can be updated with arbitrary temporal precision, but the resulting HRIR exchange is only done at processing block boundaries of the convolution. With a standard block size of  samples at 48 kHz sampling rate, this leads to a maximum onset latency of 5.3 ms until there is an audible effect of a motion of sources or the listener. In the following block, a time-domain crossfade between the new and the previous filtered signal is performed (see Convolution/Initialization), so that a discrete, instantaneous motion is completed after a maximum of two convolution processing blocks (10.6 ms for 512 samples at 48 kHz sampling rate). Additional latency from head trackers, audio buffering, etc. is not considered.\nThe rotation of the HOA content is performed at a block boundary resulting in a maximum latency of one processing block, until a motion is completed.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "C.1\t3GPP Registered URIs",
            "description": "The clause documents the registered URIs in this specification following the process in\nTable C-1 lists all registered URN values as well as:\n-\ta brief description of its functionality;\n-\ta reference to the specification or other publicly available document (if any) containing the definition;\n-\tthe name and email address of the person making the application; and\n-\tany supplementary information considered necessary to support the application.\nTable C-1: 3GPP Registered URNs\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "Table C-1: 3GPP Registered URNs",
                    "table number": 22,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "D.1 \tComparable quality viewport switching latency",
            "description": "This sub-clause illustrates how the weighted average QR value and the effective resolution can be calculated.\nThe quality level of each region is determined with its respective quality ranking (QR) value. A viewport can be covered with multiple regions. A quality level value for the viewport can be derived as weighted average of the QR values of the regions covering the viewport. The weight of each region is defined as the percentage of the viewport area covered by the corresponding region. The viewport quality level can be calculated by the following equation.\n\n: Number of regions covering the viewport\nThe given figure illustrates the viewport coverage value (in percent) of i-th quality ranking region, which represents the percentage of the viewport area that is covered by the i-th ranked quality of the image. The x-axis represents the quality ranking, ranging from 1 to 5, while the y-axis represents the viewport coverage value, ranging from 0 to 100%. The figure shows a clear correlation between the quality ranking and the viewport coverage value, indicating that higher-ranked images have a higher coverage value, meaning they are more likely to be viewed by users. This information is crucial for image quality assessment and recommendation systems, as it helps to prioritize the display of higher-quality images to users.The given figure presents the viewport coverage value (in percent) of i-th quality ranking region, which quantifies the percentage of the viewport area that is covered by the i-th ranked quality of the image. The x-axis represents the quality ranking, ranging from 1 to 5, while the y-axis represents the viewport coverage value, ranging from 0 to 100%. The figure demonstrates a clear correlation between the quality ranking and the viewport coverage value, indicating that higher-ranked images have a higher coverage value, meaning they are more likely to be viewed by users. This information is significant for image quality assessment and recommendation systems, as it helps to prioritize the display of higher-quality images to users. QR value of i-th quality ranking region\nThe viewport coverage value (in percent) of i-th quality ranking region\nFigure D.1 illustrates a viewport covered by four quality ranking 2D region, showcasing the spatial distribution of image quality across different viewing angles. The figure presents a heatmap-like representation, with darker shades indicating higher image quality and lighter shades indicating lower quality. The four quality ranking 2D regions are clearly labeled, providing a visual guide to the areas of interest for further analysis. This figure is essential for understanding the spatial variation in image quality and its impact on the overall viewing experience.\nFigure D.1-1: An example of a viewport covered by four quality ranking 2D region\nFigure D.1-1 is an example of a viewport covered by four quality ranking regions. The quality of the viewport is equal to the weighted sum of the quality ranking value and the coverage percentage value of each quality ranking region.\nThe resolution of each region is determined by its respective width and height values in pixel which are available in the quality ranking box under the name orig_width and orig_height. Note that these values are already normalized to represent the full-sphere resolution you would get if the resolution of this region would be used for the full sphere.\nThe effective resolution (i.e. the total number of original pixels) for the content visible in the viewport can be derived as the weighted average of the resolution of each region covering the viewport. The weight of each region is defined by the percentage of the viewport area covered by the corresponding region. The effective viewport resolution can be calculated by the following equation.\n\n: Number of regions covering the viewport\n: The width component of the original source pixel resolution for the i-th quality ranking region\nThe given figure illustrates the viewport coverage value (in percent) of i-th quality ranking region, which represents the percentage of the viewport area that is covered by the i-th ranked quality of the image. The x-axis represents the quality ranking, ranging from 1 to 5, while the y-axis represents the viewport coverage value, ranging from 0 to 100%. The figure shows a clear correlation between the quality ranking and the viewport coverage value, indicating that higher-ranked images have a higher coverage value, meaning they are more likely to be viewed by users. This information is crucial for image quality assessment and recommendation systems, as it helps to prioritize the display of higher-quality images to users.The given figure presents the viewport coverage value (in percent) of i-th quality ranking region, which quantifies the percentage of the viewport area that is covered by the i-th ranked quality of the image. The x-axis represents the quality ranking, ranging from 1 to 5, while the y-axis represents the viewport coverage value, ranging from 0 to 100%. The figure demonstrates a clear correlation between the quality ranking and the viewport coverage value, indicating that higher-ranked images have a higher coverage value, meaning they are more likely to be viewed by users. This information is significant for image quality assessment and recommendation systems, as it helps to prioritize the display of higher-quality images to users.: The height component of the original source pixel resolution for the i-th quality ranking region\nThe viewport coverage value (in percent) of i-th quality ranking region\n\nFigure D.1-2: An example of a source packed image with four quality ranking 2D regions with different resolutions\nFigure D.1-3: An example of a viewport covered by four different quality ranking 2D regions\nFigure D.1-2 is an example of a source with four regions with different resolution. Figure D.1-3 represents an example of a viewport which is covered by the four quality ranking 2D regions. The effective viewport resolution is equal to the weighted sum of the resolution for each quality-ranking 2D region and its corresponding viewport coverage percentage value.\nFigure D.1-4 illustrates a comparative analysis of viewport switching latency across different telecommunication systems. The graph presents the latency in milliseconds (ms) for various viewport switching methods, including direct, optimized, and optimized with a buffer. The x-axis represents the viewport size in pixels, while the y-axis quantifies the latency in milliseconds. The data points are color-coded to differentiate between the three methods, with a clear trend indicating that optimized viewport switching with a buffer results in the lowest latency, followed by optimized viewport switching, and then direct viewport switching. The figure emphasizes the importance of buffer management in reducing latency and enhancing user experience in telecommunication systems.\nFigure D.1-4: Comparable quality viewport switching latency measurement example\nFigure D.1-4 presents an example of the metric measurement operation. The viewport quality is evaluated at time t0, and then again at time t1. The media playback module renders the high-resolution sub-picture #1 at time t1. The user viewing orientation is gradually changing from sub-pic#1 to sub-pic#2 as the time progresses.\nAt time t2, the media playback module starts to render the buffered low-quality representation of sub-pic#2 as the viewport moves into sub-picture #2. At time t2, the viewport quality drops in values as compared to the viewport quality at time t1, and a new sub-picture (sub-pic #2) is rendered. A viewport switching event is identified at time t2.\nThe viewport quality values evaluated at t1 identifies the first viewport. The viewport position and viewport quality level list are assigned to the attributed Position and QualityLevel of the firstViewportItem.\nAn effective viewport resolution and viewport QR quality value for the new viewport that is comparable to that of the firstViewportItem after viewport switching time is logged at time t4. The new viewport position identifies the Position of the secondViewportItem. The corresponding QualityLevel list for the secondViewportItem is assigned.\nThe associated viewport values stored for the worst viewport quality during the switch is assigned to the field Position of the worstViewportItem. The corresponding QualityLevel list for the worstViewportItem is also assigned.\nThe comparable-quality viewport switching latency is measured as the time interval between the logged times for firstViewportItem (t1 in this example) and secondViewportItem (t4 in this example).\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "D.2\tRendered viewports",
            "description": "Error! Reference source not found. illustrates an example of clustering and the associated viewports. The first three evaluated viewports are all with the distance D (indicated by the blue circle), and are thus assigned to the same cluster. Note that the cluster center moves a bit for each new viewport which is added to the cluster.\nViewport #4 is too far away from the center of cluster #1, and thus starts a new cluster, which eventually gathers three viewport members. Then viewport #7 is too far away from the center of cluster #2, and again starts a new cluster.\nFigure D.2 - 1 illustrates a clustering example within a telecommunication network, showcasing the grouping of nodes based on their geographical locations and functional roles. The figure displays a network with multiple nodes, each represented by a distinct shape and color. The nodes are interconnected by lines, indicating the flow of data and communication between them. The clustering strategy is crucial for optimizing network performance, resource allocation, and fault management. The figure emphasizes the importance of efficient node grouping in managing network traffic and ensuring seamless communication across the network.\nFigure D.2-1: Clustering example\nFor each cluster j, the final averaged viewport parameters can be derived as follows, assuming there are N viewports in the j:th cluster. Note that the center azimuth and tilt averaging also needs to handle the special case around -180/180 degrees, as some values might be positive (e.g. 176 degrees), while others might be negative (e.g. -178 degrees). This special case is not shown in the equations below.\nNote also that the azimuth and elevation range (i.e. the visible coverage of the viewport) might often be the same for every viewport, unless the user explicitly changes the field-of-view for the device. For consistency, and to catch any during-session field-of-view changes, these two parameters should still be averaged.\nFigure D.2-2 below provides an example of duration filtering in a visual media analysis. The figure illustrates a sequence of user gaze movements, starting from the upper left (viewports #1 to #3), then a brief glance to the right (viewport #4), followed by a return to the upper left (viewports #5 and #6). Finally, the user's gaze shifts to the lower right (viewports #7 to #10). This sequence demonstrates how gaze duration and direction can be analyzed to understand user interaction with visual content.\nFigure D.2-2 below illustrates an example of the duration filtering. The user starts by looking at the upper left part of the media (viewports #1 to #3), then make a very brief glance to the right (viewport #4), and then moves back to the upper-left again (viewports #5 and #6). Then the user moves his gaze to the lower-right part (viewports #7 to #10).\nAssume here that the duration T is set to 4 times the value of the viewport sample rate X, i.e. a cluster needs to have a duration corresponding to at least four viewports to be reported. Here four clusters are formed, but before filtering only cluster #4 would be reported. After filtering, clusters #1 and #3 are close enough both in time and distance to add to each other's aggregated duration, so each of them will be assigned an aggregated duration of 5, and thus be reported. Cluster #2, the quick glance up to the right, has too short duration and will not be reported.\nFigure D.2-2 illustrates the application of duration filtering in a communication system, focusing on the removal of noise and interference from the signal. The figure depicts a typical communication channel, with the input signal passing through a duration filter. The output signal is then compared to the original, showing the improvement in signal quality after filtering. The figure highlights the importance of duration filtering in maintaining clear and reliable communication channels.\nFigure D.2-2 Duration filtering example\n\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 23,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        }
    ]
}