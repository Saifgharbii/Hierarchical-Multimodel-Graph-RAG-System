{
    "document_name": "26928-i00.docx",
    "content": [
        {
            "title": "Foreword",
            "description": "This Technical Report has been produced by the 3rd Generation Partnership Project (3GPP).\nThe contents of the present document are subject to continuing work within the TSG and may change following formal TSG approval. Should the TSG modify the contents of the present document, it will be re-released by the TSG with an identifying change of release date and an increase in version number as follows:\nVersion x.y.z\nwhere:\nx\tthe first digit:\n1\tpresented to TSG for information;\n2\tpresented to TSG for approval;\n3\tor greater indicates TSG approved document under change control.\ny\tthe second digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, etc.\nz\tthe third digit is incremented when editorial only changes have been incorporated in the document.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "Introduction",
            "description": "This Technical Report collects information on eXtended Reality (XR) in the context of 5G radio and network services. Extended reality (XR) refers to all real-and-virtual combined environments and associated human-machine interactions generated by computer technology and wearables. It includes representative forms such as augmented reality (AR), mixed reality (MR), and virtual reality (VR) and the areas interpolated among them. In this Technical Report, baseline technologies for XR type of services and applications are introduced outlining the QoE/QoS issues of XR-based services, the delivery of XR in the 5G system, and an architectural model of 5G media streaming defined in TS 26.501. In addition to the conventional service category, interactive, streaming, download, and split compute/rendering are identified as new delivery categories. A survey of 3D, XR visual and audio formats is also provided.\nUse cases and device types are classified, and processing and media centric architectures are introduced. This includes viewport independent and dependent streaming, as well as different distributed computing architectures for XR. Core use cases of XR include those unique to AR and MR in addition to those of VR discussed in 3GPP TR 26.918, ranging from offline sharing of 3D objects, real-time sharing, multimedia streaming, online gaming, mission critical applications, and multi-party call/conferences. Based on the details in the report, proposals for potential standardisation areas are documented.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "1\tScope",
            "description": "The present document collects information on eXtended Reality (XR) in the context of 5G radio and network services. The primary scope of the present document is the documentation of the following aspects:\n-\tIntroducing Extended Reality by providing definitions, core technology enablers, a summary of devices and form factors, as well as ongoing related work in 3GPP and elsewhere,\n-\tCollecting and documenting core use cases in the context of Extended Reality,\n-\tIdentifying relevant client and network architectures, APIs and media processing functions that support XR use cases,\n-\tAnalysing and identifying the media formats (including audio and video), metadata, accessibility features, interfaces and delivery procedures between client and network required to offer such an experience,\n-\tCollecting key performance indicators and Quality-of-Experience metrics for relevant XR services and the applied technology components,\n-\tDrawing conclusions on the potential needs for standardisation in 3GPP.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "2\tReferences",
            "description": "The following documents contain provisions which, through reference in this text, constitute provisions of the present document.\n-\tReferences are either specific (identified by date of publication, edition number, version number, etc.) or non-specific.\n-\tFor a specific reference, subsequent revisions do not apply.\n-\tFor a non-specific reference, the latest version applies. In the case of a reference to a 3GPP document (including a GSM document), a non-specific reference implicitly refers to the latest version of that document in the same Release as the present document.\n[1]\t3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".\n[2]\t3GPP TR 26.918: \"Virtual Reality (VR) media services over 3GPP\".\n[3]\t3GPP TS 26.118: \"3GPP Virtual reality profiles for streaming applications\".\n[4]\tARCore, https://developers.google.com/ar/\n[5]\tARKit, https://developer.apple.com/arkit/\n[6]\t3GPP TR 22.842: \"Study on Network Controlled Interactive Service in 5GS\".\n[7]\t3GPP TR 26.247: \"Transparent end-to-end Packet-switched Streaming Service (PSS); Progressive Download and Dynamic Adaptive Streaming over HTTP (3GP-DASH)\".\n[8]\t\t3GPP TS 23.501: \"System Architecture for the 5G System\".\n[9]\tSchuemie, Martijn J., Peter Van Der Straaten, Merel Krijn, and Charles A.P.G. Van Der Mast. “” CyberPsychology & Behavior, Vol. 4, No. 2. April 2001.\n[10]\tChing, Teo Choong. \"\" Medium. 27 August 2016.\n[11]\tSparks, Matt. \"Don't Break the Spell: Creating Presence in Virtual Reality\" Learning Solutions Magazine, 17 July 2017.\n[12]\t\t3GPP TS 26.501: \"5G Media Streaming Architecture\".\n[13]\t3GPP TS 22.173: \"IP Multimedia Core Network Subsystem (IMS) Multimedia Telephony Service and supplementary services; Stage 1\".\n[14]\t3GPP TS 26.114: \"IP Multimedia Subsystem (IMS); Multimedia Telephony; Media handling and interaction\".\n[15]\t3GPP TR 22.891 \"Feasibility Study on New Services and Markets Technology\".\n[16]\tKhronos, \"The OpenXR Specification\", Jan 25, 2020,\n[17]\tW3C, \"WebXR Device API\",\n[18]\tRolland, Jannick & Holloway, Richard & Fuchs, Henry. (1994). Comparison of optical and video see-through, head-mounted displays. Proceedings of SPIE - The International Society for Optical Engineering. 10.1117/12.197322.\n[19]\t\"Cloud Gaming: Architecture and Performance\", Ryan Shea and Jiangchuan Liu, Simon Fraser University; Edith C.-H. Ngai, Uppsala University; Yong Cui, Tsinghua University; IEEE Network-July/August 2013.\n[20]\tM. Claypool and K. Claypool. Latency and player actions in online games. Communications of the ACM, 49(11):40–45, 2006.\n[21]\tQuax, P., Monsieurs, P., Lamotte, W., De Vleeschauwer, D., and Degrande, N. Objective and subjective evaluation of the influence of small amounts of delay and jitter on a recent first person shooter game. In Proceedings of 3rd ACM SIGCOMM workshop on Network and system support for games (New York, NY, USA, 2004), NetGames ’04, ACM, pp. 152–156.\n[22]\tChen, K.-t., Huang, P., Wang, G.-s., Huang, C.-y., and Lei, C.-l. On the Sensitivity of Online Game Playing Time to Network QoS. Proceedings of IEEE INFOCOM 2006 00, c (2006).\n[23]\t\t3GPP TS 23.501: \"System architecture for the 5G System (5GS)\".\n[24]\t\t3GPP TS 38.300: \"NR; Overall description; Stage-2\".\n[25]\t\t3GPP TS 22.173: \"IP Multimedia Core Network Subsystem (IMS) Multimedia Telephony Service and supplementary services; Stage 1\".\n[26]\t\t3GPP TS 26.114: \"IP Multimedia Subsystem (IMS); Multimedia telephony; Media handling and interaction\".\n[27]\t3GPP TR 23.758: \"Study on application architecture for enabling Edge Applications\"\n[28]\t3GPP TR 23.748: \"Study on enhancement of support for Edge Computing in 5G Core network (5GC)\".\n[29]\t3GPP TS 23.558: \"Architecture for enabling Edge Applications (EA)\".\n[30]\tRecommendation ITU-T H.264 (04/2017): \"Advanced video coding for generic audiovisual services\" | ISO/IEC 14496-10:2014: \"Information technology – Coding of audio-visual objects – Part 10: Advanced Video Coding\".\n[31]\tRecommendation ITU-T H.265 (12/2016): \"High efficiency video coding\" | ISO/IEC 23008-2:2015: \"High Efficiency Coding and Media Delivery in Heterogeneous Environments – Part 2: High Efficiency Video Coding\".\n[32]\t3GPP TS 26.116: \"Television (TV) over 3GPP services; Video profiles\". [33]\tJens-Rainer Ohm, Gary J. Sullivan, Heiko Schwarz, Thiow Keng Tan, and Thomas Wiegand, \"Comparison of the Coding Efficiency of Video Coding Standards—Including High Efficiency Video Coding (HEVC)\" IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 22, NO. 12, DECEMBER 2012.\n[34]\tT.K. Tan, M. Mrak, R. Weerakkody, N. Ramzan, V. Baroncini, G.J. Sullivan, J.-R. Ohm, K.D. McCann, \"HEVC subjective video quality test results\", IBC2014 Conference, 2014.\n[35]\tThiow Keng Tan ; Rajitha Weerakkody ; Marta Mrak ; Naeem Ramzan ; Vittorio Baroncini, Jens-Rainer Ohm, Gary J. Sullivan, \"Video Quality Evaluation Methodology and Verification Testing of HEVC Compression Performance\" IEEE Transactions on Circuits and Systems for Video Technology, Volume: 26 , Issue: 1 , Jan. 2016.\n[37]\tISO/IEC 23090-2: \"Information technology — Coded representation of immersive media — Part 2: Omnidirectional media format\"\n[38]\tS. Schwarz et al., \"Emerging MPEG Standards for Point Cloud Compression,\" in IEEE Journal on Emerging and Selected Topics in Circuits and Systems, vol. 9, no. 1, pp. 133-148, March 2019.\n[39]\tKhronos, \"The GL Transmission Format (glTF)\", Jun 9, 2017, https://github.com/KhronosGroup/glTF/blob/master/specification/2.0/README.md\n[40]\tLong Qian, Alexander Barthel, Alex Johnson, Greg Osgood, Peter Kazanzides, Nassir Navab, and Bernhard Fuerst, \"Comparison of optical see-through head-mounted displays for surgical interventions with object-anchored 2D-display\", Int J Comput Assist Radiol Surg. 2017 Jun; 12(6): 901–910, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5891507/.\n[41]\t3GPP TS 22.261: \"Service requirements for the 5G system\".\n[42]\t3GPP, VR-IF and the Advanced Imaging Society's 2ND VR ECOSYSTEMS & STANDARDS WORKSHOP, Culver City, CA, US, https://www.vr-if.org/events/3gpp-vrif-ais-workshop/.\n[43]\tSlater, Mel and Martin Usoh. \"Body Centred Interaction in Immersive Virtual Environments.\" Body centred interaction in immersive virtual environments. In N. M. Thalmann & D. Thalmann (Eds.), Artificial Life and Virtual Reality (pp. 125-148). New York: John Wiley. 1994.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "3\tDefinitions of terms, symbols and abbreviations",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "3.1\tTerms",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the terms given in 3GPP TR 21.905 [1] and the following apply. A term defined in the present document takes precedence over the definition of the same term, if any, in 3GPP TR 21.905 [1].\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.2\tAbbreviations",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the abbreviations given in 3GPP TR 21.905 [1] and the following apply. An abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in 3GPP TR 21.905 [1].\n3DoF\tThree Degrees of Freedom\n5QI\t5G QoS Identifier\n6DoF\tThree Degrees of Freedom\nAI\tArtificial Intelligence\nAPI\tApplication Programming Interface\nAR\tAugmented Reality\nARP\tAllocation and Retention Priority\nASIC\tApplication-Specific Integrated Circuit\nASTC\tAdaptive Scalable Texture Compression\nATW\tAsynchronous TimeWarp\nAVC\tAdvanced Video Coding\nBC1\tBlock Compression for RGB\nCAD\tComputer-Aided Design\nCBR\tConstant BitRate\nCDN\tContent Delivery Network\nCPU\tCompute Processing Unit\nCTC\tCall for TeChnologies\nDASH\tDynamic Adaptive Streaming over HTTP\nDL\tDownLink\nDNS\tDomain Name System\nDoF\tDegrees of Freedom\nEAC\tEricsson Alpha Compression\nERP\tEqui-Rectangular Projection\nETC2\tEricsson Texture Compression version 2\nEVC\tEssential Video Coding\nFFS\tFor Further Study\nFLUS\tFramework for Live Uplink Streaming\nFOV\tField-Of-View\nFPS\tFrames Per Second\nGBR\tGuaranteed BitRate\nGFBR\tGuaranteed Flow Bit Rate\nGNSS\tGlobal Navigation Satellite System\nG-PCC\tGeometry-based Point Cloud Compression\nGPS\tGlobal Positioning System\nGPU\tGraphics Processing Unit\nHEVC\tHigh-Efficiency Video Coding\nHMD\tHead-Mounted Display\nHRTF\tHead-Related Transfer Function\nHTTP\tHyper-Text Transfer Protocol\nHUD\tHeads-Up Display\nIDMS\tInter-destination Multimedia Synchronization\nIMU\tInertial Measurement Unit\nIOD\tInter-aural Output Difference\nIVAS\tImmersive Voice and Audio Services\nJPEG\tJoint Photographic Experts Group\nJVET\tJoint Video Exploration Team\nLIDAR\tLight Detection and Ranging\nMCPTT\tMission Critical Push To Talk\nMCU\tMultipoint Control Unit\nMEC\tMulti-access Edge Computing\nMFBR\tMaximum Flow Bit Rate\nMMS\tMultimedia Messaging Service\nMOBA\tMultiplayer Online Battle Arena\nMPEG\tMoving Pictures Expert Group\nMR\tMixed Reality\nNBMP\tNetwork-Based Media Processing\nNCIS\tNetwork Controlled Interactive Service\nNEF\tNetwork Exposure Function\nPBR\tPhysically-Based Rendering\nPCC\tPoint Cloud Compression\nPCF\tPolicy Control Function\nPDB\tPacket Delay Budget\nPDU\tPacket Data Unit\nPER\tPacket Error Rate\nPLY\tPoLYgon\nPNG\tPortable Network Graphics\nPPI\tPixels Per Inch\nPQI\tPC5 QoS Identifier\nPSS\tPacket-Switched Streaming\nPTT\tPush To Talk\nPVRTC\tPowerVR Texture Compression\nQCI\tQoS Class Identifier\nQFI\tQoS Flow ID\nQoE\tQuality of EXperience\nQoS\tQuality of Service\nRCS\tRich Communication Service\nRGB\tRed-Green-Blue colour space\nRGBD\tRed-Green-Blue-Depth\nRPG\tRole Playing Game\nRQA\tReflective QoS Attribute\nRTP\tReal-Time Protocol\nRTS\tReal-time Strategy\nRTT\tRound Trip Time\nSCS\tSpatial Compute Server\nSDP\tSession Description Protocol\nSIP\tSession Initiation Protocol\nSLAM\tSimultaneous Localization and Mapping\nSWB\tSuper WideBand\nTCP\tTransmission Control Protocol\nToF\tTime of Flight\nTPU\tTensor Processing Unit\nUL\tUpLink\nUSB\tUniversal Serial Bus\nVCL\tVideo Coding Layer\nV-PCC\tVideo-based Point Cloud Compression\nVPS\tVisual Positioning System\nVR\tVirtual Reality\nVVC\tVersatile Video Coding\nXR\tExtended reality\nYUV\t\tLuminance-Bandwidth-Chrominance\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "4\tIntroduction to Extended Reality",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "4.1\tXR Terms and Definitions",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.1.1\tDifferent Types of Realities",
                            "text_content": "The scope of this clause is the introduction of eXtended Reality (XR) to 3GPP services and networks. eXtended Reality (XR) is an umbrella term for different types of realities as shown in Figure 4.1-1. The figure also shows different application domains of XR such as entertainment, healthcare, education, etc. The different terms are defined in the following, reusing and extending some definitions from 3GPP TR26.918 [2].\nFigure 4.1-1 illustrates the various types of realities and their respective applications, showcasing the diverse range of virtual and augmented experiences. The figure presents a comprehensive overview of the different realities, including their characteristics and use cases, providing valuable insights into the evolving landscape of immersive technologies.\nFigure 4.1-1: Different Types of Realities and some applications\nVirtual reality (VR) is a rendered version of a delivered visual and audio scene. The rendering is designed to mimic the visual and audio sensory stimuli of the real world as naturally as possible to an observer or user as they move within the limits defined by the application. Virtual reality usually, but not necessarily, requires a user to wear a head mounted display (HMD), to completely replace the user's field of view with a simulated visual component, and to wear headphones, to provide the user with the accompanying audio. Some form of head and motion tracking of the user in VR is usually also necessary to allow the simulated visual and audio components to be updated in order to ensure that, from the user's perspective, items and sound sources remain consistent with the user's movements. Additional means to interact with the virtual reality simulation may be provided but are not strictly necessary.\nAugmented reality (AR) is when a user is provided with additional information or artificially generated items or content overlaid upon their current environment. Such additional information or content will usually be visual and/or audible and their observation of their current environment may be direct, with no intermediate sensing, processing and rendering, or indirect, where their perception of their environment is relayed via sensors and may be enhanced or processed.\nMixed reality (MR) is an advanced form of AR where some virtual elements are inserted into the physical scene with the intent to provide the illusion that these elements are part of the real scene.\nExtended reality (XR) refers to all real-and-virtual combined environments and human-machine interactions generated by computer technology and wearables. It includes representative forms such as AR, MR and VR and the areas interpolated among them. The levels of virtuality range from partially sensory inputs to fully immersive VR. A key aspect of XR is the extension of human experiences especially relating to the senses of existence (represented by VR) and the acquisition of cognition (represented by AR).\nOther terms used in the context of XR are Immersion as the sense of being surrounded by the virtual environment as well as Presence providing the feeling of being physically and spatially located in the virtual environment. The sense of presence provides significant minimum performance requirements for different technologies such as tracking, latency, persistency, resolution and optics. For more details, refer to clause 4.2.\nOther relevant terms in the context of XR experiences are:\n-\tParallax is the relative movement of objects as a result of a change in point of view. When objects move relative to each other, users tend to estimate their size and distance.\n-\tOcclusion is the phenomena when one object in a 3D space is blocking another object from being viewed.\nThis document uses the acronym XR throughout to refer to equipment, applications and functions used for Virtual Reality, Augmented Reality, and other related technologies. Examples include, but are not limited to:\n-\tHead-mounted displays for Virtual Reality,\n-\tOptical see-through glasses and camera see-through HMDs for Augmented and Mixed Reality,\n-\tMobile devices with positional tracking and camera.\nAll in common with them is the ability that they offer some degree of spatial tracking and the spatial tracking results in an interaction to view some form of virtual content. More details on XR devices are provided in clause 4.8.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.1.2\tDegrees of Freedom and XR Spaces",
                            "text_content": "A user acts in and interacts with extended realities as shown in Figure 4.1-2. Actions and interactions involve movements, gestures, body reactions. Thereby, the Degrees of Freedom (DoF) describe the number of independent parameters used to define movement of a viewport in the 3D space.\nAny consistent interaction for an XR application with XR hardware is assumed to be restricted to an XR session. Once an XR session has been successfully established, it can be used to poll the viewer pose, query information about the user’s environment, and present imagery to the user.\nFigure 4.1-2 illustrates the various degrees of freedom a user experiences in extended reality (XR) environments. The figure showcases a user's position in a virtual space, with axes representing the three-dimensional movement capabilities. The horizontal axis represents the user's head movement, while the vertical axis indicates body movement. The circular area at the bottom represents the user's hand and arm movements. The figure highlights the immersive experience XR provides, allowing users to interact with virtual environments in a more natural and intuitive manner.\nFigure 4.1-2: Different degrees of freedom for a user in extended realities\nTypically, the following different types of Degrees-of-Freedom are described (and also shown in Figure 4.1-3).\n-\t3DoF: Three rotational and un-limited movements around the X, Y and Z axes (respectively pitch, yaw and roll). A typical use case is a user sitting in a chair looking at 3D 360 VR content on an HMD (see Figure 4.1-3 (a)).\n-\t3DoF+: 3DoF with additional limited translational movements (typically, head movements) along X, Y and Z axes. A typical use case is a user sitting in a chair looking at 3D 360 VR content on an HMD with the capability to slightly move his head up/down, left/right and forward/backward (see Figure 4.1-3 (b)).\n-\t6DoF: 3DoF with full translational movements along X, Y and Z axes. Beyond the 3DoF experience, it adds (i) moving up and down (elevating/heaving); (ii) moving left and right (strafing/swaying); and (iii) moving forward and backward (walking/surging). A typical use case is a user freely walking through 3D 360 VR content (physically or via dedicated user input means) displayed on an HMD (see Figure 4.1-3 (d)).\n-\tConstrained 6DoF: 6DoF with constrained translational movements along X, Y and Z axes (typically, a couple of steps walking distance). A typical use case is a user freely walking through VR content (physically or via dedicated user input means) displayed on an HMD but within a constrained walking area (see Figure 4.1-3 (c)).\n\n\nFigure 4.1-3: Different degrees of freedom\nAnother term for Constrained 6DoF is Room Scale VR being a design paradigm for XR experiences which allows users to freely walk around a play area, with their real-life motion reflected in the XR environment.\nNote: Constrained 6DoF is not intended to describe multi-room spaces, areas with uneven floor levels, or very large open areas. Content that handles those scenarios is better categorized as (unconstrained) 6DoF.\nThe Degrees of Freedom may also be used to describe the tracking capabilities of an XR device. For more details on tracking, refer to clauses 4.1.3 and 4.1.4. Content and tracking capabilities of a device do not necessarily have to match. However, the user is preferably informed by the application on any differences between content and tracking capabilities in terms of number/types of degrees of freedom. .\nSpaces provide a relation of the user’s physical environment with other tracked entities. An XR Space represents a virtual coordinate system with an origin that corresponds to a physical location. The world coordinate system is the coordinate system in which the virtual world is created. Coordinate systems are essential for operating in 3-dimensional virtual and real worlds for XR applications.\nAs an example, a coordinate system is defined by OpenXR [16] in clause 2.15 as well as for WebXR [17], both using a Cartesian right-handed coordinate system as shown in Figure 4.1-4. This coordinate system is right-handed in sense that, where +X is considered \"Right\", +Y is considered \"Up\", and -Z is considered \"Forward\".\nA coordinate system is expected to be a rectangular Cartesian in which all axes are equally scaled.\nFigure 4.1-4 illustrates a right-handed coordinate system, which is a standard for representing three-dimensional positions in a two-dimensional diagram. The system is defined by three mutually perpendicular coordinate axes, with the positive x-axis pointing to the right, the positive y-axis pointing upward, and the positive z-axis pointing forward. This system is commonly used in various fields such as engineering, physics, and computer graphics to describe and analyze three-dimensional objects and their relationships in two dimensions. The figure shows a three-dimensional object with its position and orientation relative to the coordinate axes, providing a clear and concise representation of the object's position and orientation in space.\nFigure 4.1-4: Right-Handed Coordinate system\nA three-dimensional vector is defined by the (x,y,z) coordinates. If used to represent physical distances (rather than e.g. velocity or angular velocity) and not otherwise specified, values are in meters. A position in the XR space is a 3D-vector representing a position within a space and relative to the origin.\nAn XR reference space is one of several common XR Spaces that can be used to establish a spatial relationship with the user’s physical environment. An XR reference space may be restricted, determining the ability by the user to move. This aligns with the definitions above as well as Figure 4.1-3, namely an XR reference space providing the degrees of freedom for a user.\n-\tFor 3DoF, the XR reference space is limited to a single position.\n-\tFor 3DoF+, the XR reference space is limited to a small space centered around a single position, a small bounding box, limited to positions attainable with head movements only, around a single position is provided.\n-\tFor constrained 6DoF, the XR reference space has a native bounds geometry describing the border around the space, which the user can expect to safely move within. Such borders may for example be described by polygonal boundary given as an array representing a loop of points at the edges of the safe space. The points describe offsets from the origin in meters.\n-\tFor 6DoF, the XR reference space is unlimited and basically includes the whole universe.\nA simplified diagram (mapped to 2D) on XR Spaces and and their relation to the scene is provided in Figure 4.1-5.\nFigure 4.1-5 presents a simplified illustration of XR spaces, showcasing the integration of various technologies to create an immersive virtual environment. The figure includes a user interface, a virtual environment, and a set of sensors and devices that capture and process data. The user interface allows for interaction with the virtual environment, while the sensors and devices collect data on the user's movements and actions. This data is then processed and used to create a more realistic and engaging virtual experience. The figure also highlights the importance of collaboration between different technologies and systems to create an effective XR space.\nFigure 4.1-5: Simplified Illustration of XR Spaces\nUnless the user does a reconfiguration, XR reference spaces within an XR session are static, i.e. the space the user can move in is restricted by the initial definition.\nAn XR View describes a single view into an XR scene for a given time. Each view corresponds to a display or portion of a display used by an XR device to present the portion of the scene to the user. Rendering of the content is expected to be done to well align with the view's physical output properties, including the field of view, eye offset, and other optical properties. A view, among others, has associated\n-\ta view offset, describing a position and orientation of the view in the XR reference space,\n-\tan eye describing which eye this view is expected to be shown. Displays may support stereoscopic or monoscopic viewing.\nAn XR Viewport describes a viewport, or a rectangular region, of a graphics surface. The XR viewport corresponds to the projection of the XR View onto a target display. An XR viewport is predominantly defined by the width and height of the rectangular dimensions of the viewport. In 3D computer graphics, the view frustum is the region of space in the modeled world that may appear on the screen, i.e. it is the field of view of a perspective virtual camera system. The planes that cut the frustum perpendicular to the viewing direction are called the near plane and the far plane. Objects closer to the camera than the near plane or beyond the far plane are not drawn. Sometimes, the far plane is placed infinitely far away from the camera so all objects within the frustum are drawn regardless of their distance from the camera.\nGenerally, an XR Pose describes a position and orientation in space relative to an XR Space.\n-\tThe position in the XR space is a 3D-vector representing the position within a space and relative to the origin defined by the (x,y,z) coordinates.  If used to represent physical distances, x, y, and z are in meters.\n-\tThe orientation in the XR space is a quaternion representing the orientation within a space and defined by a four-dimensional or homogeneous vector with (x,y,z,w) coordinates, with w being the real part of the quarternion and x, y and z the imaginary parts.\nUnit quaternions are used to document spatial rotations in three dimensions. Roll, pitch, and yaw as for example used in TS 26.118 [3] have limitations(for example due to the well-known gimbal lock). Hence, in computer science, engineering and XR applications, they are replaced with the more robust quaternion.\nAn XR Viewer Pose is an XR Pose describing the state of a viewer of the XR scene as tracked by the XR device. XR Viewer Poses are documented relative to an XR Reference Space.\nThe views array is a sequence of XR Views describing the viewpoints of the XR scene, relative to the XR Reference Space the XR Viewer Pose was queried with.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "",
                                    "table number": 1,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "4.1.3\tTracking and XR Viewer Pose Generation",
                            "text_content": "In XR applications, an essential element is the use of spatial tracking. Based on the tracking and the derived XR Viewer Pose, content is rendered to simulate a view of virtual content.\nXR viewer poses and motions can be sensed by Positional Tracking, i.e. the process of tracing the XR scene coordinates of moving objects in real-time, such as HMDs or motion controller peripherals. Positional Tracking allows to derive the XR Viewer Pose, i.e. the combination of position and orientation of the viewer. Different types of tracking exist:\n-\tOutside-In Tracking: a form of positional tracking and, generally, it is a method of optical tracking. Tracking sensors placed in a stationary location and oriented towards the tracked object that moves freely around a designated area defined by sensor coverage.\n-\tInside-out Tracking: a method of positional tracking commonly used in virtual reality (VR) technologies, specifically for tracking the position of head-mounted displays (HMDs) and motion controller accessories whereby the location of the cameras or other sensors that are used to determine the object's position in space are located on the device being tracked (e.g. HMD).\n-\tWorld Tracking: a method to create AR experiences that allow a user to explore virtual content in the world around them with a device's back-facing camera using a device's orientation and position, and detecting real-world surfaces, as well as known images or objects.\n-\tSimultaneous Localization and Mapping (SLAM) is the computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of the user's location within an unknown environment. For more details refer on SLAM, refer to clause 4.1.4.\nIf not mentioned otherwise, it is assumed that devices in the context of the document are able to track the XR Viewer Pose in 6DoF with sufficient accuracy as defined in clause 4.2.1.\nTo maintain a reliable registration of the virtual world with the real world as well as to ensure accurate tracking of the XR Viewer pose, XR applications require highly accurate, low-latency tracking of the device at about 1kHz sampling frequency. An XR Viewer Pose consists of the orientation (for example, 4 floating point values in OpenXR [16])) and the position (for example, 3 floating point values in OpenXR [16]). In addition, the XR Viewer Pose needs to have assigned a time stamp. The size of a XR Viewer Pose associated to time typically results in packets of size in the range of 30-100 bytes, such that the generated data is around several hundred kbit/s if delivered over the network.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.1.4\tXR Spatial Mapping and Localization",
                            "text_content": "Spatial mapping, i.e. creating a map of the surrounding area, and localization, i.e. establishing the position of users and objects within that space, are some of the key areas of XR and in particular AR. Multiple sensor inputs are combined to get a better localization accuracy, e.g., monocular/stereo/depth cameras, radio beacons, GPS, inertial sensors, etc.\nSome of the methods involved are listed below:\n1)\tSpatial anchors are used for establishing the position of a 3D object in a shared AR/MR experience, independent of the individual perspective of the users. Spatial anchors are accurate within a limited space (e.g. 3m radius for the Microsoft® Mixed Reality Toolkit). Multiple anchors may be used for larger spaces.\n2)\tSimultaneous Localization and Mapping (SLAM) is used for mapping previously unknown environments, while also maintaining the localization of the device/user within that environment.\n3)\tVisual Localization, e.g., vSLAM, Visual Positioning System (VPS), etc., perform localization using visual data from, e.g., a mobile camera, combined with other sensor data.\nSpatial mapping and localization can be done on the device. However, network elements can support the operations in different ways:\n1)\tCloud services may be used for storing, retrieving and updating spatial data. For larger public spaces, crowdsourcing may be used to keep the data updated and available to all.\n2)\tA Spatial Computing Server that collects data from multiple sources and processes it to create spatial maps including, but not limited to, visual and inertial data streamed from XR devices. The service can then provide this information to other users and also assist in their localization based on the data received from them.\nIndoor and outdoor mapping and localization are expected to have different requirements and limitations. Privacy concerns need to be explored by the service provider when scanning indoor spaces and storing spatial features, especially when it is linked to global positioning.\nTracking adds the concept of continuous localisation over time.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.2\tQuality-of-Experience for XR",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.2.1\tImmersiveness and Presence",
                            "text_content": "For providing XR experiences that make the user feel immersed and present, several relevant quality of experience factors have been collected (). Immersion is an objective description of aspects of the system such as field of view and display resolution. Presence is the feeling of being physically and spatially located in an environment. According to Slater and Usoh [43], “immersion is a necessary rather than a sufficient condition for presence - immersion describes a kind of technology, and presence describes an associated state of consciousness.” Presence is divided into 2 types: Cognitive Presence and Perceptive Presence.\nCognitive Presence is the presence of one's mind. It can be achieved by watching a compelling film or reading an engaging book. Cognitive Presence is important for an immersive experience of any kind.\nPerceptive Presence is the presence of one's senses. To accomplish perceptive presence, one's senses, sights, sound, touch and smell, have to be tricked. To create perceptive presence, the XR Device has to fool the user's senses, most notably the audio-visual system. XR Devices achieve this through positional tracking based on the movement. The goal of the system is to maintain your sense of presence and avoid breaking it.\nPerceptive Presence is the objective to be achieved by XR applications and is what is referred in the following.\nIn a paper [9] titled \"Research on Presence in Virtual Reality: A Survey\", the authors quote Matthew Lombard’s slightly more scientific definition of presence: \"Presence (a shortened version of the term “telepresence”) is a psychological state of subjective perception in which even though part or all of an individual’s current experience is generated by and/or filtered through human-made technology, part or all of the individual’s perception fails to accurately acknowledge the role of the technology in the experience. Except in the most extreme cases, the individual can indicate correctly that s/he is using the technology, but at some level, and to some degree, her/his perceptions overlook that knowledge and objects, events, entities, and environments are perceived as if the technology was not involved in the experience.\" In other words, feeling like you’re really there.\nPresence is achieved when the involuntary aspects of our reptilian corners of our brains are activated. When the user reaches out to grab the virtual apple, becomes unwilling to step off a plank or feel nervous when walking on rooftops. According to Teo Choong Ching [10], there are four components relevant for feeling present, namely the\n1.\tThe Illusion of being in a stable spatial place\n2.\tThe Illusion of self-embodiment.\n3.\tThe Illusion of Physical Interaction\n4.\tThe Illusion of Social Communication\nThe most relevant component from a the technical aspect in the context of this Technical Report is the first one. This part of presence can be broken down into three broad categories, listed in order of most important to least important for their impact on creating presence: Visual presence, Auditory presence, and sensory or haptic presence.\nTechnical Requirements for visual presence have been formulated by Valve's ™ R&D Team Brendan Iribe () from Oculus ™ as well as from experience collected from 3GPP members product development teams:\n-\tTracking\n-\t6 degrees of freedom tracking - ability to track user's head in rotational and translational movements.\n-\t360 degrees tracking - track user's head independent of the direction the user is facing.\n-\tSub-centimeter accuracy - tracking accuracy of less than a centimeter.\n-\tQuarter-degree-accurate rotation tracking\n-\tNo jitter - no shaking, image on the display has to stay perfectly still.\n-\tFor room-scale games and experiences, comfortable tracking volume - large enough space to move around and still be tracked of roughly 2m cubes. For seated games/experiences a smaller tracking volume is sufficient.\n-\tTracking needs to be done frequently to always be able to operate with the latest XR Viewer Pose. Minimum update rates as discussed above are 1000Hz and beyond. Especially rotational tracking requires high frequency.\n-\tLatency\n-\tLess than 20 ms motion-to-photon latency - less than 20 milliseconds of overall latency (from the time you move your head to when you see the display change).\n-\tMinimize the time of pose-to-render-to-photon. Rendering content as quickly as possible. Less than 50ms for render to photon in order to avoid wrongly rendered content. For more details refer to clause 4.2.2.\n-\tFuse optical tracking and inertial measurement unit (IMU) data –\n-\tMinimize loop: tracker → CPU → GPU → display → photons.\n-\tMinimize interaction delays and age of content depending on the application. For more details see 4.2.2.\n-\tPersistence\n-\tLow persistence - Turn pixels on and off every 2 - 3 ms to avoid smearing / motion blur. Pixel persistence is the amount of time per frame that the display is actually lit rather than black. “Low persistence” is simply the idea of having the screen lit for only a small fraction of the frame. The reason is that the longer a frame goes on for, the less accurate it will be compared to where you’re currently looking. The brain is receiving the same exact image for the entire frame even as you turn your head whereas in real life your view would constantly adjust.\n-\t90 Hz and beyond display refresh rate to eliminate visible flicker.\n-\tResolution\n-\tSpatial Resolution: No visible pixel structure - you cannot see the pixels. Low resolution and low pixels per inch (PPI), can cause the user to feel pixelation and feel like he or she is looking through a screen door.\n-\tIn 2014, It was thought at least 1k by 1k pixels per eye would be sufficient.\n-\tHowever, in theory, in our fovea, we need about 120 pixels per degree of view to match reality, possibly requiring significantly more than the 1k by 1k, all the way to 8k.\n-\tIn 2019, it is commonly accepted that 2k by 2k per eye provides acceptable quality. Increasing the horizontal resolution to 4k is considered a next step.\n-\tAccording to Plamer Luckey, founder of Oculus® Rift™, pixelation will not go away completely until at least 8K resolution (8196 x 4096) per eye is achieved ().\n-\tTemporal Resolution: According to , to deliver comfortable, compelling VR that truly generates presence, developers will still need to target a sustained frame rate of 90Hz and beyond, despite the usage of asynchronous time warping.\n-\tOptics\n-\tWide Field of view (FOV) is the extent of observable world at any given moment and typically 100 - 110 degrees FOV is needed. For details on FoV, see 3GPP TR 26.918 [2], clause 4.2.2.\n-\tComfortable eyebox - the minimum and maximum eye-lens distance wherein a comfortable image can be viewed through the lenses.\n-\tHigh quality calibration and correction - correction for distortion and chromatic aberration that exactly matches the lens characteristics. For details on optics, see 3GPP TR 26.918 [2], clauses 4.2.3 and 4.2.4.\nFor requirements on auditory presence, refer to 3GPP TR 26.918 [2] and [11].\nFor requirements on sensory and haptics presence, refer for example to [11].\nThe sense of presence is not only important to VR experiences, but equally so to immersive AR experiences. To achieve Presence in Augmented Reality, seamless integration of virtual content and physical environment is required. Like in VR, the virtual content has to align with user's expectations. For truly immersive AR and in particular MR, it is expected that users cannot discern virtual objects from real objects.\nAlso relevant for VR and AR, but in particular AR, is not only the awareness for the user as shown in Figure 4.1-5, but also for the environment. This includes:\n-\tSafe zone discovery\n-\tDynamic obstacle warning\n-\tGeometric and semantic environment parsing\n-\tEnvironmental lighting\n-\tWorld mapping\nFigure 4.1-5 illustrates the importance of environmental awareness in extended reality (XR) applications, showcasing how virtual and augmented reality systems interact with their surroundings. The figure presents a comprehensive overview of the XR system, highlighting the various components involved in sensing and understanding the environment. These components include sensors for capturing real-world data, processing units for analyzing the data, and actuators for responding to the environment. The figure emphasizes the need for real-time interaction between the XR system and its surroundings, as well as the role of machine learning algorithms in improving the system's ability to adapt to changing environmental conditions. Overall, the figure underscores the significance of environmental awareness in the development of effective and user-friendly XR applications.\nFigure 4.1-5: Environmental Awareness in XR Applications\nFor AR, to obtain an enhanced view of the real environment, the user may wear a see-through HMD to see 3D computer-generated objects superimposed on his/her real-world view. This see-through capability can be accomplished using either an optical see-through or a video see-through HMD. Tradeoffs between optical and video see-through HMDs with respect to technological, perceptual, and human factors issues are for example discussed in [18].\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.2.2\tInteraction Delays and Age of Content",
                            "text_content": "Beyond the sense of presence and immersiveness, the age of the content and user interaction delay are of the uttermost importance for immersive and non-immersive interactive experiences, i.e. experiences for which the user interaction with the scene impacts the content of scene (such as online gaming).\nUser interaction delay is defined as the time duration between the moment at which a user action is initiated and the time such an action is taken into account by the content creation engine. In the context of gaming, this is the time between the moment the user interacts with the game and the moment at which the game engine processes such a player response.\nAge of content is defined as the time duration between the moment a content is created and the time it is presented to the user. In the context of gaming, this is the time between the creation of a video frame by the game engine and the time at which the frame is finally presented to the player.\nThe roundtrip interaction delay is therefore the sum of the Age of Content and the User Interaction Delay. If part of the rendering is done on an XR server and the service produces a frame buffer as rendering result of the state of the content, then for raster-based split rendering (as defined in clause 6.2.5) in cloud gaming applications, the following processes contribute to such a delay:\n-\tUser Interaction Delay\n-\tcapture of user interaction in game client,\n-\tdelivery of user interaction to the game engine, i.e. to the server (aka network delay),\n-\tprocessing of user interaction by the game engine/server,\n-\tAge of Content\n-\tcreation of one or several video buffers (e.g. one for each eye) by the game engine/server,\n-\tencoding of the video buffers into a video stream frame,\n-\tdelivery of the video frame to the game client (a.k.a. network delay),\n-\tdecoding of the video frame by the game client,\n-\tpresentation of the video frame to the user (a.k.a. framerate delay).\nFor gaming, for example, references [19] and [20] provide interaction delay tolerance thresholds per game type illustrated in Table 4.2.2-1. Note that this Interaction delay refers to the roundtrip interaction delay as defined above.\nTable 4.2.2-1: Interaction delay tolerance in traditional gaming (from [19]).\n\nIn [21], the authors set up a 12 players match of Unreal Tournament 2003TM in a controlled environment. Each player is assigned a specific amount of latency and jitter for the duration of the match. After the match, the players answer a questionnaire about their experience in the game. This study still uses relatively few players, but they are able to conclude that more than 60ms of latency noticeably reduces both performance and experience of this game.\nIn general, it seems that 60 ms [18], or even 45 ms [22] are better estimates at how much latency is acceptable in the most fast-paced games than the traditionally quoted 100ms value.\nIn other cases the latency of the content is for example determined by conversational delay thresholds. Typically, around 200ms of latency is acceptable.\nOverall, different applications and use cases require different delay requirements and this phenomena should be considered.\nThe four following categories are considered with respect to roundtrip interaction delay:\n-\tUltra-Low-Latency applications: roundtrip interaction delay threshold of at most 50ms latency.\n-\tLow-Latency applications: roundtrip interaction delay threshold of at most 100ms latency.\n-\tModerate latency applications: roundtrip interaction delay threshold of at most 200ms latency.\n-\tNon- critical latency applications: roundtrip interaction delay threshold higher than 200ms latency.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 4.2.2-1: Interaction delay tolerance in traditional gaming (from [19]).",
                                    "table number": 2,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "4.3\tXR Delivery in 5G System",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.3.1\tGeneral Delivery Categories",
                            "text_content": "For the purpose of classifying use cases, this clause defines delivery categories for XR experiences. The following categories are defined:\n-\tDownload: An XR experience is downloaded and consumed offline without requiring a connection. All media and experience related traffic is downlink.\n-\t(Passive) Streaming: The experience is consumed in real-time from a network server. The user does not interact with the XR experience, or if interacting with the XR experience, the interaction is not triggering any uplink traffic. All media related traffic is downlink.\n-\tInteractive (Streaming): The experience is consumed in real-time from a network server. The user (or the device automatically) interacts with the XR experience and the interaction changes the delivered content. The traffic is predominantly downlink, but certain traffic is uplink, e.g. XR Viewer Pose information. Different flavours of interaction exist, for example viewport adaptation, gaming events, etc. Interaction delay requirements may be different, ranging from immersive latency requirements to more static selection interactions.\n-\tConversational: The experience is generated, shared and consumed in real-time from two or more participants with conversational latency requirements.\n-\tSplit Compute/Rendering: Network functions run an XR engine to support processing and pre-rendering of immersive scenes and the delivery is split into more than one connection, e.g. Split rendering, Edge Computing, etc. The latency and interaction requirements again depend on the use case and the architecture implementation.\nA more detailed analysis of architectures in the context of 5G is provided in clause 6.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.3.2\t5G System and Radio Functionalities for XR",
                            "text_content": "The integration of XR applications within the 5G System is approached following the model of 5G Media Streaming as defined in 3GPP TS 26.501 [8]. Assume a 5G-XR Application Provider being an XR Application provider that makes use of 5G System functionalities for its services. For this purpose, it provides a 5G-XR Aware Application on the UE to make use of a 5G-XR client and network functions using network interfaces and APIs, potentially defined in 5G-XR related specifications.\nThe architecture in Figure 4.3.2-1 represents potential 5G-XR functions within the 5G System (5GS) as defined in 3GPP TS 23.501 [23]. Three main functions are defined:\n5G-XR AF: An Application Function similar as defined in 3GPP TS 23.501 [23], clause 6.2.10, dedicated to 5G-XR Services.\n5G-XR AS: An Application Server dedicated to 5G-XR Services.\n5G-XR Client: A UE internal function dedicated to 5G-XR Services.\nIn the context of this Technical Report, 5G-XR AF and 5G-XR AS are initially considered Data Network (DN) functions and communicate with the UE via N6, N3 and Uu as defined in 3GPP TS 23.501 [23].\nCommunication through sidelink PC5 may be an alternative to Uu based communication.\nFunctions in trusted DNs are trusted by the operator’s network as illustrated.. Therefore, AFs in trusted DNs may directly communicate with all 5G Core functions.\nFunctions in external DNs may only communicate with 5G Core functions via the NEF using N33.\nFigure 4.3.2-1 illustrates the integration of 5G-XR functions within a 5G system, showcasing how various components work together to enable enhanced mobile broadband (eMBB), massive machine-type communications (mMTC), and ultra-reliable low latency communications (URLLC). The figure highlights the role of the 5G core network, which manages user plane and control plane functions, and the use of network slicing to create dedicated virtual networks for different use cases. Additionally, it emphasizes the importance of edge computing and the deployment of small cell and macro base stations to improve coverage and capacity. The figure also touches on the use of artificial intelligence and machine learning for network optimization and intelligent resource allocation.\nFigure 4.3.2-1: 5G-XR functions integrated in 5G System\nNOTE 1: The functions indicated by the yellow filled boxes are in potential scope of stage 3 specifications for 5G-XR. The functions indicated by the grey boxes are defined in 5G System specifications. The functions indicated by the blue boxes are assigned to the applications.\nThe above architecture is used as a starting point. With XR related functions exclusively assigned to either DN or UE. However, architectural extensions may be identified for the 3GPP system that may benefit from XR applications. Examples include the use of network slicing, edge computing or usage of 5G quality of service.\n\nFigure 4.3.2 - 2: 5G-XR Interfaces and Architecture illustrates the interfaces and architecture of a 5G-enhanced XR (Extended Reality) system. The figure showcases the integration of 5G technology with XR devices, highlighting the high-speed, low-latency communication required for seamless virtual and augmented reality experiences. The diagram includes various components such as the 5G core network, XR devices, and edge computing nodes, emphasizing the distributed nature of the system. The figure also highlights the importance of network slicing and edge computing in supporting real-time, high-performance XR applications.\nFigure 4.3.2-2: 5G-XR Interfaces and Architecture\nA basic XR architecture integrated in 5G is shown in Figure 4.3.2-2.\nThe following functions may be considered to be defined:\n-\t5G-XR Client on UE: Receiver of 5G-XR session data that may be accessed through well-defined interfaces/APIs by the 5G-XR Aware Application.\n-\tThe 5G-XR Client contains two sub-functions\n-\tXR Session Handler: A function of the UE that communicates with the 5G-XR AF in order to establish, control and support the delivery of an XR session. The XR Session Handler exposes APIs that can be used by the 5G-XR Aware Application.\n-\tXR Engine: A function of the UE that communicates with the 5G-XR Application Server in order to get access to XR related data, includes XR relevant functionalities such as sensors and tracking, processes this data and communicates with the XR Session Handler for XR session control.\n-\t5G-XR Aware Application: The 5G-XR Client is typically controlled by an external XR aware application, e.g. an App, which implements the external application service provider specific logic and enables establishing an XR session. The 5G-XR Aware Application makes use of 5G-XR Client and network functions using interfaces and APIs.\n-\t5G-XR AS: An Application Server which hosts 5G-XR media and media functions.\n-\t5G-XR Application Provider: External XR application provider that makes use of 5G-XR client and network functionalities to provide an XR experience to the 5G-XR Aware applications.\n-\t5G-XR AF: provides various control functions to the XR Session Handler on the UE and/or to the 5G-XR Application Provider. It may relay or initiate a request for different Policy or Charging Function (PCF) treatment or interact with other network functions.\nIn the context of the above, 5G radio may also be differentiated between 5G Uu and 5G Sidelink/PC5. Uu is the interface between User Equipement (UE) and Radio Access Network (RAN) as defined in 3GPP TS 38.300 [24]. Sidelink is a mode of communication whereby UEs can communicate with each other directly as defined in 3GPP TS 38.300 [24].\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.3.3\tQuality-of-Service in 5G",
                            "text_content": "Clause 5.7 of 3GPP TS 23.501 [8] explains the QoS model for 5G. The 5G QoS model is based on QoS Flows. The 5G QoS model supports both:\n-\tQoS Flows that require guaranteed flow bit rate (GBR QoS Flows)\n-\tand QoS Flows that do not require guaranteed flow bit rate (Non-GBR QoS Flows).\nThe 5G QoS model also supports Reflective QoS (see clause 5.7.5 of 3GPP TS 23.501 [8]).\nA QoS Flow ID (QFI) is used to identify a QoS Flow in the 5G System. User Plane traffic assigned to the same QoS Flow within a Protocol Data Unit (PDU) Session receives the same traffic forwarding treatment (e.g. scheduling, admission threshold).\nThe QFI may be dynamically assigned or may be equal to the 5G QoS Identifier (5QI). A QoS Flow may either be 'GBR', 'Non-GBR' or “Delay Tolerant GBR” depending on its QoS profile and it contains QoS parameters as follows:\n-\tFor each QoS Flow, the QoS profile includes the QoS parameters:\n-\t5G QoS Identifier (5QI); and\n-\tAllocation and Retention Priority (ARP).\n-\tFor each Non-GBR QoS Flow only, the QoS profile can also include the QoS parameter:\n-\tReflective QoS Attribute (RQA).\n-\tFor each GBR QoS Flow only, the QoS profile also include the QoS parameters:\n-\tGuaranteed Flow Bit Rate (GFBR) - uplink (UL) and downlink (DL); and\n-\tMaximum Flow Bit Rate (MFBR) - UL and DL; and\n-\tIn the case of a GBR QoS Flow only, the QoS profile can also include one or more of the QoS parameters:\n-\tNotification control;\n-\tMaximum Packet Loss Rate - UL and DL\nThe one-to-one mapping of standardized 5QI values to 5G QoS characteristics is specified in table 5.7.4-1 of 3GPP TS 23.501 [8] and shown below in Table 4.3.3-1.\n5QI values potentially relevant for XR applications in the context of this Technical Report are highlighted in italics.\nTable 4.3.3-1: Standardized 5QI to QoS characteristics mapping (identical to Table 5.7.4.1-1 in 3GPP TS 23.501 [10])\nThe applicability of 5QI and potential gaps for XR Services over 5G are analysed further in this Technical Report.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 4.3.3-1: Standardized 5QI to QoS characteristics mapping (identical to Table 5.7.4.1-1 in 3GPP TS 23.501 [10])",
                                    "table number": 3,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "4.3.4\t5G Media Delivery",
                            "text_content": "In the context of this Technical Report and the delivery options identified in clause 4.3.3.1, the first three basic delivery types – download, passive streaming and interactive streaming – are most suitably mapped to 5G Media Streaming as defined in 3GPP TS 26.501 [8] and associated stage 3 specifications. The applicability of 5G Media Streaming for XR applications and potential necessary extensions are identified in clauses 5, 6 and 7 of this Technical Report.\nConversational services are most suitably mapped to the Multimedia Telephony Service for IMS (MTSI) as defined in 3GPP TS 22.173 [24] with focus on XR media handling (e.g. signalling, transport, codecs, formates) when using 3GPP access, in particular 5G radio technologies. It is expected that the media handling of MTSI clients as defined in 3GPP TS 26.114 [25] may be suitably extended in order to support XR applications and services.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.3.5\tEdge Computing",
                            "text_content": "Beyond the use of Application Servers as defined in 5G Media Streaming today, the 5G-XR application may benefit from additional processing in the edge. In an example, as shown in Figure 4.3.5-1, an edge platform may be offered by the 5G network operator to support XR services served from the content provider or from the cloud.\nFigure 4.3.5 illustrates the integration of cloud and edge processing within a telecommunications network, showcasing the distribution of processing power across multiple data centers and edge servers. The figure emphasizes the strategic placement of these resources to optimize network performance, reduce latency, and improve overall system efficiency. Key visual elements include the interconnected data centers, edge servers, and the underlying network infrastructure, highlighting the complex interplay between these components in delivering high-quality service to end-users.\nFigure 4.3.5-1 Cloud and Edge Processing\nIn the context of Release-17, 3GPP work is ongoing in order to identify the integration of edge processing in 5G systems. 3GPP TR 23.748 [28] defines the necessary modifications to 5G system architecture to enhance Edge Computing. This work is currently in study phase, defining key issues and scope for Release-17. Specifically, this study is investigating mechanisms to discover connectivity to available Edge Computing resources (e.g. using DNS), mobility improvements for both UE consuming Edge Computing services and for Edge Application Servers, and for network capability exposure towards the Edge Application Server.\nIn addition, in 3GPP TR 23.758 [27] and 3GPP TS 23.558 [29] a new set of application layer interfaces for Edge Computing are identified that may potentially be useful for integration of Edge Computing. Specifically, the interfaces will enable application-layer discovery of Edge Application Servers, capability exposure towards the Edge Application Server, and procedures for onboarding, registration, and lifecycle management of Edge Applications.\nThe activities detailed in the present clause are intended to be application-neutral (i.e. to provide generic solutions for any use of Edge Computing platforms). The media aspects for using Edge Computing are not identified in these studies and information in the present Technical Report may be beneficial to contribute to Edge Computing for media processing. In particular, split Compute/Rendering architectures are not yet specified in the 5G System architecture beyond those being part of a 5G-XR aware application. Integration of computational resources into the 5G System as part of Edge Computing functionalities are currently under study in 3GPP. The present Technical Report serves to identify potentially relevant functions for XR applications when using Edge Computing and rendering.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.4\tXR Engines and Rendering",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.4.1\tIntroduction",
                            "text_content": "XR engines provide a middleware that abstracts hardware and software functionalities for developers of XR applications. In the market as understood when initially writing this report, such engines are predominantly based on proprietary and commercial solutions, but with a trend towards providing standardized abstraction layers and APIs, notably provided by Khronos' OpenXR [16] as well as W3C's WebXR [17]. An overview of the landscape as seen by the OpenXR community is shown in Figure 4.4.1-1.\nFigure 4.4.1-1 presents the current and future landscape of XR engines and ecosystems, as envisioned by OpenXR. The figure illustrates the various components and stakeholders involved in the XR ecosystem, including hardware manufacturers, software developers, and content creators. The horizontal axis represents the timeline, with the present on the left and the future on the right. The vertical axis represents the different stakeholders, with hardware manufacturers at the bottom, software developers in the middle, and content creators at the top. The figure shows how the XR ecosystem is evolving, with new technologies and partnerships emerging to drive the future of XR.\nFigure 4.4.1-1: XR engine and ecosystem landscape today and in the future as seen by OpenXR © Khronos\nAn XR engine is a software-development environment designed for people to build XR experiences such as games and other XR applications. The core functionality typically provided by XR engines include a rendering engine (\"renderer\") for 2D or 3D graphics, a physics engine or collision detection (and collision response), sound, scripting, animation, artificial intelligence, networking, streaming, memory management, threading, localization support, scene graph, and may include video support.\nTypical components are summarized below\n-\tRendering engine: This engine basically provides the functionalities as documented in clause 4.2.2 with a set of well defined APIs. In summary, the rendering engine generates animated 3D graphics by any of a number of methods (rasterization, ray-tracing etc.). Instead of being programmed and compiled to be executed on the CPU or GPU directly, most often rendering engines are built upon one or multiple rendering application programming interfaces (APIs), such as Direct3D, OpenGL, or Vulkan which provide a software abstraction of the graphics processing unit (GPU).\n-\tAudio engine: The audio engine is the component which consists of algorithms related to the loading, modifying and output of sound through the client's speaker system.  At a minimum it is able to load, decompress and play sound files. More advanced audio engines can calculate and produce such configurations as Doppler effects, echoes, pitch/amplitude adjustments, oscillation, etc. The audio engine can perform calculations on the CPU, or on a dedicated ASIC. Abstraction APIs, such as OpenAL, SDL audio, XAudio 2, Web Audio, etc. are available.\n-\tPhysics engine: The physics engine is responsible for emulating the laws of physics realistically within the XR application. Specifically, it provides a set of functions for simulating physical forces and collisions, acting on the various objects within the scene at run time.\n-\tArtificial intelligence (AI): AI is usually outsourced from the main XR engine into a special module. XR applications may implement very different AI systems, and thus, AI is considered to be specific to the particular XR application for which it is created.\nOne of the major game engines used to create several notable games such as Fortnite ™, PlayerUnknown's Battlegrounds ™, and Life is Strange 2 ™, is the Unreal Engine 4 ™. Another game engine with significant share is the Unity ™ engine. This engine is the one behind games such as Rust ™, Subnautica ™, and Life is Strange Before the Storm ™. Unity™ is a cross-platform XR engine developed by Unity Technologies, first announced and released in June 2005. A component of Unity are scriptable rendering pipelines for developers to create high-end graphics including high-end ones for consoles and PC experiences, as well as the lightweight ones for mobile, virtual reality, augmented reality, and mixed reality.\nThe Unreal Engine was first showcased in the 1998 first-person shooter game Unreal. Although initially developed for first-person shooters, it is used in a variety of other genres, including platformers, fighting games, MMORPGs, and other RPGs.\nFigure 4.4.1-2 provides an overview of typical CPU and GPU operations for XR applications.\nFigure 4.4.1-2 illustrates the parallel processing capabilities of a CPU and GPU for XR applications, showcasing how both components handle tasks concurrently. The CPU, depicted as the central processing unit, is responsible for managing overall system resources and executing instructions, while the GPU, represented as the graphics processing unit, is specialized in handling graphics-intensive tasks. The figure demonstrates how the CPU and GPU work in tandem to deliver smooth and responsive XR experiences, with the CPU managing the overall system and the GPU rendering the visual elements in real-time.\nFigure 4.4.1-2: CPU and GPU operations for XR applications\nAs mentioned above, key aspects of such XR engines and abstraction layers is the integration of advanced functionalities for new XR experiences including video, sound, scripting, networking, streaming, localization support, and scene graphs. By well-defined APIs, XR engines may also be distributed, where part of the functionality is hosted in the network on an XR Server and other parts of the functionality are carried out in the XR device.\nGPU operations and rendering is dealt with in clause 4.4.2.\nIn the remainder of this Technical Report, the term XR engine is used to provide any type of typical XR functionalities as mentioned above. A key issue is the functional integration of potentially 3GPP defined technologies, including well defined APIs and interfaces for the usability and benefit of XR application developers.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.4.2\tBriefly on Rendering Pipelines",
                            "text_content": "Rendering or graphics pipelines are basically built by a sequence of shaders that operate on different buffers in order to create a desired output. Shaders are a type of computer programs that were originally used for shading (the production of appropriate levels of light, darkness, and colour within an image), but which now perform a variety of programmable functions in various fields of computer graphics as well as image and video processing.\nThe input to the shaders is handled by the application, which decides what kind of data each stage of the rendering pipeline should operate on. Typically this data is 3D assets consisting of geometric primitives and material components. The application controls the rendering by providing the shaders with instructions describing how models should be transformed and projected on a 2D surface.\nGenerally there are 2 types of rendering pipelines (i) rasterization rendering and (ii) ray-traced rendering (a.k.a. ray-tracing) as shown in Figure 4.2-1.\n-\tRasterization is the task of taking an image described in a vector graphics format (shapes) and converting it into a raster image (a series of pixels, which, when displayed together, create the image which was represented via shapes). The rasterized image may then be displayed on a computer display, video display or printer, stored in a bitmap file format, or processed and encoded by regular 2D image and video codecs.\n-\tRay tracing heavy pipelines are typically considered as more computationally expensive thus less suitable for real-time rendering. However, in recent years real-time ray tracing has leaped forward due to improved hardware support and advances in ray tracing algorithms and post-processing.\nHowever, there are several flavors to each type of rendering and they may in some cases be combined to generate hybrid pipelines. Generally both pipelines process similar data; the input consists of geometric primitives such as triangles and their material components. The main difference is that rasterization focuses to enable real-time rendering at a desired level of quality, whereas ray tracing is likely to be used to mimic light transmission to produce more realistic images.\n\nFigure 4.4.2-1 Rasterized (left) and ray-tracing based (right) rendering\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "",
                                    "table number": 4,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "4.4.3\tReal-time 3D Rendering",
                            "text_content": "3D rendering is the process of converting 3D models into 2D images to be presented on a display. 3D rendering may include photorealistic or non-photorealistic styles. Rendering is the final process of creating the actual 2D image or animation from the prepared scene, i.e. creating the viewport. Rendering ranges from the distinctly non-realistic wireframe rendering through polygon-based rendering, to more advanced techniques such as ray tracing. The 2D rendered viewport image is simply a two dimensional array of pixels with specific colours.\nTypically, rendering needs to happen in real-time for video and interactive data. Rendering for interactive media, such as games and simulations, is calculated and displayed in real-time, at rates of approximately 20 to 120 frames per second. The primary goal is to achieve a desired level of quality at a desired minimum rendering speed. The impact of the frame rate for the rendering pipeline is discussed in details in clause 4.2.2. The rapid increase in computer processing power and in the number of new algorithms has allowed a progressively higher degree of realism even for real-time rendering. Real-time rendering is often based on rasterization and aided by the computer's GPU.\nAnimations for non-interactive media, such as feature films and video, can take much more time to render. Non real-time rendering enables use of brute-force ray tracing to obtain a higher image quality.\nHowever, in the context of XR in this Technical Report, the assumption of rendering is to be real-time to react to updated XR pose information, updates in the scene as produced, and so on.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.4.4\tNetwork Rendering and Buffer Data",
                            "text_content": "In several applications, rendering or pre-rendering is not exclusively carried out in the device GPU, but assisted or split across the network. If this is the case, then the following aspects matter:\n-\tThe type of buffers that are pre-rendered/baked in the network. Typical buffer formats are summarized below.\n-\tThe format of the buffer data. Again, some data is collected below.\n-\tThe number of parallel buffers that are handled\n-\tSpecific delay requirements of each of the buffers\n-\tThe dimensions of the buffers in terms of size and time\n-\tThe ability to compress the buffers using conventional video codecs.\nTypical buffers are summarized in the following:\n-\tVertex Buffer: A rendering resource managed by a rendering API holding vertex data. May be connected by primitive indices to assemble rendering primitives such as triangle strips.\n-\tDepth Buffer: A bitmap image holding depth values (either a Z buffer or a W buffer), used for visible surface determination, during rasterization of 3D scenes.\n-\tTexture Buffer: A region of memory (or resource) used as both a render target and a texture map. A texture map is defined as an image/rendering resource used in texture mapping, applied to 3D models and indexed by UV mapping for 3D rendering. Texture/Image represents a set of pixels. Texture buffers have assigned parameters to specify creation of an Image. It can be 1D, 2D or 3D, can have various pixel formats (like R8G8B8A8_UNORM or R32_SFLOAT) and can also consist of many discrete images, because it can have multiple array layers or MIP levels (or both). As an example, detailed formats for Vulkan are provided here:\n-\n-\n-\tFrame Buffer: a region of memory containing a bitmap that drives a video display. These frame buffers are in raster formats, i.e. they are the result of rasterization. It is a memory buffer containing a complete frame of data. Frame buffers are supported by swap chains being a series of virtual frame buffers utilized by the graphics card and graphics API for frame rate stabilization and several other functions. In every swap chain there are at least two buffers. The first frame buffer, the screen buffer, is the buffer that is rendered to the output of the video card. The remaining buffers are known as backbuffers. Each time a new frame is displayed, the first backbuffer in the swap chain takes the place of the screenbuffer, this is called presentation or swapping. A variety of other actions may be taken on the previous screenbuffer and other backbuffers (if they exist). The screen buffer may be simply overwritten or returned to the back of the swap chain for further processing. The action taken is decided by the client application and is API dependent.\n-\tUniform Buffer: A Buffer Object that is used to store uniform data for a shader program. It can be used to share uniforms between different programs, as well as quickly change between sets of uniforms for the same program object. A uniform is a global Shader variable declared with the \"uniform\" storage qualifier. These act as parameters that the user of a shader program can pass to that program. Uniforms are so named because they do not change from one shader invocation to the next within a particular rendering call.\nIn MPEG, work has started on integration of timed media into XR scenes in order to provide input to rendering buffers through network APIs, for example by retrieving 2D or 3D compressed data.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.5\t2D Compression Technologies",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.5.1\tCore Compression Technologies",
                            "text_content": "This clause provides an overview of core 2D video compression technologies that are available on mobile platforms as well as their performance. For power-efficient and best performance, encoding and decoding is preferably exclusively carried out in hardware. This clause reviews the 3GPP specifications, actual hardware availability as well as the performance of codecs.\nAs of today, two codecs are prominently referenced and available, namely H.264/AVC [30] and H.265/HEVC [31]. Both codecs are defined as part of the TV Video Profiles in 3GPP TS 26.116 [32] and are also the foundation of the VR Video Profiles in 3GPP TS 26.118 [3]. The highest defined profiles are:\n-\tH.264/AVC Progressive High Profile Level 5.1 [30] with the following additional restrictions and requirements:\n-\tthe maximum VCL Bit Rate is constrained to be 120Mbps with cpbBrVclFactor and cpbBrNalFactor being fixed to be 1250 and 1500, respectively.\n-\tthe bitstream does not contain more than 10 slices per picture\n-\tH.265/HEVC Main-10 Profile Main Tier Profile Level 5.1 [31] without any restrictions\nThese profiles and levels basically permit the delivery of video formats up to 4K at 60 frames per second. In modern mobile CPUs, the above profile/level combinations are supported, and recently even extended to support 8K video.\nAn overview of typical coding performance is provided in Table 4.5-1.\nTable 4.5-1: Expected Video coding standards performance and bitrate target\n\nA more detailed analysis of video codec performance is FFS.\nWork on video compression technologies beyond the capabilities of HEVC [31] are continued by the MPEG/ITU. For example, the Joint Video Exploration Team (JVET) initiated the work on the development of a new video coding standard, to be known as Versatile Video Coding (VVC). In addition, MPEG started working on a new video coding standard to be known as MPEG-5 Essential Video Coding (EVC) in January 2019. Also noteworthy is the improvement of encoders over time even for existing standards which also leads to bitrate reductions at the same quality.\nBased on this information it can be expected that within the time frame until 2025, video compression technology will permit bitrate reductions by a factor of 50% compared to what is today possible with HEVC [31].\nOn top of regular lossy video compression algorithms, low-latency, low-complexity and near lossless codecs are important for certain applications. As an example, JPEG XS is a recent standard for visually lossless low-latency lightweight image coding. According to , such a codec permits simple yet efficient coding, keeps latency and complexity very low and at the same time achieves visually lossless quality at compression ratios up to 10:1.\nFurthermore, for XR formats beyond regular 2D, two different approaches are taken in the compression\n1)\tusage of existing 2D codecs and providing pre- and post-processing in order to convert the signals to 3D signals\n2)\tusage of dedicated compression technologies for specific formats.\nMore details on these issues are discussed in clause 4.6.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 4.5-1: Expected Video coding standards performance and bitrate target",
                                    "table number": 5,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "4.5.2\tFormat and Parallel Decoding Challenges",
                            "text_content": "In XR type of applications, when buffers are processed by rendering engines, existing video codecs may be used to efficiently compress them when they need to be transmitted over the network. As typically a huge amount of data is exchanged and operation needs to be done in a power-efficient manner in constraint environments (see clause 4.8), XR applications rely on existing video codecs on mobile platforms, for example those codecs defined in 3GPP specifications (see clause 4.5.1). While serving an immediate need and providing a kickstart for XR type of services, such video codecs may not be fully suitable for XR applications for different reasons, some of them listed below.\nFirst of all, the formats of the buffers in XR and graphics applications may be different and more variety exists, see clause 4.4.4. Also in certain case, not only textures need to be supported, but also 3D formats, see clause 4.6.\nBeyond this, XR applications may require that multiple buffers are served and synchronized in order to render an XR experience. This results in requirements for parallel decoding of multiple streams for multiple buffers (texture, geometry, etc.) as well as multiple objects. In many cases these buffers need to be made available to the rendering engine in a synchronized manner to ensure the highest quality of the rendered scene. Furthermore, the amount of streams and data to be processed may vary heavily over the period of an XR session and requires flexible video decoding architectures, also taking into account efficient and low-latency processing.\nAs an example, MPEG is addressing several of these challenges as part of their MPEG-I project on immersive media coding. In particular, for the variety of applications, a flexible and powerful hardware based decoding and processing architecture is desirable.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.6\t3D and XR Visual Formats",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.6.1\tIntroduction",
                            "text_content": "This clause introduces 3D and XR visual formats. Both, static images as well as video formats are introduced. In all cases it is assumed that the visual signal is provided as a sequence of pictures with a specific frame rate in frames per second. The chosen frame rate may be a matter of the production of the video, or it may be based on requirements due to interactions with the content, for example in case of conversational applications or when using split rendering.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.6.2\tOmnidirectional Visual Formats",
                            "text_content": "Omnidirectional formats have been introduced in 3GPP TS 26.118 [3], clause 4.1, as well as in 3GPP TR 26.928 [2], clause 4.2.5.\nOmnidirectional visual signals are represented in a spherical coordinate space in angular coordinates (ϕ,θ). The viewing is from the origin of the sphere, looking outward. Even though a spherical coordinate is generally represented by using radius, elevation, and azimuth, it assumes that a unit sphere is used for capturing and rendering. Thus, a location of a point on the unit sphere is identified by using the sphere coordinates azimuth () and elevation ().\nFor video, such a centre point may exist for each eye, referred to as stereo signal, and the video consists of three colour components, typically expressed by the luminance (Y) and two chrominance components (U and V).\nAccording to 3GPP TS 26.118 [3], clause 4.1.3, mapping of a spherical picture to a 2D texture signal is illustrated in Figure 4.6.2-1. The most commonly used mapping from spherical to 2D is the equirectangular projection (ERP) mapping. The mapping is bijective, i.e. it may be expressed in both directions.\nFigure 4 illustrates the process of mapping spherical data to 2D representations, showcasing various examples of this transformation. The figure demonstrates how spherical data, such as satellite imagery, is converted into a 2D format suitable for analysis and visualization. The process involves techniques like projection and resampling, which are essential for handling the inherent distortions introduced by the transition from a spherical to a planar coordinate system. The figure highlights the importance of these transformations in fields such as GIS, remote sensing, and environmental monitoring, where accurate representation of spatial data is crucial.\nFigure 4.6.2-1: Examples of Spherical to 2D mappings\nAssume a 2D texture with pictureWidth and pictureHeight, being the width and height, respectively, of a monoscopic projected luma picture, in luma samples and the center point of a sample location (i,j) along the horizontal and vertical axes, respectively, then for the equirectangular projection the sphere coordinates (,) for the luma sample location, in degrees, are given by the following equations:\n= ( 0.5 − i ÷ pictureWidth ) * 360\n = ( 0.5 − j ÷ pictureHeight ) * 180\nWhereas ERP is commonly used for production formats, other mappings may be applied, especially for distribution. For more details on projection formats, refer to 3GPP TR 26.918 [2], clause 4.2.5.4.\nFor production, capturing and stitching of spherical content, refer to 3GPP TR 26.918 [2], clauses 4.2.5.2 and 4.2.5.3.\nRendering of spherical content depends on the field of view (FoV) of a rendering device. The pose together with the field of view of the device enables the system to generate the user viewport, i.e., the presented part of the content at a specific point in time. According to 3GPP TS 26.118 [3], the renderer uses the projected texture signals and rendering metadata (projection information) and provides a viewport presentation taking into account the viewport and possible other information. With the pose, a user viewport is determined by identifying the horizontal and vertical fields of view of the screen of a head-mounted display or any other display device to render the appropriate part of decoded video or audio signals. For video, textures from decoded signals are projected to the sphere with rendering metadata received from the file decoder. During the texture-to-sphere mapping, a sample of the decoded signal is remapped to a position on the sphere.\nRelated to the generic rendering approaches in clause 4.2.2,  the following steps are part of the rendering spherical media:\n-\tGenerating a 3D Mesh (set of vertexes linked into triangles) based on the projection metadata. The sphere is mapped to a mesh and the transformation of the mesh is dynamically updated based on the updated projection metadata.\n-\tMapping each vertex to a position on a 2D texture. This is again done using the available projection metadata.\n-\tRotating the camera to match the user’s head orientation. This is based on the available pose information.\n-\tComputing the viewport by using computer graphic algorithms as discussed in details in clause 4.2.2\nAccording to 3GPP TS 26.118 [3], clause 4.1.3, commonly used video encoders cannot directly encode spherical videos, but only 2D textures. However, there is a significant benefit to reuse conventional 2D video encoders. Based on this, Figure 4.6.2-2 provides the basic video signal representation in the context of omnidirectional video in the context of the present document. By pre-processing, the spherical video is mapped to a 2D texture. The 2D texture is encoded with a regular 2D video encoder and the VR rendering metadata (i.e. the data describing the mapping from the spherical coordinate to the 2D texture) is encoded and provided along with the video bitstream, such that at the receiving end the inverse process can be applied to reconstruct the spherical video.\nFigure 4 illustrates the representation of a video signal in terms of its various components, including the luminance (Y), chrominance (U and V), and the phase information. The figure shows how these components are combined to create a complete video signal, with the luminance component providing the overall brightness and contrast, and the chrominance components adding color and detail. The phase information is crucial for ensuring that the video signal is displayed correctly on the screen, as it determines the timing of the different color and brightness components. The figure also shows how the video signal is compressed using techniques such as the Discrete Cosine Transform (DCT) to reduce its size and allow for efficient transmission over digital networks.\nFigure 4.6.2-2: Video Signal Representation\nCompression, storage and data formats are defined for example in 3GPP TS 26.118 [3] as well as in ISO/IEC 23090-2 [37]. This includes viewport-independent and viewport-dependent compression formats. A principle overview of different approaches is documented in 3GPP TR 26.918 [2], clause 4.2.5.6.\nAccording to clause 4.2.1, 1k by 1k per eye is a minimum for the signal in the viewport, and for stereoscopic rendering this results in basically a signal for 2k by 1k, typically at a frame rate of 50 or 60fps.With current codecs according to clause 4.5.1, the pure viewport data can be represented with around 4-10 Mbit/s. However, a viewport typically only covers around 100 degree horizontal and 60 degree vertical. Hence, to present a full omnidirectional presentation, about 20 times more data may be necessary, leading to 80 – 200 Mbit/s. Viewport-dependent coding and delivery, in particular tiling, can support to reduce the required bitrates.\nFor use cases and applications, see 3GPP TR 26.918 [2], clause 5.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.6.3\t3D Meshes",
                            "text_content": "A polygon mesh is a collection of vertices, edges and faces that defines the shape of a polyhedral object in 3D computer graphics and solid modeling. The faces usually consist of triangles (triangle mesh), quadrilaterals (quads), or other simple convex polygons (n-gons), since this simplifies rendering, but may also be more generally composed of concave polygons, or even polygons with holes.\nObjects created with polygon meshes are represented by different types of elements. These include vertices, edges, faces, polygons and surfaces as shown in Figure 4.6.3-1. In many applications, only vertices, edges and either faces or polygons are stored.\nFigure 4.6.3-1 illustrates the essential components for mesh network representations, as detailed in the ©Wikipedia article \"Mesh_overview.jpg\". The figure showcases the nodes, links, and the central node, emphasizing the decentralized and interconnected nature of mesh networks. The nodes are depicted as small circles, while the links are represented by lines connecting them. The central node is highlighted, emphasizing its pivotal role in routing and managing the network traffic. This representation is crucial for understanding the topology and performance characteristics of mesh networks, which are widely used in various applications such as wireless sensor networks and peer-to-peer systems.\nFigure 4.6.3-1: Elements necessary for mesh representations ©Wikipedia (Mesh_overview.jpg: The original uploader was Rchoetzlein at English Wikipedia.derivative work: Lobsterbake [CC BY-SA 3.0 (])\nPolygon meshes are defined by the following elements:\n-\tVertex: A position in 3D space defined as (x,y,z) along with other information such as colour (r,g,b), normal vector and texture coordinates.\n-\tEdge: A connection between two vertices.\n-\tFace: A closed set of edges, in which a triangle face has three edges, and a quad face has four edges. A polygon is a  set of faces. In systems that support multi-sided faces, polygons and faces are equivalent. Mathematically a polygonal mesh may be considered as an , or undirected graph, with additional properties of geometry, shape and topology.\n-\tSurfaces: or smoothing groups, are useful, but not required to group smooth regions.\n-\tGroups: Some mesh formats contain groups, which define separate elements of the mesh, and are useful for determining separate sub-objects for  or separate actors for non-skeletal animation.\n-\tMaterials: defined to allow different portions of the mesh to use different  when rendered.\n-\t: Most mesh formats also support some form of  which are a separate 2D representation of the mesh \"unfolded\" to show what portion of a 2-dimensional  to apply to different polygons of the mesh. It is also possible for meshes to contain other such vertex attribute information such as colour, tangent vectors,  to control , etc (sometimes also called channels).\nMeshes are commonly produced by many different graphics engines, computer games, and so on. For more details, see also clause 4.6.7.\nMeshes can be rendered directly on GPUs that are highly optimized for mesh-based rendering.\nPolygon meshes may be represented in a variety of ways, using different methods to store the vertex, edge and face data such as face-vertex, winged, half or quad-edge meshes, corner-table or vertex-vertex meshes. Different formats also store other per vertex and materials related data in different ways. Each of the representations have particular advantages and drawbacks. The choice of the data structure is governed by the application, the required performance, the size of the data, and the operations to be performed. For example, it is easier to deal with triangles than general polygons. For certain operations it is necessary to have a fast access to topological information such as edges or neighboring faces; this requires more complex structures such as the winged-edge representation. For hardware rendering, compact, simple structures are needed and are as such commonly incorporated into low-level rendering APIs such as DirectX and OpenGL.\nMany different formats for storage and data formats exist for storing polygon mesh data, as an example PLY-format is introduced below, because it provides 3D data in a human readable format. In practice PLY-format is rarely used in real-time rendering. Typically the data format is configured to the need of the 3D engine and as such the landscape is littered with proprietary 3D formats. 3D formats may be roughly divided in two categories, run time friendly formats such as glTF and formats which enable transferring 3D assets between systems like PLY.\nThe PoLYgon (PLY) format (see http://paulbourke.net/dataformats/ply/) is used to describe a 3D object as a list of vertices, faces and other elements, along with associated attributes. A single PLY file describes exactly one 3D object. The 3D object may be generated synthetically or captured from a real scene. Attributes of the 3D object elements that might be stored with the object include: colour, surface normals, texture coordinates, transparency, etc. The format permits one object to have different properties for the front and back of a polygon.\nThe PLY does not intend to act as a Scene Graph, so it does not include transformation matrices, multiple 3D objects, modeling hierarchies, or object sub-parts. A typical PLY object definition is a list of (x,y,z,r,g,b) triples for vertices and their colour attributes (r,g,b), so they represent a point cloud. It may also include a list of faces that are described by indices into the list of the vertices. Vertices and faces are primary elements of the 3D object representation.\nPLY allows applications to create new attributes that are attached to the elements of an object. New attributes are appended to the list of attributes of an element, in a way to maintain backwards compatibility. Attributes that are not understood by a parser are simply skipped.\nFurthermore, PLY allows for extensions to create new element types and their associated attributes. Examples of such elements could be materials (ambient, diffuse and specular colours and coefficients). New elements can also be discarded by programs that do not understand them.\nA PLY file is structured as follows:\nHeader\nVertex List\nFace List\n(lists of other elements)\nThe header is a human-readable textual description of the PLY file. It contains a description of each element type, including the element's name (e.g. \"vertex\"), how many of such elements are in the object, and a list of the various attributes associated with the element. The header also indicates whether the file is in binary or ASCII format. A list of elements for each element type follows the header in the order described in the header.\nThe following is an example PLY in binary format with 19928 vertices and 39421 faces:\n\nThis example demonstrates the different components of a PLY file header. Each part of the header is a carriage-return terminated ASCII string that begins with a keyword. In case of binary representation, the file will be a mix of an ASCII header and binary representation of the elements, in little or big endian, depending on the architecture on which the PLY file has been generated. The PLY file must start with the characters \"ply\".\nThe vertex attributes listed in this example are the (x,y,z) floating point coordinates, the (nx,ny,nz) representation of the normal vectors, a 32 bit flag mask, (r,g,b) 8-bit representations of the colour of each vertex, an 8-bit representation of the transparency alpha channel. Faces are represented as a list of vertex indices with a flags attribute associated with each face.\nDifferent GPUs may support different texture formats, both raw and compressed. Raw formats include different representations of the RGB colour space, e.g. 8/16/32 bit representations of each colour component, with or without alpha channel, float or integer, regular or normalized, etc.\nTypical GPU texture compression formats include BC1, PVRTC, ETC2/EAC, and ASTC. Other image compression formats such as JPEG and PNG need to be decompressed and passed to the GPU in a format that it supports.\nRecently, the Basis Universal GPU texture format has been defined. This format also supports video texture compression. As decoding happens on the GPU, the application will benefit from reduced CPU load and CPU to GPU memory copy delay.\nBitrates and quality considerations for meshes are FFS.\nMeshes are used in many applications.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "The following is an example PLY in binary format with 19928 vertices and 39421 faces:",
                                    "table number": 6,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "4.6.4\tPoint Clouds",
                            "text_content": "A point cloud is a collection of data points defined by a given coordinates system. In a 3D coordinates system, for example, a point cloud may define the shape of some real or created physical system. Point clouds are used to create 3D meshes and other models used in 3D modeling for various fields including medical imaging, architecture, 3D printing, manufacturing, 3D gaming and various XR applications.\nPoint clouds are often aligned with 3D models or with other point clouds, a process known as point set registration. In computer vision and pattern recognition, point set registration, also known as point matching, is the process of finding a spatial transformation that aligns two-point sets. The purpose of finding such a transformation includes merging multiple data sets into a globally consistent model, and mapping a new measurement to a known data set to identify features or to estimate its pose. Point set registration is used in augmented reality.\nAn overview on point cloud definitions, formats, production and capturing systems, rendering, bitrate/quality considerations and applications is for example provided in [38]. According to this document, media-related use cases may usually contain between 100,000 and 10,000,000 point locations and colour attributes with 8-10 bits per colour component, along with as some sort of temporal information, similar to frames in a video sequence. For navigation purposes, it is possible to generate a 3D map by combining depth measurements from a high-density laser scanner, e.g. LIDAR, camera captured images and localization data measured with GPS and an inertial measurement unit (IMU). Such maps can further be combined with road markings such as lane information and road signs to create maps to enable autonomous navigation of vehicles around a city. This use case requires the capture of millions to billions of 3D points with up to 1 cm precision, together with additional attributes, namely colour with 8-12 bits per colour component, surface normals and reflectance properties attributes. According to the paper, depending on the sequence, compression factors between 1:100 to 1:500 are feasible for media-related applications. According to the same paper, bitrates for single objects with such compression methods are in the range of 8 to 20 Mbit/s.\nFor production of point clouds, see also clause 4.6.7.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.6.5\tLight Fields",
                            "text_content": "An overview on light-field technology is for example provided in https://mpeg.chiariglione.org/sites/default/files/events/7.%20MPEG127-WS_MehrdadTeratani.pdf.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.6.6\tScene Description",
                            "text_content": "Scene Descriptions are often called scene graphs due to their representation as graphs. A scene graph is a directed acyclic graph, usually just a plain tree-structure, that represents an object-based hierarchy of the geometry of a scene. The leaf nodes of the graph represent geometric primitives such as polygons. Each node in the graph holds pointers to its children. The child nodes can among others be a group of other nodes, a geometry element, a transformation matrix, etc.\nSpatial transformations are represented as nodes of the graph and represented by a transformation matrix. Other Scene Graph nodes include 3D objects or parts thereof, light sources, particle systems, viewing cameras, …\nThis structure of scene graphs has the advantage of reduced processing complexity, e.g. while traversing the graph for rendering. An example operation that is simplified by the graph representation is the culling operation, where branches of the graph are dropped from processing, if deemed that the parent node’s space is not visible or relevant (level of detail culling) to the rendering of the current view frustum.\nScene descriptions permit generation of many different 3D scenes for XR applications. As an example, glTF from Khronos [39] is a widely adopted scene description specification and is now adopted by MPEG as the baseline for their extensions to integrate real-time media into scenes.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.6.7 \tProduction and Capturing Systems for 3D Mesh and Point Clouds",
                            "text_content": "In order to capture a 3D mesh in a production facility using stereo cameras, an array of multiple cameras is placed around a recording space. A subject (for example an actor) is recorded inside the recoding space. Figure 4.6.7-1 shows an outline of one of the prototype studios as well as some example images from a production plant. In the left of Figure 4.6.7-1 a rotunda-like recording space is shown, in which multiple cameras are placed around the peripherals of the space.\nFigure 4.6.7 - 1 illustrates an example of a capture and production facility for Point Cloud/3D meshes, showcasing the process of data collection and processing. The figure depicts various stages, including data acquisition, processing, and output, highlighting the importance of accurate and efficient 3D modeling in industries such as architecture, engineering, and construction. Key components include the 3D scanner, processing station, and output devices, emphasizing the seamless integration of hardware and software to produce high-quality 3D models.\nFigure 4.6.7-1: Example of a capture and production facility for Point Cloud/ 3D meshes\nA multi-camera or multi-stereo-camera pairs setup serves as a stereo camera base unit. Each stereo camera pair records the subject(s) from a different viewpoint. For example, Figure 4.6.7-2 shows different image captures from different multi-camera setup taken at one time instance. The volumetric capture studio has an integrated illuminated background for lightning.\nFigure 4.6.7 -3 presents a 3D mesh production workflow, showcasing the process of creating a 3D model from captured images. The workflow begins with foreground and background segmentation in the keying stage, followed by the application of a depth estimator to each captured image. This depth information is then used to generate accurate depth values for each pixel in a 2D image. The resulting depth map image from a stereo camera pair is depicted in Figure 4.6.7 -4. The workflow demonstrates the use of stereo cameras to capture depth information and create a 3D model, which can be utilized for various applications such as gaming, virtual reality, and 3D modeling.\n\nFigure 4.6.7-2: Example of the 32 images captured simultaneously at one time instance in a studio\nFigure 4.6.7-3 illustrates the 3D mesh production workflow. After the capture, a foreground and background segmentation process is performed in the ’keying’ stage. A ‘depth estimator’ is applied to the captured images from each stereo pair to generate depth information with high accuracy for each pixel. From each stereo camera pair, the 3D information is stored as a colour-coded depth value in a 2D image. For example, a resulting depth map image from a stereo camera pair is shown in Figure 4.6.7-4.       \n\nFigure 4.6.7-3 provides a detailed overview of the 3D mesh production workflow, showcasing the process from capturing images to generating a final 3D mesh. The figure begins with the capture stage, where foreground and background segmentation is performed in the 'keying' phase. This is followed by the application of a 'depth estimator' to each captured image from the stereo pair, which generates depth information with high accuracy for each pixel. The 3D information is then stored as a color-coded depth value in a 2D image. For instance, Figure 4.6.7-4 illustrates a depth map image resulting from a stereo camera pair, demonstrating the depth estimation process. Overall, the figure illustrates the intricate steps involved in creating a 3D mesh from captured images, highlighting the importance of accurate depth estimation and segmentation in the production process.\nFigure 4.6.7-3: 3D mesh generation and production workflow\n\nFigure 4.6.7-4 presents a dense depth map calculated per frame for each stereo camera pair, illustrating the 3D reconstruction process in a telecommunication system. The figure showcases the arrangement of stereo cameras capturing multiple frames, with the depth information derived from the captured images. The dense depth maps are essential for accurate 3D object recognition and scene understanding, enabling enhanced communication and data processing in the telecommunication network.\nFigure 4.6.7-4: Example of a dense depth map calculated per frame for each stereo camera pair\nIn the following stage, the depth information from every stereo camera pair is merged using the initial camera calibration and a related 3D fusion process. Any 3D point which is occluded by others is filtered out, resulting in an advanced foreground segmentation.\nThe result of the 3D fusion process is a 3D Point cloud. The 3D Point cloud is further processed by different post-processing steps such as meshing, mesh reduction and texturing, etc as shown in the figure 4.6.7-5. The depth-based surface reconstruction results in a high-density mesh which consists of a high number of vertices and faces. In order to simplify the resulting high-density mesh to a single consistent mesh, a geometric simplification is performed. This process is called the mesh reduction. The simplified meshes are texturized using a 2D texture map in a common 2D image file format. In the final stage of the post-processing, the resulting meshes are temporally registered to obtain animated meshes.\nFigure 4.6.7 presents a point cloud derived from a 3D model, showcasing the detailed representation of a complex object or environment. The resulting point cloud on the left side of the figure serves as a dense, unordered collection of data points that capture the geometry and structure of the subject. On the right side, we observe the progression of 3D models through various stages of processing, including meshing, simplification, and texturing. These techniques refine the 3D model by reducing its complexity and enhancing its visual fidelity, ultimately preparing it for further analysis or rendering. The visual transformation from the raw point cloud to the refined 3D models highlights the power of computational geometry and visualization in modern data analysis and engineering tasks.\nFigure 4.6.7-5: Example of resulting point cloud (left), and 3D models such as meshing, simplification and texturing (from second left to right)\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.7\t3D and XR Audio Formats",
                    "description": "",
                    "summary": "",
                    "text_content": "For 3D and XR audio formats and systems, refer to 3GPP TR 26.918 [2], clause 4.3 as well as to 3GPP TS 26.118 [3], clause 6.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.8\tDevices and Form Factors",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.8.1\tDevice Types",
                            "text_content": "Extended reality devices are of different form factors as shown in Figure 4.8-1. These form factors may differ in processing capabilities, communication types and power consumption.\nThe majority of Virtual Reality (VR) and Augmented Reality (AR) devices are head-mounted displays (HMDs).. In AR, the display is usually transparent and digital information is superimposed onto real life objects. In VR, the display is not transparent and only virtual information and images are displayed in front of wearer's eyes. Head-mounted display ( HMD) is a device worn over the head. It features a display in front of one or both theeyes. The display streams data, images and other information in front of the wearer's eye(s). Certain HMDs have displays over both of their users' eyes, others only have a display over one of the users' eyes.\nTypical components of HMDs are listed as follows:\nOptical systems: Display and lenses\nTracking sensors and possibly additional sensors\nCameras\nXR related processing (summarized as XR engine) including GPUs, CPUs, ASICs (e.g. dedicated media encoding and decoding), etc.\nCommunication functionalities, as for example provided by a 5G System\nA smartphone (defined as XR5G-P1 device type) may be used both for AR as well as for VR (together with a card-board). In both cases, typically an XR engine/runtime is available to support processing of sensor data, viewport rendering as well as SLAM processing. In this case, the 5G modem and all media/XR processing is integrated in the device. Power consumption of such devices is important, but not ultimately critical due to the onboard battery.\nVR HMDs typically provide the a large field of view, stereoscopic 3D imagery as well as rotational, translational and positional tracking for full 6DoF experiences. For VR, the following device types are identified:\nXR5G-V1 - Simple VR Display wired: Such device types are commonly available in 2019. They only include sensors for tracking as well as a display. The remaining processing is done on a remote device, e.g. a puck or a smartphone. XR/Media Processing, connectivity and power supply are provided through wired tethering.\nXR5G-V2 - Simple VR Display wireless: Such device types are not yet available in 2019. They include sensors for tracking, a display, a wireless connection (which could be WiFi based or 5G Sidelink based), and a power supply. The remaining processing is done on a remote device, e.g. a puck or a smartphone.\nXR5G-V3 - Smart VR Viewer wireless tethering: Such device types are not yet available in 2019. They include sensors for tracking, a display, a wireless connection (which could be WiFi based or 5G Sidelink based), at least some XR processing, as well as a power supply. The remaining processing is done on a remote device, e.g. a puck or a smartphone.\nXR5G-V4 - VR HMD standalone: Such device types are commonly available in 2019, except 5G connectivity. For such devices, the 5G modem, power supply as well as all media/XR processing is expected to be integrated into a single device.\nNote that XR5G-V1 and XR5G-V2 are similar in terms of functions, but the wireless connectivity creates additional challenges and may even be done by a 5G sidelink communication.\nAn optical head-mounted display is a type of head-mounted display that projects images and allows the user to see through its display and is used in augmented reality (AR). Unlike Virtual Reality HMDs that obscures the vision of the real world, AR devices allow to see the surroundings while streaming data and image overlays in front of the eyes. Optical head-mounted displays may cover only 1 eye or both eyes. Wearers can interact with the projected digital content through input methods such as voice commands, gestures and controllers.\nFor AR Glasses,  design constraints are significantly more important. In particular, design constraints apply in terms of sleekness, weight and power. The processing power is expected to be low to avoid battery consumption and thermal dissipation. Wireless AR glasses are commercially compelling. AR is typically associated to using glasses, also referred to as optical see-through. However, AR experiences may be achieved using HMDs with video-see through functionalities. A comparison of different types is for example provided in [40]. The following device types are identified:\nXR5G-A1 - Simple AR Wearable Glass wired: Such device types are available in 2019. They include a minimum number of sensors, possibly cameras for AR localization, as well as a display. Power, XR processing and connectivity is supplied from an external source.\nXR5G-A2 - Simple AR Wearable Glass wireless: Such device types are not available in 2019 and are still far out. They would include a minimum number of sensors, possibly cameras for AR localization, power supply, as well as a wireless modem for connectivity. XR processing, AR localization as well as network connectivity is provided by external means, e.g. a puck or a smartphone.\nXR5G-A3 - Smart AR HMD video see-through: Such device types are an initial entry for AR applications. There are much closer to XR5G-V1 type of devices, but by having cameras projecting the real world on the screen, the VR device can operate as an AR device.\nXR5G-A4 - AR Wearable Glass standalone: Such device types are not available in 2019 but are under consideration. For such devices, the 5G modem, power supply, as well as all media/XR processing is expected to be integrated in a single device.\nXR5G-A5 - Smart AR Wearable Glass wireless: Such device types are not available in 2019 and are still far out. In addition to an XR5G-A2 device, such a device would include at least a certain amount of XR/Media processing capabilities such as encoders/decoders and XRprocessing.\nFigure 4 illustrates the various form factors of XR (Extended Reality) devices, showcasing their design and functionality. The figure presents a range of headsets, including standalone (S), tethered (T), and mixed reality (MR) devices, each with distinct features and use cases. The standalone headsets, represented by the letter 'S', are fully self-contained and offer the most immersive experience. Tethered headsets, denoted by 'T', are connected to external devices for processing power and are often used in professional settings. Mixed reality devices, marked by 'M', blend virtual and real-world elements, enhancing the user's interaction with the environment. The figure also highlights the importance of positional tracking systems, such as inside-out tracking (O) and external tracking (E), which enable accurate movement and positioning of the XR devices.\nFigure 4.8-1: XR Form Factors\nOne of the most important issues when considering form factors and processing is the ability of the device to dissipate power, especially when no external cooling is available. Figure 4.8-2 shows the temperature rise depending on the surface power density. As example, two points on the figure can be considered:\nAt 5C rise over ambient, power density that can be dissipated is 0.023 W/square inch. A smart phone whould have a surface area from 20 to 30 square inch, i.e. the power that can be dissipated is 0.5 to 0.75 Watt.\nAT 25C rise over ambient, power density that can be dissipated is 0.18 W/square inch. For a smartphone this would allow around 4 to 5 W continuous power dissipation. However, for an AR glass, the surface area is much smaller and so much less power can be dissipated, somewhere in the range of 1W. As an example, in a 25C room, the device enclosure surface temp would be 50C, which is already on the higher end of what is generally considered acceptable.\nFigure 4 illustrates the relationship between temperature rise and power density in a given system, showing how an increase in power density leads to a proportional rise in temperature. The graph plots the temperature rise on the y-axis against the power density on the x-axis, with various data points representing different conditions. The figure highlights the nonlinear relationship between these two parameters, emphasizing the importance of considering thermal effects when designing high-power systems.\nFigure 4.8-2: Temperature rise vs. power density\nA summary of the different device types is provided in Table 4.8-1 along with tethering examples, placement of 5G Uu modem, XR engine and localization support, power supply and typical maximum available power. In all device types, the sensors are on the device. The table also addresses the options applicable for tethering between the device carrying the 5G Uu Modem, and the XR device, if applicable. The table also addresses options for the XR engine that includes scene recognition and viewport rendering. The following definitions for the XR engine are used:\nExternal: the device only supports display and receives a fully rendered viewport data that can be displayed directly. Any scene recognition, if applicable, is not on the device.\nSplit: the external device does a pre-rendering of the viewport based on sensor information and the device does the final rendering considering the latest sensor information. Different degrees of split exist, as discussed before. Similalrly, scene recognition can be subject to split computation.\nXR device: that device does the full rendering of the viewport in the device, sensor information is only processed locally. Any scene recognition, if applicable, is on the device.\nTable 4.8-1: XR Device Types\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 4.8-1: XR Device Types",
                                    "table number": 7,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "4.8.2\tPower Consumption",
                            "text_content": "This clause addresses the available power in different XR Device types as well as the power consumption of typical XR processing functions as identified in the context of XR services.\nWhen designing media processing, XR functionality and 5G connectivity, it is important to understand the power consumption of different components that are possibly integrated in XR devices. The following should be considered:\n-\tTracking and Sensing\n-\t3DoF tracking may be done with low power consumption, e.g., below 1 Watt\n-\t6DoF tracking involving for example, capturing cameras, LEDs for eye and hand tracking, etc. are more power-consumption intense\n-\tDisplay\n-\tDisplay power consumption is critical and depends on the device.\n-\tDisplay power consumption can be in the range of 0.3W up to 1W\n-\tRender (GPU)\n-\tThe power consumption of the GPU depends on frame rates, resolution, display technology\n-\tThe power consumption can be from several mWatt to several Watt depending on the use case\n-\tCompute and Media Processing (CPU)\n-\tSimilar observation as for the GPU\n-\tIf encoding is involved, power consumption is typically higher.\n-\tConnectivity\n-\tThe power consumption of wireless contection such as 5G depends on several factors including bitrates, distance from radio access network, channel conditions, frequeny range, etc.\n-\tThe power consumption can be from several mWatt to several Watt depending on the use case.\nIt is expected that each of the components will undergo improvements to address power savings. It is important that in the development of technical specifications of XR devices, the power consumption of each component is considered.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.9\tOngoing Standardisation Work",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.9.1\tRelated Work in 3GPP",
                            "text_content": "This clause summarizes relevant 3GPP activities efforts in the context of XR.\n3GPP TR 26.918 [2] provides an introduction to Virtual Reality and 3GPP TS 26.118 [3] defines Virtual Reality Media Profiles for omnidirectional 3DoF media.\n3GPP TR 22.842 [6] on Network Controlled Interactive Service (NCIS)  analyses several use cases of NCIS as follows: NCIS Service Supporting\nNew Requirements for VR Based NCIS Service\nCloud Rendering for Games\nHigh Speed Scenario\nIoE Based Social Networking\nCommunication within NCIS group\nBased on the TR, several requirements are identified for new requirements in 3GPP TS 22.261 [41]. Also, KPIs for such services mentioned above are documented in clause 6.2 of 3GPP TR 22.842 [6], requiring additional input including some information from this TR.\nIn context of Release-17, 3GPP work is ongoing in order to identify the integration of edge processing in 5G systems. 3GPP TR 23.748 [28] defines modifications to 5GS system architecture to enhance Edge Computing. This work is currently in study phase, defining Key Issues and scope for Release-17. In addition, in 3GPP TR 23.758 [27] a new set of application layer interfaces for Edge Computing is identified that may potentially be useful for integration edge computing.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.9.2\tRelated Work External of 3GPP",
                            "text_content": "This clause summarizes relevant external standardisation efforts in the context of XR that may provide certain functionalities being of benefit for 5G-based XR applications.\nIn October 2016, MPEG initiated a new project on “Coded Representation of Immersive Media”, referred to as MPEG-I. The proposal was justified by the emergence of new devices and services that allow users to be immersed in media and to navigate in multimedia scenes. It was observed that a fragmented market exists for such devices and services, notably for content that is delivered “over the top”. The project is motivated by the lack of common standards that do not enable interoperable services and devices providing immersive and navigable experiences. The MPEG-I project is expected to enable existing services in an interoperable manner and to support the evolution of interoperable immersive media services. Enabled by the Parts of this Standard, end users are expected to be able to access interoperable content and services, and acquire devices that allow them to consume these.\nAfter the launch of the project, several phases, activities, and projects have been launched that enable services considered in MPEG-I.\nThe project is divided in tracks that enable different core experiences. Each of the phases is supported by key activities in MPEG, namely in systems, video, audio and 3D graphics-related technologies.\nCore technologies as well as additional enablers are implemented in parts of the MPEG-I standard. Currently the following 14 parts are under development:\nPart 1 – Immersive Media Architectures\nPart 2 – Omnidirectional MediA Format\nPart 3 – Versatile Video Coding\nPart 4 – Immersive Audio Coding\nPart 5 – Video-Based Point Cloud Coding (V-PCC)\nPart 6 – Immersive Media Metrics\nPart 7 – Immersive Media Metadata\nPart 8 – Network-Based Media Processing\nPart 9 – Geometry Point Cloud Coding (G-PCC)\nPart 10 – Carriage of Video-based Point Cloud Coding Data\nPart 11 – Implementation Guidelines for Network-based Media Processing\nPart 12 - Carriage of Geometry-based Point Cloud Coding Data\nPart 13 – Multi-Decoder Video Decoding Interface for Immersive Media\nPart 14 – Scene Description for MPEG Media\n\nIn addition, other technical components may be provided in existing MPEG specifications outside of MPEG-I (e.g., HEVC and AVC) in order to create interoperable immersive experiences.\nKhronos creates open standards for 3D graphics, Virtual and Augmented Reality, Parallel Computing, Neural Networks, and Vision Processing. Specifically relevant for the work on XR are the following activities:\nOpenGL® is the most widely adopted 2D and 3D graphics API in the industry, bringing thousands of applications to a wide variety of computer platforms. It is window-system and operating-system independent as well as network-transparent. OpenGL enables developers of software for PC, workstation, and supercomputing hardware to create high-performance, visually compelling graphics software applications, in markets such as CAD, content creation, energy, entertainment, game development, manufacturing, medical, and virtual reality. OpenGL exposes all the features of the latest graphics hardware.\nVulkan is a new generation graphics and compute API that provides high-efficiency, cross-platform access to modern GPUs used in a wide variety of devices from PCs and consoles to mobile phones and embedded platforms.\nOpenXR [16] is an open standard that provides high-performance access to Augmented Reality (AR) and Virtual Reality (VR)—collectively known as XR—platforms and devices.\nglTF™ (GL Transmission Format) [39] is a specification for the efficient transmission and loading of 3D scenes and models by applications. glTF minimizes both the size of 3D assets, and the runtime processing needed to unpack and use those assets. glTF defines an extensible, common publishing format for 3D content tools and services that streamlines authoring workflows and enables interoperable use of content across the industry.\n\nThe WebXR Device API Specification (https://immersive-web.github.io/webxr/) [17] provides interfaces to VR and AR hardware to allow developers to build compelling, comfortable VR/AR experiences on the web. The latest “WebXR Device API, Editor’s Draft, 10 February 2020” is available here https://immersive-web.github.io/webxr/ and provides an interface to VR/AR hardware. It is no longer marked as “UNSTABLE API”. It also provides a link to .\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.10\tXR Use Cases",
                    "description": "",
                    "summary": "",
                    "text_content": "In Annex A of this document, a significant amount of use cases are collected that serve for identifying potential interfaces, formats, protocols and requirements for the 5G system in order to support XR applications.\nTable 4.10 provides an overview of the use cases and their characterization.\nIn addition, this table more explicitly adds the device types that have been developed in clause 4.8.\nTable 4.10: Overview of Use cases as documented in Annex A\n\nThe use cases are summarized in clause 5 into several core use cases and scenarios.\n",
                    "tables": [
                        {
                            "description": "Table 4.10: Overview of Use cases as documented in Annex A",
                            "table number": 8,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.11\tSummary of Remaining Issues addressed in this Document",
                    "description": "",
                    "summary": "",
                    "text_content": "Based on these introduced technologies and the use cases, the remainder of this Technical Report is addresses the following:\nidentify the mapping of different XR use cases to the 5G System and 5G Media Delivery services according to clause 4.3.\nidentify functions, interfaces and APIs for different delivery scenarios.\ndefine high-level call flows and parameter exchange for the different service scenarios.\nfor the different scenarios, identify the formats as well as traffic requirements/properties\nidentify technical requirements on formats, processing functions, interfaces, and protocols in order to achieve adequate Quality of Experience based on the considerations in clause 4.2.\nidentify potential standardisation areas and their potential timeline.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "5\tCore Use Cases and Scenarios for Extended Reality",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "5.1\tIntroduction",
                    "description": "",
                    "summary": "",
                    "text_content": "This clause documents core consolidated use cases and scenarios for extended reality based on the underlying offered functionalities, the nature of the communication, interactivity and real-time requirements. These have been derived from the use cases collected in Annex A. Table 5.1-1 lists the core use cases and the list of the use cases in Annex A covered by each of them.\nThe following categories and their accompanying illustrations are drawn for clarity. In actual implementations, elements from more than one system may be used together. However, it is attempted to ensure that the functionalities included from any particular use case in Annex A are fully covered within a single core use case diagram.\nThe term UE is used to define a 5G-enabled user equipment that meets the capability requirements of a particular use case. A UE may for example be a mobile handset, AR glasses or an HMD with or without controllers as documented in clause 4.3.\nNote that depending on the actual UE, the usage scenario may differ slightly. Some examples of such differences are given in the accompanying text within each category. Furthermore, in certain cases the capabilities of the devices may be present on each UE and not restricted to only one side.\nTable 5.1-1: Core use case mapping to Annex A\n",
                    "tables": [
                        {
                            "description": "Table 5.1-1: Core use case mapping to Annex A",
                            "table number": 9,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "5.2\tOffline Sharing of 3D Objects",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.2.1\tSummary of Use cases",
                            "text_content": "This clause summarizes and integrates the following use cases from Annex A in a single core use case referred to as \"Offline Sharing of 3D Objects\":\nUse Case 1: 3D Image Messaging (see Annex A.2)\nUse Case 2: AR Sharing (see Annex A.3)\nUse Case 10: Online shopping from a catalogue – downloading (see Annex A.10)\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.2.2\tDescription",
                            "text_content": "Offline sharing is used for sharing 3D models/objects and 3D MR scenes amongst UEs. In Figure 5.2-1, UE A shares a 3D static/dynamic object with UE B. The 3D object can be a stored object downloaded by UE A from the cloud, or captured by the device using for example a depth camera. It may include additional information such as colour, texture, size, etc. of the 3D object, which is referred to as effects in the figure. Upon receiving, UE B can render this object (and/or 3D objects it has downloaded from the cloud) in the surrounding reality using an MR rendering engine; it can choose the desired effects for the 3D object. It can then capture the rendered MR scene and send it back to UE A. MMS is used for sharing the 3D object and the captured MR scene between the UEs. Note that the diagram is drawn for clarity and in reality, the capabilities of the devices may be present on each UE and not limited only to one side.\nThe rendering functionality depends on the type of device. For instance, flat images on a mobile phone vs. 3D rendering on AR glasses.\nFigure 5 illustrates the process of offline sharing 3D objects and MR (Mixed Reality) scenes, showcasing how data is transferred between devices without an active internet connection. The figure depicts the synchronization of 3D models and MR environments, highlighting the role of cloud storage and local caching in maintaining real-time interaction. Key components include the 3D model, MR scene, and synchronization protocol, emphasizing the importance of efficient data management for seamless cross-device experiences.\nFigure 5.2-1: Offline Sharing 3D Objects and MR Scenes\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.2.3\tPotential Normative Work",
                            "text_content": "Table 5.2-1 provides an overview of potential normative work that has been collected as part of the use case collection in Annex A and maps the potential work area to one or several use cases in Annex A.\nTable 5.2-1: Overview of potential normative work \nlinked to different offline sharing use-cases in Annex A\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.2-1: Overview of potential normative work \nlinked to different offline sharing use-cases in Annex A",
                                    "table number": 10,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "5.3\tReal-time XR Sharing",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.3.1\tSummary of Use Cases",
                            "text_content": "This clause summarizes and integrates the following use cases from Annex A in a single core use case referred to as \"Real-time XR Sharing\":\nUse Case 7: Real-time 3D Communication\nUse Case 8: AR guided assistant at remote location (industrial services)\nUse Case 11: Real-time communication with the shop assistant\nUse Case 17: AR animated avatar calls\nUse Case 23: 5G Shared Spatial Data\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.3.2\tDescription",
                            "text_content": "UE B is a device capable of XR rendering, such as AR glasses, or a mobile phone that is sending a real-time video stream of the XR experience to UE A, as illustrated in Figure 5.3-1. The XR experience is being captured and rendered by the UE B. The experience includes capture of 3D objects in a 2D scene. The rendering experience includes (real-time) 3D (static/dynamic) object placement in a 2D scene, overlay video, avatars, etc., that may be downloaded from the cloud, or sent by UE A to UE B. A bidirectional or unidirectional A/V channel may be open between the devices depending on the use case. The received objects by UE B can be influenced by UE B directly as well, based on direct or indirect input from the user. UE A also sends a real-time stream of motion signals and effects that influence the rendering of the 3D object model on UE B. Examples include: 1) head/body motion or facial expressions resulting in corresponding changes in a dynamic 3D object, e.g. an avatar, and 2) positioning and size of the 3D object within the XR scene, and subsequent changes in these parameters such as moving the object closer to a wall or making it larger. Motion data may be collected using direct input from the user interface of the device or implicitly using data from camera, gyroscopes, accelerometer, etc., including gestures. Other predefined effects for the 3D objects that can be placed on or around it can also be shared from UE A to UE B or downloaded from the cloud. Network based XR media processing may be used where required.\nIn a subset of this scenario where XR is not used, a 3D object may be rendered within the received video stream, e.g., a 3D representation of the head of a video call participant.\nShared AR experiences can be realized using a Spatial Computing Server (SCS), as shown in Figure 5.3-1. Collocated users with AR devices can view and interact with AR objects in a time synchronized and spatially correct manner using the SCS. Devices will send positional data updates (e.g. GPS, Wifi, visual scans etc.) to the SCS. The SCS provides spatial maps, spatial anchors and localization services to the devices. Spatial data may be crowd-sourced from multiple devices for higher accuracy in spatial mapping. It should also be possible to then assign spatial information to AR objects so they can be be dropped by users at specific locations for later discovery by other visitors.\n\n\nFigure 5.3-1 Real-time sharing of XR content\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.3.3\tPotential Normative Work",
                            "text_content": "Potential normative work that has been collected as part of the use case collection in Annex A is provided in the following:\n-\tSupport of static/dynamic 3D objects in MTSI (formats and transport).)\n-\tOverlaid video rendering in the network or locally.\n-\tOverlaid video rendering in the network or locally.\n-\tCoded representation of 3D depth signals and transport in MTSI.\n-\tCoded representation of XR scenes and transport in MTSI.\n-\tMTSI/FLUS uplink of XR video.\n-\tDownlink XR video with local/cloud computation and rendering.\n-\tVisual coding and transmission of avatars or cut-out heads, alpha channel coding.\n-\tTransports and potentially coding of motion data to show attentiveness.\n-\tScalable formats for storing and sharing spatial information.\n-\tData representation for spatial information.\n-\tCollected sensor data to be streamed.\n-\tContent delivery protocols to access AR maps and content items.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "5.4\tXR Multimedia Streaming",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.4.1\tSummary of Use Cases",
                            "text_content": "This clause summarizes and integrates the following use cases from Annex A in a single core use case referred to as \"XR Multimedia Streaming\":\nUse Case 3: Streaming of Immersive 6DoF\nUse Case 4: Emotional Streaming\nUse Case 20: AR Streaming with Localization Registry\nUse Case 21: Immersive 6DoF Streaming with Social Interaction\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.4.2\tDescription",
                            "text_content": "This category covers live and on-demand streaming of XR multimedia streams, which include 2D or Volumetric video (i.e., Constrained 6DoF) streams that are rendered in XR with binaural audio as well as 3DOF and 3DOF+ immersive A/V streams. It is illustrated in Figure 5.4-1, where UE is a device capable of receiving and rendering the type of stream in use. It is also capable of controlling the playback of these streams using input from handheld controllers, hand gestures, biometric readings, body and head movements etc., which is communicated to the content server. Control signals include pause, rewind, viewpoint selection or, in case of emotional streaming, adaptive content selection.\nIn another system instance, the content server can provide Inter-destination Multimedia Synchronization (n) for a group experience. A Spatial Computing Server is used by XR capable devices to register, compute, update and recall the spatial configuration of their surroundings. The service is meant for indoor spaces like a shared room or building. Appropriate surfaces may be selected for display of XR streams and saved in the Spatial Computing Server. The configuration can be shared amongst authorized users to enhance group experience when multiple users are physically sharing the same space.\nA social aspect may be added to XR multimedia streaming by receiving live social media feeds, in addition to the XR media stream, that can be rendered as an overlay. Furthermore, the users may be able to see the avatars of other remote located users consuming the same media (additionally from the same viewpoint in case of multiple viewpoints) and have audio conversations with them by establishing one or more communication channels. The social aspects are added by the cloud in the Figure 5.4-1. The XR Media storage in the cloud is for fetching XR objects, e.g., avatars. The location is shared with the cloud to establish colocation within the XR. For instance, users viewing from the same viewpoint may be considered collocated when consuming synchronized media.\nFigure 5 illustrates the process of XR multimedia streaming, showcasing the interaction between various components such as the user's XR device, the server, and the network infrastructure. The figure highlights the role of edge computing in processing and delivering content locally, reducing latency and improving the overall user experience. It also emphasizes the importance of network slicing for creating dedicated resources for different applications, ensuring smooth and efficient streaming.\nFigure 5.4-1: XR Multimedia Streaming\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.4.3\tPotential Normative Work",
                            "text_content": "For this use case, the potential normative work may cover:\n-\tCoded representation of Audio/Video Formats as well as geometry data for XR (volumetric, 3DoF+)\n-\tScene composition and description\n-\tStorage and Cloud Access Formats\n-\tTransport protocols to support any media beyond 2D streaming\n-\tDecoding, rendering and sensor APIs\n-\tBiometrics and Emotion Metadata definition and transport\n-\tSeamless splicing and smooth transitions across storylines\n-\tFormat for storing and sharing indoor spatial information.\n-\tInter-destination multimedia synchronization for group/social experience.\n-\tSocial VR Components – Merging of avatar and conversational streams to original media (e.g., overlays, etc.)\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "5.5\tOnline XR Gaming",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.5.1\tSummary of Use Cases",
                            "text_content": "This clause summarizes and integrates the following use cases from Annex A in a single core use case referred to as \"Online XR Gaming\":\nUse Case 5: Untethered Immersive Online Gaming\nUse Case 6: Immersive Game Spectator Mode\nUse Case 22: 5G Online Gaming party\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.5.2\tDescription",
                            "text_content": "The system as illustrated in Figure 5.5-1 consists of a game server capable of serving several online gamers. Each player UE receives a stream of the live game from the game server and sends control signals back to the server that influence the game play. Control signals include handheld controller inputs, biometric readings and 3DOF+ motion as required by the gameplay.\nOther users may join the live game in spectator mode and can receive the stream from the perspective of an active player or a spectator view independent of other players; viewpoint changes may be possible. The spectator may enjoy an immersive experience or 2D view depending on the device. Optionally, the spectators may interact with the players through cheering or game reward systems. In a different instance, cloud rendering could be present.\nIn an extension, the players may be co-located at a gaming party/session using device-to-device or centralized (via gaming server) communication to enhance the user experience due to improved QoS parameters. A geofenced area may be used as the game arena for an AR gaming session, with support from a Spatial Computing Server for location registration and update and a Group discovery protocol for identifying and matching players. AI methods may be required for Image & Object Recognition, XR Lighting, Occlusion Avoidance, Shared Persistence. Spatial Computing Server receives continuous updates from the players in the arena, which are used for spatial mapping and localization. The updated spatial data (e.g., maps and player locations) is shared with the other devices and also the game server.\nFigure 5 illustrates the online XR gaming process, showcasing the interaction between the user's device and the server. The figure depicts the data flow, including the user's input, server processing, and the output back to the user. Key components include the client, server, and the gaming environment. The diagram highlights the real-time nature of the interaction, emphasizing the importance of low latency and high bandwidth for an immersive gaming experience.\nFigure 5.5-1: Online XR Gaming\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.5.3\tPotential Normative Work",
                            "text_content": "For this use case, the potential normative work may cover:\n-\tStreaming protocols for online gaming\n-\tDecoding, rendering and sensor APIs\n-\tArchitectures for computing support in the network (see gap analysis in TR 22.842, clause 5.3.6)\n-\tCoded Representation of Audio/Video Formats for gaming.\n-\tDevice-to-device communication support for XR gaming.\n-\tFormat for storing and sharing location information.\n-\tNetwork support for group discovery and authentication, shared persistence and spatial mapping.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "5.6\tXR Mission Critical",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.6.1\tSummary of Use Cases",
                            "text_content": "This clause summarizes and integrates the following use cases from Annex A in a single core use case referred to as \"XR Mission Critical\":\nUse Case 9: Police Mission Critical with AR\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.6.2\tDescription",
                            "text_content": "The system shown in Figure 5.6-1 is for critical missions using XR support. In this use case a team in an indoor space is equipped with mission gear that is connected to a centralized control center. The UE in the figure represents a single team member and there may be more than one device included in the misson gear, such as AR glasses, a 360 degree helmet-mounted camera, a microphone, binaural audio headphones and other sensors. The control center/conference server performs the role of mission support by providing XR graphics such as maps, text, location pointers of other team members or other objects/people in the surrounding, etc. The mixed audio of the team members as well as audio from control center is also delivered to the UE to aid team communication and coordination. One or more drone-mounted cameras may also be used, which can be controlled by the control center or one of the members of the mission team. The control center is equipped with A/V processing capabilities to extract important information from the multitude of 360 degree video feeds, for instance, for identifying moving objects. All devices at the site(UEs and drones) use MCPTT to communicate.\n\nFigure 5-6 illustrates the critical mission scenario for an XR (Extended Reality) system, showcasing the interaction between multiple users and virtual environments. The figure depicts a network setup with a central server, edge servers, and various XR devices, highlighting the real-time data transfer and processing required for seamless user experiences. The figure emphasizes the importance of low latency and high bandwidth in ensuring the success of XR applications, particularly in mission-critical scenarios.\nFigure 5.6-1: XR Critical Mission\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.6.3\tPotential Normative Work",
                            "text_content": "For this use case, the potential normative work may cover:\n-\tMTSI/FLUS uplink 3D audio\n-\tMTSI/FLUS/MCVideo uplink XR streams\n-\tDownlink XR video with overlaid graphics with local/cloud computation and rendering\n-\tDownlink XR audio with mixed-in 3D audio objects with local/cloud computation and rendering.\n-\tMTSI/MCPTT SWB/FB voice communication\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "5.7\tXR Conference",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.7.1\tSummary of Use Cases",
                            "text_content": "This clause summarizes and integrates the following use cases from Annex A in a single core use case referred to as \"XR Conference\":\nUse Case 12: 360-degree conference meeting\nUse Case 13: 3D shared experience\nUse Case 14: 6DOF VR conferencing\nUse Case 15: XR Meeting\nUse Case 16: Convention / Poster Session\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.7.2\tDescription",
                            "text_content": "This system caters for an XR conference with multiple physically co-located and remote participants using XR to create telepresence. The shared conference space can be, 1) a physical space that is shared by local participants and sent as an immersive stream to the remote participants, 2) a virtual space that has the same layout as the physical space so that the physically present (local) and remote participants have a similar experience while moving in the space (e.g. captured via a 360-degree camera), and 3) a virtual space that is retrieved from an application server (AS). In any of the 3 options the conference space might be extended with other Media Objects (e.g. a presentation video) retrieved from an application server (AS). The UE functionality can be split into two parts one for user media capture and one for rendering. In practice, these functions may (and usually will) be integrated within a single device (e.g. a smartphone) possibly augmented by peripheral devices like a wireless camera. Another option is that they are indeed separated, e.g. using a dedicated capture device (like a 360-degree camera) and a XR rendering device (like AR Glasses, mobile phone, VR HMD, holographic display, etc.). However, it should also be considered that some UEs will render the immersive communication experience on traditional displays.\nFigure 5.7-1 illustrates the system. Virtual spaces and avatars are retrieved from the Application Server by the Conference Server. A Spatial Computing Server is used for storing the layout of the physical space, when used. Remote participants would be seen as avatars within the XR experience located at their relative position in the shared space. Alternatively, they may be represented as themselves using a live video feed and video extraction to isolate the person from the background and using a photo-realistic representation of their face in place of the HMD. The required video processing is located in the conference server in Figure 5.7-1. For example, a Network Media Processing function may perform any media and/or metadata processing that is required to place, a certain user and multiple other users consistently into a virtual environment. A typical example would be a Multipoint Control Unit (MCU) function that combines and processes the video captured from the various users in order to reduce the resource requirements of the clients. Finally, the Conference Server takes care of all the session signalling to set up channels for the exchange of media data and metadata. It performs Session Control functions for the scene configuration, network media processing and common scene media.\nRemote participants are free to move around when 6DOF motion is used. The conference server processes the audio streams from the participants to create an XR experience. Participants will hear binaural audio of all participants according to their position and a 360-degree surround sound, if needed. If a physical space is used, the conference server would also receive and process input from one or multiple 360-degree A/V capture devices and RGB+depth camera. Note that when 6DOF is supported, all remote participants can move freely within the confines of the designated space, moving from one room to another when there are multiple rooms defined in the space. Using motion signals, relative positioning and location information, it would be possible for participants (local + remote) to form smaller groups for discussion within the XR space as would happen in a real space. The conversation/real-time XR stream shown in the figure is a mix of VR (remote user) or AR (local user) media, room layout (virtual/physical) and mixed binaural audio. The presentation pointer data may be sent from one of the UEs while presenting a shared presentation/poster for highlighting specific parts.\nA top-view of the conference space showing its layout and the current positions of the participants can be viewed by participants and is indicated as part of the XR stream label in the figure (but as separate physical stream). The conference server should also provide IDMS.\nFigure 5 illustrates the utilization of XR (Extended Reality) technology in a conference setting, showcasing a virtual environment where participants can engage in immersive meetings. The figure displays a 3D representation of a conference room, complete with virtual avatars of the attendees. The layout includes a central podium for presentations, seating arrangements, and interactive elements such as whiteboards and multimedia displays. The figure emphasizes the potential of XR technology to enhance collaboration and communication in remote work and virtual events.\nFigure 5.7-1: XR Conference\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.7.3\tPotential Normative Work",
                            "text_content": "Table 5.7-1 provides an overview of potential normative work that has been collected as part of the use case collection in Annex A and maps the potential work area to one or several use cases in Annex A.\nTable 5.7-1: Overview of potential normative work \nlinked to different conversational/conferencing use-cases in Annex A\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.7-1: Overview of potential normative work \nlinked to different conversational/conferencing use-cases in Annex A",
                                    "table number": 11,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "5.8\tSpatial Audio Multiparty Call",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.8.1\tSummary of Use Cases",
                            "text_content": "This clause summarizes and integrates the following use cases from Annex A in a single core use case referred to as \"Spatial Audio Multiparty Call\":\nUse Case 18: AR avatar multi-party calls\nUse Case 19: Front-facing camera video multi-party calls\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.8.2\tDescription",
                            "text_content": "The system shown in Figure 5.8-1 illustrates an AR multiparty call. Each party can see the other parties using 2D video streams captured by the front facing camera of a mobile phone. Alternatively, they can be displayed as avatars, for instance, when a pair of AR glasses are used as UE. Each party hears spatial audio with the audio of the other parties originating from where their avatar/video is placed on the display. Motion such as head turns are tracked to create a realistic impression that the audio is originating from that virtual direction.\nIn a special case, the avatars and the audio of the other parties on party A's display is based on their actual geolocation and the relative direction they are with respect to party A. The same would be true for all parties. UEs also have the ability to switch to PTT to suppress surrounding sound if they wish. They may use the \"hear what I hear\" function to send a 3D audio of their surroundings to the other parties.\n\nFigure 5.8-1: Spatial Audio Multiparty Call\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.8.3\tPotential Normative Work",
                            "text_content": "For this use case, the potential normative work may cover:\n-\tVisual coding and transmission of avatars\n-\tCoding of cut-out heads, alpha channel coding\n-\tAudio coding and transmission of mono objects and 3D audio for streams from all participants.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "6\tMapping Extended Reality to 5G",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "6.1\tIntroduction",
                    "description": "",
                    "summary": "",
                    "text_content": "Based on the technologies introduced in clause 4 as well as the core use cases and scenarios introduced in clause 5, this clause maps a set of core technologies to 5G media centric architectures.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "6.2\tXR Processing and Media Centric Architectures",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.2.1\tIntroduction",
                            "text_content": "This clause focuses on rendering and media centric architectures. The architectures are simplified and illustrative, they only consider an XR server and an XR device to identify the functions in XR servers and XR devices that communicate and exchange information, possibly over a 5GS communication. The baseline technologies are introduced in clause 4. These architectures focus on processes where the following main tasks are carried out:\n-\tDisplay\n-\tTracking and Pose Generation\n-\tViewport Rendering\n-\tCapture of real-world content\n-\tMedia encoding\n-\tMedia decoding\n-\tMedia content delivery\n-\t5GS communication\n-\tMedia Formats, metadata and other data delivered on communication links\n-\tSpatial Location Processing\nThe section also discusses benefits and challenges of the different approaches in terms of required bitrates, latencies, reliability, etc. A main aspect to be addressed in the following are the processes that are involved in the motion-to-photon/sound latency and how the processed may impact the XR viewport rendering.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.2\tViewport-Independent delivery",
                            "text_content": "In the viewport independent delivery case, following the architecture in 3GPP TS 26.118 [3], clause 4.3, tracking and sensor information is only processed in the XR device as shown in Figure 6.2.2-1. This means that the entire XR scene is delivered and decoded.\nFigure 6.2.2-1 illustrates the viewport independent delivery process, showcasing the various stages of content transformation and optimization for different display devices. The figure highlights the role of media processing units (MPUs) and content adaptation tools in ensuring a seamless viewing experience across various screen sizes and resolutions. The figure emphasizes the importance of dynamic content adaptation and efficient resource allocation to maintain high-quality visuals and minimize buffering times.\nFigure 6.2.2-1: Viewport Independent Delivery\nUse cases that may be addressed partially or completely by this delivery architecture are summarized in clause 5.4.\nThe basic procedures follow the procedures of 5G Media Streaming in 3GPP TS 26.501 [12], clause 5. Both, on-demand and live streaming may be considered.\nNo content format for 6DoF streaming is fully defined yet, but the content may for example be a scene for which foreground 3D objects, for example represented by point-clouds, are mixed with background content, for example a omnidirectional scene. The combination of the content may be provided by a scene description that places the object in the 6DoF scene. Typically, the data needs to be streamed into buffers that are jointly rendered.\nDue to the significant amount of data that needs to be processed in the device, hardware supported decoding and rendering is necessary.\nSuch an approach typically requires the delivery and decoding of several video and audio streams in parallel.\nIn the case of viewport-independent streaming, processing of updated pose information is only done locally in the XR device. Delivery latency requirements are independent of the motion-to-photon latency.\nIn order to provide sufficient content quality, the video material is referably encoded such that the QoE parameters as defined as defined in clause 4.2 can be fulfilled. The necessary QoS and bitrates on the 5G System depend depend on the type of the XR media as well as on the streaming protocol. Based on information from the workshop \"Immersive Media meets 5G\" in April 2019 as well as from publicly announced demos, that based on today's equipment and the one available over the next 2-3 years, around 100 Mbps are sufficient bitrates to address high-quality 6DOF VR services. This is expected to allow 2k per eye streaming at 90 fps (see clause 4.2) using existing video codecs (see clause 4.5). The QoE requirements may increase further, for example higher resolution and frame rate, but with the advance of new compression tools, this is expected to be compensated.\nThe XR media delivery are typically built based on download or adaptive streaming such as DASH (see for example TS 26.118 [3] and TS 26.247 [7]), such that one can adjust quality to the available bitrate to a large extent.\nSuitable 5QI values for adaptive streaming over HTTP are 6, 8, or 9 as defined in  clause 4.3.3.\nIf other protocols are applied for streaming, then suitable 5QIs would be for FFS.\nIn the context of the present document, relevant 3D media formats, efficient compression, adaptive delivery as well as the perceived quality of the XR media is of key relevance.\nIn the context of this delivery scenario, the following potential standardisation needs are identified:\nVery high-bitrate and efficient/scalable streaming protocols\n6DoF Scene Description and XR media integration\nVideo and audio codec extensions to efficiently code and render graphics centric formats (2D, meshes, point clouds)\nSupport of decoding platforms that support the challenges documented in clause 4.5.2.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.3\tViewport-dependent Streaming",
                            "text_content": "In the viewport dependent delivery case, following the architecture in TS 26.118 [3], clause 4.3, the tracking information is predominantly processed in the XR device, but the current pose information is provided to the XR delivery engine in order to include the pose information in the adaptive media requests. In an extension to this in the case of XR and 6DoF, the XR pose and additional information may be shared with the XR content delivery in order to only access the information that is relevant for the current viewports. According to Figure 6.2.3-1, the tracking and sensor data is processed in the XR device for XR rendering, and the media is adaptively delivered/requested based on the XR viewport. A reduced or a viewport optimized scene is delivered and also only a reduced scene is processed. Examples include an object that is not visible is not delivered, or only delivered in low quality, or that only the part of the object that is in the viewport is delivered with the highest quality.\nFigure 6.2.3-2 Viewport-dependent Streaming illustrates the process of viewport-dependent streaming in a multimedia system, focusing on the adaptive bitrate streaming over HTTP Live Streaming (HLS). The figure depicts the interaction between the client, the server, and the content delivery network (CDN), highlighting the dynamic adjustment of video quality based on the user's viewport size and network conditions. The figure emphasizes the importance of efficient video streaming, ensuring a smooth playback experience while minimizing bandwidth usage and latency.\nFigure 6.2.3-2 Viewport-dependent Streaming\nUse cases that may be addressed partially or completely by this delivery architecture are summarized in clause 5.4.\nThe basic procedures follow the procedures of 5G Media Streaming in TS 26.501 [12], clause 5. Both, on-demand and live streaming may be considered.\nIn addition, the request for data is accompanied with information from the XR engine.\nThe same formats as discussed in clause 6.2.2.4 apply.\nCompared to the viewport independent delivery in clause 6.2.2, for viewport dependent streaming, updated tracking and sensor information impacts the network interactivity. Typically, due to updated pose information, HTTP/TCP level information and responses are exchanged every 100-200 ms in viewport-dependent streaming.\nFrom analysis in TR 26.918 [2] and other experience as for example documented the workshop \"Immersive Media meets 5G\" in April 2019\" [42], such approaches can reduce the required bitrate compared to viewport independent streaming by a factor of 2 to 4 at the same rendered quality.\nIt is important to note that viewport-dependent streaming technologies are typically also built based on adaptive streaming allowing to adjust quality to the available bitrate. The knowledge of tracking information in the XR Delivery receiver just adds another adaptation parameter. However, generally such systems may be flexibly designed taking into account a combination/tradeoff of bitrates, latencies, complexity and quality.\nIn the context of the this architecture, the following potential standardisation needs are identified:\nThe same aspects as defined in clause 6.2.2.6\nIn addition, more flexible data structures and access to these data structures as well as concurrentdecoding and streaming of smaller units of data, such as tile-based structures, may be defined.\nIf other protocols than adaptive streaming over HTTP would be applied, then suitable 5QIs would be for FFS.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.4\tViewport Rendering in Network",
                            "text_content": "In a architecture as shown in figure 6.2.4-1 below, the viewport is entirely rendered in the XR server. The XR server generates the XR Media on the fly based on incoming Tracking and Sensor information, for example a game engine. The generated XR media is provided for the viewport in a 2D format (flattened), encoded and delivered over the 5G network. The tracking and sensor information is delivered in the reverse direction. In the XR device, the media decoders decode the media and the viewport is directly rendered without using the viewport information.\nFigure 6.2.4 presents a viewport rendering in a network simulation, showcasing the flow of data packets through various network elements. The figure illustrates the interaction between the network nodes, including switches, routers, and firewalls, as well as the transmission mediums such as Ethernet cables and fiber optic lines. The visualization aids in understanding the performance and behavior of network traffic, enabling network administrators to optimize and troubleshoot network operations effectively.\nFigure 6.2.4-1 Viewport rendering in Network\nThe following call flow highlights the key steps:\n1)\tAn XR device connects to the network and XR Media Generation application\na)\tSends static XR device information and capabilities (supported decoders, viewport)\n2)\tBased on this information, XR server sets up encoder and formats\n3)\tLoop\na)\tXR device collects pose (or a predicted pose)\nb)\tXR Pose is sent to XR Server\nc)\tThe XR Server uses the pose to generate/compose the viewport\nd)\tXR Viewport is encoded with regular media encoders\ne)\tThe compressed video is sent to XR Device\nf)\tThe XR device decompresses video and directly renders viewport\nSuch an architecture enables simple clients, but has significantly challenges on compression and transport to fulfill the latency requirements. Latencies should be kept low for each processing step including delivery, to make sure that the cumulative delay for all the processing steps (including tracking, pose delivery, viewport rendering, media encoding, media delivery, media decoding and display) is within the immersive motion-to-photon latency upper limit of 20ms.\nThe following three cases, with different media delivery bitrates, are considered:\n1)\tAround 100 Mbps: In this case, the XR device needs to perform certain amount of processing and decoding.\n2)\tAround 1 Gbps: In this case, only lightweight and low-latency compression (e.g. intra only) may be used to provide sufficiently high quality (4k or even 8k at sufficiently high frame rates above 60 fps) and sufficiently low latency (immersive limits of less than 20ms for motion to photon) for such applications. It is still expected that some processing (e.g. decoding) by the XR device is needed.\n3)\tAround 10 Gbps or even more: A full \"USB-C like\" wireless connection, providing functionalities that currently can only be provided by cable, possibly uncompressed formats such as 8K. The processing requirements for the XR device in this case may be minimal.\nNote that the lightweight compression or no compression in cases 2) and 3) can help to reduce processing delays.\nIn addition, the formats exported from Game engines needs to be supported by the respective media encoders.\nNote that in this case for XR-based services, the motion-to-photon latency determines the maximum latency requirements for the content. This means that 20ms as defined in clause 4.5.1 are the end-to-end latency targets, including the uplink streaming of the pose information.\nThis is different, if the content is rendered on a flat device (for example for cloud/edge gaming applications on smartphones), for which not motion-to-photon latency determines the latency requirements, but the roundtrip interaction delay. In this case, the requirements from clause 4.5.2 apply, typically a 50ms latency is a requirement for most advanced games.\nOn Formats and codecs:\nFrom the analysis, for case 1, similar aspects as defined in clause 6.2.2.6 apply for the formats.\nFor cases 2 and 3, formats are of less relevance for 3GPP as such formats are typically defined by other consortia, if at all.\nOn network support:\nNetwork rendering for cloud gaming on flat screens is expected to be of significant relevance. In this case the end-to-end latency (action to photon) is determined by the roundtrip interaction delay, i.e. 50ms (see 4.5.2). 5QIs to support such latencies as well as guaranteed bitrates are considered of relevance. Required bitrates follow case 1) from above.\nNetwork rendering for XR services would require an end-to-end latency including motion-to-photon (including network rendering, encoding, delivery and decoding) of 20ms to meet the immersive limits and it is expected that the bitrates would be higher due to low-complexity and low-latency encoding, following case 2) and 3) from above. Hence,\n5QIs and QoS would be necessary, that provides significantly lower latency than 10ms in both directions and the same time provides a stable and high bitrate in the range of 0.1 – 1 Gbps according to case 2).\nIt is not expected to be practical for Uu-based communication to achieve such low-latencies at very high bitrates (mostly case 3, e.g. 1Gbps and higher) in the short term, but final studies on this matter are FFS.\nHowever, sidelink-based based communication addressing network rendering is expected to be feasible in the 5G architecture and is subject to active work in 3GPP.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.5\tRaster-based Split Rendering",
                            "text_content": "Raster-based split rendering refers to the case where the XR Server runs an XR engine to generate the XR Scene based on information coming from an XR device. The XR Server rasterizes the XR viewport and does XR pre-rendering.\nAccording to Figure 6.2.5-1, the viewport is pre-dominantly rendered in the XR server, but the device is able to do latest pose correction, for example by asynchronuous time-warping (see clause 4.1) or other XR pose correction to address changes in the pose.\n-\tXR graphics workload is split into rendering workload on  a powerful XR server (in the cloud or the edge) and pose correction (such as ATW) on the XR device\n-\tLow motion-to-photon latency is preserved via on device Asynchronous Time Warping (ATW) or other pose correction methods.\nAs ATW is applied the motion-to-photon latency requirements (of at most 20 ms) are met by XR device internal processing. What determines the network requirements for split rendering is time of pose-to-render-to-photon and the roundtrip interaction delay. According to clause 4.5, the latency is typically 50-60ms. This determines the latency requirements for the 5G delivery.\nFigure 6.2.5 -1 illustrates the process of split rendering with asynchronous time warping (ATW) correction in a telecommunication context. The figure showcases the synchronization of two separate rendering processes, where one process is responsible for the foreground while the other handles the background. This approach allows for the efficient rendering of complex scenes with dynamic elements. The asynchronous nature of the processes enables real-time interaction and minimizes the impact of latency on the overall rendering quality. The figure also highlights the use of ATW correction, which is employed to align the two rendering processes and ensure a seamless transition between the foreground and background. This technique is crucial for maintaining the visual coherence of the rendered scene, especially in applications where precise synchronization is essential.\nFigure 6.2.5-1: Split Rendering with Asynchronous Time Warping (ATW) Correction\nThe use cases in clause 5.5 may be addressed by this architecture.\nThe following call flow highlights the key steps:\n1)\tAn XR Device connects to the network and joins XR application\na)\tSends static device information and capabilities (supported decoders, viewport)\n2)\tBased on this information, the XR server sets up encoders and formats\n3)\tLoop\na)\tXR Device collects XR pose (or a predicted XR pose)\nb)\tXR Pose is sent to XR Server\nc)\tThe XR Server uses the pose to pre-render the XR viewport\nd)\tXR Viewport is encoded with 2D media encoders\ne)\tThe compressed media is sent to XR device along with XR pose that it was rendered for\nf)\tThe XR device decompresses video\ng)\tThe XR device uses the XR pose provided with the video frame and the actual XR pose for an improved prediction using  and to correct the local pose, e.g. using ATW.\nRasterized 3D scenes available in frame buffers (see clause 4.4) are provided by the XR engine and need to be encoded, distributed and decoded. According to clause 4.2.1, relevant formats for frame buffers are 2k by 2k per eye, potentially even higher. Frame rates are expected to be at least 60fps, potentially higher up to 90 fps.The formats of frame buffers are regular texture video signals that are then directly rendered. As the processing is graphics centric, formats beyond commonly used 4:2:0 signals and YUV signals may be considered.\nWith the use of time warp, the latency requirements follow those documented in clause 4.2.2, i.e. the end-to-end latency between the user motion and the rendering is 50ms.\nIt is known from experiments that with H.264/AVC the bitrates are in the order of 50 Mbps per eye buffer. It is expect that this can be reduced to lower bitrates with improved compression tools (see clause 4.5) but higher quality requirements may absorb the gains. It is also known that this is both content and user movements dependent, but it is known from experiments that 50 - 100 Mbps is a valid target bitrate for split rendering.\nRegular stereo audio signals are considered, requiring bitrates that are negligible compared to the video signals.\n5QI values exist that may address the use case, such 5QI value number 80 with 10ms, however this is part of the non-GBR bearers (see clause). In addition, it is unclear whether the 10ms with such high bitrates and low required error rates may be too stringent and resource consuming. Hence, for simple split rendering in the context of the requirements in this clause, suitable 5QIs may have to be defined addressing the latency requirements in the range of 10-20ms and bitrate guarantees to be able to stream 50 to 100 Mbps consistently.\nThe uplink is predominantly the pose information, see clause 4.1 for details. Data rates are several 100 kbit/s and the latency should be small in order to not add to the overall target latency.\nIn the context of this architecture, the following potential standardisation needs are identified:\nRegular 2D video encoders and decoders that are capable encode and decode 2K per eye as well as 90 fps and are capable to encode typical graphics frame buffer signals.\nPose information in the uplink at sufficiently high frequency\nContent Delivery protocols to support the delivery requirements\nEdge computing discovery and capability discovery\nA simple XR split rendering application framework for single buffer streaming\nNew 5QIs and QoS support in 5G System for split rendering addressing latency requirements in the range of 10-20ms and bitrate guarantees to be able to stream 50 to 100 Mbps consistently.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.6\tGeneralized XR Split Rendering",
                            "text_content": "In Figure 6.2.6-1, an architecture is shown for which the XR server pre-renders the 3D scene into a simpler format to be processed by the device (e.g. it may provide additional metadata that is delivered with the pre-rendered version). The device recovers the baked media and does the final rendering based on local correction on the actual pose.\n-\tXR graphics workload is split into rendering workload on a powerful XR server and simpler XR processing on the XR device\n-\tThis approach enables to relax the latency requirements to maintain a full immersive experience as time-critical adjustment to the correct pose is done in the device.\n-\tthis approach may provide more flexibility in terms of bitrates, latency requirements, processing, etc. than the single buffer split rendering in clause 6.2.5.\nFigure 6.2.6-1 illustrates the process of VR split rendering with XR viewport rendering in a device, showcasing how multiple virtual reality (VR) experiences are managed and displayed simultaneously. The figure highlights the role of the XR viewport, which acts as a unified interface for users to navigate and interact with various VR content. The diagram emphasizes the importance of efficient resource allocation and synchronization among different VR applications, ensuring a smooth and immersive user experience.\nFigure 6.2.6-1: VR Split Rendering with XR Viewport Rendering in Device\nSuch an approach needs careful considerations on the formats of projected media and their compression with media decoders. Also important is distribution of latencies to different components of the system. More details and breakdown of the architectures is necessary. The interfaces in the device however are aligned with the general structure defined above.\nIn general, the similar requirements and considerations as in clause 6.2.5 apply, but a more flexible framework may be considered by providing not only 2D frame buffers, but different buffers that are split over the network.\nThe use cases in clause 5.5 may be addressed by this architecture.\nThe following call flow highlights the key steps:\n1)\tAn XR Device connects to the network and joins XR application\na)\tSends static device information (supported decoders, viewport, supported formats)\n2)\tBased on this information, network server sets up encoder and formats\n3)\tLoop\na)\tXR Device collects XR pose (or a predicted XR pose)\nb)\tXR Pose is sent to XR Server\nc)\tThe XR Server uses the pose to pre-render the XR viewport by creating one or multiple rendering buffers, possibly with different update frequencies\nd)\tThe rendering buffers are encoded with 2D and 3D media encoders\ne)\tThe compressed media is sent to XR device along with additional metadata that describes the media\nf)\tThe XR device decompresses the multiple buffers and adds this to the XR rendering engine.\ng)\tThe XR rendering engine takes the buffers, the rendering pose assigned to the buffers and the latest XR pose to create the finally rendered viewport.\nIn this context, the buffers may not only be 2D texture or frame buffers as in case of clause 6.2.5, but may include geometric data, 3D data, meshes and so on. Also multiple objects may be generated. The content formats discussed in clause 4.6 apply.\nWith the use of different buffers, the latency requirements follow those documented in clause 4.5.2, i.e. the end-to-end latency between the user motion and the rendering is 50ms. However, it may well be that the update frequency of certain buffers is less. This may result in differentiated QoS requirements for different encoded media, for example in terms of latency, bitrates, etc.\nMore details are FFS.\nIn the context of the this architecture, the following potential standardisation needs are identified:\nSimilar aspects as defined clause 6.2.5.6\nFlexible 2D and 3D formats that can be shared over the network to serve device rendering buffers\nFormats and decoding capabilities as defined in clause 4.5.2\nEdge computing discovery and capability discovery for Generalized Split rendering\nA generalized XR split rendering application framework\nMore flexible 5QIs and QoS support in 5G System for generalized split rendering addressing differentiated latency requirements in the range of 10ms up to potentially several 100ms and with bitrate guarantees.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.7\tXR Distributed Computing",
                            "text_content": "This clause provides the architecture for extended reality applications which supports the XR split rendering. The workload for XR processing is split into workloads on XR server and the device. The below Figure 6.2.7-1 shows a high-level structure of the XR distributed computing architecture which describes their components and interfaces.\nFigure 6.2.7 -1 presents a detailed XR (Extended Reality) distributed computing architecture, illustrating the integration of various components such as the XR headsets, edge servers, and cloud-based processing units. The figure emphasizes the seamless interaction between the hardware and software layers, showcasing how data is processed and transmitted efficiently across the network. Key elements include the use of low-latency communication protocols and the strategic placement of edge servers to reduce latency and improve user experience. The figure also highlights the importance of security measures in maintaining privacy and data integrity in XR applications.\nFigure 6.2.7-1: XR Distributed Computing Architecture\nAn XR client may have following capabilities.\n-\tXR capture\n-\tSensor data processing (e.g., AR pose tracking)\n-\tXR scene generation\n-\tXR rendering.\n-\t2D or 3D Media decoding\n-\tMetadata (including scene description) processing\n-\t5G delivery\nAn XR edge server may have following capabilities.\n-\tSensor data processing\n-\tXR scene generation\n-\t2D or 3D media encoding\n-\tMetadata (including scene description) generation\n-\t5G delivery\nAn XR client connects to the network and joins XR rendering application. The XR client sends static device information (e.g., sensors, supported decoders, display configuration) to the XR edge server. Based on this information, the XR edge server sets up encoder and formats.\nWhen the XR client has a set of sensors (e.g., trackers and capturing devices, it collects sensor data from sensors. The collected sensor data is processed either locally or at the XR edge server. The collected sensor data or locally processed information (e.g., a current AR pose) is sent to the XR edge server. The XR edge server uses the information to generate the XR scene. The XR edge server converts the XR scene into a simpler format as 2D or 3D media with metadata (including scene description). The media component is compressed, and the compressed media stream and metadata are delivered to the XR client. The XR client generates the XR scene by compositing locally generated or received media and metadata and renders the XR viewport via the XR display (e.g., HMD, AR glass).\nFor example, the XR client captures the 2D video stream from a camera and sends the captured stream to the XR edge server. The XR edge server performs the AR tracking and generates the AR scene which a 3D object is overlaid over a certain position in the 2D video based on the AR tracking information. The 3D object or 2D video for the AR scene are encoded with 2D/3D media encoders, and the scene description or the metadata is generated. The compressed media and metadata are sent to the XR client. The XR client decodes the media or metadata and generates an AR scene which overlays the 3D object in the 2D video., A user viewport is determined by horizontal/vertical field of view of the screen of a head-mounted display or any other display device. The appropriate part of AR scene for the user viewport is rendered and displayed.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.8\tXR Conversational",
                            "text_content": "In Figure 6.2.8-1, a general architecture for XR conversational and conference services is depicted. As stated, these services are an extension on the current MTSI work, using the IMS for session signalling. In order to support XR conversational services (in 5G), extensions are needed in the signalling to enable VR/AR specific attributes, and the media and metadata need to support the right codecs, profiles and metadata. A new interface is the interface to the network (based) media processing. This is an interface similar to that to an SRF, but is expected to be much more extensive to support various types of media processing. This interface can be based on the work in MPEG on Network Based Media Processing.\nFigure 6.2.8 illustrates the general architecture for XR conversational and conference services, showcasing the integration of various components such as the XR client, XR server, and the underlying network infrastructure. The figure emphasizes the role of the XR server in managing sessions and providing content, while the XR client handles user interaction. The network infrastructure, depicted through the use of arrows and labels, highlights the importance of low-latency and high-bandwidth connections to support seamless communication and content delivery. The figure also highlights the potential for edge computing and caching solutions to enhance performance and reduce network congestion.\nFigure 6.2.8-1 General architecture for XR conversational and conference services\nTypical steps for call session setup follow normal IMS procedures, in case the clients have a simple peer-2-peer call and also do all processing themselves (simplified procedure as follows):\nThe first client initiates call setup (SIP INVITE);\nThe IMS (i.e. central session control) routes the call setup to the second client, ensuring proper bandwidth reservations in the network;\nThe second client, after call acceptance, responds to the call setup (200 OK);\nThe network controls all bandwidth reservations;\nCall setup is completed.\nBut, given mobile clients, their limited processing capabilities, battery capacity and potentially problems with heat dissipation, processing might be moved to the network. Typical processing for XR conferencing:\nForeground/background segmentation;\nHMD removal, i.e. replacing a users HMD with a 3D model of the actual face, possibly including eye tracking / reinsertion;\n3D avatar reconstruction, i.e. using RGB + depth cameras or multiple cameras to create 3D user video avatars;\nSupport for multiple users with a (centralised or distributed) VR conferencing bridge, stitching multiple user captures together;\nCreating a self-view, i.e. local 3D user avatar from the user’s own perspective.\nIn such network-processing scenario, setup is somewhat extended:\nFirst a client initiates the call setup;\nBased on the call setup, the session control triggers network based media processing, reserves resources in the network, incl. media processing resources;\nSession control forwards call setup to the second client;\nAfter call acceptance, both first and second client are connected to the network processor.\nSession control instruct the network processor on the actual processing and the stream forwarding, i.e. which input streams go to which clients.\nSpecific details here are for further study. Routing media streams can be performed in various ways, using existing methods for stream transcoding, or more centralised conferencing signalling. The interface to the media processor can be based on the existing MRF interface. But, given the developments within MPEG on Network Based Media Processing, maybe a new interface should be defined.\nFor the routing of signalling, many options allready exist within the IMS. Re-use and perhaps slight modifications are expected to be sufficient to cover the various use cases defined here. For the SDP within the signalling, more modifications are expected. Besides support for new types of media and thus new types of codecs (point cloud streaming, mesh streaming, depth streaming) and profiles for those codecs, new types of metadata also need to be supported. Calibration data (i.e. calibration of HMD position vs camera position), HMD information (position, orientation, characteristics), camera information (position/orientation, lens/sensor characteristics, settings), user information (3D model, IOD, HRTF, etc) can all be used to perform or improve the media processing.\nAlso, there are different media types, both for the environment and for the user avatars. A virtual environment can consist of a rendered environment, a 360 photo or video, or some hybrid. User avatars can be graphical avatars, video based, 3D video avatars, rendered avatars. Devices can be a single device (one mobile in an HMD enclosure, potentially with a separate bluetooth camera) or can be multiple devices (separate stand-alone VR HMD, multiple (smartphones as cameras).\nAdditional aspects to be taken into account are:\nplacement of media processor: central vs edge, centralised vs distributed.\ndelay aspects for communication purposes. Ideally, delay is kept to a minimum, i.e. <150 ms one-way delay. Given the required processing, this is a challenge, and will effect e.g. codec choices and rendering choices.\nFor XR Conversational services, we can consider 3 bandwidth cases according to the type of capture/user representation transmitted (with almost constant bandwidth on the upload):\n2D+/RGBD: >2.7Mbit/s (1 camera), >5.4Mbit/s (2 Camera)\n3D Mesh: ~30 Mbit/s\n3D VPCC / GPCC: 5-50 Mbit/s (CTC MPEG)\nFurthermore we can assume that joining an communication experience session will result in a download peek at the beginning of the session to download the environment and associated media objects within the XR application. Throughout a XR communication experience session the download might vary depanding on the amound of users represented and the upload format of those users.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.3\tSummary of Traffic Characteristics",
                    "description": "",
                    "summary": "",
                    "text_content": "This clause summarizes initial typical traffics characteristics for different architectures, based on the findings in this clause. The parameters relate to the 5G PDU session QoS as introduced in clause 4.3.\nTable 6.3-1 provides tnitial traffic characteristics for different architectures based on the findings in clause 6.2. Whereas some initial aspects are collected, many issues are FFS.\nLegend:\n-\tDL: Downlink,\n-\tUL: Uplink,\n-\tPDB: Packet delay budget,\n-\tPER: Packet Error Rate,\n-\tRTT: Round Trip Time\nNote: Either RTT applies, or UL and DL PDB applies separately, but RTT and UL/DL PDB cannot apply simultaneously\nTable 6.3-1: Initial Traffic Characteristics for different architectures\n",
                    "tables": [
                        {
                            "description": "Table 6.3-1: Initial Traffic Characteristics for different architectures",
                            "table number": 12,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "6.4\tAnalysis of existing 5QIs",
                    "description": "",
                    "summary": "",
                    "text_content": "As a summary of the above, existing 5QIs may be used for adaptive streaming over HTTP applications as defined in clause 6.2.2 and 6.2.3.\nFor other types of services, new 5QIs for Uu-based communication are considered beneficial, among others\nIf other protocols than adaptive streaming over HTTP would be applied, then suitable 5QIs would be for FFS.\nNew 5QIs and QoS support in 5G System for network and split rendering addressing latency requirements in the range of 10-20ms and bitrate guarantees to be able to stream 50 to 100 Mbps consistently\nMore flexible 5QIs and QoS support in 5G System for generalized split rendering addressing differentiated latency requirements in the range of 10ms to several 100ms and with bitrate guarantees.\nError rates are FFS.\nThe data rate, latency and PER for different architectures as introduced in clause 6.3 are FFS.\nFor sidelink based communication, new PQI/QoS parameters may be defined as well. Details are FFS.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "7\tPotential Standardisation Areas",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "7.1\tGeneral",
                    "description": "",
                    "summary": "",
                    "text_content": "This clause documents and clusters potential standardisation areas in the context of this Technical Report.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.2\tXR-Centric Device Types and Architectures",
                    "description": "",
                    "summary": "",
                    "text_content": "As documented in clause 4.5, XR centric devices are the key enablers for XR services. Key aspects for XR devices are:\nRendering-centric device architectures using sophisticated GPU functionalities, see clause 4.4.\nSupport for Tracking, in particular inside-out tracking in the device\nHeavily power-constrained at least for certain form factors\nSupport for multiple decoding formats and parallel decoding of media streams following the challenges documented in clause 4.5.2\nIn addition, device characteristics can be quite different. Hence, the device types developed in clause 4.8 serve as a starting point for different device types. A more formal definition of XR devices types is considered useful.\nIn any work considered in 3GPP, end-points compatible to Khronos-based graphics and XR functionalities should be considered. A framework for interfacing device centric XR functionalities with 5G System and radio functionalities is a relevant standardisation effort.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.3\tExtensions to 5G Media Streaming for XR/6DoF Media",
                    "description": "",
                    "summary": "",
                    "text_content": "Streaming XR and 6DoF is considered in several use cases evaluated in this Technical report. With the establishment of 5G Media Streaming in Release-16 in TS 26.501 and the stage-3 specifications, extensions of 5G Media Streaming to support XR experiences is useful. MPEG develops several new formats and codecs specifically addressing 3D and XR, but also proprietary formats exist that make use of regular hardware supported standardized media codecs and rendering functionalities. All of them have in common that they rely on existing and emerging device architectures that make use of existing video codecs and GPU rendering. In addition, the use of multiple video codecs in parallel is commonly applied. XR/6DoF Streaming is based on CDNs and HTTP delivery, however new functionalities are required.\nExtensions to 5G Media streaming may be done in ordered to support the delivery of different XR/DoF media in 5G Systems. Relevant aspects are:\nAdditional media decoding capabilities including higher profile and levels, the use of multiple decoders in parallel to interface with rendering architectures, as well more flexible formats following clause 4.5.2\nSupport for more flexible delivery protocols allowing parallel download of different objects and parts of objects\nViewport-dependent streaming as documented in clause 6.2.3\nPotentially new 5QIs and radio capabilities to support higher bitrate streaming\nBiometrics and Emotion Metadata definition and transport\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.4\tRaster-based Split Rendering with Pose Correction",
                    "description": "",
                    "summary": "",
                    "text_content": "Split Rendering is a promising technology to support online gaming in power- and resource constrained devices. Split rendering requires the use of edge computing as the pose-to-render-to-photon is expected to be below 50ms. Rendering of rasterized frame buffers in the network allows to support XR devices with existing codecs using 5G System and radio capabilities. Relevant aspects for single-buffet split rendering include:\nA simple XR split rendering application framework where a single frame buffer per media per eye is shared and final pose correction is done in the XR device\n2D video encoders and decoders that are capable encode and decode 2K per eye as well as 90 fps as well as typical 2D rasterized graphics centric formats\nIntegration of audio into split rendering architectures\nFormats and protocols for XR Pose information delivery and possibly other metadata in the uplink at sufficiently high frequency\nContent Delivery protocols that support split rendering\n5QIs and other 5GS/Radio capabilities that support split rendering\nEdge computing discovery and capability discovery based on work in SA2 and SA6 (see clause 4.3.6)\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.5\tXR conference applications",
                    "description": "",
                    "summary": "",
                    "text_content": "XR conversational applications within real or computer generated virtual environments is considered in several use cases evaluated in this Technical report. Some work is already ongoing in IVAS, ITT4RT, however, potential additional normative work includes;\n-\tStudy the mapping of XR conference applications to the 5G system architecture including Media streaming and MTSI.\nSupport for media processing in the network (e.g. NBMP) (e.g. foreground/background segmentation of the user capture, replacement of the user with a photo-realistic representation of their face, etc.)\n6DOF metadata framework and a 6DOF capable renderer for immersive voice and audio.\nSupport of static/dynamic 3D objects’ formats and transport for real-time sharing\nTransport of collected data from multiple sensors (e.g. for spatial mapping)\nFormat for storing and sharing spatial information (e.g. indoor spatial data).\nContent Delivery protocols that support XR conversational cases\n5QIs and other 5GS/Radio capabilities that support XR conversational cases\nEdge computing discovery and capability discovery based on work in SA2 and SA6 (see clause 4.3.6)\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.6\tAugmented Reality for New Form Factors",
                    "description": "",
                    "summary": "",
                    "text_content": "Augmented reality was discussed in details in this report. Glass-type AR/MR UEs with standalone capability, i.e., that can be connected directly to 3GPP networks are interesting emerging devices. Such a type is classified as XR5G-A5 in Table 4.3-1. However, it would also be necessary to consider situations where XR5G-A5 UEs have to fall back to XR5G-A1 or XR5G-A2, i.e., to the wired or wirelessly tethered modes, e.g., from NR Uu to 5G sidelink or IEEE 802.11ad/y.  Furthermore, an evolution path for devices under the category XR5G-A3 and XR5G-A4 should be considered. Further studies are encouraged, among others\nBasic use cases: a set of use cases relevant for XR5G-A5 can be selected from Table 5.1-1. The preferred cases will be those capable of delivering experiences previous or existing services could not support, e.g., real-time sharing or streaming 3D objects. They also have to be easier to realize in the environments of glasses that are more limited than those of phones.\nMedia formats and profiles: for the selected use cases, available formats and profiles of the media/data can be discussed. Sharing of XR and 3D Data is of interest for personal and enterprise use cases as documented in scenario 5.2. Properly generated XR data can be used in AR applications on smartphone devices as well as on AR glasses. Exchange formats for AR-based applications are relevant, for example for services such as MMS.\nTransport technologies and protocols: in case the selected use cases include relocation or delivery of 3D or XR media/data over 3GPP networks, combinations of transport protocols, radio access and core network technologies that support the use cases at relevant QoS can be discussed. If existing technologies and protocols cannot serve the use cases properly, such gaps can be taken into account in the consideration of normative works.\nForm factor-related issues: in Table 4.3-1, typical maximum transmit power of XR5G-A5 is 0.5-2W while phone types transmit at 3-5 W. However, if XR5G-A5 is implemented in a form factor of typical glasses, i.e., smaller than goggles or HMDs and with a weight less than 100 g, its cellular modems and antennas are located near the face. In this case, XR5G-A5 UEs can have more constraints on transmit power and it would be necessary to develop solutions to overcome it, e.g. considering situations where XR5G-A5 UEs have to fall back to XR5G-A2 from NR Uu to 5G sidelink or IEEE 802.11ad/y. Furthermore, an evolution path for devices under the category XR5G-A3 (3-7W) and XR5G-A4 (2-4W) should be considered.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.7\tTraffic Characteristics and Models for XR Services",
                    "description": "",
                    "summary": "",
                    "text_content": "As identified in the course of the development of this report, there is significant interest in 3GPP radio and system groups on the traffic characteristics for XR services. This effort should be a prime work for 3GPP to collect realistic traffic characteristics for typical XR services. Of specific interest for other groups in 3GPP is a characterization of traffic of an XR service in the following domains:\n-\tDownlink data rate ranges\n-\tUplink data rate ranges\n-\tMaximum packet delay budget in uplink and downlink\n-\tMaximum Packet Error Rate,\n-\tMaximum Round Trip Time\n- \tTraffic Characteristics on IP level in uplink and downlink in terms of packet sizes, and temporal characteristics.\nSuch characteristics are expected to be available for at least the following applications\n-\tViewport independent 6DoF Streaming\n-\tViewport dependent 6DoF Streaming\n- \tSingle Buffer split rendering for online cloud gaming\n-\tXR conversational services\nThe Technical Report on Typical Traffic Characteristics in TR 26.925 should be updated to address any findings to support the 3GPP groups.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.8\tSocial XR",
                    "description": "",
                    "summary": "",
                    "text_content": "Social XR is used as an umbrella term for combining, delivering, decoding and rendering XR objects (avatars, conversational, sound sources, streaming live content, etc.) originating from different sources into a single user experience. Social XR may be VR centric, but also may apply to AR and MR.\nSocial XR is expected to integrate multiple XR functionalities such a 6DoF streaming with XR conversational services. Some normative work may include:\n-\tSocial XR Components – Merging of avatar and conversational streams to original media (e.g., overlays, etc.)\n-\tParallel decoding of multiple independently generated sources.\n- \tProper annotation and metadata for each object to place it into scene.\n-\tDescription and rendering of multiple objects into a Social XR experience.\nDetails are FFS.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.9\tGeneralized Split and Cloud Rendering and Processing",
                    "description": "",
                    "summary": "",
                    "text_content": "Edge/Cloud processing and rendering is a promising technology to support online gaming in power- and resource constrained devices. Relevant aspects for generalized cloud/split rendering include:\nA generalized XR cloud and split rendering application framework based on a scene description\nSupport for 3D formats in split and cloud rendering approaches\nFormats and protocols for XR Pose information delivery and possibly other metadata in the uplink at sufficiently high frequency\nContent Delivery protocols that support generalized split/cloud rendering\nDistributions of processing resources across different resources in the 5G system network, in the application provider domain (cloud) and the XR device.\nSupporting the establishment of Processing Workflows across distributed resources and managing those\n5QIs and other 5GS/Radio capabilities that support generalized split/cloud rendering by coordination with other groups\nEdge computing discovery and capability discovery based on work in SA2 and SA6 (see clause 4.3.6)\nIt is recommended that this area is studied in more details to identify key issues.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "8\tConclusions and Proposed Next Steps",
            "description": "In this study, frameworks for eXtended Reality (XR) have been analysed. XR referes a larger concept for representing reality that includes the virtual, augmented, and mixed realities. After defining key terms and outlining the QoE/QoS issues of XR-based services, the delivery of XR in the 5G system is discussed, following an architectural model of 5G media streaming defined in TS 26.501. In addition to the conventional service categories, conversational, interactive, streaming, and download, split compute/rendering is identified as a new delivery category. A survey of 3D, XR visual and audio formats was provided.\nUse cases and device types have been classified, and processing and media centric architectures are introduced. This includes viewport independent and dependent streaming, as well as different distributed computing architecture for XR. Core use cases of XR include those unique to AR and MR in addition to those of VR discussed in TR 26.918, ranging from offline sharing of 3D objects, real-time sharing, multimedia streaming, online gaming, mission critical applications, and multi-party call/conferences.\nBased on the details in the report, the following is proposed:\nIn the short-term:\nDevelop a flexible XR centric device reference architecture as well as a collection of device requirements and recommendations for XR device classes based on the considerations in clause 7.2. Device classes should include VR device for 6DoF streaming and XR online gaming (XR5G-V4), as well as AR devices (XR5G-A1, XR5G-A4 and XR5G-A5).\nDevelop a framework and basic functionalities for raster-based Split Rendering for Online Gaming according to the considerations in clause 7.4.\nDocument typical XR traffic characteristics in 3GPP TR 26.925 based on the initial considerations in this report, in particular clause 7.7 and support other 3GPP groups in designing systems for XR services and applications.\nAddress simple extensions to MTSI to support XR conversational services based on the considerations in clause 7.5\nStudy detailed functionalities and requirements for glass-type AR/MR UEs with standalone capability according to clause 7.6 and addresses exchange formats for AR centric media, taking into account different processing capabilities of AR devices.\nIn the mid-term:\nBased on the work developed in the short-term for raster-based split rendering, an extended Split and Cloud Rendering and Processing should be defined based on the considerations in clause 7.9, preferably preceded by a dedicated study\nAddress simple extensions to 5G Media Streaming to support 6DoF Streaming based on considerations in clause 7.3. Stage-2 aspects related to TS26.501 should be considered first before starting detailed stage-3 work.\nBased on the work developed in the shorter time frame above, address the considerations in more detailed considerations inclause 7.5 and clause 7.8 on Social XR\nThe work should be carried out in close coordination with other groups in 3GPP on XR related matter, edge computing and rendering as well in communication with experts in MPEG on the MPEG-I project as well as with Khronos on their work on OpenXR, glTF and Vulkan/OpenGL.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.1\tIntroduction and Template",
            "description": "In order to collect relevant service scenario and core use cases in the context of XR, this Annex documents collected individual use cases and the established processes to collect those use cases.\nThe following procedure was applied for adding to the present document:\n-\tThere is consensus that the use case is understood, relevant and in scope of the Study Item\n-\tA feasibility study is provided and considered sufficient. Some examples on what is expected on feasibility is provided below.\n-\tHow could the use case be implemented based on technologies available today or expected to be available in a foreseeable timeline, at most within 3 years?\n-\tWhat are the technology challenges to make this use case happen?\n-\tDo you have any implementation information?\n-\tDemos\n-\tProof of concept\n-\tExisting services\n-\tReferences\n-\tCould a reduced experience of the use case be implemented in an earlier timeframe or is it even available today?\n-\tBeyond use case description and feasibility, the template includes sufficient information on\n-\tCategorization: Type, Degrees of Freedom, Delivery Type, Device\n-\tPreconditions: What is necessary to make this work?\n-\tQoS Considerations: What network capabilities are needed, e.g. bitrate, latency, etc.?\n-\tQoE Considerations: What is expected that the user is satisfied with the quality?\n-\tPotential Standardisation Status and Needs: This may include 3GPP relevant standards or external standards\nFor use cases that are moved to the present document, in the course of the study item, is expected that the following aspects are addressed:\n1)\tThe use case is mapped to one or multiple architectures.\n2)\tFor each use case the functions and interfaces are defined, and the requirements are developed to address the use case.\n3)\tSpecific requirements include\na)\tArchitectural requirements\nb)\tNetwork and QoS requirements\nc)\tMedia Processing requirements\nd)\tMore detailed QoE requirements\nThe template provided in Table A.1-1 is recommended to be used for this collection.\nTable A.1-1 Proposed Use Case Collection Template\n\nTable A.1-2 provides an overview of the use cases and their characterization.\nTable A.1-2: Overview of Use cases\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "Table A.1-1 Proposed Use Case Collection Template",
                    "table number": 13,
                    "summary": "",
                    "name": ""
                },
                {
                    "description": "Table A.1-2: Overview of Use cases",
                    "table number": 14,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.2\tUse Case 1: 3D Image Messaging",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 15,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.3\tUse Case 2: AR Sharing",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 16,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.4\tUse Case 3: Streaming of Immersive 6DoF",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 17,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.5\tUse Case 4: Emotional Streaming",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 18,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.6\tUse Case 5: Untethered Immersive Online Gaming",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 19,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.7\tUse Case 6: Immersive Game Spectator Mode",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 20,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.8\tUse Case 7: Real-time 3D Communication",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 21,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.9\tUse Case 8: AR guided assistant at remote location (industrial services)",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 22,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.10\tUse Case 9: Police Critical Mission with AR",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 23,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.11\tUse Case 10: Online shopping from a catalogue – downloading",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 24,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.12\tUse Case 11: Real-time communication with the shop assistant",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 25,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.13\tUse Case 12: 360-degree conference meeting",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 26,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.14\tUse Case 13: 3D shared experience",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 27,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.15\tUse Case 14:\t6DOF VR conferencing",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 28,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.16\tUse Case 15: XR Meeting",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 29,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.17\tUse Case 16: Convention / Poster Session",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 30,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.18\tUse Case 17:\tAR animated avatar calls",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 31,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.19\tUse Case 18: AR avatar multi-party calls",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 32,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.20\tUse Case 19: Front-facing camera video multi-party calls",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 33,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.21\tUse Case 20: AR Streaming with Localization Registry",
            "description": "\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 34,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.22\tUse Case 21: Immersive 6DoF Streaming with Social Interaction",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 35,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.23\tUse Case 22: 5G Online Gaming party",
            "description": "\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 36,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.24\tUse Case 23: 5G Shared Spatial Data",
            "description": "\n\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 37,
                    "summary": "",
                    "name": ""
                },
                {
                    "description": "",
                    "table number": 38,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        }
    ]
}