{
    "document_name": "26926-i10.docx",
    "content": [
        {
            "title": "Foreword",
            "description": "This Technical Report has been produced by the 3rd Generation Partnership Project (3GPP).\nThe contents of the present document are subject to continuing work within the TSG and may change following formal TSG approval. Should the TSG modify the contents of the present document, it will be re-released by the TSG with an identifying change of release date and an increase in version number as follows:\nVersion x.y.z\nwhere:\nx\tthe first digit:\n1\tpresented to TSG for information;\n2\tpresented to TSG for approval;\n3\tor greater indicates TSG approved document under change control.\ny\tthe second digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, etc.\nz\tthe third digit is incremented when editorial only changes have been incorporated in the document.\nIn the present document, modal verbs have the following meanings:\nshall\t\tindicates a mandatory requirement to do something\nshall not\tindicates an interdiction (prohibition) to do something\nThe constructions \"shall\" and \"shall not\" are confined to the context of normative provisions, and do not appear in Technical Reports.\nThe constructions \"must\" and \"must not\" are not used as substitutes for \"shall\" and \"shall not\". Their use is avoided insofar as possible, and they are not used in a normative context except in a direct citation from an external, referenced, non-3GPP document, or so as to maintain continuity of style when extending or modifying the provisions of such a referenced document.\nshould\t\tindicates a recommendation to do something\nshould not\tindicates a recommendation not to do something\nmay\t\tindicates permission to do something\nneed not\tindicates permission not to do something\nThe construction \"may not\" is ambiguous and is not used in normative elements. The unambiguous constructions \"might not\" or \"shall not\" are used instead, depending upon the meaning intended.\ncan\t\tindicates that something is possible\ncannot\t\tindicates that something is impossible\nThe constructions \"can\" and \"cannot\" are not substitutes for \"may\" and \"need not\".\nwill\t\tindicates that something is certain or expected to happen as a result of action taken by an agency the behaviour of which is outside the scope of the present document\nwill not\t\tindicates that something is certain or expected not to happen as a result of action taken by an agency the behaviour of which is outside the scope of the present document\nmight\tindicates a likelihood that something will happen as a result of action taken by some agency the behaviour of which is outside the scope of the present document\nmight not\tindicates a likelihood that something will not happen as a result of action taken by some agency the behaviour of which is outside the scope of the present document\nIn addition:\nis\t(or any other verb in the indicative mood) indicates a statement of fact\nis not\t(or any other negative verb in the indicative mood) indicates a statement of fact\nThe constructions \"is\" and \"is not\" do not indicate requirements.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "Introduction",
            "description": "Media services in general, but also in particular eXtended Reality (XR) and Cloud Gaming are some of the most important 5G media applications under consideration in the industry. XR is an umbrella term for different types of realities and refers to all real-and-virtual combined environments and human-machine interactions generated by computer technology and wearables. It includes representative forms such as Augmented Reality (AR), Mixed Reality (MR) and Virtual Reality (VR) and the areas interpolated among them.\nOn XR and Cloud Gaming traffic with high throughput, low latency and high reliability requirements, it is important to consider system aspects of such services. If the traffic requirements of the XR and Cloud Gaming service are flexible (e.g., the underlying architecture allows adaptation of content), then the capacity of the service can be studied by assessing the delay, throughput and reliability variations with increasing number of users in the system. In order to properly study this, detailed traffic characteristics are necessary.\nBased on this, the present document provides traffic models and quality evaluation methods for different media and eXtended Reality (XR) Services. In order to address this, generic modelling considerations are introduced and for different services reference designs, simulation models and suitable quality metrics are reported. This information permits to obtain accurate information on exact bitrate and delay requirements in uplink and downlink, develop detailed traffic traces, develop suitable statistical models for media and XR traffic and to evaluate the expected media quality of such services. The information may be used by other 3GPP groups in order to assess media quality for different configuration of 5G System parameters, as well as for evaluating the requirements in terms of QoS for XR and media services.\n\n\n\n\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "1\tScope",
            "description": "The present document provides traffic models and quality evaluation methods for different media and eXtended Reality (XR) Services. In order to address this, generic modelling considerations are introduced and for different services reference designs, simulation models and suitable quality metrics are reported. This information permits to obtain accurate information on exact bitrate and delay requirements in uplink and downlink, develop detailed traffic traces, develop suitable statistical models for media and XR traffic and to evaluate the expected media quality of such services. The information may be used by other 3GPP groups in order to assess media quality for different configuration of 5G System parameters, as well as for evaluating the requirements in terms of QoS for XR and media services.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "2\tReferences",
            "description": "The following documents contain provisions which, through reference in this text, constitute provisions of the present document.\n-\tReferences are either specific (identified by date of publication, edition number, version number, etc.) or non-specific.\n-\tFor a specific reference, subsequent revisions do not apply.\n-\tFor a non-specific reference, the latest version applies. In the case of a reference to a 3GPP document (including a GSM document), a non-specific reference implicitly refers to the latest version of that document in the same Release as the present document.\n[1]\t3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".\n[2]\t3GPP TR 26.925, \"Typical traffic characteristics of media services on 3GPP networks\"\n[3]\t3GPP TR 26.928, \"Extended Reality (XR) in 5G\"\n[4]\t3GPP TR 38.838, \"Study on XR (Extended Reality) Evaluations for NR\"\n[5]\t3GPP TS 26.501, \"System architecture for the 5G System (5GS)\"\n[6]\t3GPP TS 26.118, \"Virtual Reality (VR) profiles for streaming applications\"\n[7]\tVR-IF Guidelines, https://www.vr-if.org/wp-content/uploads/vrif2020.180.00-Guidelines-2.3_clean.pdf\n[8]\tIETF RFC 5052, Forward Error Correction (FEC) Building Block\n[9]\tIETF RFC 6330, RaptorQ Forward Error Correction Scheme for Object Delivery\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "3\tDefinitions of terms, symbols and abbreviations",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "3.1\tTerms",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the terms given in 3GPP TR 21.905 [1] and the following apply. A term defined in the present document takes precedence over the definition of the same term, if any, in 3GPP TR 21.905 [1].\ntrace: a well-defined format to describe a sequence of timed data units together with relevant metadata for system simulation.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.2\tSymbols",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the following symbols apply:\nvoid\n\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.3\tAbbreviations",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the abbreviations given in 3GPP TR 21.905 [1] and the following apply. An abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in 3GPP TR 21.905 [1].\nAAC\tAdvanced Audio Coding\nACK\tACKnowledgment message\nADU\tApplication Data Unit\nALR\tArea Loss Rate\nAMR\tAdaptive Multi Rate\nAPI\tApplication Programming Interface\nATW\tAsynchronous Time Warping\nAVC\tAdvanced Video Coding\nCAR\tCorrect Area Rate\nCAT\tCATegory\nCBR\tConstant BitRate\nCDF\tCumulative Distribution Function\nCDN\tContent Delivery Network\nCGI\tComputer Graphics and Images\nCQP\tContant QP\nCRF\tConstant Rate Factor\nCSV\tComma Separated Version\nCTU\tCoding Tree Unit\nCU\tCoding Unit\nDAR\tDAmage Rate\nDASH\tDynamic Adaptive Streaming over HTTP\nDRX\tDiscontinous Receive\nERP\tEqui-Rectangular Projection\nEVS\tEnhanced Voice Service\nFEC\tForward Error Correction\nFFS\tFor Further Study\nFOV\tFielf-of-View\nFPS\tFirst Person Shooter\nGBR\tGuaranteed Bit Rate\nGDR\tGradual Decoder Refresh\nGFBR\tGuaranteed Flow Bit Rate\nHARQ\tHybrid Automatic Repeat equest\nHDR\tHigh Dynamic Range\nHEVC\tHigh Efficiency Video Coding\nHLS\tHTTP Live Streaming\nHTTP\tHyperText Transfer Protocol\nIAT\tInter arrival times\nIBC\tInternational Broadcasting Convention\nIDR\tInstantenous Decoder Refresh\nIVAS\tImmersive Voice and Audio Service\nLDR\tLoss and Damage Rate\nMDBV\tMaximum Data Burst Volume\nMFBR\tMaximum Flow BitRate\nMMO\tMassive Multiplayer Online\nMOS \tMean Opinion Score\nMPEG\tMoving Pictures Expert Group\nMTHR\tMedium-to-High Ratio\nMTU\tMaximum Transfer Unit\nNACK\tNegation ACKnowledgement message\nOMAF\tOmnidirectional MediA Format\nPDB\tPacket Delay Budget\nPDCP\tPacket Data Convergence Protocol\nPDU\tPacket Data Unit\nPER\tPacket Error Rate\nPLR\tPacket Loss Rate\nPOC\tPicture Order Count\nPSNR\tPeak Signal to Noise Ratio\nQFI\tQoS Flow Indicator\nQP\tQuantization Parameter\nRAN\tRadio Access Network\nRLC\tRadio Link Control\nRPG\tRole-Player Game\nRPSNR\tRecovered PSNR\nRTCP\tReal-Time Control Protocol\nRTP\tReal-Time Protocol\nRTS\tReal-Time Strategy\nRTT\tRound Trip Time\nSLR\tSlice Loss Rate\nSSIM\tStructural Similarity Index Measure\nSTD\tStandard Deviation\nTCP\tTransmission Control Protocol\nUDP\tUser Datagram Protocol\nUPF\tUser Plane Function\nVBR\tVariable BitRate\nVDP\tViewport DePendent\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "4\tOverview and Scope",
            "description": "In an initial version of TR26.925, Typical Traffic Characteristics for Operator and Third-Party Services have been collected. The work was initiated based on communication between SA4 and SA1. During the course of the work, additional requests from SA1 were received that have been partially addressed in the initial version of TR26.925.\n3GPP TSG SA WG4 (Codec) addressed their mandate on \"Guidance to other 3GPP groups concerning required QoS parameters and other system implications, including channel coding requirements, imposed by different multimedia codecs in both circuit-switched and packet-switched environments.\"\nFurthermore, during the study for eXtended Reality (XR) over 5G (FS_5GXR) documented in TR26.928, several initial considerations for XR services including cloud gaming had been collected. Specifically, parameters such as downlink and uplink bitrates, packet delay budgets, error rates and round-trip times are collected, but several of those for different cases are kept FFS and need more work.\nIn particular, eXtended Reality (XR) and Cloud Gaming are some of the most important 5G media applications under consideration in the industry. XR is an umbrella term for different types of realities and refers to all real-and-virtual combined environments and human-machine interactions generated by computer technology and wearables. It includes representative forms such as Augmented Reality (AR), Mixed Reality (MR) and Virtual Reality (VR) and the areas interpolated among them.\nThis Technical Report attempts to provide the following information:\n-\tCollect and document traffic characteristics including for different services, but not limited to\n-\tDownlink data rate ranges\n-\tUplink data rate ranges\n-\tMaximum packet delay budget in uplink and downlink\n-\tMaximum Packet Error Rate,\n-\tMaximum Round Trip Time\n-\tTraffic Characteristics on IP level in uplink and downlink in terms of packet sizes, and temporal characteristics. XR Services and Cloud Gaming based on the initial information documented in TR26.928 including.\n-\tCollect additional information, such as codecs and protocols in use.\n-\tProvide the information from above at least for the following services (initial services)\n-\tViewport independent 6DoF Streaming\n-\tViewport dependent 6DoF Streaming\n-\tSimple Single Buffer split rendering for online cloud gaming\n-\tCloud gaming\n-\tMTSI-based XR conversational services\n-\tIdentify additional relevant XR and other media services and document their traffic characteristics\n-\tDocument additional developments in the industry that impact traffic characteristics in future networks\n-\tIdentify the applicability of existing 5QIs/PQIs for such services and potentially identify requirements for new 5QIs/PQIs or QoS related parameters.\nThe work is expected to be carried out with other 3GPP groups and external organizations on relevant aspects related to the scope of the work.\nIn addition, extensions to TR 26.925 are developed to document traffic characteristics for XR applications.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "5\tGeneral Design and Modelling",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "5.1\tIntroduction",
                    "description": "",
                    "summary": "",
                    "text_content": "The system and RAN model used in this Technical Report for XR Traffic analysis follows the 5G System architecture as defined in TS 23.501 [5.1i]. In particular, the user plane aspects are considered as showing in Figure 5.1-1 for which an XR Server is in the external DN or in a trusted DN, and the XR Device is a 5G UE. The exchange of data is assumed to be carried out using the 5G System. Interfaces to the 5G Core functions for charging, QoS control, etc. are not considered in the below diagram, for details refer to TS 23.501.\nThe figure depicts a simplified architecture for XR end-to-end services, illustrating the various components and their interactions. It includes a base station (BS), a user equipment (UE), a network control center (NCC), and a network edge device (NED). The figure highlights the importance of network slicing, network slicing, and network slicing in providing flexible and efficient XR services.\nFigure 5.1-1 Simplified Architecture for XR end-to-end services\nIn order to identify the traffic characteristics and the impacts of media and XR applications on 5G Systems and radio access networks, an appropriate modelling of both, the XR traffic as well as of the 5G System is desirable, also to identify the interaction of the two components. Note that the XR Server may be hosted in the cloud or may be hosted on an edge.\nDetails on the modelling of the 5G System and RAN as well as simulation parameters are provided in clause 5.2.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "5.2\tEnd-to-end Modelling of XR Traffic",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.2.1\tArchitecture and System Model",
                            "text_content": "In order to support modelling of end-to-end XR Traffic, an abstracted architecture and system model is introduced in Figure 5.2.1-1. This system is used as a baseline and may be refined for specific traffic classes. The system follows the basic building blocks from clause 5.1 but provides details for each of those aspects. In order to support simpler system modelling and simulation, the interfaces are supported by traces. Traces define a well-defined format to describe a sequence of timed data units together with relevant metadata for system simulation. An overview of traces is introduced in clause 5.2.2, details are provided in each of the functions.\nIn order to split the tasks across different 3GPP working groups and companies, an approach as introduced in Figure 5.2.1-1 is considered:\n-\tA content model based on traces may be provided based on company input providing sufficient information on a rendered application, e.g. a game or a scene.\n-\tBased on these content model traces, codec centric experts defines a content encoding and delivery model taking into account a system design, for example based on TR26.928 [3]. The system model includes encoding, delivery, decoding and also a quality definition, providing packet traces.\n-\t5G System and RAN simulations may be carried out by the 3GPP radio experts using the packet traces to simulate different traffic characteristics and evaluate different performance options.\nAn overview of the functions is provided below:\n1)\tContent Model: Provides typical characteristics for the XR content model for video, audio and potentially other data. This content is rendered for XR/CG consumption – for details refer to clause 5.5.\n2)\tContent Encoding Model: Provides the details for the content encoding in order to meet certain objectives. This includes the generation of sequences of application data units (slices, video frames, audio frames, etc.) and the incurred timestamp of each of the units is available. For details refer to clause 5.6.\n3)\tContent Delivery Model: provides details on content delivery, for example packetization, delay jitter, but possibly also more sophisticated models such as retransmission, TCP operations and so on. This also includes emulation of 5G Core Network. It produces packet traces (timestamp and size of each packet) based on traces of application data units. For details refer to clause 5.7.\n4)\tCore Network Model: This model emulates the core network behaviour in terms of delay, latency and packet losses in the core network, i.e. the interface between the UPF and gNB. For details refer to clause 5.4.\n5)\tPacket Radio Model: The packet radio model receives sequences/traces of packets at a given time and of a specified size. The packets may have additional metadata assigned that can potentially be used by the radio simulator. The packet traces are provided for multiple users and reflect typical traffic characteristics. The RAN simulator provides packet traces after delivery that reflect the occurred delays and losses for each user. For details refer to clause 5.4.\n6)\tContent Receiver Model: The content receiver converts the packet traces into application units taking into account delays and losses occurred. It may also model additional functions such as retransmissions or FEC, if applicable. For details refer to clause 5.7.\n7)\tContent Decoding Model: The decoding model uses the received application units to model the reconstruction of timed data (video frames, audio) considering delays, losses and also content model properties (error propagation, refresh data and so on). For details refer to clause 5.6.\n8)\tQuality Evaluation: A quality evaluation tool is provided that takes into account traces to compute different quality metrics based on packet, application unit and media quality. For details refer to clause 5.8.\n9)\tUplink Model: A model for the uplink traffic in a similar fashion also providing packet traces. For details refer to clause 5.9.\nSeveral of the functions are initialized by appropriate configurations. An overview of configurations is introduced in each of the functions, if needed.\nFigure 5.2.1-1 General architecture and system model for XR Traffic\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.2.2\tInterfaces and Traces",
                            "text_content": "Generally, traces include the following information:\n-\tTime series of data units\n-\tSize of each data units\n-\tAssociation to the application\n-\tAdditional \"metadata\"\nTraces can also directly be fed to a quality evaluation tool obtain the application quality based on the simulation as shown in clause 5.8. The benefit of traces compared to first order statistical model are as follows:\n-\ttraces to represent time correlations.\n-\ttraces can be used to assign additional metadata in the time series\n-\tstatistical models can be developed based on time series\n-\ttraces can be directly fed to quality evaluation tool\nThe trace formats are defined as CSV files and follow the recommendation in  \"Common Format and MIME Type for CSV Files\". For CSV files, headers are added to identify the data types. From existing CSV types, the online tool  has been used to\n1)\tValidate the csv file\n2)\tCreate a schema for the files.\nFigure 5.2.1-1 shows different traces, associated to different interfaces. The following traces are defined:\n1)\tV-Traces: Video traces provide a time series of the statistics/complexity of a video frame. Such a V-Trace is expected to provide sufficient information such that a coding model can create suitable statistics statics for encoded video streams at the output. For details refer to clause 5.5.\n2)\tS-Traces:  Slice traces provide a time series of encoded video application data units, referred to as slices as known from H.264/AVC and H.265/HEVC suitable application data units as outputs from a video codecs. S-Traces are expected to provide sufficient information to the content delivery module to create IP packets. For details refer to clause 5.6.\n3)\tP-Traces: P Traces provide a time series of IP packets, possibly associated with different application flows and/or QoS Flows. In addition, they provide metadata information that may be beneficial for advanced delivery. For details refer to clause 5.3.\n4)\tP’-Traces have the same format as P-Traces, but in addition may include losses and delays observed due to the delivery over a network.\n5)\tS’-Traces are have the same format as S-Traces, but in addition may include information on losses and distortions due to the delivery over a network. For details refer to clause 5.7.\n6)\tV’-Traces: Video receive traces provide a time series of received video frames together with an associated encoding and delivery quality. For details refer to clause 5.6.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.2.3\tBenefits and Limitations",
                            "text_content": "This aspect is for further study.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "5.3\tP-Traces",
                    "description": "",
                    "summary": "",
                    "text_content": "Packet Traces (P-Traces) are defined to emulate the arrival of IP packets with associated metadata at certain network nodes. A P-Trace format together with semantics is defined in Table 5.3-1.\nThe first row just keeps an index of the packets.\nThe second to fourth data row information is considered to be available in a general delivery environment and are summarized as baseline parameters.\n-\tavailability_time: A relative time from the start of the trace when this packet is available in the delivery function\n-\tsize: the size of the packet in octets\n-\tflow_id: an id to a flow in order to differentiate different QoS flows to identify a PDU session aligned with the QFI.\nTable 5.3-1 P-Trace format\n\nIn addition, it is considered that an application may provide additional metadata per packet that if present, may be used by the delivery system. Different definitions are provided in the P-Trace format. The signaling of this data a 5G System is for further study. This information would not be available to RAN delivery in the incoming IP packets. These parameters are summarized as cross-layer parameters.\n5GS through the QoS model may provide the ability that application streams are separated into different streams or the streams are at least marked accordingly. Details would need further discussion with SA2 on how application traffic and packet properties or stream properties can be matched to the 5G System architecture. This aspect may be considered, once it is identified that certain “cross-layer” markings make sense.\nBased on this, it is suitable to use the basic packet trace structure as defined in Table 5.3-1 and that RAN1 can assume that RAN1 may have access to the above packet information. The P-Trace may be extended based on new findings.\nFor any system simulations, it is important that simulation are carried out only using the baseline parameters. Additional cross-layer parameters may be used as defined above. New packet-based markers may be added.\nIt is also considered that realization in the 5G System is only addressed once the benefits of an approach are identified.\n",
                    "tables": [
                        {
                            "description": "Table 5.3-1 P-Trace format",
                            "table number": 5,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "5.4\t5G System Model and Simulation",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.4.1\t5G System Model",
                            "text_content": "In order to further model the 5G system, an alignment with the 5GS QoS model as defined in clause 5.7 of TS 23.501 is considered appropriate. The interface between the application domain and the 5G System is assumed to be based on QoS Flows. A QoS Flow is the finest granularity of QoS differentiation in the PDU Session. A QoS Flow ID (QFI) is used to identify a QoS Flow in the 5G System. User Plane traffic with the same QFI within a PDU Session receives the same traffic forwarding treatment (e.g. scheduling, admission threshold).\nThe QFI is carried in an encapsulation header on N3 without any changes to the e2e packet header and is used to uniquely identify a PDU Session. The principle for classification and marking of User Plane traffic and mapping of QoS Flows to AN resources is illustrated in Figure 5.7.1.5-1 of TS 23.501 and repeated in Figure 5.1-2.\nThe figure depicts a flow chart illustrating the classification and User Plane marking process for QoS Flows and mapping them to AN Resources. It includes key steps such as defining the QoS Flow, mapping it to AN Resources, and ensuring that the mapping is accurate and consistent. The figure is crucial for understanding the process and ensuring that QoS Flows are correctly classified and mapped to appropriate AN Resources.\nFigure 5.2.1-1: The principle for classification and User Plane marking for QoS Flows and mapping to AN Resources (see TS 23.501 [5.1i], Figure 5.7.1.5-1)\nIn the downlink, incoming data packets are classified by the UPF based on the Packet Filter Sets of the DL PDRs in the order of their precedence. The UPF conveys the classification of the User Plane traffic belonging to a QoS Flow through an N3 User Plane marking using a QFI. The AN binds QoS Flows to AN resources (i.e. Data Radio Bearers of in the case of 3GPP RAN).\nIn UL, for relevant PDU Session of Type IP, the UE evaluates UL packets against the UL Packet Filters in the Packet Filter Set in the QoS rules based on the precedence value of QoS rules in increasing order until a matching QoS rule is found. The UE uses the QFI in the corresponding matching QoS rule to bind the UL packet to a QoS Flow. The UE then binds QoS Flows to AN resources.\nTS 23.501 defines the following QoS characteristics:\n-\tClause 5.7.3.2: Resource type (Non-GBR, GBR, Delay-critical GBR)\n-\tA GBR QoS Flow uses either the GBR resource type or the Delay-critical GBR resource type. The definition of PDB and PER are different for GBR and Delay-critical GBR resource types, and the MDBV parameter applies only to the Delay-critical GBR resource type.\n-\tA Non-GBR QoS Flow uses only the Non-GBR resource type.\n-\tClause 5.7.3.3: Priority Level;\n-\tThe Priority Level associated with 5G QoS characteristics indicates a priority in scheduling resources among QoS Flows. The lowest Priority Level value corresponds to the highest priority.\n-\tClause 5.7.3.4: Packet Delay Budget (including Core Network Packet Delay Budget);\n-\tThe Packet Delay Budget (PDB) defines an upper bound for the time that a packet may be delayed between the UE and the N6 termination point at the UPF. For a certain 5QI the value of the PDB is the same in UL and DL. In the case of 3GPP access, the PDB is used to support the configuration of scheduling and link layer functions (e.g., the setting of scheduling priority weights and HARQ target operating points).\n-\tFor GBR QoS Flows using the Delay-critical resource type, a packet delayed more than PDB is counted as lost if the data burst is not exceeding the MDBV within the period of PDB and the QoS Flow is not exceeding the GFBR. For GBR QoS Flows with GBR resource type not exceeding GFBR, 98 percent of the packets shall not experience a delay exceeding the 5QI's PDB.\n-\tClause 5.7.3.5: Packet Error Rate;\n-\tThe Packet Error Rate (PER) defines an upper bound for the rate of PDUs (e.g. IP packets) that have been processed by the sender of a link layer protocol (e.g. RLC in RAN of a 3GPP access) but that are not successfully delivered by the corresponding receiver to the upper layer (e.g. PDCP in RAN of a 3GPP access).\n-\tThus, the PER defines an upper bound for a rate of non-congestion related packet losses. The purpose of the PER is to allow for appropriate link layer protocol configurations (e.g. RLC and HARQ in RAN of a 3GPP access). For every 5QI the value of the PER is the same in UL and DL. For GBR QoS Flows with Delay-critical GBR resource type, a packet which is delayed more than PDB is counted as lost, and included in the PER unless the data burst is exceeding the MDBV within the period of PDB or the QoS Flow is exceeding the GFBR.\n-\tClause 5.7.3.6: Averaging window (for GBR and Delay-critical GBR resource type only);\n- \tEach GBR QoS Flow shall be associated with an Averaging window. The Averaging window represents the duration over which the GFBR and MFBR shall be calculated (e.g. in the (R)AN, UPF, UE).\n-\tClause 5.7.3.7: Maximum Data Burst Volume (for Delay-critical GBR resource type only).\n-\tEach GBR QoS Flow with Delay-critical resource type shall be associated with a Maximum Data Burst Volume (MDBV).\n-\tMDBV denotes the largest amount of data that the 5G-AN is required to serve within a period of 5G-AN PDB.\n-\tEvery standardized 5QI (of Delay-critical GBR resource type) is associated with a default value for the MDBV (specified in QoS characteristics Table 5.7.4.1). The MDBV may also be signalled together with a standardized 5QI to the (R)AN, and if it is received, it shall be used instead of the default value.\n-\tThe MDBV may also be signalled together with a pre-configured 5QI to the (R)AN, and if it is received, it shall be used instead of the pre-configured value\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.4.2\tCore Network Model",
                            "text_content": "The core network is expected to operate on constant bitrate channel from the UPF to the gNB.\nIn the model showing in Figure 5.4.2-1, the core network model gets as input a P-Trace and delivers a P-Trace. Packets provided to the core network are expected to be delivered from the UPF to the gNB at a constant bitrate of bitrate R, whereby each packet is delivered at a delivery time being the later of the two\n1)\tThe arrival time at the UPF\n2)\tThe delivery end time of the previous packet\nThe arrival time at the gNB is modelled as the sum of the delivery time and the packet size divided by the bitrate R.\nThe core network delivery is considered to be error-free.\nThe figure depicts a core network model with various components such as core switches, optical line terminals (OLTs), and distributed nodes. The layered design aligns with SDN principles, allowing for efficient network management and failover.\nFigure 5.4.2-1 Core Network Model\nThe modelling of such a delay is for example show in Figure 5.4.2-2 for different packet size models.\nThe figure depicts a core network model with packet size and latency additions, illustrating the impact of these factors on network performance.The figure depicts a core network model with packet size and latency additions, illustrating the impact of these factors on network performance.\nFigure 5.4.2-2 Packet size and latency addition in core network model\nThe configuration parameters for the core network model are\n-\tthe input packet trace\n-\tthe core network bitrate\n- \tthe output packet trace\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.4.3\tRAN Model",
                            "text_content": "The RAN delivery may be modelled by a function that receives P-Traces as input for each simulated user as shown in Figure 5.4.3-1 and produces a P’-Trace at the output for the primary user. The other users are only used from statistical competing traffic. Configuration parameters for the simulation are left to RAN experts.\nThe 3-1  Packet Radio Network Model illustrates the communication architecture of a 3GPP-compliant network, with a focus on packet radio technology. It demonstrates the use of radio frequency (RF) signals to transmit data, with a focus on the 3GPP 3-1 standard. The model includes various components such as base stations (BSs), mobile stations (MSs), and radio access network (RAN) elements. The diagram highlights the use of packet radio technology, which allows for efficient data transmission and efficient use of spectrum.\nFigure 5.4.3-1 Packet Radio Network Model\nHowever, the usage of traces for RAN simulations is complex and there is a preference to operate with statistical models that emulate the arrival time and size of the packets. The main reason is for example, that RAN1 needs to simulate multiple cells at the same time, for example 63, for interference considerations. A statistical model is preferred.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.4.4\tTest Channels",
                            "text_content": "P-Traces as produced by XR applications may be used by RAN for complex RAN simulations. However, at the same time it is important to evaluate different XR application and encoding configurations that result in a P-Trace and then compare different settings. For this purpose, test channels are defined addressing two aspects:\n-\tPermit to emulate typical radio conditions in terms delays and losses.\n-\tPermit to evaluate the application quality for different representative radio conditions.\nThe test channel aligns with the 5G System and QoS Model as introduced in clause 5.4.1.\nA test channel is defined by the following models:\n-\tPacket Error Rate:\n-\tDefinition: the rate of PDUs (e.g. IP packets) that have been processed by the sender of a link layer protocol (e.g. RLC in RAN of a 3GPP access) but that are not successfully delivered by the corresponding receiver to the upper layer (e.g. PDCP in RAN of a 3GPP access).\n-\tModel: iid loss model independent of the packet size with parameter PLR\n-\tPacket Delay:\n-\tDefinition: the time that a packet may be delayed between the UE and the N6 termination point at the UPF.\n-\tModel: iid distributed latency between 0 and a max_delay\nOther parameters and modelling aspects for test channels are for further study.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "5.5\tContent Modelling",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.5.1\tIntroduction",
                            "text_content": "In order to evaluate the traffic characteristics and the quality of XR applications, it is relevant to operate with realistic source data. Preferably, the traffic of existing services is evaluated, for example taking P-Traces from existing XR services. However, in doing so, there are limitations\n1)\tin accessing the data\n2)\tin terms of legal restrictions on providing such data\n3)\tin terms of making use of such data for quality evaluation\n4)\tin terms of understanding the structure and details of the contained traffic\n5)\tin terms of understanding the options and impact of different system configurations\n6)\tand for other reasons\nBased on this, an approach is proposed in this report as shown in Figure 5.5.1-1.\nThe figure depicts a content modelling approach, illustrating the various stages involved in the content modelling process. It highlights the importance of content-based filtering, which is crucial for content-based filtering, and the use of content-based filtering in the context of content-based filtering.\nFigure 5.5.1-1 Content Modelling Approach\nIt is assumed that the raw media data is generated based on an XR/game session for one or several popular games, using typical pose traces and interactions, for example by a human playing the game. This information may be running over several minutes or even hours in order to create sufficiently representative statistics.\nThe output of the XR rendering engine is video representing texture, primitive and eye buffers.\nFor service and applications not relying on XR engines, other approaches may be taken. This is for further study and discussed as part of the specific traffic characteristics.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.5.2\tV-Trace Generation",
                            "text_content": "The basic idea behind the generation of V-Traces is motivated by two main aspects:\n1) \tHandling of several minutes or hours of raw data from a game engine is too significant. A statistical version is preferred in order to properly handle the amount of data\n2)\tRepresentative games are typically attached with copyright and licensing terms and hence, the raw media data cannot be shared publicly.\nBased on this V-Trace generation has been initiated by\n-\tusing a representative game\n-\tusing a repeatable pose and interaction trace for the game\n-\tusing a high-quality output of a game engine for each eye buffer, encoded for example in H.264(AVC)\n-\tdecode these video sequences and store this raw data for proper model encoding to generate a trace. As an example, the decoded video sequence is 2K x 2K at 120 fps for each source buffer.\nA shorter version of the raw data is then encoded in with different parameters to create some statistical output that allows to estimate the performance of a video codec when used with different configurations. For this purpose, initially a set of encoding parameters are used to understand the impact of each of the parameters for the resulting bitrate and quality. This information is then used for the modelling.\nBased on the above model findings, it considered to only generate two V-Traces for the entire sequence to keep data handling manageable as shown in Figure 5.5.2-1. One V-Trace is generated for Intra coding only, the other one is generated for predictive coding.\nThe V-Traces are combined to document relevant statistics for each frame.\n\nThe figure depicts a V-Trace generation process, illustrating the steps involved in generating a V-Trace for a specific scenario. The V-Trace is a crucial tool in network troubleshooting, allowing network engineers to identify and resolve issues more efficiently. The figure includes various components such as the V-Trace generator, the V-Trace table, and the V-Trace output. The V-Trace table is a structured representation of the V-Trace, allowing engineers to quickly access and analyze the data. The V-Trace generator is responsible for generating the V-Trace, which is a sequence of V-Trace values. The V-Trace output is a visual representation of the V-Trace, providing a clear and concise overview of the generated V-Trace.\nFigure 5.5.2-1 V-Trace Generation\nFor the purpose of this TR, a modelling is provided Annex A.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.5.3\tV-Trace Format",
                            "text_content": "For each frame in the video sequence, the information as documented in Table 5.5.3-1 is provided.\nTable 5.5.3-1 V-Trace Format\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.5.3-1 V-Trace Format",
                                    "table number": 6,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "5.5.4\tOther media types",
                            "text_content": "For other media types, no dedicated trace format is defined.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "5.6\tMedia Coding and Decoding Modelling",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.6.1 \tGeneral",
                            "text_content": "Media may be encoded in many different ways in order to meet application, service and delivery requirements. Whereas decoders in media coding are typically fully specified, there typically exist significant options in the encoding, in particular for video, but also in general, apply different encoding methods and functionalities.\nThese functionalities may impact:\n1)\tBitrate of the media stream, both in terms of short-term (for example over a window of 1 second) as well as average bitrate of the media stream. The bitrate may also be dynamically adjusted, for example based on content properties and/or network conditions. Changes in bitrate impact the quality of the media stream.\n2)\tDelays in encoder or decoder due to processing or algorithmic functionalities. For example the time when captures/generated media sample hits encoder and the time when it leaves the decoder may be configurable. Permitting longer delays typically results in better quality.\n3)\tOutput data unit properties determining the size of the generated data units and packets. Adding constraints typically results in additional bitrates\n4)\tError resilience and random access includes functionalities in the media stream that support mitigation of potential losses of data. Adding such functionalities typically increases the bitrate or reduces the quality.\nMore details for video media type are provided in clause 5.6.2. A configuration overview for video is provided in clause 5.6.3. Trace formats are defined in clause 5.6.4.\nOther media types are defined in clause 5.6.5.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.6.2\tVideo Coding and Decoding",
                            "text_content": "Figure 5.6.2-1 provides an overview of content encoding and decoding models with focus on video. Encoding makes use of V-Traces and based on configurations and possibly feedback from the decoder, encoding of individual media samples/frames is carried out. The encoder produces a sequence of slices according to the format defined in clause 5.6.3. After delivery, a content decoding model generates a sequence of V’-Traces that can be used by a Quality evaluation tool. The decoding model may also provide feedback to the encoder, for example on lost or late frames.\nThe figure depicts a detailed model of media coding and decoding in a 5G network, illustrating the various steps involved in the transmission and reception of data. Key components include the base station (gNB), user equipment (UE), and scatterers, which are essential for signal propagation and reception. The diagram highlights beamforming techniques to mitigate interference, ensuring reliable communication.\nFigure 5.6.2-1 Media Coding and Decoding Modelling\n\nAs an example, rate control plays an important role in video coding, although it’s not a normative tool for any video coding standard. In video communications, rate control ensures that the coded bitstream can be transmitted successfully and make full use of the limited bandwidth while maximizing the quality. The rate control not only determines the quality, it also determines the latency when for example delivering a video bitstream via constant bitrate link.\nFor video encoding, among others the following parameters may be considered:\n-\tCodec in use, for example H.264/AVC or H.265/HEVC\n-\tEncoding patterns:\n-\tUsage of B and P pictures\n-\tLatency settings: P pictures only, look-ahead units only 0 (for minimum latency), i.e. no future frames are taken into account in the encoding\n-\tRate control to be used, for example\n-\tCBR with bitrate\n-\tCapped VBR with bitrate\n-\tconstant rate factor (CRF), i.e. constant quality with weighted rate adjustments\n-\tconstant QP (CQP), i.e. constant quantization parameters, or\n-\tfeedback-based using information on loss and delay statistics\nFor a detailed modelling refer to clause 5.6.2.3\n-\tSlice settings, examples\n-\t1 slice per frame,\n-\t1 slice per every 16 pixel row,\n-\t8 slices per video frame\n-\tIntra settings and error resilience:\n-\tRegular IDR Patterns, e.g. 16-th frame,\n-\tRegular Gradual Decoder Refresh Pattern,\n-\tadaptive Intra,\n-\tfeedback based Intra,\n-\tfeedback based predication and ACK-based,\n-\tfeedback-based prediction and NACK based\n-\tComplexity settings for encoders\n-\tPre-Encoding delays: generation timestamp of the rendered frame until this frame is encoded (add models)\n-\tconstant delay\n-\tequally distributed between a min and max delay\n-\ttruncated Gaussian model with mean, variance and maximum delay\n-\tEncoding delays: the time of encoder from receiving the frame until a slice is produced (add models).\n-\tconstant delay\n-\tequally distributed between a min and max delay\n-\ttruncated Gaussian model with mean, variance and maximum delay.\n-\tHandling of multiple video buffers, for example left and right eye\n-\tAll buffers operate on the same sampling/render time with the same pre-encoding delay\n-\tBuffers are staggered, i.e. sampling/render time is uniformly distributed across the sampling rate.\nBased on these settings, a slice trace S-Trace can be generated resulting in the format as defined clause 5.6.4. A slice delivery unit now uses a slice trace and provides as output an S’-Trace, for more details refer to clause 5.7. The delivery impacts the slice delivery in one or multiple of the following ways:\n1)\tThe slice is delayed, i.e. a delivery timestamp is assigned to the slice in the S’-Trace. This can be used to determine if the slice is received in time\n2)\tThe slice is corrupted, i.e. the slice is entirely, only a correct prefix of the slice available or the slice was fully received for the decoding model.\nA decoding model now takes into account the S’-Trace as well as the V-Trace in order to generate a decoding model. For this, coding units in a video frame may be viewed individually and get assigned a state:\n-\tA coding unit spanning a certain area of the video frame is correct if and only if it is received correctly and it predicts for a non-damaged coding unit. Predicting from non-damaged units means that both spatial prediction as well as temporal prediction coding units are correct\n-\tIf any of the two conditions do not hold, the coding unit state is damaged.\nNote that only non-predicted coding units will eventually remove damaged areas. The resulting trace documenting correct and damaged coding units is referred to as V’-Trace, see clause 5.6.4 for more details.\nAn important factor for the quality of the video is the percentage of lost area.\nA specific algorithm to model the generation of slices as well as the decoding of received slices is provided in Annex A.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.6.3\tVideo Coding and Decoding Configuration",
                            "text_content": "Detailed video encoding configuration parameters may be derived from the list of parameters provided in clause 5.6.2. An example encoding parameters are provided in Table 5.6.3-1. The below example sets the following parameters:\n-\tTwo source buffers, each defined by V-Traces, width, height, frame rates\n-\tslice configuration mode, 8 rows are configured.\n-\tBitrate mode is configured to CBR, with parameters for the rate control\n-\tError resilience settings using 1 intra rotating intra slice for every frame, restricted to no error propagation, also referred to gradual decoder refresh (GDR).\n-\tParameters for encoder pre-delay and encoder processing delay\n- \tstaggering of buffers are set to be true, i.e. the frames of each buffer are sent with different presentation times.\nTable 5.6.3-1 Example video encoding configuration\n\nFor video coding, each received S’-trace would be decoded independently and converted into a V’-Trace. The V’-Trace holds status of each coding unit it is determined if the coding unit is correct or damaged according to clause 5.6.2. No specific other configuration parameters are provided.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.6.3-1 Example video encoding configuration",
                                    "table number": 7,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "5.6.4 \tS-Trace and V’-Trace Format",
                            "text_content": "For each generated slice, an S-Trace format is provided according to Table 5.6.4-1.\nTable 5.6.4-1 S-Trace Format\n\nFor each frame, an entry into a binary file is generated that documents the information according to Table 5.6.4-2.\nTable 5.6.4-2 Frame-related metadata from encoding\n\nFor each slice after delivery, on entry in the V’-Trace is provided according to the format provided in Table 5.6.4-3.\nTable 5.6.4-3 V'-Trace Format\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.6.4-1 S-Trace Format",
                                    "table number": 8,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 5.6.4-2 Frame-related metadata from encoding",
                                    "table number": 9,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 5.6.4-3 V'-Trace Format",
                                    "table number": 10,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "5.6.5\tOther media types",
                            "text_content": "For other media types, no dedicated modelling is defined.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "5.7\tContent Delivery Modelling",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.7.1\tGeneral",
                            "text_content": "Content delivery deals with the content delivery protocol and packetization, delay jitter, but possibly also more sophisticated models such as RTP retransmission, TCP operations and so on. Typically, an application data unit is received by the content delivery system and is then mapped to one or several transport packets including information on timing, etc. At the receiving end, the received packets are used to re-construct the application data units.\nThere may be network deployments that uniformly support the transport of packets with larger Maximum Transfer Unit (MTU) sizes (for example with ethernet jumbo frames of transport MTU size up to 9216 octets), but according to TS 23.501, clause 5.6.10.4 and Annex J, the typical link MTU size is 1358 bytes. This means that for typical network deployments the IP packets size larger than 1358 bytes will imply IP fragmentation.\nThe sending part of the content delivery can typically be modelled by at least the following parameters:\n-\tMaximum Transfer Unit: This determines the maximum packet size of the application. Typically, an application data unit is segmented into multiple packets for transfer for which none of the packets exceeds the maximum transfer unit. The delivery This is addressed in clause 5.7.2\n-\tBitrate restrictions: the available bitrate from packager to UPF, i.e.  from the content delivery sender to the 5G System.\n-\tPotential retransmissions of packets. This is addressed in clause 5.7.3.\n-\tPotentially adding forward error correction on application layer for one or several application data units. This is addressed in clause 5.7.4.\nThe receiving part of the content delivery can typically be modelled by at least the following parameters:\n-\tSlice recovery strategy: This determines how a slice is recovered from lost and received packets assigned to a slice, for details for different configurations refer to clauses 5.7.2, 5.7.3 and 5.7.4.\n-\tMaximum Latency: This determines the maximum permitted latency for a slice to be useful and if exceeded, the slice will be declared as lost.\nA content delivery modelling is provided in Figure 5.7.1-1. An S-Trace is received by the content delivery model sender which converts the data to a packet trace (P-Trace) according to clause 5.3. After delivery through the 5G-System model, a P’-Trace is provided to the content delivery model sender which has the same format as the P-Trace. From the P’-Trace, an S’-Trace is reconstructed following the format defined in 5.7.5.\nThe figure depicts a content delivery model, illustrating the various components and their interactions. It includes a content delivery network (CDN), a content delivery network (CDN), a content delivery network (CDN), and a content delivery network (CDN). The CDN is responsible for caching and distributing content to users, ensuring fast and efficient delivery of multimedia content. The figure also highlights the importance of CDN in improving the user experience by reducing latency and improving the overall performance of the network.\nFigure 5.7.1-1 Content Delivery Modelling\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.7.2\tContent Delivery Modelling for ADU Fragmentation",
                            "text_content": "Payload formats of modern video codecs, in particular H.264/AVC and H.265/HEVC allow fragmentation of a single application data unit (ADU) into multiple transport units. Such a fragment of an ADU consists of an integer number of consecutive octets of that ADU unit.  Each octet of the ADU is part of exactly one fragment of that ADU unit. Fragments of the same ADU are typically sent in consecutive order, for example with ascending RTP sequence numbers. Based on this approach, every ADU can be mapped to a packet transport stream for which a maximum transfer unit (MTU) of a specific size exist, for example something like 1500 bytes. In addition, mapping a ADU to an IP packet system, results in some overhead for the packet header, for IPv4 with RTP/UDP typically 40 bytes.\nSecondly, the availability time of the generated packets to RAN layer depends on the delivery from the encoder to the UPF/RAN.\nBased on this, from an S-Traces, a P-Trace can be generated based on the following parameters:\n-\tMaxSize: maximum size of an output packet in bytes (0 means infinite)\n-\tPacketOverhead: packet overhead added in bytes (default 40 bytes)\n-\tPacket Delivery Bitrate: Provides the bitrate at which the packets are delivered from Encoder to RAN. Packets arrive this value divided by the size of the packet.\nNOTE: assumes 1 hop with a bitrate as above. Consideration of multiple hops is not included.\nFigure 5.7.2-1 provides an illustration on the content delivery modelling delays and packets based on the measurements presented in Annex B. A video frame may be delayed by the encoder. ADUs/slices are produced and are delayed by the encoder. The resulting packets are fragmented and then delivered, possibly delayed by the connection from the encoder to the UPF/RAN.\n\n\n\n\nFigure 5.7.2-1 Packet sizes and delays\nA possible configuration is provided in Table 5.7.2-1. In this case the trace data from an S-Trace is used and ADU fragmentation and packetization is done using a maxSize of 1468 and an overhead of 40. The access bitrate from encoder to RAN is assumed to be 10 Mbit/s and may result in packet delays when delivered.\nTable 5.7.2-1 Example Content Delivery sender configuration\n\nAt the receiving end the packets assigned to an ADU are recovered to reconstruct an ADU. For the case of ADU fragmentation, the ADU is reconstructed by removing the header from each packet and concatenating the fragment of the ADUs in sequence order. Only when all packets associated to the ADU are available, the ADU is available to the next processing unit, i.e. the timestamp is determined by the last received packet associated to the ADU.\nIf one or more packets associated to the ADU are lost, then the timestamp of the loss is the time at which the first lost packet is detected. However, as in order delivery cannot be assumed, a maximum delay of an ADU needs to be set, typically compared to the render time, after which only received packets are processed as part of the ADU. For the potential recovery of the ADU one of two different modes of recovery apply:\n-\tADU loss: If one of the packets associated to the ADU is lost, the entire ADU is lost.\n-\tSuffix loss: If one packet is lost, then only the information from packets are earlier in the concatenated ADU are used to generate a partially received ADU.\nIn addition, a maximum ADU delay is set after which the data in the ADU compared to the render time is no longer helpful and the ADU is discarded.\nA possible configuration is provided in Table 5.7.2-2. In this case the trace data from an P’-Trace is analysed and all buffer S’-Traces are generated. The loss mode is set to ADU to indicate that any lost fragment results in a loss of the entire ADU. ADUs not recovered within the maxDelay of 60 are considered as lost.\nTable 5.7.2-2 Example Content Delivery receiver configuration\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.7.2-1 Example Content Delivery sender configuration",
                                    "table number": 11,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 5.7.2-2 Example Content Delivery receiver configuration",
                                    "table number": 12,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "5.7.3\tContent Delivery Modelling for Retransmission",
                            "text_content": "This work is for further study.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.7.4\tContent Delivery Modelling for Application Layer FEC",
                            "text_content": "Commercial XR split rendering and cloud gaming services use Application Layer Forward Error Correction (FEC).\nFor example, Nvidia CloudXR™ supports FEC as indicated here . Other cloud gaming and XR services also report about the use of FEC.\nIn the following a possible implementation for application layer FEC assuming the system model for XR traffic (see Figure 5.2.1-1) is described. Commercially available XR split rendering and cloud gaming services as introduced above follow the same or at least similar principles.\nIn this case, the Application Data Units (ADUs) are not sent directly to the network, but they are added to a source block that then generates packets of basically equal size in order to then distribute the content. The basic concept is shown in Figure 5.7.4-1.\nEach Application Data Unit (for example a video frame, or an object) has assigned a size F and additional properties, for example the type of the ADU, its importance, its delay constraints and so on. The properties are typically different for each ADU. Each ADU forms a source block with K encoding symbols, each of size T. Typically, the number of K is different for each ADU in a sequence of ADUs. Each of the initial K encoding symbol forms the payload of K source packets, whereby each packet may include some of the properties, and includes the source block size K as well as the encoding symbol id (ESI). The size of the object, F, may be carried as part of the source block as shown in Figure 1. In addition to K source packets, N-K repair packets may be sent as part of this ADU. The repair packets would be assigned to the same ADU, for example using a unique Transport Object Identifier (TOI) for ADU.\nThe figure depicts a packet generation process for application layer FEC in a 5G network. It illustrates the steps involved in generating a packet, including the selection of a frame type, encoding, and transmission. The figure also shows the use of FEC techniques to improve the reliability and efficiency of data transmission.\nFigure 5.7.4-1 Packet Generation for Application Layer FEC\nAt the receiving end, assuming that the code is maximum distance separable (MDS) as the case for RaptorQ or Reed-Solomon codes, i.e. K out of the N packets are sufficient to recover the ADU, the receiver collects K symbols, determines the symbol size T based on the payload size, applies FEC decoding, recovers the source block, reads the size F from the K-th source symbols and recovers the ADU for the next layer in the protocol stack.\nSuch a system is aligned with Forward Error Correction (FEC) Building Block as defined in RFC 5052 [7].\nThe FEC Payload ID (i.e. the information carried in every packet header), essentially only requires carrying the encoding symbol ID and the source block size K. As an example, for RaptorQ as defined in RFC 6330 [8], the maximum source block size is 56403, i.e. 16 bits are sufficient. It is also expected that to signal the ESI, 1 or 2 bytes would be sufficient for most applications. In addition, a TOI may be carried, again using 1 or 2 bytes. While the above FEC configuration only serves as one reference, it may be considered as typical implementation.\nAn example sender configuration for FEC is provided in Table 5.7.4-1\nTable 5.7.4-1 Example Content Delivery sender configuration for Application Layer FEC\n\nAt the receiver, if one or more packets associated to the ADU with are lost, then the timestamp of the loss is the time at which the first lost packet is detected. However, as in-order delivery cannot be assumed, a maximum delay of an ADU needs to be set, typically compared to the render time, after which only received packets are processed as part of the ADU. If at least K packets are received for an ADU within the time budget, the ADU can be fully recovered. If less than K packets are received, the ADU cannot be recovered and one of the following two error handling modes can be configured:\n-\tADU loss: If more than N-K of the packets associated to the ADU are lost, the entire ADU is lost.\n-\tSuffix loss: If more than N-K of the packets associated to the ADU are lost, then only the correct prefix preceding the first loss of a source packet is used to generate a partially received ADU.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.7.4-1 Example Content Delivery sender configuration for Application Layer FEC",
                                    "table number": 13,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "5.7.5\tS’-Trace Format",
                            "text_content": "The S’-Trace format after the content delivery modelling receiver is shown in Table 5.7.4-3.\nTable 5.7.4-3 S’-Trace Format\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.7.4-3 S’-Trace Format",
                                    "table number": 14,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "5.8\tQuality Metrics and Computation",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.8.1\tGeneral",
                            "text_content": "Quality Evaluation is based on two aspects, namely the encoding quality and the quality degradation due to lost or late data. An evaluation framework is shown in Figure 5.8-1. The quality evaluation takes into account information from\n-\tP’-Traces\n-\tS’-Traces\n-\tV’-Traces\nFor each user and each buffer, the quality is provided individually following the format of a Quality Trace Q-Trace as shown in Figure 5.8.1-1.\nTable 5.8.1-1 Q-Trace Format\n\nThe figure depicts the packet sizes and delays in an 8-1 network, illustrating the impact of packet size on latency and the importance of maintaining a balance between packet size and delay to ensure efficient communication.\nFigure 5.8-1 Packet sizes and delays\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.8.1-1 Q-Trace Format",
                                    "table number": 15,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "5.8.2\tTest Channels",
                            "text_content": "In order to evaluate the performance of certain configurations, test channel are defined and applied to P-Traces. The test channel address two aspects:\n-\tPermit to emulate typical radio conditions in terms delays and losses.\n-\tPermit to evaluate the application quality for different representative radio conditions for different different application configurations.\nThe test channel is aligned with the QoS Model as defined in TS 23.501, clause 5.7. Note that TS 23.501 defines the following QoS characteristics:\n-\tClause 5.7.3.2: Resource type (Non-GBR, GBR, Delay-critical GBR)\n-\tA GBR QoS Flow uses either the GBR resource type or the Delay-critical GBR resource type. The definition of PDB and PER are different for GBR and Delay-critical GBR resource types, and the MDBV parameter applies only to the Delay-critical GBR resource type.\n-\tA Non-GBR QoS Flow uses only the Non-GBR resource type.\n-\tClause 5.7.3.3: Priority Level: The Priority Level associated with 5G QoS characteristics indicates a priority in scheduling resources among QoS Flows. The lowest Priority Level value corresponds to the highest priority.\n-\tClause 5.7.3.4: Packet Delay Budget (including Core Network Packet Delay Budget):\n-\tThe Packet Delay Budget (PDB) defines an upper bound for the time that a packet may be delayed between the UE and the N6 termination point at the UPF. For a certain 5QI the value of the PDB is the same in UL and DL. In the case of 3GPP access, the PDB is used to support the configuration of scheduling and link layer functions (e.g., the setting of scheduling priority weights and HARQ target operating points).\n-\tFor GBR QoS Flows using the Delay-critical resource type, a packet delayed more than PDB is counted as lost if the data burst is not exceeding the MDBV within the period of PDB and the QoS Flow is not exceeding the GFBR. For GBR QoS Flows with GBR resource type not exceeding GFBR, 98 percent of the packets shall not experience a delay exceeding the 5QI's PDB.\n-\tClause 5.7.3.5: Packet Error Rate;\n-\tThe Packet Error Rate (PER) defines an upper bound for the rate of PDUs (e.g. IP packets) that have been processed by the sender of a link layer protocol (e.g. RLC in RAN of a 3GPP access) but that are not successfully delivered by the corresponding receiver to the upper layer (e.g. PDCP in RAN of a 3GPP access).\n-\tThus, the PER defines an upper bound for a rate of non-congestion related packet losses. The purpose of the PER is to allow for appropriate link layer protocol configurations (e.g. RLC and HARQ in RAN of a 3GPP access). For every 5QI the value of the PER is the same in UL and DL. For GBR QoS Flows with Delay-critical GBR resource type, a packet which is delayed more than PDB is counted as lost, and included in the PER unless the data burst is exceeding the MDBV within the period of PDB or the QoS Flow is exceeding the GFBR.\n-\tClause 5.7.3.6: Averaging window (for GBR and Delay-critical GBR resource type only): Each GBR QoS Flow shall be associated with an Averaging window. The Averaging window represents the duration over which the  GFBR and MFBR shall be calculated (e.g. in the (R)AN, UPF, UE).\n-\tClause 5.7.3.7: Maximum Data Burst Volume (for Delay-critical GBR resource type only):\n-\tEach GBR QoS Flow with Delay-critical resource type shall be associated with a Maximum Data Burst Volume (MDBV).\n-\tMDBV denotes the largest amount of data that the 5G-AN is required to serve within a period of 5G-AN PDB.\n-\tEvery standardized 5QI (of Delay-critical GBR resource type) is associated with a default value for the MDBV (specified in QoS characteristics Table 5.7.4.1). The MDBV may also be signalled together with a standardized 5QI to the (R)AN, and if it is received, it shall be used instead of the default value.\n-\tThe MDBV may also be signalled together with a pre-configured 5QI to the (R)AN, and if it is received, it shall be used instead of the pre-configured value\nFor the initial version of a test channel, the following model and parameters are defined:\n1)\tPacket Error Rate:\na.\tParameters: Packet Error Rate\nb. \tModel: iid losses independent of the packet size\n2)\tPacket delays:\na.\tParameter: max_delay\nb.\tModel: iid distributed latency between 0 and a max_value\nAdditional aspects of the QoS model and the RAN model are for further study.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "5.9\tUplink Modeling",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.9.1\tConsiderations",
                            "text_content": "In several scenarios, only pose and action data is sent in the uplink channel. Pose information may be used by the edge/cloud for network rendering. In general, the sampling rates are controlled by the application for which the pose information is applied. The more frequently the pose information is captured, the more accurate the rendered scenes and gaming control are. According to TR 26.928, in order to always be able to respond to the latest XR Viewer Pose, tracking needs to be done frequently and the minimum update rates should be 1000Hz and beyond. However, this applies to the hardware tracking of the device. Hence, an XR application can continuously query the XR Runtime (for example using an OpenXR API) to provide the viewer pose for a particular display time. This time is typically the target display time for a frame to be rendered. Repeatedly querying the pose for the same display time may not necessarily return the same result. Instead, the pose prediction gets increasingly accurate as the function is called closer to the given time for which a prediction is made. The application may also query the XR runtime for the predicted pose at different display times.\nIn case the pose is used for pre-rendering in the network (edge/cloud), an accurate and most recent pose information is preferable. There is a tradeoff between how often the latest pose is sent and whether it is sent for only one predicted display time or several consecutive display times. As a first estimate it can be assumed that sending a viewer pose aligned with the frame rate of the rendered video may be sufficient, for example at 60fps. The size of such information is typically 32 bytes per pose, and with several poses sent and header overhead, it may be up to few 100 bytes in a single flow. With such assumption the mapping to bitrates, periodicity and PDB is straightforward.\nIn addition to pose information, media uplink may be sent, for example data from cameras. In this case, a similar modelling as for downlink media applies.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.9.2\tPose Uplink Model",
                            "text_content": "According to TR 26.928, clause 4.1.3, the following applies:\nTo maintain a reliable registration of the virtual world with the real world as well as to ensure accurate tracking of the XR Viewer pose, XR applications require highly accurate, low-latency tracking of the device at about 1kHz sampling frequency. An XR Viewer Pose consists of the orientation (for example, 4 floating point values in OpenXR) and the position (for example, 3 floating point values in OpenXR). In addition, the XR Viewer Pose needs to have assigned a time stamp. The size of a XR Viewer Pose associated to time typically results in packets of size in the range of 30-100 bytes, such that the generated data is around several hundred kbit/s if delivered over the network.\nIn addition, according to clause 5.9.1, an application permits to control the uplink pose information sending.\nBased on these considerations, the following is proposed for the uplink modelling:\n1)\tXR Viewer Pose information is provided to the application in 1kHz sampling frequency\n2)\tThe size of the XR Viewer Pose packets are considered 100 byte (60 payload, 40 header)\n3)\tThe Pose is sent with a frequency of controlled by the application, but we assume alignment with the rendering frame rate, i.e. 10 ms.\nIn order to minimize the rendering impact, the expected latency of the pose information is at most 10ms in the uplink. This means that the content is rendered with a pose of typically 10-15ms age.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "6\tXR Split Rendering",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "6.1\tIntroduction",
                    "description": "",
                    "summary": "",
                    "text_content": "Raster-based split rendering was introduced in TR 26.928 as an important scenario in order to serve end devices with restricted capabilities.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "6.2\tReference System Design",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.2.1\tOverview",
                            "text_content": "The system design for split rendering follows the discussion and requirements from TR26.928 [3], clause 6.2.5. The architecture us shown in Figure 6.2.1-1.\nThe figure depicts a split rendering with asynchronous time warping (ATW) correction, which is a technique used in computer graphics to improve the quality of images by warping the image to correct for the effects of time warping. This technique is particularly useful in applications where the image is not perfectly aligned with the original, such as in video editing or gaming. The figure shows the process of warping the image to correct for the effects of time warping, and the results are shown in the final image.\nFigure 6.2.1-1 Split Rendering with Asynchronous Time Warping (ATW) Correction\nRaster-based split rendering refers to the case where the XR Server runs an XR engine to generate the XR Scene based on information coming from an XR device. The XR Server rasterizes the XR viewport and does XR pre-rendering.\nAccording to Figure 6.2.1-1, the viewport is pre-dominantly rendered in the XR server, but the device is able to do latest pose correction, for example by asynchronuous time-warping (see clause 4.1 of TR26.928) or other XR pose correction to address changes in the pose.\n-\tXR graphics workload is split into rendering workload on a powerful XR server (in the cloud or the edge) and pose correction (such as ATW) on the XR device\n-\tLow motion-to-photon latency is preserved via on device Asynchronous Time Warping (ATW) or other pose correction methods.\nThe following call flow highlights the key steps:\n1.\tAn XR Device connects to the network and joins XR application\na)\tSends static device information and capabilities (supported decoders, viewport)\n2. Based on this information, the XR server sets up encoders and formats\n3. Loop\na) \tXR Device collects XR pose (or a predicted XR pose)\nb) \tXR Pose is sent to XR Serve\nc) \tThe XR Server uses the pose to pre-render the XR viewport\nd)\tXR Viewport is encoded with 2D media encoders\ne)\tThe compressed media is sent to XR device along with XR pose that it was rendered for\nf)\tThe XR device decompresses video\ng)\tThe XR device uses the XR pose provided with the video frame and the actual XR pose for an improved\nprediction using and to correct the local pose, e.g. using ATW.\nAccording to TR 26.928, clause 4.2.2, the relevant processing and delay components are summarized as follows:\n-\tUser interaction delay is defined as the time duration between the moment at which a user action is initiated and the time such an action is taken into account by the content creation engine. In the context of gaming, this is the time between the moment the user interacts with the game and the moment at which the game engine processes such a player response.\n-\tAge of content is defined as the time duration between the moment a content is created and the time it is presented to the user. In the context of gaming, this is the time between the creation of a video frame by the game engine and the time at which the frame is finally presented to the player.\nThe roundtrip interaction delay is therefore the sum of the Age of Content and the User Interaction Delay. If part of the rendering is done on an XR server and the service produces a frame buffer as rendering result of the state of the content, then for raster-based split rendering, the following processes contribute to such a delay:\n-\tUser Interaction Delay (Pose and other interactions)\n-\tcapture of user interaction in game client,\n-\tdelivery of user interaction to the game engine, i.e. to the server (aka network delay),\n-\tprocessing of user interaction by the game engine/server,\n-\tAge of Content\n-\tcreation of one or several video buffers (e.g. one for each eye) by the game engine/server,\n-\tencoding of the video buffers into a video stream frame,\n-\tdelivery of the video frame to the game client (a.k.a. network delay),\n-\tdecoding of the video frame by the game client,\n-\tpresentation of the video frame to the user (a.k.a. framerate delay).\nAs ATW is applied the motion-to-photon latency requirements (of at most 20 ms) are met by XR device internal processing. What determines the network requirements for split rendering is time of pose-to-render-to-photon and the roundtrip interaction delay. According to clause TR 26.928, clause 4.5, the permitted downlink latency is typically 50-60ms.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.2\tConsidered Content Formats",
                            "text_content": "Rasterized 3D scenes available in frame buffers (see clause 4.4 or TR 26.928 [3]) are provided by the XR engine and need to be encoded, distributed and decoded. According to TR 26.928, clause 4.2.1, relevant formats for frame buffers are 2k by 2k per eye, potentially even higher. Frame rates are expected to be at least 60fps, potentially higher up to 90 fps. The formats of frame buffers are regular texture video signals that are then directly rendered. As the processing is graphics centric, formats beyond commonly used 4:2:0 signals and YUV signals may be considered.\nIn practical considerations, the NVIDIA Encoding functions may be used. The parameters of such an encoder are documented here .\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.3\tConsidered System Parameters",
                            "text_content": "Based on the discussion on clause 6.2.1 and 6.2.2, several parameters are relevant for the overall system design.\n-\tGame: Type of game, state of game, multi-user actions, etc.\n-\tUser Interaction:\n-\t6DOF pose based on head and body movement,\n-\tGame interactions by controllers\n-\tFormats of rasterized video signal. Typical parameters are:\n-\t1.5K x 1.5K per eye at 60, 90, 120fps\n-\t2K x 2K at 60, 90, 120fps\n-\tYUV 4:2:0 or 4:4:4\n-\tEncoder configuration\n-\tCodec: H.264/AVC or H.265/HEVC\n-\tBitrate: Bitrate setting to a specific value (e.g. 50 Mbit/s)\n-\tRate control: CBR, Capped VBR, Feedback based, CRF, QP\n-\tSlice settings: 1 per frame, 1 per MB row, X per frame\n-\tIntra settings and error resilience: Regular IDR, GDR Pattern, adaptive Intra, feedback based Intra, feedback based predication and ACK-based, feedback-based prediction and NACK based\n-\tLatency settings: P pictures only, look-ahead units\n-\tComplexity settings for encoder\n-\tContent Delivery\n-\tSending of eye buffers\n-\tAt the same time\n-\tstaggered\n-\tSlice to IP mapping: Fragmentation\n-\tRTP-based time codes and packet numbering\n-\tRTP/RTCP-based feedback ACK/NACK\n-\tRTP/RTCP-based feedback on bitrate\n-\t5G System/RAN Configuration:\n-\tQoS Settings (5QI): GBR, Latency, Loss Rate\n-\tHARQ transmissions, scheduling, etc.\n-\tContent Delivery Receiver configuration:\n-\tLoss Detection: sequence numbers\n-\tDelay/Latency handling\n-\tError Resilience\n-\tATW\n-\tQuality Aspects\n-\tVideo quality (encoded)\n-\tLost data\n-\tImmersiveness\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.3\tSimulation System",
                    "description": "",
                    "summary": "",
                    "text_content": "Figure 6.3-1 shows the simulation system used for XR split rendering based on the architecture in Figure 5.2.1-1. The following instantiation is provided:\n-\tTwo S-Traces are generated to address a video output for each eye buffer\n-\tNo feedback on video encoding or content delivery is considered\nThe XR Split Rendering Simulation System, depicted in Figure 6.3-1, is a cutting-edge technology used in the field of computer graphics and virtual reality. This system utilizes a split rendering technique to create high-quality, immersive visual experiences. The system's architecture is designed to efficiently process and render complex scenes, ensuring smooth and realistic visual output. The use of XR technology in this simulation system demonstrates the potential for advanced visual rendering capabilities in the future of computer graphics and virtual reality.\nFigure 6.3-1 XR Split Rendering Simulation System\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "6.4\tRecommended Configurations",
                    "description": "",
                    "summary": "",
                    "text_content": "The following parameters are recommended and fixed for the simulartion:\n1)\tContent Model:\na.\tRendered scene output with 2 eye buffers at 2Kx2K at 60 fps, 8bit.\nb.\tContent and Trace Preview is here:\nc.\tNo audio\nNOTE: Audio Modeling may be added by providing another P-Trace for audio, for example every 20ms a packet is provided according to an AMR/EVS/AAC model. Typical bitrates are in the range of 15-80 kbit/s which is negligible compared to video.\n2)\tEncoding Model\na.\tEncoding Models according to clause 5.5.\nb.\tHEVC, target bitrate 30 Mbit/s or 45 Mbit/s (CBR, capped VBR), equally split across eye buffers, independently encoded.\nc.\tSlice based encoding (8 slices) or 1 frame\nd.\tIntra Refresh (1 slice per frame) or every 8th frame.\ne.\tLeft and right eye buffer are independently encoded.\nf.\tPre-encoding delay: Encoder pre-delay is varying between 10 to 20ms\ng.\tEncoding delay is modelled to vary with mean 4/slice_numbers and standard deviation 3/slice_numbers and maximum being the frame interval (aligned with clause 5.7.2 and Annex B).\n3)\tContent Delivery Model\na.\tContent Delivery Model see clause 5.7.2 with, detailed configurations in Table 6.4-1.\nb.\tPacket MaxSize 1500 byte (cloud) or unlimited (edge)\nc.\tEdge/Cloud to gNB bitrate is 1.5 media bitrate => delay variance\n4)\tDelivery receiver\na.\tModeling according to clause 5.6.\nb.\tPackets are dropped if late\nc.\tSlice loss model (1 lost packets per slice results in slice loss)\nd.\tTimestamp of slice if time stamp of latest packet of slice\ne.\tMaximum latency for slice: 60ms (see TR 26.928, clause 4 and 6.2.5.1)\n5)\tDecoding Model\na.\tModeling according to clause 5.6.\nb.\tTakes into account slice structure, spatial and temporal error propagation, intra refresh\n6)\tQuality evaluation tool.\na.\tModeling according to clause 5.8.\nb.\tThe following metrics are considered for each user and buffer\ni.\tIP Packet loss rate\nii.\tIP Packet late rate\niii.\tSlice loss rate\niv.\tArea loss rate (total amount of Coding Units)\nv.\tArea damage rate (total amount of Coding Units)\nvi.\tAverage encoded PSNR\nvii.\tAverage PSNR\na.\tAverage over all buffers\nb.\tMulti-user\ni.\tAverage over all users\nii.\tPercentile of support\n7)\tA model for the uplink traffic in a similar fashion also providing packet traces.\na.\tXR Pose is sent uplink\nb.\tDetails are in clause 5.8.\nc.\tThe uplink bitrate for the pose if 200 kbit/s CBR, with 4ms packet interval and packet size 100 byte. This means that the content is rendered with a pose of typically 10-15ms age.\nIn addition, the following configurations are considered:\n-\tStart frame for N=16 users at different starting positions [1,1801,901,2701,451,2251,1351,3251,226,2026,1126,2926,676,2476,1576,3476]\n-\tSlice and Error Resilience:\n-\tFor configuration 1, 2, 3, 4, and 6 use slice setting 8 with one intra slice per frame\n-\tFor configuration 5: No slice setting with one I-frame very 8 frames\n-\tBitrates:\n-\tFor configurations 1, 2, 5 and 6 capped VBR at 30 Mbit/s total with window size 12 frame (200ms) (target frame size is 31,250 byte per buffer, but buffer may exceed slightly – only over 200ms it is not exceeding). (aligns with findings in Annex B).\n-\tFor configurations 3 and 4: Constant bitrate at 30 Mbit/s total with window size 1 frame (maximum frame size is 31,250 byte per buffer, almost constant).\n-\tDelay:\n-\tEncoder pre-delay is varying between 10 to 20ms.\n-\tEncoding delay is modelled to vary with mean 4/slice_numbers and std 3/slice_numbers and maximum being the frame interval.\n-\tBuffer Interleaving:\n- \tFor configuration 1, 2, 3, 4 and 5, buffer interleaving set to false.\n-\tFor configuration 6, buffer interleaving set to false.\n- \tContent Delivery Modeling\n-\tFor configurations 1, 3, 5, and 6, 1500 byte max packet size (addressing the cloud server case in Annex B)\n-\tFor configurations 2 and 4, unlimited packet size, i.e. each slice results in a packet (addressing the edge serve case in Annex B)\n-\tThe bitrate is set to an excess of 1.5, i.e. 45 Mbit/s for 30 Mbit/s content bitrate and 67.5 Mbit/s for 45 Mbit/s content bitrate. This aligns with the parameters in Annex B.\nBeyond the above, the configurations provided in Table 6.4-1 are recommended with priority according to order.\nTable 6.4-1 XR Split Rendering Recommended Configurations\n\nThe following provides an idea of the latencies of packets from the time that a frame is rendered in the XR server to the time the packet arrives at the gNB/Radio:\n-\tMin-Latency:\n-\tPre-encoding delay: 10ms\n-\tEncoding delay: 2ms\n-\tEncoder-to-gNB delay: 3ms\n-\tTotal delay: 15ms\n-\tMax-Latency:\n-\tPre-encoding delay: 20ms\n-\tEncoding delay: 17ms (considered exceptionally large)\n-\tEncoder-to-gNB delay: 10ms (considered exceptionally large)\n-\tTotal delay: 47ms\n-\tTypical-Latency:\n-\tPre-encoding delay: 15ms\n-\tEncoding delay: 4ms\n-\tEncoder-to-gNB delay: 11ms\n-\tTotal delay: 30ms\nNote that\n-\tthe maximum delay of 47ms aligns with the 32ms from Annex B assuming without constant delay of around 15ms.\n-\ttypical latency of 30ms aligns with the 32ms from Annex B assuming without constant delay of around 15ms\n",
                    "tables": [
                        {
                            "description": "Table 6.4-1 XR Split Rendering Recommended Configurations",
                            "table number": 16,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "6.5\tTraces and Statistical Models",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.5.1\tTraces",
                            "text_content": "A set of P-Traces for 16 users are provided, each 1 minute duration for the 8 configurations as provided in clause 6.4.\nThe traces can be accessed here\n\nFigure 6.5.1.2-1 provides the S-Trace statistics for both eyes and one user 0. The bitrate over time varies, but is bound to 30 Mbit/s for almost all cases. Frame size variation is more distributed than VR2-3, with much smaller frames occasionally. Due to window size 12 (i.e. 200ms), larger frame sizes occur. Slice size distribution also follows the peak at 3500 bytes with distributions up to 12000 bytes.\nOverall, the total bitrate is likely too low to express best quality, as the rate control forces the frame size to be constant for most of the time.\nThe figure depicts a S-Trace Statistics for VR2-1, which is a 5G network scenario. It shows the number of S-Trace events, the duration of each event, and the number of S-Trace events per second. The figure also includes a legend to explain the different colors and symbols used in the S-Trace statistics.\nFigure 6.5.1.2-1 S-Trace Statistics for VR2-1\nFigure 6.5.1.3-1 and Figure 6.5.1.3-2 provide the S-Trace and P-Trace statistics for VR2-7 for both eyes and one user 0, respectively. The bitrate over time varies, but is bound to 45 Mbit/s. This also shows significantly more variance. Frame size variation is more distributed than VR2-1, with much smaller frames occasionally. Due to window size 12 (i.e. 200ms), larger frame sizes occur. Slice size distribution also follows the peak at 5500 bytes with distributions up to 12000 bytes.\nThe figure depicts a S-Trace Statistics for initial VR2-7, which is a crucial component in the analysis of telecommunication signals. The S-Trace Statistics provides a comprehensive view of the signal quality, including the number of transmitted packets, the number of received packets, and the number of dropped packets. This information is essential for understanding the performance of the network and identifying potential issues. The figure also includes a legend to help interpret the data, and a legend key to explain the different colors and symbols used in the S-Trace Statistics. Overall, the figure provides a valuable tool for network engineers and analysts to monitor and optimize the performance of their telecommunication systems.\nFigure 6.5.1.3-1  S-Trace Statistics for initial VR2-7\nThe figure depicts a P-Trace Statistics for initial VR2-7, which is a crucial component in the analysis of telecommunication signals. The P-Trace is a trace that represents the path of a signal through a network, and it is used to identify and analyze the performance of the network. The figure shows the distribution of P-Traces for different VR2-7 nodes, which can be used to understand the network's performance and identify potential issues. The P-Trace Statistics provides valuable insights into the network's performance, such as the number of P-Traces, the length of the P-Traces, and the distribution of P-Traces across the network. This information can be used to optimize the network and improve its performance.\nFigure 6.5.1.3-1 P-Trace Statistics for initial VR2-7\nThis bitrate of 45 Mbit/s seems to better express a realistic content model.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.2\tStatistical Models",
                            "text_content": "Analyzing the initial traces that were derived and they are attached to this document for VR2-3 and VR2-4.\nWe extract several statistical models for VR2-3 and VR2-4 in Figure 6.5.2-1 and Figure 6.5.2-2, respectively.\n-\tInter arrival distribution: what is the typical difference between packet arrival\n-\tPacket size distribution: what are the different packet sizes\n-\tPacket latency distributions\nThe figure depicts VR2-3 statistics, including the number of users, the number of sessions, the average session duration, and the number of sessions per user. The statistics are presented in a clear and concise manner, making it easy to understand the performance of VR2-3.\nFigure 6.5.2-1 VR2-3 Statistics\n\nThe figure depicts VR2-4 statistics, which are essential for understanding the performance of a network. The graph shows the number of successful transmissions, the number of dropped packets, and the average packet loss rate for VR2-4. This information is crucial for network management and optimization.\nFigure 6.5.2-2 VR2-4 Statistics\nObservations:\n-\tThe traffic statistics can be mapped to statistics, but we need to know which statistics are needed\n-\tIt is clear that second order statistics are not represented in the above distributions\n-\tIt is also clear that depending on the configurations, the statistics are quite different.\nMore data can be extracted based on the attached traces and distributions.\nBased on the observations it is agreed\n-\tthat SA4 develops statistical models based on traces\n-\tthat RAN1 may develop statistical models based on traces\n-\tthat RAN1 may ask SA4 to provide statistical models for specific setups and parameters, if needed, for example\n-\tStatistical models for packets associated to I-frames and P-frames\n-\tStatistical models for slices and video frames\n-\tStatistical models for different importance settings\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.3\tAnalysis of the Traces and Statistical Models",
                            "text_content": "Statistical models are in Table 5.1.1.5-1 in TR 38.838 [4], repeated below in Table 6.5.3.1-1. The size of a frame is determined by the given data rates and frame rates, which is modelled as a random variable following truncated Gaussian distribution, as shown in Table 5.1.1.5-1 in TR 38.838 [4].\nNote that a frame in the context of TR 38.838 is defined as the collection of all packets of a packet trace with the same timestamp for presentation.\nTable 6.5.3.1-1: Statistical parameter values for dual eye buffer frame size (TR 38.838)\nAn analysis was done using the traces defined in clause 6.5.1 and the statistical model.  Table 6.5.3.1-2 show the frame size distribution for the P-trace according to the traces. It can be seen that the frame size distribution based on the P-traces in clause 6.5.1 are aligned with frame size model in TR 38.838 [4].\nTable 6.5.3.1-1 Frame size distribution deriving from the P-trace files\nFigure 6.5.3.1-1 shows the distribution of the slice size for different P-Traces.\n\nFigure 6.5.3.1-1 The statistic of packet size by using the P-trace data from SA4\nFrom the figure the following is observed:\n-\tVR2-1, VR2-2, VR2-3, VR2-4 and VR2-5 share similar frame size distribution as they all operate at the same bitrate of 30 Mbit/s and basically every frame uses the same bitrate to maximize the quality within the delay constraints. VR2-1, VR2-2 and VR2-5 have higher likelihood of smaller packets as they operate in capped VBR.\n-\tVR2-6 has lower frame sizes, because it sends the buffers in a staggered manner, resulting in twice as many frames, but each at half size.\n-\tVR2-7 and VR2-8 operate at higher bitrate and hence the quality for several frames is higher making use of the higher bit budget, but some frames do not need the full bit budget and hence stay at lower size.\nAn analysis was done using the traces defined in clause 6.5.1 for jitter statistical modelling. Table 6.5.3.2-1 shows the jitter distribution for the P-trace according to the traces.\nTable 6.5.3.2-14: Jitter distribution deriving from the P-trace files\nFigure 6.5.3.2-1 shows the distribution of the jitter value for different P-traces.\n\nFigure 6.5.3.2-1 The statistic of jitter from the P-trace files\nFrom Table 6.5.3.2-1 and Figure 6.5.3.2-1 the following is observed:\n- \tVR2-1, VR2-2, VR2-3, VR2-4, VR2-5, VR2-6, VR2-7 and VR2-8 share similar jitter distribution with Mean 0ms and STD 5ms in rang [-8, 8] ms, since they all experience the same encoder and transport network.\nNote that the jitter statistical model refers to the description in TR 38.838 [4]. The jitter is modelled as a random variable added on top of periodic arrivals, which follows truncated Gaussian distribution.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.5.3.1-1: Statistical parameter values for dual eye buffer frame size (TR 38.838)",
                                    "table number": 17,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 6.5.3.1-1 Frame size distribution deriving from the P-trace files",
                                    "table number": 18,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 6.5.3.2-14: Jitter distribution deriving from the P-trace files",
                                    "table number": 19,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "6.6\tTest Channel Results",
                    "description": "",
                    "summary": "",
                    "text_content": "In clause 5.8.2, test channels are introduced in order to\n-\tPermit to emulate typical radio conditions in terms delays and losses.\n-\tPermit to evaluate the application quality for different representative radio conditions.\nThe 3GPP radio experts are expected to simulate based on statistical models whereby the statistical models may for example differentiate packet classes, as an example, different radio QoS is associated to different packets.\n3GPP radio experts may ask 3GPP application service experts to evaluate the simulation and evaluation results for different test channels in order to identify the benefit of specific radio settings. For this purpose, 3GPP radio experts should support 3GPP application service experts to define test channels models and configurations. Examples are\n-\tUse Test channel with PLR 0.1% and iid delay between 0 and 20ms and simulate the quality\n-\tUse Test channel with\n-\tPLR 0.1% and iid delay between 0 and 20ms for packets marked with type P\n-\tPLR 0.01% and iid delay between 0 and 20ms for packets marked with type I\n-\tand provide the resulting quality\nIt is agreed\n-\tthat 3GPP application service experts evaluates the quality of different configurations and radio settings through test channels\n-\tthat 3GPP radio experts supports 3GPP application service experts in the development of different test channel models\n-\tthat 3GPP radio experts may ask 3GPP application service experts to evaluate the simulation and evaluation results for different test channels in order to identify the benefit of specific radio settings, if needed.\nTable 6.6-14: traces and configuration for P-file generation\n\nBased on the traces and configuration as available in Table 6.6-1 we have produced some initial results with the following test channel.\n-\t\"loss_rate\": 0.01,\n-\t\"max_delay_ms\": 50,\n-\t\"max_bitrate\": 60000000,\nThe results were produced for 4 users each. We observe that the loss rates and delays may be too high, but it was for an initial plausibility checking.\nThe results are provided here: https://dash.akamaized.net/WAVE/3GPP/XRTraffic/Test-Results/sample-results-2021-03-31-01.zip and include P’, S’, V’, and Q traces as well as inter arrival plots.\nTable 6.6-2 provides the averaged results for the 4 users and for each of the eye buffers.\nTable 6.6-24: traces and configuration for P-file generaration\n\nThe following is observed:\n1.\tThe packet loss rates are too high and in particular the channel latencies are too high to get meaningful results\n-\tThe packet late rate is too high\n-\tIn between 11 to 50% of the slices are lost\n-\tThe Loss and Damaged Area is 70 to 85%\n2.\tThere is a problem with the PSNR for config 7 and 8, which is lower than for lower bitrates. We are investigating this.\n3.\tBest performing is config 6, which does eye buffer interleaving. In this case, the second eye buffer is not blocked in the sending for the right as they are staggered. This is a problem in all other configurations that the second eye buffer causes the packet and slice late losses.\n4.\tWorst performing is the single slice configuration as a single loss results in the loss of an entire frame.\n5.\tThe same packet loss rate for larger packets (i.e. mapped to a slice) results in better quality as for smaller packets (as each loss aggregates later to a slice loss).\n6.\tWe also identified that handling the frame data too complex, so we plan to create a binary pseudo bitstream.\nThe following is concluded:\n1.\tWe are moving into a good direction, but some open issues still\n2.\tWe need to fix the bug on PSNR\n3.\tWe need to use identify good channel parameters for\na.\tPLR = 1e-3, 1e-4, 1e-5\nb.\tLatencies = 10ms, 20ms, 30ms (iid), 10ms (iid) for 98% of packets\nWe need to identify to what extent these configurations are useful.\n",
                    "tables": [
                        {
                            "description": "Table 6.6-14: traces and configuration for P-file generation",
                            "table number": 20,
                            "summary": "",
                            "name": ""
                        },
                        {
                            "description": "Table 6.6-24: traces and configuration for P-file generaration",
                            "table number": 21,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "7\tCloud Gaming",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "7.1\tIntroduction",
                    "description": "",
                    "summary": "",
                    "text_content": "Cloud Gaming according to TR 26.928 refers to the case that rendering of the views is done in the cloud. It differentiates from clause 6 XR Split Rendering in a sense that it is expected to played on a single screen device, e.g. a smartphone or tablet.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.2\tReference System Design",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "7.2.1\tOverview",
                            "text_content": "The system design for cloud gaming is provided in Figure 7.2.1-1 and is similar to what is provided for XR Split rendering.\nThe figure depicts a reference architecture for cloud gaming, illustrating the various components and their interconnections. It includes cloud servers, cloud gaming platforms, and cloud gaming clients, all interconnected through a network infrastructure. The architecture is designed to support high-performance gaming experiences, with a focus on scalability, reliability, and security.\nFigure 7.2.1-1 Cloud Gaming Reference Architecture\nThe cloud scene is rendered in the device. The following assumptions are taken. The Game Server runs a game engine to generate the game scene based on information coming from a gaming end device, and in multi-user cases also what comes from other users. The gaming server rasterizes the viewport and does a scene pre-rendering.\nAccording to Figure 7.2.1-1, the viewport is pre-dominantly rendered in the gaming server, but the device is able to do latest pose correction, for example by asynchronous time-warping (see clause 4.1 of TR26.928) or other pose correction to address changes in the pose.\n-\tGraphics workload is split into rendering workload on a powerful cloud gaming server (in the cloud or the edge) and pose correction (such as ATW) on the gaming device\n-\tLow motion-to-photon latency is preserved via on device Asynchronous Time Warping (ATW) or other pose correction methods.\nThe following call flow highlights the key steps:\n1)\tA gaming device connects to the network and joins a gaming application\na)\tSends static device information and capabilities (supported decoders, viewport)\n2)\tBased on this information, the gaming server sets up encoders and formats\n3)\tLoop\na)\tgaming end device collects pose and controller information\nb)\tpose and controller information is sent to gaming server\nc)\tThe gaming uses the pose to pre-render the gaming viewport\nd)\tthe scene viewport is encoded with 2D media encoders\ne)\tThe compressed media is sent to gaming device along with the pose that it was rendered for\nf)\tThe gaming device decompresses video\ng)\tThe gaming device uses the pose provided with the video frame and the actual pose for an improved prediction using and to correct the local pose, e.g. using ATW.\nAccording to TR 26.928, clause 4.2.2, the relevant processing and delay components are summarized as follows:\n-\tUser interaction delay is defined as the time duration between the moment at which a user action is initiated and the time such an action is taken into account by the content creation engine. In the context of gaming, this is the time between the moment the user interacts with the game and the moment at which the game engine processes such a player response.\n-\tAge of content is defined as the time duration between the moment a content is created and the time it is presented to the user. In the context of gaming, this is the time between the creation of a video frame by the game engine and the time at which the frame is finally presented to the player.\nThe roundtrip interaction delay is therefore the sum of the Age of Content and the User Interaction Delay. If part of the rendering is done on a gaming server and the service produces a frame buffer as rendering result of the state of the content, then for raster-based split rendering (as defined in clause 6.2.5) in cloud gaming applications, the following processes contribute to such a delay:\n-\tUser Interaction Delay (Pose and other interactions)\n-\tcapture of user interaction in game client,\n-\tdelivery of user interaction to the game engine, i.e. to the server (aka network delay),\n-\tprocessing of user interaction by the game engine/server,\n-\tAge of Content\n-\tcreation of one or several video buffers (e.g. one for each eye) by the game engine/server,\n-\tencoding of the video buffers into a video stream frame,\n-\tdelivery of the video frame to the game client (a.k.a. network delay),\n-\tdecoding of the video frame by the game client,\n-\tpresentation of the video frame to the user (a.k.a. framerate delay).\nAs ATW is applied corrections to pose are applied by device internal processing. What determines the network requirements for split rendering is time of pose-to-render-to-photon and the roundtrip interaction delay. According to clause TR 26.928, clause 4.5, the permitted downlink latency is typically 50-60ms.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.2.2\tConsidered Content Formats",
                            "text_content": "Video games have different characteristics that are important to take into account when encoding the rasterized frames produced by the game engine. In TR 26.928 [6], clause 4.2.2, a few different types of games and their interaction delay tolerance are documented. However, TR 26.928 [6] does not differentiate the characteristics of the content. This aspect is addressed in the following.\nIn particular, the following characteristics are important:\n-\tDynamicity of content: how frequent rasterized frames change when compared to previous frame\n-\tComplexity of content: how much content changes between frames and how complex such changes are\n-\tType of content: traditional CGI, photo-realistic CGI or natural images/video\nDepending on these characteristics as well as the interaction delay tolerance, video games can be organized into different categories as document in the remainder of this clause.\nA detailed analysis is provided in TR26.955, clause 6.6.\nThis category includes games such as board games, turn-by-turn strategy games, management/simulation games or non-realtime role-playing games (RPG) in which content may not change over several consecutive frames and changes are typically limited.\nThis category also includes games such as adventure games, casual games, or platform games in which although content may change at every single frame, changes are limited to animation of sprites or simple global movements of the content.\nThe common characteristics of the games in this category is that their playability can support longer interaction delay tolerance (500 – 1000ms according to TR 26.928 [6]) and their content is typically considered to video encode.\nThis category includes games such as fighting games, racing games, real-time strategy (RTS) games or real-time RPGs in which content is very dynamic but changes are either limited or simple transforms.\nThe common characteristics of the games in this category is that their playability requires shorter interaction delay tolerances (100ms according to TR 26.928 [6]) while their content is still considered simple to video encode (with high benefits from prediction coding).\nThis category includes games such as first-person shooters (FPS), Massive Multiplayer Online (MMO) games and racing games in which content is very dynamic with possibly very significant changes regularly in the content.\nThe common characteristics of the games in this category is that their playability requires shorter interaction delay tolerances (100ms according to TR 26.928) and their content is typically considered as complex content to video encode.\nThe main characteristics of the games in this category is that their content is typically considered as more complex content to video encode.\nXR Game content is covered as part of XR Split rendering and not further discussed.\nTable 7.2.2.7-1 provides an overview of the different source signal properties for Online Gaming. This information is used to select proper test sequences.\nTable 7.2.2.7-1 Online Gaming source properties\n\nIn practical considerations, the NVIDIA Encoding functions may be used. The parameters of such an encoder are documented here .\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 7.2.2.7-1 Online Gaming source properties",
                                    "table number": 22,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "7.2.3\tConsidered System Parameters",
                            "text_content": "Based on the discussion on clause 7.2.1 and 7.2.2, several parameters are relevant for the overall system design.\n-\tGame:\n-\tType of game\n-\tstate of game,\n-\tmulti-user actions, etc.\n-\tUser Interaction:\n-\tGame pose by the device,\n-\tGame interactions by controllers\n-\tFormats of rasterized video signal. Typical parameters are:\n-\tSee above\n-\tEncoder configuration\n-\tCodec: H.264/AVC or H.265/HEVC\n-\tBitrate: Bitrate setting to a specific value (e.g. 50 Mbit/s)\n-\tRate control: CBR, Capped VBR, Feedback based, CRF, QP\n-\tSlice settings: 1 per frame, 1 per MB row, X per frame\n-\tIntra settings and error resilience: Regular IDR, GDR Pattern, adaptive Intra, feedback based Intra, feedback based predication and ACK-based, feedback-based prediction and NACK based\n-\tLatency settings: P pictures only, look-ahead units\n-\tComplexity settings for encoder\n-\tContent Delivery\n-\tSlice to IP mapping: Fragmentation\n-\tRTP-based time codes and packet numbering\n-\tRTP/RTCP-based feedback ACK/NACK\n-\tRTP/RTCP-based feedback on bitrate\n-\t5G System/RAN Configuration:\n-\tQoS Settings (5QI): GBR, Latency, Loss Rate\n-\tHARQ transmissions, scheduling, etc.\n-\tContent Delivery Receiver configuration:\n-\tLoss Detection: sequence numbers\n-\tDelay/Latency handling\n-\tError Resilience\n-\tATW\n-\tQuality Aspects\n-\tVideo quality (encoded)\n-\tLost data\n-\timmersiveness\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "7.3\tSimulation System",
                    "description": "",
                    "summary": "",
                    "text_content": "Figure 7.3-1 shows the simulation system used for Cloud gaming based on the architecture in Figure 5.2.1-1. The following instantiation is provided:\n-\tSingle S-Traces is generated to address a video output for a screen\n-\tNo feedback on video encoding or content delivery is considered\nThe figure depicts a cloud gaming simulation system, showcasing a virtual environment with various elements such as a virtual screen, game controllers, and a cloud-based server. The system is designed to simulate real-world gaming experiences, allowing users to interact with the game through the cloud. The figure highlights the importance of cloud gaming in the future of gaming, as it provides a more immersive and accessible experience for users.\nFigure 7.3-1 Cloud Gaming Simulation System\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.4\tRecommended Configurations",
                    "description": "",
                    "summary": "",
                    "text_content": "The following parameters are considered:\n1)\tContent Model:\na.\tGame output with 1920 x 1080 and 4096 x 2048 at 60fps.\nb.\tContent and Trace Preview is here:\ni.\tBaolei HD (to be produced)\nii\tBaolei 4K (to be produced)\nc.\tNo audio\n2)\tEncoding Model\na.\tHEVC encoding model as defined in 5.6\nb.\tPre-encoding delay: Encoder pre-delay is varying between 10 to 20ms\nc.\tEncoding delay is modelled to vary with truncated Gaussian\n3)\tContent Delivery Model\na.\tEdge/Cloud to gNB bitrate is 1.5 media bitrate\n4)\tDelivery receiver\na.\tSlice loss model (1 lost packets per slice results in slice loss)\nb.\tMaximum latency for slice: 80ms (see TR 26.928, clause 4 and 6.2.5.1)\nBeyond the above, the configuration options provided in Table 7.4-1 are recommended with priority according to order.\nTable 7.4-1 Online Gaming source properties\n\nIn total there are 4 configurations. Additional configurations may be added.\nFor uplink modelling, no specific content modelling is considered.\n1)\tA model for the uplink traffic in a similar fashion also providing packet traces.\na.\tGame pose and actions are sent uplink\nb.\tDetails are in clause 5.8.\nc.\tThe uplink bitrate for the pose is 200 kbit/s CBR, with 4ms packet interval and packet size 100 byte. This means that the content is rendered with a pose of typically 10-15ms age.\nd.\tThe E2E Requirements are\ni.\tBitrate: 200 kbit/s\nii.\tPLR: 1e-3\niii.\t10ms\nIn addition, the configurations as documented in clause 6.4 are to be considered.\n",
                    "tables": [
                        {
                            "description": "Table 7.4-1 Online Gaming source properties",
                            "table number": 23,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.5\tTraces and Statistical Models",
                    "description": "",
                    "summary": "",
                    "text_content": "Traces and statistical models for cloud gaming are for further study.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.6\tTest Channel Results",
                    "description": "",
                    "summary": "",
                    "text_content": "Test Channel Results for cloud gaming are for further study.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "8\tAR Conversational",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "8.1\tIntroduction",
                    "description": "",
                    "summary": "",
                    "text_content": "In contrast to the scenarios presented in clause 6 and 7, AR Conversational provides audio and video also in the uplink. A more specific example is the use case 8 in TR 26.928 [3]  (AR guided assistant at remote location (industrial services)), which requires one or two video streams in the uplink. The use case states:\n-\tA XR device with glasses of some sort boasting view-port dependent streaming and split rendering\n-\tThe device has one 2D camera which is used for conveying the local scene to a remote location. (Use case 8 includes depth information but this could be disregarded for the purposes of this study.)\nFor example, it is expected that mobile network operators may configure 5G networks for less capacity in the uplink than the downlink, thus it would be relevant to confirm whether we can expect bottlenecks in the uplink, related to XR use cases. Also, it would be relevant to confirm whether the transmission of time-critical pose information in the uplink is impacted by the additional uplink video traffic.\nAs uplink video may in some use cases happen at the same time as split rendering in the downlink, it is relevant to add an uplink video signal with suitable characteristics to the scenario for AR Conversational.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "8.2\tReference System Design",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "8.2.1\tOverview",
                            "text_content": "In order to re-use an existing architecture, the XR distributed computing architecture in TR 26.928 [3] is applicable. The camera in the XR device may be viewed as a \"sensor\" in this context as shown in Figure 8.2.1-1.\nThe AR Conversational Reference System (AR-CRS) is a distributed compute system designed for XR applications, utilizing AR technology to enhance user engagement and interaction.\nFigure 8.2.1-1 AR Conversational reference system as XR distributed compute system\nThe figure depicts a detailed system design and modeling assumptions for a 5G network, including the use of base stations (gNB), user equipment (UE), and scatterers. The diagram illustrates the multi-path signal propagation in a 5G network, highlighting beamforming techniques to mitigate interference.\nDetailed system design and modeling assumptions are provided in clause 8.3.\nThe figure depicts a conversational reference system, which is a system that allows users to interact with a system through natural language. It is designed to provide a user-friendly interface for users to communicate with the system, allowing them to ask questions, provide feedback, and receive information in a conversational manner. The system is designed to be user-friendly and intuitive, making it easy for users to navigate and interact with the system. The figure shows the different components of the system, such as the user interface, the conversational agent, and the response system, which are all designed to work together to provide a seamless and efficient user experience.\nFigure 8.2.1-1 AR Conversational reference system\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "8.2.2\tConsidered Content Formats",
                            "text_content": "As a starting point consider 4K 60 fps, at 0, 10 and 25 Mbps, based on 2D video as documented in TR 26.928 [3] are considered and provided in Figure 8.2.2-1.\nThe figure depicts various content formats for AR conversational, including 2D and 3D content, as well as AR content formats for different devices and platforms.\nFigure 8.2.2-1 Content formats for AR conversational\nThis is based on the highest profiles in TS 26.118 [6] for VR streaming may be a starting point to derive also the 2D uplink video. In practice the rate would be highly use-case-dependent, it is advisable to select realistic values covering a variety of use case. The considered Content formats and properties are provided in  Table 8.2.2-1.\nTable 8.2.2-1 Considered content formats and properties\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 8.2.2-1 Considered content formats and properties",
                                    "table number": 24,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                },
                {
                    "title": "8.3\tSimulation System",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "8.3.1\tOverview",
                            "text_content": "A simulation overview for downlink and uplink is provided.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "8.3.2\tSimulation Downlink",
                            "text_content": "Downlink simulation model is provided in the following\n1)\tContent Model:\na. \tVideo is identical to split rendering simulation in clause 6.3\nb. \tAudio\ni.\tMax Sampling Rate: 48 kHz\nii.\tInter-frame time: 20-21.3 ms\nNOTE: \tFor the simulation purposes, the inter-frame time can be assumed to be 21.3 ms considering MPEG-H, or if we consider that the actual conversational audio codec might be a different one, we could assume 20 ms, as this has so far been used for several 3GPP speech codecs\nc.\tData Stream\n-\tInter-frame time: 10 ms\n2)\tEncoding Model\na.\tVideo is identical to split rendering simulation in clause 6.3\nb.\tAudio\n-\tOperation Point (following 3GPP TS 26.118 [6] Table 6.1-1): 3GPP MPEG-H Audio (this Operation Point is specified for VR streaming in SA4 and can be used for simulation purposes for conversational services since the IVAS Codec is not yet available)\n-\tAverage data rate : 256 / 512 kbps\n-\tPacket Loss rate should be below 1e-3\nc.\tData Stream\n-\tAverage data Rate : <0.5 Mbps\n-\tPacket Loss rate should be below 1e-3\n3)\tContent Delivery Model\na.\tVideo is identical to split rendering simulation in clause 6.3\nb.\tAudio and data are for further study\n4)\tDelivery receiver\na.\tVideo is identical to split rendering simulation in clause 6.3\nb.\tAudio and data are for further study\n5)\tDecoding Model\na.\tVideo is identical to split rendering simulation in clause 6.3\nb.\tAudio and data are for further study\n6)\tQuality evaluation tool.\na.\tVideo is identical to split rendering simulation in clause 6.3\nb.\tThe following metrics are considered for each user and buffer\n-\tIP Packet loss rate\n-\tIP Packet late rate\n-\tSlice loss rate\n-\tArea loss rate (total amount of Coding Units)\n-\tArea damage rate (total amount of Coding Units)\n-\tAverage encoded PSNR\n-\tAverage PSNR\nc.\tAverage over all buffers\nd.\tMulti-user\n-\tAverage over all users\n-\tPercentile of support\ne.\tAudio and Data are for further study.\n7)\tA model for the uplink traffic in a similar fashion also providing packet traces.\na.\tUplink Pose information\n-\tidentical to split rendering simulation in clause 6.3\nb.\tReverse uplink audio and video encoding see clause 8.3.3\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "8.3.3\tSimulation Uplink",
                            "text_content": "The simulation for the reversed uplink is considered as follows:\n1)\tContent Model:\na.\tVideo\n-\tCamera Signal with 1920 x 1080 at 60fps.\n-\tContent and Trace Preview is for further study\nb.\tAudio\n-\tMax Sampling Rate: 48 kHz\n-\tInter-frame time: 20-21.3 ms\nNOTE: \tFor the simulation purposes, the inter-frame time can be assumed to be 21.3 ms considering MPEG-H, or if we consider that the actual conversational audio codec might be a different one, we could assume 20 ms, as this has so far been used for several 3GPP speech codecs\nc.\tData Stream\n-\tInter-frame time: 10 ms\n2)\tEncoding Model\na.\tVideo: Encoding Models see clause 6.3 with the following\n-\tHEVC, target bitrate 10 Mbit/s (capped VBR) or AVC target bitrate 20 Mbit/s (capped VBR).\n-\tSlice based encoding (4 slices) or 1 frame\n-\tIntra Refresh (1 slice per frame) or every 60th frame.\n-\tPre-encoding delay: Encoder pre-delay is varying between 10 to 20ms\n-\tEncoding delay is modelled to vary with mean 4/slice_numbers and std 3/slice_numbers and maximum being the frame interval (aligned with )\nb.\tAudio\n-\tOperation Point (following 3GPP TS 26.118 [6] Table 6.1-1): 3GPP MPEG-H Audio (this Operation Point is specified for VR streaming in SA4 and can be used for simulation purposes for conversational services since the IVAS_Codec is not yet available)\n-\tAverage data Rate : 256 / 512 kbps\n-\tPacket Loss rate should be below 1e-3\nc.\tData Stream\n-\tAverage data Rate : <0.5 Mbps\n-\tPacket Loss rate should be below 1e-3\n3)\tContent Delivery Model\na.\tVideo is identical to split rendering simulation in clause 6.3\nb.\tAudio and data are for further study\n4)\tDelivery receiver\na.\tVideo is identical to split rendering simulation in clause 6.3\twith following parameters\n-\tPackets are dropped if late\n-\tSlice loss model (1 lost packets per slice results in slice loss)\n-\tTimestamp of slice if time stamp of latest packet of slice\n-\tMaximum latency for slice: 80ms (see TR 26.928 [3], clause 4 and 6.2.5.1)\nb.\tAudio and data are for further study\n5)\tDecoding Model\na.\tVideo is identical to split rendering simulation in clause 6.3\nb.\tAudio and data are for further study\n6)\tQuality evaluation tool.\na.\tVideo is identical to split rendering simulation in clause 6.3\nb.\tThe following metrics are considered for each user and buffer\n-\tIP Packet loss rate\n-\tIP Packet late rate\n-\tSlice loss rate\n-\tArea loss rate (total amount of Coding Units)\n-\tArea damage rate (total amount of Coding Units)\n-\tAverage encoded PSNR\n-\tAverage PSNR\nc.\tAverage over all buffers\nd.\tMulti-user\n-\tAverage over all users\n-\tPercentile of support\nb.\tAudio and data are for further study\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "8.4\tSimulation Framework",
                    "description": "",
                    "summary": "",
                    "text_content": "This work is for further study.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "9\tViewport Dependent VR Streaming",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "9.1\tOverview",
                    "description": "",
                    "summary": "",
                    "text_content": "The system design for viewport dependent 3DoF streaming is basically similar to the discussion and requirements from TR26.928 [3], clause 6.2.3. The architecture is shown in Figure 9.1-1.\n\nThe figure depicts a viewport-dependent streaming defined in TR26.928, which is a standard for defining streaming in the context of a 3D viewport. The viewport-dependent streaming is a technique used to ensure that the streaming process is optimized for the specific viewport being used, which can be important for applications that require high-quality video playback. The figure includes a legend that explains the different types of streaming parameters, such as the number of streams, the frame rate, and the resolution, and how they are defined in the TR26.928 standard.\nFigure 9.1-1 Viewport-dependent Streaming defined in TR26.928\nViewport Dependent 3DoF Streaming refers to the case where the XR server runs a 3DoF XR media generation engine, and the XR device collects local 3DoF tracking information to get the viewport related content to display.\nAccording to Figure 1, the XR media is generated, encoded, and delivered by XR server depending on the adaptive media request sent by the XR device. The device works in two channels. One is to accept the XR media stream and the workflow of decoding, rendering, displaying is followed. Another is to use the tracking sensors to collect 3DoF parameters continuously, and ‘translate’ them into adaptive media request which will be sent to XR server by using 5GS delivery system.\nThe following call flow highlights the key steps:\n1. \tAn XR device connects to the network and joins Viewport Dependent 3DoF Streaming application\n2.\tThe device sends static device information and capabilities to server (supported decoders, display resolution)\n3.\tBased on these information, the XR server sets up encoders and formats, and pre-spilts the XR scene into blocks (e.g. several tiles used in HEVC encoders)\n4.\tLoop\na.\tThe device collects XR 3DoF pose\nb.\tXR pose is converted to media request by the device in order to cover the viewport of user and match the network condition\nc.\tThe XR server responses to these requests and provides the media segments\nd.\tCompressed media is sent to XR device\ne.\tThe device decodes the media content\nf.\tThe device displays the media\nAccording to TR 26.928, clause 6.2.3.5, for viewport dependent streaming, updated tracking and sensor information impacts the network interactivity. Typically, due to updated pose information, HTTP/TCP level information and responses are exchanged every 100-200 ms in viewport-dependent streaming.\nIn actual implementation process, in order to ensure the smooth transmission and viewing, sometimes it is adopted to transmit a channel of background data independent to the viewport in addition to the data described above, so as to replace these data to fill the playback when network condition is poor.\nAttached to this document, please find a paper from IBC 2017 addressing network optimizations for VR Streaming. An author of the paper provided information beyond the paper.\nIt is considered that the model for VDP 3DOF streaming also applies for VDP 6DOF streaming.\nFurther information is provided based on the from the VR-IF Guidelines [7] for 3DoF Viewport dependent Streaming:\n1)\tDownlink data rate ranges\n-\tfor tiled streaming, 4K around 5 Mbit/s, up to 10.\n-\t8K between 10 and 13, but can go under for static scenes and over for dynamic ones.\n-\t5.5K stereo is the same as 8K mono.\n-\tInterestingly, for tiled streaming 360 or 180 make little difference if the viewport is the same.\n2)\tMaximum packet delay budget in uplink and downlink  for mass distribution I don’t care about the uplink much as long as the delay is not too jittery. Relate this to the segment size in an HLS pull – 2 secs is a good target segment size.\n3)\tMaximum Packet Error Rate,  for mass distribution applications, preferably use something reliable, like HLS push or pull or a proprietary reliable transport.\n4)\tMaximum Round Trip Time\n-\tfor viewport-dependent, minimizing round-trip is crucial especially with head motion. That said there are no maximums.\n-\tThere is a degradation as it increases. You should think in the order of frame duration, and know that head motion is about 500 degrees per second max, so 60 msec per tile, and that eyes also take some 60 msec to adapt to a new position, so you have a few frames to work with.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "9.2\tSystem Parameters",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "9.2.1\tContent Delivery Setting",
                            "text_content": "According to 3GPP TS 26.118 [6] and TR 26.923 [3], the tiled stream approach can be used for VR 360 video delivery. It allows emphasizing the current user viewport through transmitting non-viewport samples with decreased resolution, i.e. selecting the tiles from the viewport at a high-resolution version and the tiles that do not belong to the viewport at a lower resolution.\nWith tiled streaming approach, one or multiple decoders are needed, which depends on each of the tiles is encoded as motion-constrained HEVC tiles or separate video streams. When using motion constrained tile HEVC streams, the varying spatial resolution in video picture can be achieved by merging all tiles into a single common bitstream with one decoder needed. When using separately encoded video streams, several decoders are required at the receiver side.\nSimilarly to the separately encoded video streams, GSMA Cloud AR/VR white paper  proposes the Tile Wise Streaming according to MPEG OMAF specification [D]. The tiled based approach with separately encoded video streams are used for Viewport Dependent 3DoF Streaming delivery method. The details can be summarized as below:\n1.\tSplit the original 8K 360° panoramic video into multiple tiles (such as 42 tiles) and generate a low-resolution 360° panoramic video (such as a 4K 360° panoramic video);\n2.\tAll tiles and the low-resolution panoramic video are packaged in DASH using spatial relationship descriptors and distributed via the CDN;\n3.\tThe client player retrieves low-resolution panoramic video and those tiles which are located at the user’s field of view. When the user turns around and causes the field of view to change, the client requests new tiles from the CDN for the new field of view. The multiple tiles would be obtained separately and merged into the FoV-Area pictures in the receiver side.  Because it requires a certain amount of time from when the new tiles are requested until they can be rendered, the low-resolution panoramic video will be displayed until the new tiles can be played.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "9.2.2\tConsidered Content Formats",
                            "text_content": "In this scenario, the vast majority of transmitted media content is 2D or 3D 360 degree video. According to TR 26.928, clause 4.2.1, it is commonly accepted that 2k by 2k per eye provides acceptable quality. Thus the equivalent content resolution will be 6k~8k at 90 ~ 110 degree FOV. Framerate is 60 fps for common cases and 30 fps for some low demand services.\nH.264/AVC Progressive High Profile Level 5.1 with additional restrictions or H.265/HEVC Main-10 Profile Main Tier Profile Level 5.1 are recommended as encoding codecs by TR 26.928 in clause 4.5.1. Network adaptive encapsulation format is usually used to ensure smooth transmission, e.g. MPEG-DASH.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "9.2.3\tConsidered System Parameters",
                            "text_content": "The following system parameters are considered.\n-\tStreaming: Types of streaming: video-on-demand\n-\tFrame rate setting: 30fps\n-\tFormats of transmitted content: ERP, 8K(7680 * 3840) , YUV 4:2:0\n- \tVideo Encoder configuration\n-\tCodec:H.265/HEVC\n-\tRate control: VBR\n-\tTile settings: 42 per frame\n-\tTiles of FoV Area: 18\n-\tComplexity settings for encoder\n-\tSegment Length: 1.067s\n-\tAudio Encoder configuration\n-\tMax Sampling Rate: 48 kHz\n-\tOperation Point (following 3GPP TS 26.118 [6] Table 6.1-1): 3GPP MPEG-H Audio\n-\tContent Delivery\n-\tMPEG-DASH-based media delivery (tiled based approach with separately encoded video streams)\n-\tRAN Configuration :\n-\tQoS Settings (5QI): GBR, Latency, Loss Rate, Mobility, Power Consumption\n-\tReceiver configuration:\n-\tBuffer size: 3s\n-\tE2E Downlink Budget:\n-\t50ms\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "9.2.4\tOutput traffic characteristics",
                            "text_content": "The following encoder output traffic characteristics are considered\n-\tData rate range:\n-\tper tiled streaming: 0.71~1.43 Mbps\n-\tFoV Area Streaming: (0.71~1.43)*18 Mbps\n-\tlow-resolution 4K omnidirectional streaming: 6-8Mbps\n-\tPeriodical segment request: 1s per request\n-\tPer tiled Segment size range: 8000 ~ 180000 byte\n-\tLow resolution 4K omnidirectional segment size range: 848386~1681920 byte\n-\tPacket size distribution: fixed size as 1500 bytes\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "9.2.5\tAdditional Discussion",
                            "text_content": "Figure 9.2.5-1 provides an overview of the system design and delay components involved in viewport-dependent streaming according to the above referred paper.\nThe figure depicts a Tiled VR Streaming system, highlighting the various latency factors that can impact the system's performance. The system is composed of a tiled VR display, a VR headset, and a VR streaming server. The latency factors include the VR headset's latency, the VR streaming server's latency, and the network latency. The figure also includes a visual representation of the system's architecture, showing the connections between the different components.\nFigure 9.2.5-1 Overview of latency factors in a Tiled VR Streaming system (see [7])\nFurthermore, Figure 9.2.5-2 provides the network exchange and caching process.\n\nThe figure depicts a caching process for VR streaming, as referenced in [7]. The process involves the use of caching servers to store and deliver VR content to users, reducing the need for frequent network traffic and improving user experience.\nFigure 9.2.5-2\tCaching Process for VR Streaming (see [7])\nFinally, clause 5 of the attached paper includes a typical set of test conditions. Tests with simulated (i.e., recorded), but significant head motion using 30 frames per second, 8k x 4k panorama encoded using 96 high-resolution tiles. The content was encoded using separate HEVC encoders for each of the tiles, with tile size fixed at 512 x 512 pixels. The bitrate from the edge to the end user was between 12 and 16 Mbit/s.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "9.3\tSystem Design and Simulation Model",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "9.3.1\tInitial Considerations",
                            "text_content": "The simulation model is similar to the one provided in Figure 9.2-1, but traces are not provided.\n1)\tContent Model:\n-\tNot considered\n2)\tEncoding Model\n-\t96 tiles\n-\tHEVC\n3)\tContent Delivery Model\na)\tOptions:\n-\tHTTP/1.1. (HTTP + TCP/IP)\n-\tHTTP3 (HTTP2 + QUIC + UDP)\nb) \t96 concurrent requests every 2 seconds\nc) \t20 in viewport at 500 kbit/s (Gaussian distributed with variance 100 kbit/s)\nd)\t6 background tiles 500 kbit/s (Gaussian distributed with variance 100 kbit/s)\ne)\tPacket size is 1500 byte, provided in sequence\nf)\tServer to gNB bitrate is 1.5 times of the media bitrate\ng)\tPacket Trace settings\n4)\tDelivery receiver\na)\tHTTP1.1 => Object is impacted by the lost packet for each object\n-\tModel needs to be defined based on traces\nb)\tHTTP3 => Object is impacted by the lost packet for each object\n-\tModel needs to be defined based on traces\nd)\tObjects are either lost or delayed\n5)\tDecoding Model\n-\tnone\n6)\tQuality evaluation tool. The following metrics are considered for each user and buffer\na)\tIP Packet loss rate\nb)\tViewport object loss rate\nc)\tNon-viewport object loss rate\nd)\tStreaming Metrics\n-\tVideo stuck rate\n-\tPercentage of video playing stuck time\n-\tMTHR delay: average video time from low definition to high definition (FOV)\n-\tTimes of black edge/shadow/blur when turning head\ne)\tAverage over all buffers\nf)\tMulti-user\n-\tAverage over all users\n-\tPercentile of support\n7)\tA model for the uplink traffic in a similar fashion also providing packet traces.\na)\t26 HTTP requests are sent\nb)\tHTTP/1.1 and HTTP 3 feedback for TCP/IP and QUIC\nc)\tUplink traffic is minimal\nd)\tThe end-to-end RTT delay and packet loss rate recorded by the test under different configurations can also be used as the actual network conditions.\ne)\tInteraction: turning frequency\n-\tLow frequency turned: like every 10s\n-\thigh frequency turned: like every ls\n8)\tPotential RAN Configurations\n-\tto be decided by RAN\n-\tDevices: support QoS Profile. The recommended configuration of 5QI is as follows：\n-\tSince the 5QI have mapping relation to characteristics of 5G QoS, the RAN configuration, such as Packet Delay Budget (PDB) and Packet Error Rate (PER), can be configured according to the value of 5QI. In the TS 23.501, table 5.7.4-1. shows standardized 5QI to QoS characteristics mapping. It should be noted that the PDB value in this table includes AN-PDB and CN-PDB which is a fixed value generally.\nA summary is provided as follows:\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "-\tDevices: support QoS Profile. The recommended configuration of 5QI is as follows：",
                                    "table number": 25,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "A summary is provided as follows:",
                                    "table number": 26,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "9.3.2\tOpen Issues",
                            "text_content": "Among others, the following issues are for further study\n-\tLate and loss model for objects based on HTTP/1.1 and HTTP3 taking into account radio model\n-\tUplink traffic model for objects based on HTTP/1.1 and HTTP3\n-\tDefinition of late threshold\n-\tQuality impact for lost and late objects in viewport and not in viewport\n-\tPacket trace model and mapping.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "10\tXR Distributed Computing",
            "description": "Traffic models XR Distributed computing are for further study.\n\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.1\tIntroduction",
            "description": "Content encoding with different parameters is complex and so is decoding modelling with errors. Hence a simplified model is derived using model encoding and decoding. Different aspects are modelled in the following.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.2\tEncoding modeling",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "A.2.1\tImpact of encoding parameters",
                    "description": "",
                    "summary": "",
                    "text_content": "A version of the video sequence is sent through a model encoder for identifying the impacts of different parameter settings. The following x.265 parameters are used resulting in total with 24 different configurations. The configurations take into account different frame rates, different quality factors, an all intra and an all inter predicted sequence, as well as no slices or 8 slices, as follows:\n./x265 --input input.yuv --input-res 2048x2048 --preset medium --fps [30,60] --crf [22, 28,34] --keyint [-1,1] --slice [1,8] --csv /csv_analfiles/[i,p]_[30,60]_[22,28,34]_[1,8].csv --csv-log-level 2 --log-level full  --rc-lookahead 0 --no-deblock  --bframes 0  --frames 480 --psnr --ssim  --output /output/xxxx.h265\nA summary of the bitrates and resulting quality is provided in Table A.2.1-1 for each of the 24 parameter settings.\nTable A.2.1-1 Bitrates and resulting quality for different configurations\n\nAn analysis of the bitrate decrease per CRF/QP increase is shown in Table A.2.1-2.\nTable A.2.1-2 Bitrate decrease per CRF/QP increase\n\nThe bitrate increase for I-frames vs P-frames for different configurations is shown in Table A.2.1-3.\nTable A.2.1-3 Bitrate decrease per CRF/QP increase\n\nFinally, the bitrate increase for 60fps vs 30fps is shown in Table A.2.1-4.\nTable A.2.1-4 Bitrate increase for 60 fps vs. 30fps\n\nFrom these results some initial conclusions:\n-\tSlices as available in x265 being constrained across pictures are very costly if used without prediction across slice boundaries\n-\tPrediction over 2 P-frames results roughly in factor 2\n-\tRate decrease per CRF/QP for I is consistently 2 for QP increase 6\n-\tRate decrease per CRF/QP for P is consistently between 3 and 4 for QP increase 6\n-\tBitrate increase for I frame coding when doubling frame rate is typically in the range of 2.\n-\tBitrate increase for P frame coding when doubling frame rate is typically in the range of 1.4.\n",
                    "tables": [
                        {
                            "description": "Table A.2.1-1 Bitrates and resulting quality for different configurations",
                            "table number": 27,
                            "summary": "",
                            "name": ""
                        },
                        {
                            "description": "Table A.2.1-2 Bitrate decrease per CRF/QP increase",
                            "table number": 28,
                            "summary": "",
                            "name": ""
                        },
                        {
                            "description": "Table A.2.1-3 Bitrate decrease per CRF/QP increase",
                            "table number": 29,
                            "summary": "",
                            "name": ""
                        },
                        {
                            "description": "Table A.2.1-4 Bitrate increase for 60 fps vs. 30fps",
                            "table number": 30,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "A.2.2\tGlobal Configuration",
                    "description": "",
                    "summary": "",
                    "text_content": "The following global configuration parameters are considered (considered, preferred, ignored)\n-\tBitrate Control (one of the following):\n-\tConstant Bitrate – bitrate & buffer\n-\tFeedback-based Variable Bitrate - dynamic\n-\tConstant Rate Factor – rate factor with CRF (default: CRFref is used)\n-\tSlice Setting (one of the following)\n-\tDefault – no slices\n-\tnumber of slices – number of slices (typical numbers are 4, 8, 16)\n-\tmaximum slice size – number in bytes\n-\tError Resilience (one of the following)\n-\tIntra-refresh frame parameter would be the period (default is no intra refresh)\n-\tIntra-refresh slice: period (1  1 slice every 1 frame with the slice being picked as POC mod #slices ) 2  1 slice every 2 frames)\n-\tFeedback-based\n-\tMode:\n-\tintra refresh: add an intra for the lost slice\n-\tACK-mode: only use acknowledge slices in prediction\n-\tNACK-mode: use an old reference frame or intra in case of loss\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "A.2.3\tDynamic status:",
                    "description": "",
                    "summary": "",
                    "text_content": "Dynamic feedback status may be considered as follows\n-\tMax number of bits for next frame (external rate control)\n-\tframe Number, slice number ACK/NACK/unknown\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "A.2.4\tContent Encoding Modelling",
                    "description": "",
                    "summary": "",
                    "text_content": "The content encoding per frame is modelled as shown in Listing A.2.4-1.\nListing A.2.4-1 Content Encoding per frame\n\n\nThe following issues are not yet included:\n-\tModelling of encoding times – no priority\n-\tModel encoding delay.\n-\ttwo independent buffers for left and right eye need to be addressed.\n-\tfor now create the same process for\n-\tOption 1: Same timing\n-\tOption 2: staggered left right at half the frame rate\n-\tCan a new PSNR model for the updated QP be derived?\n-\tcheck for model\n-\tIn the absence of a model, 1 QP step up reduces PSNR by 1dB (linear)\n-\tWhat about slice modelling and all the issues that we saw\n-\tIgnore slice modelling for now, no bitrate change compared to not using slices.\n-\tAlso ACK/NACK based feedback needs to be addressed.\n-\tNACK already addressed above.\n-\tACK you only take acknowledged slice/frames for references – this basically extends the reference frame the feedback delay per slice.\n",
                    "tables": [
                        {
                            "description": "Listing A.2.4-1 Content Encoding per frame",
                            "table number": 31,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "A.2.5\tRate Control",
                    "description": "",
                    "summary": "",
                    "text_content": "The rate control is modelled as shown in Listing A.2.5-1.\nListing A.2.5-1 Rate Control Modelling\n\n",
                    "tables": [
                        {
                            "description": "Listing A.2.5-1 Rate Control Modelling",
                            "table number": 32,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "A.3\tDecoding modeling",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "A.3.1\tOverview",
                    "description": "",
                    "summary": "",
                    "text_content": "Quality Evaluation is based on two aspects, namely the encoding quality and the quality degradation due to lost slices. This evaluation is shown in Figure A.3.1-1.\nThe figure depicts a content delivery, decoding, and quality modeling system, illustrating the various components involved in the process. It includes a content delivery network (CDN), a decoding network, and a quality modeling network, all interconnected to ensure efficient and reliable delivery of content.\nFigure A.3.1-1\tContent Delivery, decoding and quality modeling\nThe following simulation is defined for identifying damaged macroblocks:\n-\tKeep a state for each macroblock\n-\tDamaged\n-\tCorrect\n-\tMacroblock is damaged\n-\tIf it is part of a slice that is lost for this transmission\n-\tIf it is correctly received, but it predicts from a wrong macroblock\n-\tMacroblock is correct\n-\tIf it is received correctly and it predicts for a non-damaged macroblock\n-\tPredicting from non-damaged macroblock means\n-\tSpatial prediction is correct\n-\tTemporal prediction is correct\n-\tRecovering MBs done by Intra Refresh and predicting from correct MBs again.\nDepending on the configuration and the setting of the delivered video quality, different results may be obtained. An example is shown in Figure A.3.1-2.\nA quality threshold may for example be to have at most 0.1 % of damaged video area. Also the quality of the original content may be a threshold. Details are ffs.\nThe given telecommunication figure, labeled as \"Figure A.3.1-2,\" depicts a potential evaluation graph for different configurations. The figure illustrates the potential impact of various factors on the performance of a communication system, such as the number of users, the distance between users, and the type of network infrastructure. The graph is designed to help network operators and engineers make informed decisions about the optimal configuration for their network.\nFigure A.3.1-2\tPotential evaluation graph for different configurations\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "A.3.2\tSlice Recovery",
                    "description": "",
                    "summary": "",
                    "text_content": "At the recovery the following happens for recovering RTP/IP packet traces:\n-\tif one packet of a slice is lost the entire slice is lost\n-\tthe delay of the latest packet determines the arrival time of the slice.\n-\tDump the following information:\n-\tSlice Timing/frame count\n-\tLeft or right eye\n-\tSlice availability (after the slice timing) relative to 0.\n-\tThe time it took through system\n-\tIt can also be infinite = lost\n-\tQuality/QPnew\n-\tNew PSNR – add a function\n-\tSlice size\n-\tSlice type\n-\tCTU types\nThe result is a slice delay trace\n-\tSlice Timing/frame count\n-\tLeft or right eye\n-\tSlice availability (after the slice timing) relative to 0.\n-\tThe time it took through system\n-\tIt can also be infinite = lost\n-\tQuality/QPnew\n-\tNew PSNR – add a function\n-\tSlice size\n-\tSlice type\n-\tCTU types\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "A.3.3\tQuality Measurement",
                    "description": "",
                    "summary": "",
                    "text_content": "The quality measurement algorithm is provided in Listing A.3.3-1.\nListing A.2.5-1 Quality Measurement Algorithm\n\n",
                    "tables": [
                        {
                            "description": "Listing A.2.5-1 Quality Measurement Algorithm",
                            "table number": 33,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "A.3.4\tVideo Decoding and Reconstruction",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "A.3.4.1\tOverview",
                            "text_content": "Video decoding and reconstruction primarily addresses to identify areas of the image that are correct and those that are damaged. It also includes the encoding quality of correctly received images.\nThe reconstruction quality evaluation is based on two aspects, namely the encoding quality and the quality degradation due to lost and late packets. This evaluation is shown in Figure A.3.4.1-1.\n\nVideo decoding is based on input from S’-Trace and S-Trace, resulting in V’-Trace.\nThe following simulation is proposed for identifying damaged CUs:\n-\tKeep a state for each CU\n-\tDamaged\n-\tCorrect\n-\tCU is damaged\n-\tIf it is part of a slice that is lost for this transmission\n-\tIf it is correctly received, but it predicts from a wrong CU\n-\tCU is correct\n-\tIf it is received correctly and it predicts for a non-damaged CU\n-\tPredicting from non-damaged CU means\n-\tSpatial prediction is correct\n-\tTemporal prediction is correct\n-\tRecovering CU done by Intra Refresh and predicting from correct CUs again.\nDetailed modelling is provided in clause A.3.4.3.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "A.3.4.2\tConfiguration",
                            "text_content": "No configuration is applied, but the input parameters are the S and S’-Trace.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "A.3.4.3\tModelling",
                            "text_content": "Input information\n-\tS-Trace\n-\tSlice index\n-\tSlice metadata for reconstruction\n-\tFor every CU\n-\tSize\n-\tMode\n-\tReference\n-\tQuality/QPnew\n-\tPSNR/PSNRnew\n-\tS’-Trace\n-\tSlice index\n-\tSlice availability time\n-\tRecovered slice position (0 => lost, in between, full)\nRun the following algorithm:\n-\tInput parameters.\n-\tParameters of source\n-\tResolution\n-\treference frames\n-\tS’-Trace\n-\tCreate a map of slices, CU maps (64 x 64) and reference frames\n-\tExample: 2048 x 2048, 8 slices, 3 reference frames\n-\tAddresses for 2048 / (8 * 64) = 4 rows with 32 CUs for 8 slices in 3 frames maintained.\n-\tFor each CU of each frame, store mode:\n-\tCorrect\n-\tDamaged\n-\tUnavailable\n-\tInitialize all CUs as unavailable\n-\tFor each frame i\n-\tGet all slices from trace for the frame\n-\tFor all slices that are lost\n-\tMark all CUs as unavailable\n-\tIndicate the slice loss for feedback\n-\tFor all slices are received\n-\tIndicate the slice received for “feedback”\n-\tIf it is an intra CU, mark it correct\n-\tIf it is an inter CU and it references a damaged or unavailable CU, mark it as damaged, otherwise mark it as correct\n-\tReferencing is determined as follows (note a better model may be developed in the future)\n-\tThe CU in the new frame references the CU at the same position in the referencing frame is 100%\n-\tThe probability of referencing a neighbouring CU top/bottom/left/right is 50%\n-\tThe probability of referencing a neighbouring CU is 50%, if one of the two top/botton/left/right is referenced, and 100% if both are referenced, and is 0% if none are referenced.\n-\tCompute the totally unavailable and damaged CUs in this frame\n-\tCompute the average PSNR for this frame\n-\tavPSNR = PSNR * correctCUs/totalCUs + PSNRwrong (1- correctCUs/totalCUs) with PSNRwrong = 0\n-\tRun this independently for each eye buffer\n-\tDump this information into a V’-Trace according to format in 5.6.4.\n-\tAdd the availability time stamp\n\nThe figure A.3.4 depicts the probability of wrong CU (Customer Unit) propagation interception, showing the probability of an interception event occurring at a specific CU location. The figure illustrates the probability distribution of interception events, with the x-axis representing the probability of an interception event occurring at a specific CU location, and the y-axis representing the probability of an interception event occurring at a different CU location. The figure provides a visual representation of the interception probability distribution, allowing for a better understanding of the interception probability at different CU locations.\nFigure A.3.4.3-1 Probability for wrong CU propagation inter\nThe figure A.3.4. 3 - 2 depicts the probability for wrong CU (Customer Unit) propagation within a 3GPP network. The figure shows the probability distribution of the probability of a wrong CU propagation within a 3GPP network, with the probability of a wrong CU propagation being 3 - 2. This figure is used to understand the probability of a wrong CU propagation within a 3GPP network and to design and implement network security measures.\nFigure A.3.4.3-2 Probability for wrong CU propagation intra\n\n\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "B.1\tIntroduction",
            "description": "In order to have some insights into realistic traffic models for cloud gaming and XR, measurements are derived traffic models from different gaming platforms such as Stadia ™, GEFORCE Now ™, PS Now ™ and different game types such as role player games or racing. It is considered to use rendering in the edge (with the operators network) and rendering in the cloud with a public Internet connection.\nThe following assumptions where taken for a cloud gaming:\n-\tPlatform: Google Stadia\n-\tGame: misc. e.g. Grid (Racing)\n-\tFrame rate: 60fps\n-\tScreen resolution: 4K\n-\tCapturing packets from Router\n-\tWiFi AP: .11ac, 5GHz, 80MHz, 867Mbps\n-\tWireline: Data rate: 70Mbps DL / 20Mbps UL\n-\tPing: 20ms\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "B.2\tResults",
            "description": "From the results of the measurement a bursty traffic model is obtained with burst periodicity of indirectly proportional to the frame rate of the video content. Packets are of fixed size, with almost fixed inter-packets arrival time. The burst Length follows truncated Gaussian distribution.\nThe figure B.2-1 illustrates a traffic model for a 5G network, depicting the various types of traffic and their respective paths. The model includes base stations (BSs), user equipment (UE), and scatterers, with different types of traffic such as voice, video, and data. The figure highlights the importance of beamforming techniques to mitigate interference and ensure reliable communication.\nFigure B.2-1 Illustration of Traffic model\nFigure B.2-2 provides the results for different games on different platforms in terms of throughput, downlink packet sizes and burst lengths for:\n-\tCAT-B: High dynamicity & low/med complexity games\n-\tCAT-C: High dynamicity & high complexity games\nThe figure B.2 presents a comparison of the throughput, packet size, and burst length for different games and platforms. It highlights the impact of network congestion on the performance of these games, with a focus on the burst length. The figure illustrates the importance of network optimization and capacity planning in ensuring smooth gameplay for gamers.\nFigure B.2-2Statistics for different games and platforms: Throughput, packet size, burst length\nFigure B.2-3 provides the results for different games on different platforms in terms of Inter arrival times (IAT), jitter, and burst IAT.\nThe figure B.2 presents statistics for different games and platforms, including Inter arrival times (IAT), jitter, and burst IAT. IAT refers to the time it takes for a packet to be transmitted and received, while jitter and burst IAT represent the variability in the timing of packets. The figure illustrates the impact of these metrics on the performance of the network, highlighting the importance of optimizing these parameters for optimal gaming experiences.\nFigure B.2-3Statistics for different games and platforms: Inter arrival times (IAT), jitter, burst IAT\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "B.3\tDerived Traffic Models",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "B.3.1\tTraffic Models",
                    "description": "",
                    "summary": "",
                    "text_content": "Based on the measurement and results, the following traffic models are proposed for cloud-gamin:\n-\tPacket Size & Traffic Shape\n-\tFixed packet size. E.g. 1500 bytes\n-\tBursty (Segmentation based on Ethernet MTU size limit)\n-\tArrival time\n-\tInter-burst period of 1/FR sec\n-\tBurst length follows truncated Gaussian distribution and depends on channel conditions and games/XR req.\n- \tJitter\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.3.2\tParameters",
                    "description": "",
                    "summary": "",
                    "text_content": "Figure B.3.2-1 provides recommended parameters for cloud-gaming traffic\nThe figure B.3.2-1 presents recommended parameters for cloud-gaming traffic, highlighting the importance of network performance and security in this high-demand application.\nFigure B.3.2-1 recommended parameters for cloud-gaming traffic\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "B.3.3\tJitter Modelling",
                    "description": "",
                    "summary": "",
                    "text_content": "Jitter is the effect of variable packet delays observed in the system. Reasons of jitter\n-\tXR Server at the Cloud (outside 3GPP network): Variation in frame encoding time, Network routing and congestion, gNB buffering latency\n-\tXR server at the Edge (within 3GPP network): Network routing and gNB buffering latency\nJitter affects QoE, i.e. results indicate that, even for slight variations of the underlying jitter distribution, the QoE distribution shows significant variations. Jitter can lead to high power consumption due to uncertainty in the arrival time of the periodic traffic could impact the DRX operation affecting power consumption. Edge compute may reduce baseline latency, but congestion in access still causes some jitter.\n\n\n",
                    "tables": [
                        {
                            "description": "",
                            "table number": 34,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        }
    ]
}