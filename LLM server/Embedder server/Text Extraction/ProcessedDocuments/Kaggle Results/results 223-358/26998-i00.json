{
    "document_name": "26998-i00.docx",
    "content": [
        {
            "title": "Foreword",
            "description": "This Technical Report has been produced by the 3rd Generation Partnership Project (3GPP).\nThe contents of the present document are subject to continuing work within the TSG and may change following formal TSG approval. Should the TSG modify the contents of the present document, it will be re-released by the TSG with an identifying change of release date and an increase in version number as follows:\nVersion x.y.z\nwhere:\nx\tthe first digit:\n1\tpresented to TSG for information;\n2\tpresented to TSG for approval;\n3\tor greater indicates TSG approved document under change control.\ny\tthe second digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, etc.\nz\tthe third digit is incremented when editorial only changes have been incorporated in the document.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "Introduction",
            "description": "Augmented Reality (AR) and Mixed Reality (MR) promise to provide new experiences for immersive media services. The form factors of the devices for these services are typically not expected to deviate significantly from those of typical glasses, resulting in less physical space for the various required components such as sensors, circuit boards, antennas, cameras, and batteries, when compared with typical smartphones. Such physical limitations also reduce the media processing and communication capabilities that may be supported by AR/MR devices, in some cases requiring the devices to offload certain processing functions to a tethered device and/or a server.\nThis report addresses the integration of such new devices into 5G system networks and identifies potential needs for specifications to support AR glasses and AR/MR experiences in 5G.\nThe focus of this document is on general system aspects, especially targeting visual rendering on glasses, and may not be equally balanced or equally precise on all media types (e.g. on haptics, GPUs).\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "1\tScope",
            "description": "The present document collects information on glass-type AR/MR devices in the context of 5G radio and network services. The primary scope of this Technical Report is the documentation of the following aspects:\n-\tProviding formal definitions for the functional structures of AR glasses, including their capabilities and constraints,\n-\tDocumenting core use cases for AR services over 5G and defining relevant processing functions and reference architectures,\n-\tIdentifying media exchange formats and profiles relevant to the core use cases,\n-\tIdentifying necessary content delivery transport protocols and capability exchange mechanisms, as well as suitable 5G system functionalities (including device, edge, and network) and required QoS (including radio access and core network technologies),\n-\tIdentifying key performance indicators and quality of experience factors,\n-\tIdentifying relevant radio and system parameters (required bitrates, latencies, loss rates, range, etc.) to support the identified AR use cases and the required QoE,\n-\tProviding a detailed overall power analysis for media AR related processing and communication.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "2\tReferences",
            "description": "The following documents contain provisions which, through reference in this text, constitute provisions of the present document.\n-\tReferences are either specific (identified by date of publication, edition number, version number, etc.) or non-specific.\n-\tFor a specific reference, subsequent revisions do not apply.\n-\tFor a non-specific reference, the latest version applies. In the case of a reference to a 3GPP document (including a GSM document), a non-specific reference implicitly refers to the latest version of that document in the same Release as the present document.\n[1]\t3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".\n[2]\t3GPP TR 26.928: “Extended Reality (XR) in 5G”\n[3]\tWireless Broadband Alliance, “5G and Wi-Fi RAN Convergence”, April 2021.\n[4]\tKhronos Group, The OpenXR Specification, 1.0,\n[5]\tW3C, WebXR Device API, W3C Working Group Draft,\n[6]\tISO/IEC 23090-13:2022 DIS: “Information technology — Coded representation of immersive media — Part 13: Video Decoding Interface for Immersive Media”\n[7]\tMiscrosoft Azure KinectTM DK documentation,\n[8]\tGoogle ARCoreTM: Use Depth in your Android app,\n[9]\tMicrosoft Azure KinectTM DK documentation: Use Azure Kinect Sensor SDK image transformations,\n[10]\tDaniel Wagner, Louahab Noui, Adrian Stannard, \"Why is making good AR displays so hard?\", LinkedIn Blog, August 7, 2019,\n[11]\tDaniel Wagner, \"MOTION TO PHOTON LATENCY IN MOBILE AR AND VR\", Medium Blog, August 20, 2018,\n[12]\tYodayoda, \"Why loop closure is so important for global mapping\", Medium Blog, December 24, 2020,\n[13]\t3GPP TS 22.261: “Service requirements for the 5G system”\n[14]\t3GPP TR 22.873: “Study on evolution of the IP Multimedia Subsystem (IMS) multimedia telephony service”\n[15]\t3GPP TS 26.114: “IP Multimedia Subsystem (IMS); Multimedia telephony; Media handling and interaction”\n[16]\t3GPP TR 38.838: “Study on XR (Extended Reality) evaluations for NR”\n[17]\tISO/IEC 23090-2:2021: “Information technology — Coded representation of immersive media — Part 2: Omnidirectional media format”\n[18]\tISO/IEC 23090-3:2021: “Information technology — Coded representation of immersive media — Part 3: Versatile video coding”\n[19]\tISO/IEC 23090-5:2021: “Information technology — Coded representation of immersive media — Part 5: Visual volumetric video-based coding (V3C) and video-based point cloud compression (V-PCC)”\n[20]\tISO/IEC 23090-8:2020: “Information technology — Coded representation of immersive media — Part 8: Network based media processing”\n[21]\tETSI GS ISG ARF 003 v1.1.1 (2020-03): “Augmented Reality Framework (ARF) AR framework architecture”\n[22]\tKhronos Group, The GL Transmission Format (glTF) 2.0 Specification,\n[23]\tISO/IEC 23090-14:2022: “Information technology — Coded representation of immersive media — Part 14: Scene Description for MPEG-I Media”\n[24]\tISO/IEC 23090-10:2021: “Information technology — Coded representation of immersive media — Part 10: Carriage of Visual Volumetric Video-based Coding Data”\n[25]\tISO/IEC 23090-18:2021: “Information technology — Coded representation of immersive media — Part 18: Carriage of Geometry-based Point Cloud Compression Data”\n[26]\t3GPP TS 26.501: “5G Media Streaming (5GMS); General description and architecture”\n[27]\tH. Chen, Y. Dai, H. Meng, Y. Chen and T. Li, \"Understanding the Characteristics of Mobile Augmented Reality Applications,\" 2018 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), 2018, pp. 128-138.\n[28]\tS. Kang, H. Choi, “Fire in Your Hands: Understanding Thermal Behavior of Smartphones”, The 25th Annual International Conference on Mobile Computing and Networking (MobiCom '19)\n[29]\tT. Chihara, A. Seo, “Evaluation of physical workload affected by mass and center of mass of head-mounted display”, Applied Ergonomics, Volume 68, pp. 204-212, 2018\n[30]\tGoogle Draco:\n[31]\tT.Ebner, O.Schreer, I. Feldmann, P.Kauff, T.v.Unger, “m42921 HHI Point cloud dataset of boxing trainer”, MPEG 123rd meeting, Ljubljana, Slovenia\n[32]\tScene understanding,\n[33]\tSerhan Gül, Dimitri Podborski, Jangwoo Son, Gurdeep Singh Bhullar, Thomas Buchholz, Thomas Schierl, Cornelius Hellge, “Cloud Rendering-based Volumetric Video Streaming System for Mixed Reality Services”, Proceedings of the 11th ACM Multimedia Systems Conference (MMSys'20), June 2020\n[34]\tScene lighting:\n[35]\tPBR material:\n[36]\tColour Material:\n[37]\tS. N. B. Gunkel, H. M. Stokking, M. J. Prins, N. van der Stap, F.B.T. Haar, and O.A. Niamut, 2018, June. Virtual Reality Conferencing: Multi-user immersive VR experiences on the web. In Proceedings of the 9th ACM Multimedia Systems Conference (pp. 498-501). ACM.\n[38]\tDijkstra-Soudarissanane, Sylvie, et al. \"Multi-sensor capture and network processing for virtual reality conferencing.\" Proceedings of the 10th ACM Multimedia Systems Conference. 2019.\n[39]\tVRTogether, a media project funded by the European Commission as part of the H2020 program, , November 2020.\n[40]\tMPEG131 Press Release: Point Cloud Compression – WG11 (MPEG) promotes a Video-based Point Cloud Compression Technology to the FDIS stage:\n[41]\t3GPP TR 23.701: “Study on Web Real Time Communication (WebRTC) access to IP Multimedia Subsystem (IMS); Stage 2”\n[42]\t3GPP TR 23.706: “Study on enhancements to Web Real Time Communication (WebRTC) access to IP Multimedia Subsystem (IMS); Stage 2”\n[43]\t3GPP TS 24.371: “Web Real-Time Communications (WebRTC) access to the IP Multimedia (IM) Core Network (CN) subsystem (IMS); Stage 3; Protocol specification”\n[44]\tIETF RFC 8831: WebRTC Data Channels\n[45]\tIETF RFC 8864: Negotiation Data Channels Using the Session Description Protocol (SDP)\n[46]\tIETF RFC 8827: WebRTC Security Architecture\n[47]\tISO/IEC 23090-6:2021: “Information technology — Coded representation of immersive media — Part 6: Immersive media metrics”\n[48]\t3GPP TR 26.926: “Traffic Models and Quality Evaluation Methods for Media and XR Services in 5G Systems”\n[49]\tOscar Falmer: “AR Headsets Landscape”, 10th September 2021,\n[50]\tOscar Falmer: “Mobile AR Features Landscape”, 20th September 2021,\n[51]\tMicrosoft, Coordinate Systems,\n[52]\tGoogle WebRTC project update & Stadia review,\n[53]\tJoint MPEG/Khronos/3GPP Workshop on \"Streamed Media in Immersive Scene Descriptions\", September 29/30, 2021,\n[54]\tMPEG Systems Output WG3 N21042 \"Report of Joint Workshop on Streamed Media in Immersive Scene Descriptions\", MPEG#136, October 2021,\n[55]\t3GPP TS 23.501: “System architecture for the 5G System (5GS)”\n[56]\t3GPP TS 26.260: “Objective test methodologies for the evaluation of immersive audio systems”\n[57]\t3GPP TS 26.261: “Terminal audio quality performance requirements for immersive audio services”\n[58] \tYounes, Georges, et al. \"Keyframe-based monocular SLAM: design, survey, and future directions.\" Robotics and Autonomous Systems 98 (2017): 67-88.\n[59] \tDavid G. Lowe, \"Distinctive Image Features from Scale-Invariant Keypoints\".\n[60] \tHerber Bay et. al., \" SURF: Speeded Up Robust Features”.\n[61] \tE. Rublee, V. Rabaud, K. Konolige and G. Bradski, \"ORB: An efficient alternative to SIFT or SURF,\" 2011 International Conference on Computer Vision, 2011, pp. 2564-2571, doi: 10.1109/ICCV.2011.6126544.\n[62]\tETSI GS ISG ARF 004 v1.1.1 (2020-08): “Augmented Reality Framework (ARF) Interoperability Requirements for AR components, systems and services”\n[63]\tMicrosoft Flight Simulator,\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "3\tDefinitions, symbols and abbreviations",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "3.1\tDefinitions",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the terms and definitions given in 3GPP TR 21.905 [1] and the following apply. A term defined in the present document takes precedence over the definition of the same term, if any, in 3GPP TR 21.905 [1].\n5G AR/MR media service enabler: A 5G AR/MR media service enabler is supporting an AR/MR application to provide AR/MR experience using at least partially 5G System tools.\n5G System (Uu): Modem and system functionalities to support 5G-based delivery using the Uu radio interface.\nAR/MR Application: a software application that integrates audio-visual content into the user’s real-world environment.\nAR/MR content: AR/MR content consists of a scene with typically one or more AR objects and is agnostic to a specific service.\nAR Data: Data generated by the AR Runtime that is accessible through API by an AR/MR application such as pose information, sensors outputs, and camera outputs.\nAR Media Delivery Pipeline: pipeline for accessing AR scenes and related media over the network.\nAR/MR object: An AR/MR object provides a component of an AR scene agnostic to a renderer capability.\nAR Runtime: a set of functions that interface with a platform to perform commonly required operations such as accessing controller/peripheral state, getting current and/or predicted tracking positions, general spatial computing, and submitting rendered frames to the display processing unit.\nLightweight Scene Manager: A scene manager that is capable to handle a limited set of 3D media and typically requires some form of pre-rendering in a network element such as the edge or cloud.\nMedia Access Function: A set of functions that enables access to media and other AR related data that is needed in the scene manager or AR Runtime in order to provide an AR experience.\nNOTE:\tIn the context of this report, the Media Access function typically uses 5G system functionalities to access media.\nPeripherals: The collection of sensors, cameras, displays and other functionalities on the device that provide a physical connection to the environment.\nScene Manager: a set of functions that supports the application in arranging the logical and spatial representation of a multisensorial scene with support of the AR Runtime.\nSimplified Entry Point: An entry point that is generated by 5G cloud/edge processes to support offloading processing workloads from UE by lowering the complexity of the AR/MR content.\nSpatial Computing: AR functions which process sensor data to generate information about the world 3D space surrounding the AR user.\nXR Spatial Description: a data structure describing the spatial organisation of the real world using anchors, trackables, camera parameters and visual features.\nXR Spatial Compute Pipeline: pipeline that uses sensor data to provide an understanding of the physical space surrounding the device and uses XR Spatial Description information from the network.\nXR Spatial Compute server: an edge or cloud server that provides spatial computing AR functions.\nXR Spatial Description server: a cloud server for storing, updating and retrieving XR Spatial Description.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.2\tSymbols",
                    "description": "",
                    "summary": "",
                    "text_content": "Void\n\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.3\tAbbreviations",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the abbreviations given in 3GPP TR 21.905 [1] and the following apply. An abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in 3GPP TR 21.905 [1].\n5GMS\t5G Media Streaming\nAAC\tAdvanced Audio Coding\nAF\tApplication Function\nAGW\tAccess GateWay\nAPI\tApplication Programming Interface\nAR\tAugmented Reality\nARF\tAugmented Reality Framework\nAS\tApplication Server\nATIAS\tTerminal Audio quality performance and Test methods for Immersive Audio Services\nATW\tAsynchronous Time Warp\nAVC\tAdvanced Video Coding\nBLE\tBluetooth Low Energy\nBMFF\tBased Media File Format\nBoW\tBag-Of-visual-Words\nCAD\tComputer Aided Design\nCGI\tComputer Generated Imagery\nCMAF\tCommon Media Application Format\nCoM\tCentre of Mass\nCPU\tCentral Processing Unit\nCSCF\tCall Session Control Function\nDASH\tDynamic Adaptive Streaming over HTTP\nDC\tData Channel\nDCMTSI\tData Channel Multimedia Telephony Service over IMS\nDIS\tDraft International Standard\nDoF\tDegree of Freedom\nDRX\tDiscontinuous Reception\nDTLS\tDatagram Transport Layer Security\nEAS\tEdge Application Server\nEDGAR\tEDGe-Dependent AR\nEEL\tEnd-to-End Latency\neMMTEL\tEvolution of IMS Multimedia Telephony Service\nEMSA\tStreaming Architecture extensions for Edge processing\nERP\tEquirectangular Projection\nEVC\tEssential Video Coding\nFDIS\tFinal Draft International Standard\nFFS\tFor Future Study\nFLUS\tFramework for Live Uplink Streaming\nFoV\tField of View\nFPS\tFrame Per Second\nG-PCC\tGeometry-based Point Cloud Compression\nGBR\tGuaranteed Bit Rate\nglTF\tGraphics Library Transmission Format\nGPS\tGlobal Positioning System\nGPU\tGraphics Processing Unit\nHDCA\tHigh Density Camera Array\nHEVC\tHigh Efficiency Video Coding\nHLS\tHTTP Live Streaming\nHMD\tHead-Mounted Display\nHOA\tHigher-Order Ambisonics\nHTML\tHyperText Markup Language\nHTTP\tHyperText Transfer Protocol\nICE\tInteractive Connectivity Establishment\nIMS\tIP Multimedia Subsystem\nIMU\tInertial Measurement Unit\nISG\tIndustry Specification Group\nISOBMFF\tISO Based Media File Format\nITT4RT\tImmersive Teleconferencing and Telepresence for Remote Terminals\nIVAS\tImmersive Voice and Audio Services\nJPEG\tJoint Photographic Experts Group\nJSON\tJavaScript Object Notation\nKPI\tKey Performance Indicator\nLI\tLawful Interception\nLIDAR\tLIght Detection And Ranging\nLSR\tLate Stage Reprojection\nMAF\tMedia Access Function\nMBS\tMulticast and Broadcast Services\nMIoT\tMobile Internet of Things\nMMT\tMPEG Media Transport\nMPD\tMedia Presentation Description\nMPR\tMaximum Power Reduction\nMR\tMixed Reality\nMRF\tMultimedia Resource Function\nMSH\tMedia Session Handler\nMTSI\tMultimedia Telephony Service over IMS\nNAT\tNetwork Address Translation\nNBMP\tNetwork Based Media Processing\nOHMD\tOptical Head Mount Display\nOMAF\tOmnidirectional MediA Format\nOpenGL\tOpen Graphics Library\nOTT\tOver-The-Top\nPBR\t\tPhysically-Based Rendering\nPCC\tPoint Cloud Compression\nPCF\tPolicy Control Function\nPDU\tProtocol Data Unit\nPLY\tPoLYgon file format\nPRACK\tProvisional Response Acknowledgement\nRAN\tRadio Access Network\nRFC\tRequest For Comments\nRP\tReference Point\nRTC\tReal-Time Communication\nRTP\tReal-time Transport Protocol\nSDK\tSoftware Development Kit\nSDP\tSession Description Protocol\nSIFT\tScale Invariant Feature Transform\nSID\tStudy Item Description\nSIP\tSession Initiation Protocol\nSLAM\tSimultaneous Localization And Mapping\nSRTCP\tSecure Real-time Transport Control Protocol\nSRTP\tSecure Real-time Transport Protocol\nSSE\tServer-Sent Events\nSTAR\tSTandalone AR\nSTUN\tSession Traversal Utilities for NAT\nSURF\tSpeeded Up Robust Features\nTCP\tTransmission Control Protocol\nTLS\tTransport Layer Security\nToF\tTime of Flight\nTPU\tTensor Processing Unit\nTURN\tTraversal Using Relays around NAT\nUE\tUser Equipment\nUSB\tUniversal Serial Bus\nV3C\tVisual Volumetric Video-based Coding\nV-PCC\tVideo-based Point Cloud Compression\nVDE\tVideo Decoding Engine\nVDI\tVideo Decoding Interface\nVPU\tVision Processing Unit\nVVC\tVersatile Video Coding\nWebRTC\tWeb Real-Time Communication\nWLAR\tWireLess tethered AR\nWTAR\tWired Tethered AR\nXHR\tXML HTTP Request\nXMPP\teXtensible Messaging and Presence Protocol\nXR\t\teXtended Reality\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "4\tIntroduction to Glass-type AR/MR Devices",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "4.1\tGeneral",
                    "description": "",
                    "summary": "",
                    "text_content": "After a series of feasibility studies and normative work on Virtual Reality (VR), the feasibility study on eXtended Reality (XR) in 5G (FS_XR5G), documented in TR 26.928 [2], analysed the frameworks for eXtended Reality (XR) as a conceptual umbrella for representing the virtual, augmented, and mixed realities in a consistent fashion. This study, 5G glass-type Augmented Reality (AR) / Mixed Reality (MR) classified the XR devices into different form factors, including AR glasses, also referred to as optical head-mounted displays (OHMDs) and pointed out their power and tethering issues with the sidelink. A key aspect of this study is to identify the details of AR glasses including the capabilities for communication, processing, media handling, and offloading of power consumption.\nAugmented reality composites virtual objects with reality. The compositing is a combination of light from real world and light presented on display to make them visible together to a user's eyes. Challenges in AR include the prediction of the real-world image visible to the human eye, and the accurate positioning and presentation of the virtual image on the display of AR glasses. To display a virtual image in front of a user's eyes, an optical see-through display is installed between the real world and the user's eyes, typically and most conveniently done with AR glasses.\nIn order to track the real-world space in which to place the virtual objects, sensors and in particular cameras are required to capture a live-action image visible to the human eye. Typically, multiple sensors and cameras are needed to construct a three-dimensional geometry around the user, called the user’s geometry. The perception of geometry and the mapping of AR glass in geometry is referred to Simultaneous Localization and Mapping (SLAM), also introduced in some details in TR 26.928.\nWhen AR objects are placed in the user’s geometry, these objects are anchored to a part of the real-world geometry. With users moving, maintaining augmentation and consistency between reality and the user's geometry is challenging and requires continuous data flows and processing. In order to support devices with low power consumption, split processing such as split rendering or split perception are technologies to offload processing to powerful network servers. Such split processing approaches are considered beneficial or even essential for satisfying AR experiences, but add new necessary processes and challenges. This encoding, transmission, decoding, correction of rendering data and sensor/camera data over 5G networks, altogether pose challenges on bitrates, reliability requirements and latencies.\nBased on the findings in clause 8 of TR 26.928, this clause follows up on some parts of the conclusions and proposed short term actions:\n-\tDevelop a flexible XR centric device reference architecture as well as a collection of device requirements and recommendations for XR device classes based on the considerations in clause 7.2 of TR 26.928.\n-\tStudy detailed functionalities and requirements for glass-type AR/MR UEs with standalone capabilities according to clause 7.6 of TR 26.928 and address exchange formats for AR centric media, taking into account different processing capabilities of AR devices.\nThree different types of device reference architectures are identified in this clause. One major distinction among these types is the device capabilities of whether stand-alone processing of required AR media processing (in clause 4.2.2.2) or having dependencies on an entity in charge of offloading of power consuming processes, which the entity may be a cloud/edge service (in clause 4.2.2.3) and 5G wireless connectivity (in clause 4.2.2.4).\nFor the detailed functionalities for the device reference architecture of AR glasses, AR runtime (in clause 4.2.3) is identified for AR/MR system capability discovery, AR/MR session management, tracking of surrounding area, and rendering of AR/MR content in scene graph. Scene manager (in clause 4.2.4) is able to process a scene graph and render the corresponding 3D scene. 5G Media Access Function (in clause 4.2.4) is identified to support AR UE and the scene manager to access and stream components of AR content (in clause 4.4).\nAR content consists of one or more AR objects in terms of primitives (in clause 4.4.4) and their spatial and temporal compositions described by a scene description (in clause 4.4.2). Processing of AR/MR functions may require additional metadata (in clause 4.4.3) to properly recognize user’s pose and surrounding area.\nKey performance indicators and metrics for AR/MR based on TR 26.928 are provided (in clause 4.5) and related works (in clause 4.6) on AR/MR in 3GPP, MPEG, and ETSI are identified for the considerations on collaborative work on device function architecture and AR content formats and codecs.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.2\tDevice Functional Architecture",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.2.1\tDevice Functions",
                            "text_content": "AR glasses contain various functions that are used to support a variety of different AR services as highlighted by the different use cases in clause 5. AR devices share some common functionalities to create AR/XR experiences. Figure 4.2.1-1 provides a basic overview of the relevant functions of an AR device.\nThe primary defined functions are\n-\tAR/MR Application: a software application that integrates audio-visual content into the user’s real-world environment.\n-\tAR Runtime: a set of functions that interface with a platform to perform commonly required operations such as accessing controller/peripheral state, getting current and/or predicted tracking positions, general spatial computing, and submitting rendered frames to the display processing unit.\n-\tMedia Access Function: A set of functions that enables access to media and other AR related data that is needed in the scene manager or AR Runtime in order to provide an AR experience. In the context of this report, the Media Access function typically uses 5G system functionalities to access media.\n-\tPeripherals: The collection of sensors, cameras, displays and other functionalities on the device that provide a physical connection to the environment.\n-\tScene Manager: a set of functions that supports the application in arranging the logical and spatial representation of a multisensorial scene based on support from the AR Runtime.\n\nFigure 4.2.1-1: 5G AR Device Functions\nNOTE:\tMicrophones are not shown in Figure 4.2.1-1. This figure is focusing on the receiving side of an AR device; in conversational use cases, microphones are also present on the device.\nThe various functions that are essential for enabling AR glass-related services within an AR device functional structure include:\na)\tTracking and sensing (assigned to the AR Runtime)\n-\tInside-out tracking for 6DoF user position\n-\tEye Tracking\n-\tHand Tracking\n-\tSensors\nb)\tCapturing (assigned to the peripherals)\n-\tVision camera: capturing (in addition to tracking and sensing) of the user’s surroundings for vision related functions\n-\tMedia camera: capturing of scenes or objects for media data generation where required\nNOTE:\tVision and media camera logical functions may be mapped to the same physical camera, or to separate cameras. Camera devices may also be attached to other device hardware (AR glasses or smartphone), or exist as a separate external device.\n-\tMicrophones: capturing of audio sources including environmental audio sources as well as users’ voice.\nc)\tAR Runtime functions\n-\tXR Spatial Compute: AR functions which process sensor data to generate information about the world 3D space surrounding the AR user. It includes functions such as SLAM for spatial mapping (creating a map of the surrounding area) and localization (establishing the position of users and objects within that space), 3D reconstruction and semantic perception.\n-\tPose corrector: function for pose correction that helps stabilise AR media when the user moves. Typically, this is done by asynchronous time warping (ATW) or late stage reprojection (LSR).\n-\tSemantic perception: process of converting signals captured on the AR glass into semantical concepts. Typically uses some sort of Artificial Intelligence (AI) and/or Machine Learning (ML). Examples include object recognition, object classification, etc.\nd)\tScene Manager\n-\tScene graph handler: a function that supports the management of a scene graph that represents an object-based hierarchy of the geometry of a scene and permits interaction with the scene.\n-\tCompositor: compositing layers of images at different levels of depth for presentation\n-\tImmersive media renderer: the generation of one (monoscopic displays) or two (stereoscopic displays) eye buffers from the visual content, typically using GPUs.  Rendering operations may be different depending on the rendering pipeline of the media and may include audio, 2D or 3D visual rendering, as well as pose correction functionalities. The immersive media renderer also includes rendering of other senses such as haptics.\ne)\tMedia Access Function includes\n-\tTethering and network interfaces for AR/MR immersive content delivery\n>\tThe AR glasses may be tethered through non-5G connectivity (wired, WiFi)\n>\tThe AR glasses may be tethered through 5G connectivity\n>\tThe AR glasses may be tethered through different flavours of 5G connectivity\n-\tContent Delivery: Connectivity and protocol framework to deliver the content and provide functionalities such as synchronization, encapsulation, loss and jitter management, bandwidth management, etc.\n-\tDigital representation and delivery of scene graphs and XR Spatial Descriptions\n-\tCodecs to compress the media provided in the scene.\n>\t2D media codecs\n>\tImmersive media decoders: media decoders to decode compressed immersive media as inputs to the immersive media renderer.  Immersive media decoders include both 2D and 3D visual media and mono, stereo and/or spatial/audio media decoder functionalities.\n>\tImmersive media encoders: encoders providing compressed versions of visual/audio immersive media data.\n-\tMedia Session Handler: A service on the device that connects to 5G System Network functions, typically AFs, in order to support the delivery and QoS requirements for the media delivery. This may include prioritization, QoS requests, edge capability discovery, etc.\n-\tOther media-delivery related functions such as security, encryption, etc.\nf)\tPhysical Rendering (assigned to the peripherals)\n-\tDisplay: Optical see-through displays allow the user to see the real world “directly” (through a set of optical elements though). AR displays add virtual content by adding additional light on top of the light coming in from the real-world.\n-\tSpeakers: Speakers that allow to render the audio content to provide an immersive experience. A typical physical implementation are headphones.\ng)\tAR/MR Application with additional unassigned functions\n-\tAn application that makes use of the AR and MR functionalities on the device and the network to provide an AR/MR user experience.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.2.2\tGeneric reference device functional structure device types",
                            "text_content": "In TR 26.928, clause 4.8, different AR and VR device types had been introduced. This clause provides an update and refinement in particular for AR devices. The focus in this clause mostly on functional components and not on physical implementation of the glass/HMD. Also, in this context the device is viewed as a UE, i.e. which functions are included in the UE.\nA summary of the different device types is provided in Table 4.2.2.1-1. The table also covers:\n-\thow the devices are connected to get access to information,\n-\twhere the 5G Uu modem is expected to be placed,\n-\twhere the AR Runtime (as specified in 4.2.1) is placed,\n-\twhere the Scene Manager (as specified in 4.2.1) is placed,\n-\twhere the AR/MR application is running,\n-\twhere the power supply/battery is placed.\nExamples for current AR devices with assigned properties are for example provided in [49]. For all glass device types, it is assumed that sensors, cameras and microphones are on the device.\nThe definition for Split AR/MR in Table 4.2.2.1-1 is as follows:\n-\tSplit: the tethered device (phone/puck) or external entity (cloud/edge) does some power-intense processing (e.g., a pre-rendering of the viewport based on sensor and pose information), and the AR/MR device and/or tethered device performs post-processing considering the latest sensor information (e.g. warping to apply pose correction). Different degrees of split workflow exist, between different devices and entities. Similarly, vision engine functionalities and other AR/MR functions (such as AR/MR media reconstruction, encoding and decoding) may be subject to split computation.\nTable 4.2.2.1-1: 5G Augmented Reality device types\n\nThe Wired Tethered AR UE device type is for reference purposes only and not considered in this document as it is not included as part of the study item objectives.\nGenerally, the STAR and WLAR device according to Table 4.2.2.1-1 are expected to have similar functionalities from a 5G System perspective.\nBased on this, the focus is on three main different device types in the remainder of this document following the rows 1 to 3 in Table 4.2.2.1-1.\nFigure 4.2.2.2-1 provides a functional structure for Type 1: 5G STandalone AR (STAR) UE.\n\nFigure 4.2.2.2-1: Functional structure for Type 1: 5G STandalone AR (STAR) UE\nNOTE:\tMicrophones are not shown in Figure 4.2.2.2-1, the focus is on the receiving side of an AR UE.\nMain characteristics of Type 1: 5G STandalone AR (STAR) UE:\n-\tThe STAR UE is a regular 5G UE. 5G connectivity is provided through an embedded 5G modem.\n-\tThe AR Runtime is local and uses input from sensors, audio inputs or video inputs. XR Spatial Compute is primarily local, but may access or share information on the network.\n-\tThe AR Scene Manager is local and provides immersive rendering capabilities. Support of compute on the network may be provided, but scenes may typically be composed on the UE.\n-\tThe AR/MR application is resident on the device.\n-\tAn AR/MR application provider is providing a service and the service may be supported/assisted by network-based AR functions and rendering.\n-\tDue to the amount of processing required, such devices are likely to require a higher power consumption in comparison to the other device types.\n-\tAs the device includes all UE functionalities, the application resides and pre-dominantly is executed on the device and all essential AR/MR functions are available for typical media processing use cases, the device referred to as STandalone AR (STAR) UE.\n-\tMedia Access Functions are provided that support the delivery of media content components over the 5G system. For details refer to clause 4.2.6.\n-\tThe application may also communicate though the 5G System using a dedicated interface.\nFigure 4.2.2.3-1 provides a functional structure for Type 2: 5G EDGe-Dependent AR (EDGAR) UE.\nThe figure depicts a functional structure for Type 2 5G EDGe-Dependent AR (EDGAR) UE, illustrating the various components and their interconnections.\nFigure 4.2.2.3-1: Functional structure for Type 2: 5G EDGe-Dependent AR (EDGAR) UE\nNOTE:\tMicrophones are not shown in Figure 4.2.2.3-1, the focus is on the receiving side of an AR UE.\nMain characteristics of Type 2: 5G EDGe-Dependent AR (EDGAR) UE:\n-\tThe 5G EDGAR UE is a regular 5G UE. 5G connectivity is provided through an embedded 5G modem and 5G System components.\n-\tThe AR Runtime is local and uses data from sensors, audio inputs or video inputs. The AR Runtime, in particular the XR Spatial Compute, may be assisted by the cloud/edge application for example spatial localization and mapping provided by a spatial computing service.\n-\tMedia processing is local, the device needs to embed all media codecs required for decoding pre-rendered 2D view.\n-\tA Lightweight Scene Manager is local to the AR/MR device, but the main scene management and composition is done on the cloud/edge. A scene description is generated and exchanged to establish the split work flow.\n-\tThe main AR/MR application resides on the cloud/edge, but a basic application functionality is on the UE to support regular UE functionalities and launching services and applications.\n-\tPower consumption on such glasses must be low enough to fit the form factors. Heat dissipation is essential.\n-\tMedia Access Functions are provided that support the delivery of media content components over the 5G system, in particular cloud and split rendering supporting functions. Media Access Functions are divided in control on M5 (Media Session Handler and Media AF) and user data on M4 (Media Client and Media Application Server). Detailed requirements are for study in this report.\n-\tWhile the EDGAR UE may have additional functionalities, for example those available in a STAR UE, generally for media centric or compute heavy use cases processing needs to be supported by the edge, hence referred to as EDGe-Dependent AR (EDGAR) UE.\n-\tThe application may also communicate though the 5G System using a dedicated interface.\nThis clause introduces the 5G WireLess Tethered AR UE. Two sub-types are differentiated:\n-\tSplit Rendering WLAR UE. In this case the 5G phone that includes the modem also acts to support rendering of complex scenes and provides the pre-rendered data to the glass\n-\tRelay WLAR UE: In this case, the 5G phone acts as a relay to provide IP connectivity.\nFigure 4.2.2.4-1 provides a functional structure for Type 3a: 5G Split Rendering WireLess Tethered AR UE.\nThe figure depicts a functional structure for 5G split rendering wire-less tethered AR (Augmented Reality) UE, showcasing the various components and their interconnections.\nFigure 4.2.2.4-1: Functional structure for Type 3a: 5G Split Rendering WireLess Tethered AR UE\nNOTE:\tMicrophones are not shown in Figure 4.2.2.4-1, the focus is on the receiving side of an AR UE.\nMain characteristics of Type 3a: 5G Split Rendering WireLess Tethered AR UE:\n-\t5G connectivity is provided through a tethered device which embeds the 5G modem. Wireless tethered connectivity is provided through WiFi or 5G sidelink. BLE (Bluetooth Low Energy) connectivity may be used for audio. The motion-to-render-to-photon loop runs from the glass to the phone. While the connectivity is outside of the 5G Uu domain, it is still expected that for proper performance when used for split rendering, a stable and constant delay link may be setup on the tethered connection.\n-\tThe AR Runtime is local and uses from sensors, audio inputs or video inputs, but may be assisted by functionalities on phone.\n-\tWhile media processing (for 2D media) may be done on the AR glasses, energy intensive AR/MR media processing may be done on the AR/MR tethered device or split.\n-\tSome devices might have limited support for immersive media decoding and rendering and may need to rely on 5G cloud/edge\n-\tWhile such devices are likely to use significantly less processing than Type 1: 5G STAR devices by making use of the processing capabilities of the tethered device, they still support a lot of local media and AR/MR processing. Such devices are expected to provide 8-10h of battery life while keeping a significantly low weight.\n-\tThe tethered glass itself is not a regular 5G UE, but the combination of the glass and the phone results in a regular 5G UE.\n-\tMedia Access Functions are provided that support the delivery of media content components over the 5G system. Examples of the Media Access Functions are 5GMS functions, MTSI functions, web-connectivity or edge-related client functions. Detailed requirements are for study in this report.\nFigure 4.2.2.4-2 provides a functional structure for Type 3b: 5G Relay WireLess Tethered AR UE.\nThe figure depicts a functional structure for Type 3b: 5G Relay WireLess Tethered AR UE, illustrating the various components and their interconnections.\nFigure 4.2.2.4-2: Functional structure for Type 3b: 5G Relay WireLess Tethered AR UE\nNOTE:\tMicrophones are not shown in Figure 4.2.2.4-2, the focus is on the receiving side of an AR UE.\nMain characteristics of Type 3b: 5G Relay WireLess Tethered AR UE:\n-\t5G connectivity is provided through a tethered device which embeds the 5G modem. Wireless tethered connectivity is through WiFi or 5G sidelink. BLE (Bluetooth Low Energy) connectivity may be used for audio.\n-\tThe 5G Phone acts as a relay to forward IP packets. The 5G Phone runs a Media Session Handler including EDGE functionalities to support QoS control on the 5G System. To support proper end-to-end QoS, the media session handling needs to take into account the constraints of the tethering link to provide sufficient QoS on the 5G System link to provide adequate QoE for the end user. Details on the exact function of the relay, for example of it is on IP layer (layer 3) or on lower layer is for further study.\n-\tMedia Access functions are provided on the glass device to support the delivery of media content components over the 5G and wireless tethered link.\n-\tThe motion-to-render-to-photon loop runs from the glass to the edge and hence includes in total 4 wireless links. It is expected that for proper performance when used for split rendering, a stable and constant delay end to end link needs to be setup.\n-\tThe AR Runtime is local and uses from sensors, audio inputs or video inputs, but may be assisted by functionalities on phone.\n-\tMedia Processing is either done on the glass device or it is split with the network. In particular, relevant is that many devices have limited support for immersive media decoding and rendering and may need to rely on 5G cloud/edge.\n-\tWhile such devices are likely to use significantly less processing than Type 1: 5G STAR devices by making use of the processing capabilities of the tethered device, they still support a lot of local media and AR/MR processing. Such devices are expected to provide 8-10h of battery life while keeping a significantly low weight.\n-\tThe tethered glass itself is not a regular 5G UE, but the combination of the glass and the phone results in a regular 5G UE.\n-\tFor services with low latency requirements, such as MTSI or those provided by FLUS, it may be necessary to take the status of wireless connectivity into account when configuring the services, such that the link between AR glass and 5G phone is not overly loaded. Although some work on the convergence of different acces networks is defined in [3], the coordination of the operation of Uu and wireless connectivity in such services is FFS.\nA key challenge for WLAR and WTAR UEs is to properly estimate the required QoS allocations for the AR sessions. The QoS allocation must take into account the wireless/wired tethering link from the glass to the UE. This applies to all QoS parameters, namely bitrate, packet loss, delay, and jitter. The following diagram depicts a breakdown of the components contributing to the end-to-end delay as an example:\nThe figure depicts the end-to-end delay breakdown of a 5G network, highlighting the various components involved in signal propagation, such as base stations (gNB), user equipment (UE), and scatterers. The diagram illustrates the use of beamforming techniques to mitigate interference, and highlights the importance of redundancy paths to ensure failover reliability.\nFigure 4.2.2.4-3: End-to-end delay breakdown to components\nFor a smooth operation of the AR session, the UE must estimate the impact of the tethering link on the overal QoS requirements. This corresponds to the Dn,1 component in the example figure. The MSH on the UE is the best entity to perform such estimates, which it may do by:\n-\tRunning some measurement tests for latency, packet loss, and bitrate\n-\tExchanging information with the radio and/or AF on the QoS policy\nThe MSH may regularly adjust its QoS allocation based on the observation of the status of the tethering link, thus targeting a consistent end-to-end QoS experience.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 4.2.2.1-1: 5G Augmented Reality device types",
                                    "table number": 3,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "4.2.3\tAR Runtime",
                            "text_content": "The AR Runtime is a device-resident software or firmware that implements a set of APIs to provide access to the underlying AR/MR hardware. These APIs are referred to as AR Runtime APIs. An AR Runtime typically provides the following functions:\n-\tSystem capability discovery: allows applications to discover capabilities of the AR glasses\n-\tSession management: manages an AR session and its state\n-\tInput and Haptics: receives information about user’s actions, e.g. through usage of trackpads, and passes that information to the application. On request by the application, it may trigger haptics feedback using the AR glasses and associated hardware.\n-\tRendering: synchronizes the display and renders the composited frame onto the AR glasses displays.\n-\tSpatial Computing: processes sensor data to generate information about the world 3D space surrounding the AR user. Spatial computing includes functions such as\n>\tTracking to estimate the movement of the AR device at a high frequency\n>\tRelocalization to estimate the pose of the AR device at initialization, when tracking is lost or regularly to correct the drift of the tracking.\n>\tMapping, for reconstructing the surrounding space, for example through triangulation of identified points. This reconstruction may be sparse for localization purposes, or dense for visualization.\n>\tA combination of tracking, mapping and relocalization functions, for example through Simultaneous Localization and Mapping (SLAM) to build a map of the environment and establish the position of users and objects within that environment\n>\tSemantic perception: to process the captured information into semantical concepts, typically uses some sort of Artificial Intelligence (AI) and/or Machine Learning (ML). Examples include object or user activity segmentation, recognition, and classification.\nSpatial computing functions typically include data exchange and requires network architecture. Clause 4.2.5 provides more details on XR Spatial computing.\nAR runtimes are usually extensible to add support for a wide range of AR glasses and controllers that are on the market or that might be released in the future. This will allow different vendors to add custom functionality such as gaze tracking, hand control, new reference spaces, etc.\nTwo key representative and standardized AR runtimes are Khronos defined OpenXR [4] and W3C defined WebXR [5]. More details are provided in clause 4.6.4.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.2.4\tScene Manager",
                            "text_content": "A Scene Manager is a software component that is able to process a scene description and renders the corresponding 3D scene. The Scene Manager parses a scene description document to create a scene graph representation of the scene. For each node of the scene graph, it adds the associated media components for correct rendering of the corresponding object.\nTo render the scene, the Scene Manager typically uses a Graphics Engine that may be accessed by well-specified APIs such as defined by Vulkan, OpenGL, Metal, DirectX, etc. Spatial audio is also handled by the Scene Manager based on a description of the audio scene. Other media types may be added as well.\nThe Scene Manager is able to understand the capabilities of the underlying hardware and system, to appropriately adjust the scene complexity and rendering process. The Scene Manager may, for instance, delegate some of the rendering tasks to an edge or remote server. As an example, the Scene Manager may only be capable of rendering a flattened 3D scene that has a single node with depth and colour information. The light computation, animations, and flattening of the scene may be delegated to an edge server.\nClause 4.6.5 provides a description of glTF2.0 and the extensions defined by the MPEG-I Scene Description, which may serve as a reference for the entry point towards a Scene Manager.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.2.5\tXR Spatial Computing",
                            "text_content": "XR Spatial computing summarizes functions which process sensor data to generate information about the world 3D space surrounding the AR user. It includes functions such as SLAM for spatial mapping (creating a map of the surrounding area) and localization (establishing the position of users and objects within that space), 3D reconstruction and semantic perception. This requires accurately localizing the AR device worn by the end-user in relation to a spatial coordinate system of the real-world space. Vision-based localization systems reconstruct a sparse spatial mapping of the real-world space in parallel (e.g. SLAM). Beyond the localization within a world coordinate system based on a sparse spatial map, additionally dense spatial mapping of objects is essential in order to place 3D objects on real surfaces, but also provides the ability to occlude objects behind surfaces, doing physics-based interactions based on surface properties, providing navigation functions or providing a visualization of the surface. Thirdly, for the purpose of understanding and perceiving the scene semantically, machine-learning and/or artificial intelligence may be used to provide context of the observed scene. The output of spatial computing is spatial mapping information that is organized in a data structure called the XR Spatial Description for storing and exchanging the information. Further details on XR Spatial Description formats are provided in clause 4.4.7.\nXR Spatial Compute processes may be carried out entirely on the AR device. However, it may be beneficial or necessary to use cloud or edge resources to support spatial computing functions. At least two primary scenarios may be differentiated:\n1)\tSpatial computing is done on the AR device, but an XR Spatial Description server is used for storage and retrieval of XR Spatial Description.\n2)\tAt least parts of the spatial compute functions are offloaded to a XR Spatial Compute server\nBoth cases are discussed further in the following.\nTypically, the device stores a certain amount of XR Spatial Description locally on the device. However, in particular to create a world-experience, the AR device may not be able to store all information related to XR Spatial Description on the device, and hence, such information may continuously be updated by downloading or streaming updated information from an XR Spatial Description server as shown in Figure 4.2.5-1. In addition, the device may use personalized storage on the cloud to offload device-generated XR spatial information components. This may for example include so-called key frames, i.e. frames that are useful to provide accurate spatial mapping with relevant key points.\nThe architecture in Figure 4.2.5-1 relates to the case where XR Spatial computing is done standalone on the device, and hence we refer to this as STAR architecture in context of XR Spatial computing.\nThe figure depicts a functional diagram for XR S, a platform for 5G network and cloud support. It illustrates the network/cloud integration, highlighting the role of the network controller (NCC), the network service provider (NSP), and the cloud service provider (CSP). The diagram shows the communication between the NCC, CSP, and the network, as well as the communication between the CSP and the network. The CSP is responsible for managing the network and cloud services, while the NCC is responsible for managing the network and cloud services. The CSP is responsible for managing the network and cloud services, while the NCC is responsible for managing the network and cloud services. The CSP is responsible for managing the network and cloud services, while the NCC is responsible for managing the network and cloud services. The CSP is responsible for managing the network and cloud services, while the NCC is responsible for managing the network and cloud services. The CSP is responsible for managing the network and cloud services, while the NCC is responsible for managing the network and cloud services. The CSP is responsible for managing the network and cloud services, while the NCC is responsible for managing the network and cloud services. The CSP is responsible for managing the network and cloud services, while the N\nFigure 4.2.5-1 Functional diagram for XR Spatial computing with network/cloud support\nIf the device is limited in processing power, or if complex XR compute functionalities need to be carried out, the XR compute function on the device may be assisted by or even depend on compute resources in the network, for example on the edge or cloud. Figure 4.2.5-2 provide a basic architecture for the case where XR Spatial computing is delegated partially or completely to an XR Spatial computing edge server.\n-\tThe device sends sensor data or pre-processed sensor data (e.g. captured frames or visual features extracted from such frames) to the XR Spatial Compute server.\n-\tThe XR Spatial Compute server carries out supporting functions to extract relevant information and returns directly XR Spatial Compute-related AR Runtime data (according to the AR Runtime API), e.g. pose information, or pre-computed XR Spatial information that is used by a lightweight XR Spatial Compute function on the device to create AR Runtime data. Pre-computed XR Spatial information may, for example, be dense map of segmented objects for visualization, labels or id of recognized objects, 2D contours of recognized object to highlight them, or labels of the recognized user activity.\n-\tThe XR spatial edge compute server may further fetch the XR Spatial Description from the XR Spatial Description server and perform spatial computing based on device sensor data.\nThe figure depicts a functional diagram for spatial computing with XR Spatial Compute, illustrating the various components and their interactions. The diagram includes spatial computing nodes, spatial compute servers, and spatial compute edge devices, all interconnected through spatial compute networks. The spatial compute servers are responsible for processing spatial data, while the spatial compute edge devices are designed to process spatial data at the edge of the network. The spatial compute networks are designed to provide high-speed, low-latency data transmission, enabling efficient spatial computing.\nFigure 4.2.5-2 Functional diagram for spatial computing with XR Spatial Compute edge\nThe architecture in Figure 4.2.5-2 relates to the case for XR Spatial computing depends on an edge function, and hence we refer to this as EDGAR architecture in context of XR Spatial computing.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.2.6\t5G Media Access Function",
                            "text_content": "The Media Access Function supports the AR UE to access and stream media. For this purpose, a Media Access Function as shown in Figure 4.2.6-1 includes:\n-\tCodecs: used to compress and decompress the rich media. In several cases, not only a single instance of a codec per media type is needed, but multiple ones.\n-\tContent Delivery Protocol: Container format and protocol to deliver media content between the UE and the network according to the requirements of the application. This includes timing, synchronization, reliability, reporting and other features.\n-\t5G connectivity: a modem and 5G System functionalities that allow the UE to connect to a 5G network and get access to the features and service offered by the 5G System.\n-\tMedia Session Handler: A generic function on the device to setup 5G System capabilities. This may setup edge functionalities, provide QoS support, support reporting, etc.\n-\tContent protection and decryption: This function handles protection of content from being played on unauthorized devices.\nFunctions are needed in both uplink and downlink, depending on use cases and scenarios.\nExample for Media Access Functions are\n-\t5GMSd client that includes a Media Session Handler and a Media Player as defined in TS 26.501 and TS 26.512.\n-\t5GMSu client that includes a Media Session Handler and a Media Streamer as defined in TS 26.501 and TS 26.512.\n-\tA real-time communication client that includes either uplink or downlink, or both to support more latency critical communication services.\n-\tA combination of the above based on the needs of the XR application. An XR scene may have a mix of static, streaming, and real-time media that require the usage of multiple transport channels and protocol stacks.\nIn all cases, the basic function of Media Session Handler and a delivery client (which includes content delivery protocols and codecs) is expected to be maintained. The Media Session Handler is a generic function to support 5G System integration.\nAs a subject of this report, the needs to support different types of instantiations is for codecs, delivery protocols, session handling and so is identified. Not all components are necessarily required for all scenarios and even further, not all functions may be available on all device types.\nThe figure depicts a media access function (MAF) for AR (Augmented Reality) in a 6-1 network. The MAF is a crucial component in AR applications, enabling seamless communication between the user and the AR content. The figure illustrates the various layers of the MAF, including the media access point (MAP), the media access control (MAC), and the media access control protocol (MAPP). The figure also highlights the importance of network slicing and network slicing policy in managing the network resources for AR applications.\nFigure 4.2.6-1 Media Access function for AR\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.3\tBasic Processes for delivering an AR experience",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.3.1\tIntroduction",
                            "text_content": "In this clause we provide a typical workflow for setting up a session between a 5G UE and network/cloud for receiving AR scenes from a scene provider. The basic workflow is provided in Figure 4.3.1-1. In this case, the following steps happen:\n1.\tThe application contacts the application provider to fetch the entry point for the content. The acquisition of the entry point may be performed in different ways and is considered out of scope. An entry point may for example be a URL to a scene description.\n2.\tSession set up:\n2a.\tIn case when the entry point is a URL of a scene description, the application initializes the Scene Manager using the acquired entry point.\n2b.\tThe Scene Manager retrieves the scene description from the scene provider based on the entry point information.\nNOTE:\tThe scene provider can be mapped to a functional entity in the cloud/edge. Architecture mappings are described in clause 6.\n2c.\tThe Scene Manager parses the entry point and creates the immersive scene.\n2d.\tThe Scene Manager requests the creation of a local AR/MR session from the AR Runtime.\n2e.\t\tThe AR Runtime creates a local AR/MR session and performs registration with the local environment.\nThen steps 3 and 4 run in parallel:\n3:\tAR Media Delivery Pipeline: In case when entry point is a scene URL, a delivery session - for accessing scenes (new scenes or scene updates) and related media over the network is established. This can basically use the MAF as well as the scene manager and the corresponding network functions. Details are introduced in clause 4.3.2.\nNOTE:\tThe realization of AR media delivery pipeline may vary in different architectures as shown in clause 6.\n4:\tXR Spatial Compute Pipeline: A pipeline that uses sensor data to provide an understanding of the physical space surrounding the device to determine the device’s position and orientation and placement of AR objects in reference to the real world and uses XR Spatial Description information from the network to support this process. This uses the XR Spatial Description functions as introduced in clause 4.4.7. Details are introduced in clause 4.3.3.\n5:\tSteps 3 and 4 run independently, but the results of both pipelines (e.g., media organized in a scene graph and pose of the AR device) are inputs of the AR/MR Scene Manager function. This function handles the common processing of the two asynchronous pipelines to create an AR experience.\nThe figure depicts a basic workflow for delivering an augmented reality (AR) experience, illustrating the steps involved in creating and delivering an AR experience. The workflow includes the following steps: \n\n1. **Gathering Data**: The user selects a location and selects a specific object to view in the AR experience.\n2. **Creating the AR Experience**: The AR experience is created using a combination of 3D modeling, 3D graphics, and 3D audio.\n3. **Deploying the AR Experience**: The AR experience is deployed to the user's device, which includes the necessary software and hardware.\n4. **Monitoring and Updating**: The AR experience is monitored and updated as needed to ensure the user's experience is consistent and enjoyable.\n\nThis workflow ensures that the AR experience is delivered accurately and efficiently, providing a seamless and immersive experience for the user.\nFigure 4.3.1-1: Basic workflow for delivering an AR experience\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.3.2\tAR Media Delivery Pipeline",
                            "text_content": "In this clause, we provide the detailed processes to set up AR Media Delivery Pipeline as addressed in step 3 of Figure 4.3.1-1. This generic basic process may be extended to address specific applications and use cases. The call flow as shown in Figure 4.3.2-1 aligns with the STAR/EDGAR architecture and serves as a baseline for defining use-case specific call flows.\nFigure 4.3.2-1: Functional diagram for AR Media Delivery Pipeline\nFor an AR Media Delivery Pipeline:\n1.\tThe Scene Manager initializes AR delivery session.\n2.\tThe MAF establishes AR delivery session.\n3.\tThe MAF may receive updates to the scene description from the scene provider\n4.\tThe MAF passes the scene update to the Scene Manager.\n5.\tThe Scene Manager updates the current scene.\n6.\tThe Scene Manager acquires the latest pose information and the user’s actions\n7.\tThe Scene Manager in the device shares that information with the Scene Manager in edge/cloud\nThe media rendering loop consists of the following steps. Note that steps 8, 9 and 10 are running as 3 parallel loops:\n8.\tFor each new object in the scene:\na.\tThe Scene Manager triggers the MAF to fetch the related media.\nb.\tThe MAF creates a dedicated media pipeline to process the input.\nc.\tThe MAF establishes a transport session for each component of the media object.\n9.\tFor each transport session:\na.\tThe media pipeline fetches the media data. It could be static, segmented, or real-time media streams.\nb.\tThe media pipeline processes the media and makes it available in buffers.\n10.\tFor each object to be rendered:\na.\tThe Scene Manager gets processed media data from the media pipeline buffers\nb.\tThe Scene Manager reconstructs and renders the object\n11.\tThe Scene Manager passes the rendered frame to the AR/MR Runtime for display on the user’s HMD.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.3.3\tXR Spatial Compute Pipeline",
                            "text_content": "In this clause, we provide the detailed processes to set up an XR Spatial Compute Pipeline for spatial description related data as addressed in step 4 of Figure 4.3.1-1. This generic basic process may be extended to address specific applications and use cases. The call flow as shown in Figure 4.3.3-1 aligns with the STAR/EDGAR architecture and serves as a baseline for defining use-case specific call flows.\nFigure 4.3.3-1 Functional diagram for XR Spatial Compute Pipeline\nFor a XR Spatial Compute downlink delivery session:\n1.\tThe XR Spatial Compute function in the AR Runtime asks the MAF to establish a XR Spatial Compute downlink delivery session\n2. The MAF communicates with the network to establish the proper resources and QoS\n3. The XR Spatial Compute function requests access to XR Spatial Description information\n4. An XR Spatial Description downlink delivery session is established across the XR Spatial Compute server, the media delivery function, the media access function and XR Spatial Compute function on the device.\n5. XR Spatial Description information is delivered in this downlink delivery session\nFor a XR Spatial Compute uplink delivery session:\n6. The XR Spatial Compute function in the AR Runtime asks the MAF to establish a XR Spatial Compute uplink delivery session\n7. The MAF communicates with the network to establish the proper resources and QoS\n8. The MAF established an appropriate uplink delivery pipeline\n9. An XR Spatial Description uplink delivery session is established across the XR Spatial Compute function on the device, the media access function, the media delivery function and the XR Spatial Compute server.\n10. Spatial compute information is upstreamed to the XR Spatial Compute server.\n11. Data is continuously exchanged between the Scene Manager and the AR Runtime\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.4\tAR content formats and codecs",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.4.1\tOverview",
                            "text_content": "A 5G AR/MR application provider offers an AR/MR experience to a 5G UE using an AR/MR application and 5G System and media functionalities. AR/MR content is typically agnostic to a delivery architecture and consists of one or more AR/MR objects, each of which usually corresponds to an immersive media type in clause 4.4.4 and may include their spatial and temporal compositions. The delivery of an immersive media adaptive to device capability and network bandwidth may be enabled by a delivery manifest in clause 4.4.5. Processing of AR/MR functions in 5GMS AS may require additional metadata in clause 4.4.3 to properly recognize user’s pose and surroundings.\nAR/MR functions include encoding, decoding, rendering and compositing of AR/MR object, after which localization and correction is performed based on the user’s pose information.\nSTAR-based architecture has both basic AR functions and AR/MR functions on the device. EDGAR-based architecture has only basic AR functions on the device.\nSince AR/MR functions are on-device for the STAR-based architecture, immersive media including 2D media is considered as the input media for the architecture.\nExamples of immersive media are 2D/3D media such as overlay graphics and drawing of instructions (UC#16 in Annex A.1), 3D media such as furniture, a house and an animated representation of 3D modeled person (UC#17 in Annex A.2), a photorealistic volumetric video of a person (UC#18 in Annex A.3), a 3D volumetric representation of conference participants (UC#19 in Annex A.4), 2D video, and volumetric information and simple textual overlays (UC#20 in Annex A.5).\nFor the EDGAR-based architecture, basic AR functions are on-device therefore 2D media and additional information (such as depth map) generated from immersive media renderer are considered as the input media for basic AR functions. A rasterized and physically-based rendering (PBR) image is an example of 2D media.\nA study into the existing technologies to be considered as inputs to each function and device type are identified and presented as a non-exclusive list below.\n-\tseveral visual media representation formats were documented in clause 4.4.4.\n-\tseveral delivery manifests were documented in clause 4.4.5.\n-\tseveral scene description formats were documented in clause 4.4.2.\n-\tmetadata such as user pose information and camera information were documented in clause 4.4.3.\n-\tmanagement and coordination of multiple media decoders are documented in clause 4.4.6, respectively.\nIn order to integrate real-time media into AR scenes, a Media Access Function (MAF) provides the ability to access media and adds it to the AR scene. The MAF instantiates and manages Media Pipelines. A media pipeline typically handles content of an attribute/component of an object/mesh that is part of the scene graph. The media pipeline produces content in the format indicated by the scene description file. For real-time media, the formatted frame is then pushed into the circular buffer. Media Pipelines are typically highly optimized and customized for the type and format of media that is being fetched. Typically, for one scene, multiple media decoders of the same media type are needed to run in parallel. If the media decoders share the same hardware decoding platform on the UE, the MAF may also coordinate the different instances of media decoders to optimise the use of the hardware platform thus avoiding negative effects of resource competition or possible synchronization issues. MPEG-I Video Decoding Interface (ISO/IEC 23090-13 [6]) is an example specification that may fulfil this task of coordination. More information is available in clause 4.6.6. General considerations and challenges related to media decoder management is described in clause 4.4.6. Media Pipelines also maintain sync information (time and space) and passes that information as buffer metadata to the scene manager.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.4.2\tScene Graph and Scene Description",
                            "text_content": "A scene description may correspond to an AR/MR content. A volumetric media containing the primitives ranging from one vertex to a complex object may be described by a scene description. For the use cases listed in Table 5-1, scene description is useful to locate AR/MR objects in user’s world. A scene description typically has a tree or a graph structure which of each leaf represents a component of a scene. A primitive or a group of primitives are referenced as a leaf node of the scene tree. A skeleton to allow for motion rigging or an animation of motion of the skeleton in time may present an animation of volumetric presentation.\n-\tFormats for scene description\nKhronos glTF2.0 and MPEG Scene description (ISO/IEC 23090-14) are examples of scene description technologies. They have a tree structure and internal/external resource references. There are many types of leaf of the tree. For example, a Node is one type of leaf under a Scene. A node may have a Camera as a subsidiary leaf. The node with camera represents one of the rendering frustum/viewport to be used by a scene renderer (i.e., immersive media renderer). Any translation/rotation/scaling of the node affects position and direction of its subsidiary, in this example, a camera. A node with mesh may be used as an anchor that represents AR object with its location and direction in geometric space.\nMPEG Scene description is an extension of glTF2.0. It is extended to support MPEG immersive media. MPEG_media and MPEG_scene_description are the major changes to provide support of media access link including manifest, and temporal update of the scene description itself.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.4.3\tMetadata",
                            "text_content": "User’s position may be represented as a geolocation with longitude and latitude. The position may also be represented as a point in a scene. The scene may be represented as a bounding box on a geometry which represents the user’s real environment. When an AR/MR device reports the user position to obtain a split rendering of the immersive media from a server, the device calculating the user pose is expected to be either a geolocation, a point in a scene or a point in a user’s geometry. Depending on the representation, the server is assumed to be aware of the underlying scene or the geometry. A device is expected to update whenever there is any change in the scene or the geometry through user interaction (e.g., rotating a scene by hand gesture) and/or SLAM (e.g., finer modelling of surrounding environment).\nA direction may be represented with a rotation matrix, or roll, pitch, and yaw. The direction is relative to a scene/geometry and the scene/geometry has an origin and default direction of the three axes.\nThe device representing a user’s pose moves continuously, and if the device is worn on the user’s head, it is assumed that he or she frequently turns their head around. A set of position and direction information is only meaningful at a certain moment in time. Since the device reports the user pose at around a frequency of 1 KHz, any pose information would need to include a timestamp to specify when it was measured or created. A pose corrector (e.g., ATW and LSR) in a server may estimate the user’s future pose, whilst a pose corrector in a device may correct the received rendered image to fit the latest user pose.\n-\tFormats for user pose\nA position in Cartesian coordinate system may be represented by either X, Y and Z or by a translation matrix. A direction may be represented by a rotation matrix or by quaternions.\nOpenXR describes a possible format for user pose [4]. It consists of 4 quaternions for orientation and 3 vectors for position. Timestamp is represented by a 64 bit monotonically increasing nano-second-based integer.\nImmersive media is captured by camera(s). The camera parameters such as focal length, principal points, calibration parameters and the pose of the camera all contribute in understanding the relevance between points in the volumetric scene and pixels in the captured image. Photogrammetry is the technology used to construct immersive media from a continuous capturing of images. Depth sensor-based cameras may be used to capture immersive media from one capturing of the volumetric scene\n-\tFormats for camera information\nCamera intrinsic parameters may be represented by a camera matrix. Extrinsic parameters may be represented by a transform matrix.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.4.4\tMedia Formats/Primitives in AR Scenes",
                            "text_content": "An AR/MR object may be represented in a form of 2D media. One camera or one view frustum in a scene may return a perspective planar projection of the volumetric scene. Such a 2D capture consists of pixels with colour attributes (e.g., RGB).\nEach pixel (a) may represent a measure of the distance between the surface of an AR object, point (A) and the camera centre. Conventionally, the distance is represented by the coordinate of the point on the z-axis obtained by the orthogonal projection of the point (A) on this axis, here denoted as the point (A’). The measured distance is thus the length of the segment (CA’) as depicted in Figure 4.4.4-1.\nThe figure depicts a pixel representation of depth images, illustrating the intricate details of the human visual system.\nFigure 4.4.4-1: Pixel representation of depth images\nThis convention is used for commercially available frameworks handling depth images such as the Microsoft Azure KinectTM SDK [7] and the Google ARCoreTM [8]. According to the documentation of the Azure KinectTM SDK, the depth sensor uses the Time-of-Flight (ToF) technique to measure the distance bewteen the camera and a light-reflecting point in the scene. The documentation further specifies that “these measurements are processed to generate a depth map. A depth map is a set of Z-coordinate values for every pixel of the image, measured in units of millimeters”. Similarly, the Google ARCoreTM documentation explains that “when working with the Depth API, it is important to understand that the depth values are not the length of the ray CA itself, but the projection of it” onto the z-axis.\nAdditionally, sensor API may provide the image from the viewpoint of the depth sensor which is thus not aligned with the viewpoint of RGB camera which is necessarily few millimetres away due to physical constraints. In this case, an alignment operation is necessary in order to guarantee the correspondence between a pixel of the depth image and a pixel of the RGB picture. For instance, the Azure Kinect SDK provides the k4a_transformation_depth_image_to_color_camera() and k4a_transformation_color_image_to_depth_camera() functions which generate a depth image aligned with the colour picture and a colour image aligned with the depth image, respectively. More details and illustrations are provided in [9].\nA depth map thus contains pixels with the distance attribute (e.g., depth). Distance is one-dimensional information and may be represented in an absolute/relative or linear/non-linear manner. Metadata to explain the depth map may be provided.\nThe capturing of a volumetric scene may also be expressed as an omnidirectional image in a spherical coordinate system. Equirectangular Projection (ERP) is an example of projection methods to map a spherical coordinate system into a cylindrical coordinate system. The surface of the cylindrical coordinate system is considered as 2D media.\nCapturing of a volumetric scene may be further improved/elevated with hundreds of cameras in an array; High Density Camera Array (HDCA) or lenticular are methods to capture rays of light. Each point on surface of a volumetric scene has countless rays of colours in multiple different directions. Each position of a camera captures a different colour from the same point surface of the volumetric scene. 2D images from the camera array may be packed together to form a larger plenoptic image.\nFrom another perspective, 2D media is the output of the immersive media renderer. One view frustum that represents the user’s viewport is placed in a scene, and in turn, a perspective or an orthogonal projection of the volumetric media may be produced. To minimise motion sickness, a pose corrector function performs a correction of the 2D media at the last stage of presentation. The pose corrector may require additional information such as the estimated or measured user pose that was used for the rendering of the 2D media. For the case that the latest user pose does not match with the estimated user pose, additional information that provides knowledge on the geometry, such as a depth map, may be delivered from immersive media renderer.\nImmersive media may be considered as an AR/MR object and may be used to provide an immersive experience to users. The immersive experience may include a volumetric presentation of such media. The volumetric presentation does not bind to a specific display technology. For example, a mobile phone may be used to present either the whole AR media, or a part of the AR media. Users may see a volumetric presentation of a part of the AR media augmented in real space. Therefore, immersive media includes not only volumetric media formats such as omnidirectional visual formatsERP image, 3D meshesPrimitives, point cloudsPrimitives, light fieldsPlenopotic image, scene description, and 3D audio formats, but also 2D video2D image as studied in TR 26.928.\n-\tFormats for 2D media\nStill image formats may be used for 2D media. The 2D media may have metadata for each image or for a sequence of images. For example, pose information describes the rendering parameter of one image. The frame rate or timestamp of each image are typically valid for a sequence of such images.\n-\tPrimitives\n3D meshes and point clouds consists of thousands and millions of primitives such as vertex, edge, face, attribute and texture. Primitives are the very basic elements in all volumetric presentation. A vertex is a point in volumetric space, and contains position information in terms of three axes in coordinate system. In a Cartesian coordinate system, X, Y, and Z make the position information for a vertex. A vertex may have one or more attributes. Colour and reflectance are typical examples of attributes. An edge is a line between two vertices. A face is a triangle or a rectangle formed by three or four vertices. The area of a face is filled by interpolated colour of vertex attributes or from textures.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.4.5\tCompression Formats",
                            "text_content": "An elementary stream is an output of a media encoder. Immersive media and 2D media in clause 4.4.4 have relevant technologies to encode each media format as follows.\n-\t2D Video codecs\nThere are differences in terms of context of 2D media, such as RGB image versus depth map image, one planar perspective camera image versus ERP, or one camera image versus HDCA plenoptic image. Such differences may be considered in the proper encoder/decoder coding tools. In general, 2D video codecs may encode 2D media types listed in clause 4.4.4. AVC and HEVC are industry wide examples of 2D video codecs.\n-\tMPEG OMAF (Omnidirectional MediA Format)\nOMAF consists of two parts; the first part is a pre-processing which includes a packing and projection of spherical volumetric media onto a 2D image, and the second part is an encapsulation of the compressed 2D frame packed image with metadata signalling the projection.\nFor the compression of the 2D images, 2D video codecs may be considered and the pre-processing operations are agnostic to specific 2D codec technology.\n-\tMPEG V3C and V-PCC\nV3C and V-PCC consists of two parts; the first part is a pre-processing which includes the decomposition of a part of the volumetric media into the planar projection, a patch, of different characteristics such as texture, geometry and occupancy. The second part is an encoding of 2D patch packing images, with metadata for signalling the decomposition.\nFor the encoding of the 2D images, 2D video codecs may be considered and the pre-processing operations are agnostic to specific 2D codec technology.\n-\tMPEG G-PCC\nG-PCC divides volumetric media into multiple sub-blocks. Triangle (Trisoup) or leaf (Octree) are used as the units of the divisions. A volumetric media is subdivided recursively until no more sub-blocks are left. The dimension (or level) of the tree is relatively large, such as 224. Tools including arithmetic encoding are used to encode all the tree information into the bitstream.\nAn encapsulation format encapsulates an elementary stream with its coding structure information and metadata information. ISOBMFF (ISO based Media File Format, ISO/IEC 14496-12) is one of encapsulation format technology. DASH initialization/media segment and CMAF track are the extensions of ISOBMFF for both adaptive streaming and storage purpose. They are extended to provide partial access of a media fragment on time axis.\nA delivery manifest provides a description of media service consisting of multiple media components such as video and audio. Adaptation to device capability or network bandwidth is key features of a delivery manifest. In a delivery manifest, there is a group of multiple different encodings of the same media component context with the description of the encoding variations. An encapsulation format for an adaptive streaming is used to allow temporal access of media fragment to enable adaptive switching of a group of different encodings. MPD (Media Presentation Description) for DASH is one of delivery manifest for the purpose.\n-\tFile formats for Primitives\nOBJ, PLY, and GPU command buffer in OpenGL-based languages (e.g., glTF Buffer) are methods of encapsulating the primitives. A sequence of primitive files – such as multiple OBJs, PLYs or a set of GPU command buffers in a time may present an animation of volumetric presentation.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.4.6\tMultiple Media Decoders management and coordination",
                            "text_content": "The use of hardware video decoding platform is essential for the decoding of AR/MR content when it comes to power consumption, fast and scheduled decoding as well as battery usage. Modern hardware video decoding platform typically offer the capability to instantiate multiple decoders of the same media type at the same time and run multiple decoding instances in parallel. A typical example is the decoding of different components of the same AR/MR object, or the presentation of multiple objects in a scene. As a result, AR/MR application typically runs several decoder instances, in some cases using the same codec for different instances, in others different codecs for different streams. Note that this issue not only exists for video, but for any media type, in particular also for object-based audio. Under this high demand, there may be a resource competition and scheduled issues for the hardware decoding platform.\nFrom an application perspective, there are different cases as well. There may exist cases for which even several applications are competing for the hardware decoding platform, for example an application renders a scene, but other applications provide overlays and notifications on top of the existing scene. A possible solution is to handle the coordination at the operating system level by setting priority to each application.\nHowever, a single AR/MR application accessing and managing several decoding instances is a more typical and prominent case. It is thus important that the performance of the different decoder instances running is in line with the expectations and the needs of the AR/MR applications such that the AR/MR applications may optimise the usage of the hardware decoding platform when possible.\nThe first question from the AR/MR application point of view is to determine the number of decoder instances to instantiate. To this end, the AR/MR application may determine the number of AR/MR objects to be presented as well as the number of elementary streams contained in each AR/MR object. The hardware decoding platform is typically exposing a capability query API which lists the supported codec. This information enables the AR/MR application to calculate how many AR/MR objects may be simultaneously decoded and with which quality. In addition, there may be cases wherein different elementary streams from the same AR/MR object may be jointly decoded as part of a single elementary stream hence streamlining the rest of the pipeline by effectively decreasing the number of decoder instances and output buffers needed. When this is the case, the AR/MR application may instruct the hardware decoding platform to merge those input elementary streams.\nAt runtime, the AR/MR application expects the decoded frames for each AR/MR object to be ready at the same point in time so that further processing of this AR/MR object may happen without loss of frames or delay introduced due to buffering. However, the concurrent decoder instances may exhibit different performance in terms of decoding delay for each frame. Therefore, it is useful for the AR/MR application to be able to signal to the hardware video decoding platform that certain decoder instances form a group that expected to be treated collectively in terms of output synchronisation.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.4.7\tXR Spatial Description",
                            "text_content": "XR Spatial Description is a data structure (typically organized in a graph) describing the spatial organisation of the real world using:\n-\tVisual features, keyframes and spatial maps as described in more details in clause 4.4.7.3.\n-\tSpatial anchors and trackables as described in more details in clause 4.4.7.4.\n-\tCamera parameters as defined in clause 4.4.3.2\nXR Spatial Description is derived from or needs to be processed together with camera and sensor information. Typical raw sensor data is summarized in clause 4.4.7.2.\nXR Spatial Description describes the real-world including information that is used for the estimation of position and orientation (pose estimation) of AR devices for the purpose of registration, tracking and positioning, and provides a coordinate reference system in relation to the real world. Generally, it may be used for spatial computing as described in clause 4.2.5.\nThe XR Spatial Description may be downloaded to the AR device and reside on the device. However, to support mobility and different environments, XR Spatial Description may have to be exchanged over the network and hence a formalized representation of XR Spatial Description may be needed. In this case, XR Spatial Description data has to be downloaded and updated periodically from a XR Spatial Description server.\nIn addition, the AR function may send XR Spatial Description updates to a XR Spatial Description server. Such data may be derived from XR Spatial Compute functions, e.g., updated visual spatial features, keyframes attached to camera parameters, or sub-parts of an XR Spatial Description. The server may use these XR Spatial Description updates to improve the XR Spatial Description for future use of the same user or by other users.\nAs the data needs to be updated, exchanged as well as stored on the device and the XR Spatial Description server, an efficient and flexible representation of XR Spatial Description is desired. For example, the description needs to be serialized and fragmented to be properly accessed and downloaded over the network.\nThe size of the XR Spatial Description depends on several parameters, for example, size of the area covered by the XR application, number of supported viewpoints in the area, the amount of keyframes that are provided, etc. The size may for example be from 10MByte for a small room to several hundred MBytes for a building. For a global-scale spatial map, the amount of data would be massively larger. As an example, the Microsoft™ Flight Simulator is around 2 Million GByte [63]. Regular exchange of data with the network is needed, details on the frequency, the latency requirements, and the bitrate requirements typically depend on the application, but more details are for further study.\nAs an example, the ETSI ISG ARF 004 [62] uses the term World Graph for XR Spatial Description. It defines the relative position of trackables and world anchors by 3D transforms. In this case, the World Graph is similar to a scene graph including trackables (embedding their features), and spatial anchors representing the real world. This information may be used by the AR Runtime for spatial compute functions including activity and object detection, object recognition, and pose estimation using trackables [21].  At the end of 2021, no non-proprietary XR Spatial Description formats are known.\nIn this clause we provide an overview of different sensors that may provide input data for spatial compute AR functions. All device-captured data require a common timeline and a common coordinate system in order to be meaningful for XR Spatial Compute processing. If the data is processed in the networked, such time and spatial synchronization information is expected to be maintained.\nAR Glasses typically include multiple cameras (for example one device supporting 7 cameras) to build precise motion tracking and gesture recognition. Generally, these camera feeds are processed on the device, but they may be sent across the network to support spatial compute functions. Different cameras exist on a single device, namely\n-\tMonochrome image capture cameras,\n-\tRGB image capture cameras,\n-\tInfrared capture cameras.\nOptical 3D sensors may be used to capture and reconstruct three-dimensional depth of objects. Depending on the source of the radiation, optical 3D sensors may be divided in two sub-categories; passive and active systems. Stereoscopic systems, Shape-from-Silhouettes (SfS), and Shape-from-Texture (SfT) are examples of passive systems, which do not emit any kind of radiation themselves. The sensors collect images of the scene, eventually from different points of view or with different optical setups. Then the images are analysed in order to compute the 3D depth of points in the scene. On the contrary, active systems emit some kind of radiation and the interaction between the object and the radiation is captured by a sensor. From the analysis of the captured data, knowing the features of the emitted radiation, the coordinates of the points are obtained. Time-of-Flight (ToF), phase shift, and active triangulation are examples of active systems. The typical output of an optical 3D sensor is a depth map image as described in clause 4.4.4.\nLight Detection And Ranging (LiDAR) may be another option to measure distances (ranging) by illuminating the target with a light and then measuring the reflection with an optical sensor. In practice, LiDAR cameras operate in the ultraviolet, visible or infrared spectrum. Since the laser light used is typically collimated, the LiDAR camera needs to scan the scene in order to generate an image with a usable Field-of-View. The output of a LiDAR acquisition is a point cloud which may then be enriched with other sensor data such as RGB data.\nDevices may also include microphones. A typical setup is a two-channel microphone array for audio input. Multichannel microphones or even Higher-Order Ambisonics (HOA) microphone arrays may be supported as well. The resulting signals are two- or multi-channel audio signals or HOA signals.\nTypical sensor and fusion data are accelerometer, gyroscope, and magnetometer samples. This information includes, for example, angular velocity from gyroscopes, accelerometer data including the effect of gravity, as well as statistical data around the measurements. Detailed representations are for further study.\nVisual features, keyframes, and spatial maps are used for mapping the real world, typically as part of the SLAM process.\nVisual features are characteristics of real-world elements that are searched, recognized and tracked in 2D images captured by the AR device as the user moves in a real environment. These images provide a view of the same real world elements, captured from different positions (as indicated by the camera parameters attached to them) from a single moving camera or multiple cameras. Visual features are generally extracted from points that are recognizable in multiple images.\nFrom the captured images of the real world, keyframes that include one or multiple visual features may be stored for later use. Visual features from the captured frames may be matched by comparing those frames with keyframes available to the AR Runtime in order to support the SLAM process. Keyframes have attached camera information defined in 4.4.3.2 to triangulate 3D points correctly from multiple cameras. These 3D points, triangulated from matching visual features are called spatial features.\nFinally, a spatial map may be generated from keyframes and their matched visual features. A spatial map is, thus, a digital representation of the real world surrounding users consisting of at least one spatial feature cloud, e.g., 3D points (vector of 3 floats) with their associated descriptors such as SIFT [59], SURF [60], or ORB [61]. The geometrical part of the spatial map may be represented as a sparse or dense point cloud or a mesh. The mapping process may be performed either at runtime or offline. The spatial map is then used at runtime to relocalize and thus register the AR device by matching the visual features extracted from the current captured frames with spatial features stored in the spatial map. The spatial mapping approach described herein is one of well-known keyframe-based SLAM techniques [58].\nThe descriptors of features, whether visual or spatial, are generally vectors of numbers (e.g., vector of 128 floats for SIFT, vector of 64 floats for SURF, vector of 32 integers for ORB). Note that other features such as 3D segments (e.g., a 3D starting point and a 3D ending point) may also be used. During the localization process, the visual features extracted from the current frame, captured by the device, are matched with the spatial features of the map, resulting in 2D-3D correspondences used to estimate the pose of the cameras. However, since the 2D-3D matching process consists of comparing the descriptors of visual features extracted from the current image and those of the spatial features of the map, the complexity may quickly increase for maps containing several hundred thousand or even millions of spatial features. To accelerate the 2D-3D matching process, a spatial map typically also includes the following metadata:\n-\tInformation required for keyframe retrieval. For example, a keyframe retrieval uses Bag-Of-visual-Words (BoW) model. In this case, the information consists of the vocabulary of the BoW model and corresponding descriptor for each keyframe (vector of occurrence counts of a vocabulary in the keyframe). Depending on the visual descriptor used, the vocabulary size is usually a 10-100 MByte, and this vocabulary may be reused.\n-\tThe visual features for each keyframes (e.g. 2D points with their associated descriptors such as SURF, SIFT, ORB represented by a vector of numbers). The number of features extracted per keyframe varies between 200 and 1000.\n-\tA vector pair (identifier of the visual features, identifier of the spatial features) that matches the visual features of keyframes with the spatial features of the spatial feature cloud.\nUsing this metadata, instead of comparing all descriptors of visual features extracted from the current frame with all spatial feature descriptors (from the spatial feature cloud of the spatial map), reduces the otherwise high computational complexity.\nThe vision-based localization system may then accelerate the matching between visual and spatial features by:\n-\tMatching the closest keyframe to the current frame by retrieving it with the BoW model\n-\tMatching the visual features between the current frame and the retrieved keyframe\n-\tMatching the visual features between the current frame and spatial feature cloud (knowing matches between visual features of the keyframes and spatial features of the spatial feature cloud)\nFigure 4.4.7.3-1 illustrates the localization process of a captured 2D frame using a spatial map. The figure shows a current frame with visual features highlighted in green. The visual features from the current frame are matched with the spatial features and keyframe information stored in the spatial map to estimate the pose of the camera when it captured the frame.\nThe figure depicts a camera pose estimation system that utilizes feature matching between a 2D captured frame and a spatial map. The system uses a feature matching algorithm to estimate the camera pose, taking into account the spatial map's features. The spatial map is a 3D representation of the environment, and the feature matching algorithm is used to match the 2D captured frame with the spatial map. The system is designed to be efficient and accurate, with a high level of accuracy in estimating the camera pose.\nFigure 4.4.7.3-1 Camera pose estimation by features matching between a 2D captured frame and a spatial map\nAR objects are positioned in reference to the real world (e.g., placing a vase on a table) using spatial anchors and trackables.\nA spatial anchor provides a fixed position and orientation in the real world based on a common frame of reference that may be used by multiple AR devices. Spatial anchors are also used independently of other spaces in case global coordinates are available to the device. In this case, the anchors are treated as global anchors as they have global coordinates for which positions are determined.\nHowever, in many cases an accurate global coordinate system is not available. In this case, spatial anchors refer to trackables for accurate positioning relative to the physical space. Trackables are elements of the real world for which features (visual or non-visual) are available and/or could be extracted. A trackable may for example be a spatial map that defines a full environment composed of floor walls and furniture in the real world consisting of several 3D points with visual features. However, there are other types of trackables, for example:\n-\tA controller with LEDs that may be tracked by an AR headset’s vision sensor. The feature in this case is the constellation of LEDs.\n-\tA fiducial marker that is detected as a black and white pattern by an AR device vision sensor. The feature in this case is the black and white pattern.\n-\tHands visible through an AR headset’s vision sensor. The feature is a learnt model for hands.\nAll of the above examples give a position of the trackable in reference to the position of the sensor (generally embedded in the AR headset).\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.5\tKey Performance Indicators and Metrics for AR",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.5.1\tSummary of TR 26.928",
                            "text_content": "In TR 26.928 [2], clause 4.2 quality experience for XR is summarized. In order to provide the feeling of presence in immersive scenes, this clause provides a summary. TR 26.928 has some focus on VR and HMDs.\nTable 4.5.1-1: KPIs from TR 26.928 with focus on VR and HMDs\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 4.5.1-1: KPIs from TR 26.928 with focus on VR and HMDs",
                                    "table number": 4,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "4.5.2\tUpdated KPIs for AR",
                            "text_content": "In TR 26.928 [2], some high-level statements on experience KPIs for AR are provided. To achieve Presence in Augmented Reality, seamless integration of virtual content and physical environment is required. Like in VR, the virtual content has to align with user's expectations. For truly immersive AR and in particular MR, it is expected that users cannot discern virtual objects from real objects.\nAlso relevant for VR and AR, but in particular AR, is not only the awareness for the user for the environment. This includes, safe zone discovery, dynamic obstacle warning, geometric and semantic environment parsing, environmental lighting and world mapping.\nBased on updated information, Table 4.6.2-1 provides new KPIs with focus on AR and in particular glasses. For some background and additional details refer for example to [10], [11], [49], [50], and [51].\nTable 4.5.2-1 Updated KPIs based on TR 26.928 with focus on AR glasses\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 4.5.2-1 Updated KPIs based on TR 26.928 with focus on AR glasses",
                                    "table number": 5,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "4.5.3\tTypical Latencies in networked AR Services",
                            "text_content": "Building on top of the architectures introduced in clause 4.2 in this document as well as the latency considerations in TR 26.928 [2], Figure 4.5.3-1 provides a summary of different latencies involved networked AR services. Based on TR 26.928 as well as Table 4.5.2-1, two relevant latency requirements for adequate user experience matter:\n-\tmotion-to-photon latency being less 20ms, but preferably even single digit latency below 10ms.\n-\tpose-to-render-to-photon latency: as small as 50-60ms\nIt is important to note that the motion-to-photon latency is primarily a function of the device implementation as it is basically covered within the AR runtime. What matters and is relevant is the time used to provide the pose information from the AR runtime to the renderer and the renderer using this pose to generate the displayed media. Final pose correction to the latest pose may always be done in the AR runtime.\nFigure 4.5.3-1 provides different latency critical uplink and downlink operations, depending on where the rendering is done, locally, in the edge or in the cloud. If done in the edge or cloud, rendered data needs to be delivered in low-latency and high-quality over the network. The typical operations in this case include:\n-\tpose detection in the UE\n-\tsending the pose through a 5G uplink network to the edge of cloud.\n-\trendering the scene in the edge or cloud\n-\tcompressing and encrypting the rendered scene and delivering to the UE\n-\tdecrypting and decompressing the rendered scene\n-\tcomposition of the scene\n-\tapplying the latest pose in the pose correction and display the immersive media.\nNote that Figure 4.5.3-1 also adds buffers that are typically handled by the AR Run time, namely eye and depth as well as sound buffers.\nThe figure depicts typical latencies in networked AR services, showing the impact of latency on user experience.\nFigure 4.5.3-1: Typical Latencies in networked AR services\nIt is ultimately relevant that in case of networking the rendering loop, the processes in the loop are executed such that the end-to-end latency requirements for the pose-to-render-to-photon latency are ensured. Clearly the “closer” the rendering happens at the AR UE, the easier it is to meet latency requirements. However, with proper support of 5G system and media functionalities, these networked AR challenges are solved. This is subject of the remaining discussion of this report.\nWith reference to TR 26.928 [2], other types of latencies impact the user experience, for example when used for cloud gaming, interactive scenes or in case of real-time network-based processing of sensor data. These aspects are not specific to AR but are also relevant. Some more details are provided in clause 6 for the different scenarios.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.6\tRelated Work",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.6.1\t3GPP",
                            "text_content": "This clause documents the 3GPP activity related to services using AR/MR device.\n-\t3GPP TR 26.928 [2] provides an introduction to XR including AR and a mapping to 5G media centric architectures. It also specified the core use cases for XR and device types.\n-\t3GPP TS 22.261 [13] identified use cases and requirements for 5G systems including AR and 3GPP TR 22.873 [14] documents new scenarios of AR communication for IMS Multimedia Telephony service.\n-\t3GPP SA4 is working on the documentation of 360-degree video support to MTSI in 3GPP TS 26.114 [15]. It will provide the recommendations of codec configuration and signalling mechanisms for viewport-dependent media delivery.\n-\t3GPP TR 26.926 [48] provides Traffic Models and Quality Evaluation Methods for Media and XR Services in 5G Systems.\n-\tIn the context of Release-17, 3GPP RAN work [16] identified traffic models for XR application and an evaluation methodology to access the XR performance.\n-\t3GPP SA4 is working on the development of the EVS Codec Extension for Immersive Voice and Audio Services (IVAS) codec. It targets encoding/decoding/rendering of speech, music and generic sound, with low latency operation and support of high error robustness under various transmission conditions, The IVAS codec is expected to provide support for a range of service capabilities, e.g., from mono to stereo to fully immersive audio, implementable on a wide range of UEs. The work on IVAS is expected to provide support for MTSI services and potentially streaming services through the definition of a new immersive audio media component.\nNOTE:\tThe integration of IVAS into the architectures developed in this report is for further study.\n-\tIn the context of Release-18 under the Terminal Audio quality performance and Test methods for Immersive Audio Services (ATIAS) work item, 3GPP SA4 is working on the specification of test methods in 3GPP TS 26.260 [56] and requirements in TS 26.261 [57] for immersive audio.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.6.2\tMPEG",
                            "text_content": "MPEG has developed a suite of standards for immersive media with a project of MPEG-I (ISO/IEC 23090 Coded Representation of Immersive Media). It contains all the media related components, including video, audio, and system for AR/MR as well as 360-degree video.\n-\tPart 1 – Immersive Media Architectures: Provides the structure of MPEG-I, core use cases and scenarios, and definitions of terminologies for immersive media\n-\tPart 2 – Omnidirectional MediA Format (OMAF): Defines a media format that enables omnidirectional media applications (360-degree video) with support of 3DoF, 3DoF+, and 6DoF based on ISOBMFF. The second edition of OMAF was published in 2021 [17].\n-\tPart 3 – Versatile Video Coding (VVC): Describes the 2D video compression standard, providing the improved compression performance and new functionalities as compared to HEVC. The first edition was published in 2021 [18].\n-\tPart 4 – Immersive Audio Coding: It provides the compression and rendering technologies to deliver 6DoF immersive audio experience.\n-\tPart 5 – Visual Volumetric Video-based Coding (V3C) and Video-based Point Cloud Compression (V-PCC): It defines the coding technologies for point cloud media data, utilizing the legacy and future 2D video coding standards. The first edition was published in 2021 [19] and the second edition for dynamic mesh compression is developing.\n-\tPart 6 – Immersive Media Metrics: It specifies a list of media metric and a measurement framework to evaluate the immersive media quality and experience. The first edition was published in 2021 [47].\n-\tPart 7 – Immersive Media Metadata: It defines common immersive media metadata to be referenced to various other standards.\n-\tPart 8 – Network-Based Media Processing (NBMP): It defines a media framework to support media processing for immersive media which may be performed in the network entities. It also specifies the composition of network-based media processing services and provides the common interfaces. The first edition was published in 2020 [20], and currently developing media processing entity capabilities and split rendering support as the second amendment.\n-\tPart 9 – Geometry-based Point Cloud Compression (G-PCC): It defines the coding technologies for point cloud media data, using techniques that traverse directly the 3D space in order to create the predictors for compression.\n-\tPart 10 – Carriage of Visual Volumetric Video-based Coding Data: It specifies the storage format for V3C and V-PCC coded data. It also supports flexible extraction of component streams at delivery and/or decoding time.\n-\tPart 11 – Implementation Guidelines for Network-based Media Processing\n-\tPart 12 – Immersive Video: It provides coding technology of multiple texture and depth views representing immersive video for 6DoF.\n-\tPart 13 – Video Decoding Interface for Immersive Media: It provides the interface and operation of video engines to support flexible use of media decoder. MPEG Systems has also initiated the next phase of development for extending MPEG-I Scene Description including the support of additional immersive media codecs, support for haptics, AR anchoring, user representation and avatars, as well as interactivity.\n-\tPart 14 – Scene Description for MPEG Media: It describes the spatial-temporal relationship among individual media objects to be integrated.\n-\tPart 15 – Conformance Testing for Versatile Video Coding\n-\tPart 16 – Reference Software for Versatile Video Coding\n-\tPart 17 – Reference Software and Conformance for Omnidirectional MediA Format\n-\tPart 18 – Carriage of Geometry-based Point Cloud Compression Data\n-\tPart 19 – Reference Software for V-PCC\n-\tPart 20 – Conformance for V-PCC\n-\tPart 21 – Reference Software for G-PCC\n-\tPart 22 – Conformance for G-PCC\n-\tPart 23 – Conformance and Reference Software for MPEG Immersive Video\n- \tPart 24 – Conformance and Reference Software for Scene Description for MPEG Media\n-\tPart 25 – Conformance and Reference Software for Carriage of Visual Volumetric Video-based Coding Data\n-\tPart 26 – Conformance and Reference Software for Carriage of Geometry-based Point Cloud Compression Data\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.6.3\tETSI Industry Specification Group",
                            "text_content": "ETSI Industry Specification Group AR Framework (ISG ARF) has developed a framework for AR components and systems [21]. It introduces the characteristics of an AR system and describes the functional building blocks of the AR reference architecture and their mutual relationships. The generic nature of the architecture is validated by mapping the workflow of several use cases to the components of this framework architecture.\nThe ETSI AR Framework Architecture describes a system composed of hardware and software components as well as data describing the real world and virtual content. The architecture is composed of three layers as described in clause 4 of [21] and illustrated in Figure 4.6.3-1.\n-\tHardware layer including:\n>\tTracking Sensors: These sensors aim to localize (position and orientation) the AR system in real-time in order to register virtual contents with the real environment. Most of AR systems such as smartphones, tablets or see-through glasses embed at least one or several vision sensors (generally monochrome or RGB cameras) as well as an inertial measurement unit and a GPS™. However, specific and/or recent systems use complementary sensors such as dedicated vision sensors (e.g. depth sensors and event cameras), or exteroceptive sensors (e.g. Infrared/laser tracking, Li-Fi™ and Wi-Fi™).\n>\tProcessing Units: Computer vision, machine learning-based inference as well as 3D rendering are processing operations requiring significant computing resources optimized thanks to dedicated processor architectures (e.g. GPU, VPU and TPU). These processing units may be embedded in the device, may be remote and/or distributed.\n>\tRendering Interfaces: Virtual content require interfaces to be rendered to the user so that he or she may perceive them as part of the real world. As each rendering device has its own characteristics, the signals generated by the rendering software generally need to be transformed in order to adapt them to each specific rendering hardware.\n-\tSoftware layer including:\n>\tVision Engine: This software aims to mix the virtual content with the real world. It consists of localizing (position and orientation) the AR device relative to the real world reference, localizing specific real objects relatively to the AR device, reconstructing a 3D representation of the real world or analysing the real world (e.g. objects detection, segmentation, classification and tracking). This software component essentially uses vision sensors signals as input, but not only (e.g. fusion of visual information with inertial measurements or initialization with a GPS), it benefits from the hardware optimization offered by the various dedicated processors embedded in the device or remote, and will deliver to the rendering engine all information required to adapt the rendering for a consistent combination of virtual content with the real world.\n>\t3D Rendering Engine: This software maintains an up-to-date internal 3D representation of the virtual scene augmenting the real world. This internal representation is updated in real-time according to various inputs such as user's interactions, virtual objects behaviour, the last user viewpoint estimated by the Vision Engine, an update of the World Knowledge to manage for example occlusions between real and virtual elements, etc. This internal representation of the virtual content is accessible by the renderer (e.g. video, audio or haptic) which produces thanks to dedicated hardware (e.g. Graphic Processing unit) data (e.g. 2D images, sounds or forces) ready to be played by the Rendering Interfaces (e.g. screens, headphones or a force-feedback arm).\n-\tData layer including:\n>\tWorld Knowledge: This World Knowledge represents the information either generated by the Vision Engine or imported from external tools to provide information about the real world or a part of this world (CAD model, markers, etc.). This World Knowledge corresponds to the digital representation of the real space used for different usages such as localization, world analysis, 3D reconstruction, etc.\n>\tInteractive Content: These Interactive Content represent the virtual content mixed to the perception of the real world. These contents may be interactive or dynamic, meaning that they include both 3D contents, their animations, their behaviour regarding input events such as user's interactions. These Interactive Contents could be extracted from external authoring tools requiring to adapt original content to AR application (e.g. 3D model simplification, fusion, and instruction guidelines conversion).\nThe figure depicts a global overview of an AR system, showcasing its architecture, components, and key features. It includes a 3D representation of the system, highlighting the various layers and components such as the user interface, sensors, cameras, and processing units. The figure also illustrates the integration of AR technology with other systems, such as the internet of things (IoT) and machine learning, to provide a comprehensive understanding of the system's capabilities and potential applications.\nFigure 4.6.3-1: Global overview of the architecture of an AR system\nIn the ETSI AR functional architecture, there are eleven logical functions as illustrated in Figure 4.6.3-2. Each function is composed of two or more subfunctions.\nThe figure depicts a functional reference architecture for a 6.3-2 network, illustrating the various components and their interconnections. The diagram includes key elements such as the core switches, optical line terminals (OLTs), and distributed nodes, which are essential for ensuring network reliability and failover capabilities. The layered design of the architecture aligns with SDN principles, promoting efficient network management and control.\nFigure 4.6.3-2: Diagram of the functional reference architecture\nThe ETSI ISG ARF has validated that the functional architecture covers all requirements for AR experience delivery in a variety of use cases. The logical functions are connected by Reference Point (RP). An RP in the AR functional architecture is located at the juncture of two non-overlapping functions and represents the interactions between those functions.\nDetails for each of the eleven functions and their subfunctions are described in clause 5 of [21] and details of each of the 18 RPs are described in clause 6 of [21].\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.6.4\tWork related to AR Runtime",
                            "text_content": "OpenXR [4] is an API that is developed by the Khronos Group for developing XR applications that address a wide range of XR devices. XR refers to a mix of real and virtual world environments that are generated by computers through interactions by humans. XR includes technologies such as virtual reality (VR), augmented reality (AR) and mixed reality (MR). OpenXR is the interface between an application and XR runtime. The runtime handles functionality such as frame composition, user-triggered actions, and tracking information.\nOpenXR is designed to be a layered API, which means that a user or application may insert API layers between the application and the runtime implementation. These API layers provide additional functionality by intercepting OpenXR functions from the layer above and then performing different operations than would otherwise be performed without the layer. In the simplest cases, the layer simply calls the next layer down with the same arguments, but a more complex layer may implement API functionality that is not present in the layers or runtime below it. This mechanism is essentially an architected \"function shimming\" or \"intercept\" feature that is designed into OpenXR and meant to replace more informal methods of \"hooking\" API calls.\nApplications may determine the API layers that are available to them by calling the  function to obtain a list of available API layers. Applications then may select the desired API layers from this list and provide them to the  function when creating an instance.\nAPI layers may implement OpenXR functions that may or may not be supported by the underlying runtime. In order to expose these new features, the API layer must expose this functionality in the form of an OpenXR . It must not expose new OpenXR functions without an associated extension.\nAn OpenXR instance is an object that allows an OpenXR application to communicate with an OpenXR runtime. The application accomplishes this communication by calling  and receiving a handle to the resulting  object.\nThe  object stores and tracks OpenXR-related application state, without storing any such state in the application’s global address space. This allows the application to create multiple instances as well as safely encapsulate the application’s OpenXR state since this object is opaque to the application. OpenXR runtimes may limit the number of simultaneous  objects that may be created and used, but they must support the creation and usage of at least one  object per process.\nSpaces are represented by  handles, which the application creates and then uses in API calls. Whenever an application calls a function that returns coordinates, it provides an  to specify the frame of reference in which those coordinates will be expressed. Similarly, when providing coordinates to a function, the application specifies which  the runtime to be used to interpret those coordinates.\nOpenXR defines a set of well-known reference spaces that applications use to bootstrap their spatial reasoning. These reference spaces are: VIEW, LOCAL and STAGE. Each reference space has a well-defined meaning, which establishes where its origin is positioned and how its axes are oriented.\nRuntimes whose tracking systems improve their understanding of the world over time may track spaces independently. For example, even though a LOCAL space and a STAGE space each map their origin to a static position in the world, a runtime with an inside-out tracking system may introduce slight adjustments to the origin of each space on a continuous basis to keep each origin in place.\nBeyond the well-known reference spaces, runtimes expose other independently tracked spaces, such as a pose action space that tracks the pose of a motion controller over time.\nFigure 4.6.4.1-1 depicts the lifecycle of an application that uses OpenXR for interaction and rendering with/to an HMD.\nThe figure depicts the application lifecycle of an OpenXR application, including the development, testing, and deployment stages. It illustrates the various steps involved in creating and deploying an OpenXR application, such as creating a new project, setting up a development environment, and integrating the application with the OpenXR framework. The figure also includes a visual representation of the application's lifecycle, including the creation of a new project, the development of the application, and the deployment of the application to a target platform.\nFigure 4.6.4.1-1: OpenXR application lifecycle\nWebXR [5] is a set of APIs that are developed by the W3C to provide support for augmented reality (AR) and virtual reality (VR) in web environments, hence the name WebXR for cross reality in the web. When a WebXR session is created, the mode of the session is indicated, i.e. whether it is an AR or VR session. VR sessions may be consumed in 2 ways, inline and immersive. In the inline mode, the VR content is rendered on the 2D screen as part of the web document. In the immersive mode, the content is rendered on an HMD with an immersive 3DoF experience. AR sessions are always immersive.\nA typical lifecycle of a WebXR application will start by checking for availability of the WebXR API support in the current browser. When the user requests the activation of a WebXR functionality, an XRSession with the desired mode is created. The XRSession instance is then used to request a frame to render using the requestAnimationFrame call. Complex scenes may require threaded rendering, which may be achieved through the usage of Worker instances. WebGL is then ultimately used to render to the provided frame. When calling the requestAnimationFrame, the application provides a callback function that will be called when a new frame is about to be rendered. The callback function will receive a timestamp, indicating the current timestamp of the XR pose. It also receives an XRFrame, which holds information about the current XR poses for all objects that are being tracked by the session. This information is then used to perform correct rendering by the application. The XRFrame offers two main functions, the getPose and getViewerPose. The getPose functions returns the relationship between two XRSpaces, which are passed in as input to that function. The getVeiwerPose returns the viewer’s pose in relationship to a reference XRSpace that is passed to the function call.\nWebXR defines a set of reference XRSpace(s) as described in the Table 4.6.4.2-1:\nTable 4.6.4.2-1: Reference XRSpace in WebXR\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 4.6.4.2-1: Reference XRSpace in WebXR",
                                    "table number": 6,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "4.6.5\tMPEG Scene Description",
                            "text_content": "A key technology in enabling immersive 3D user experiences is scene description. Scene description is used to describe the composition of a 3D scene, referencing and positioning the different 2D and 3D assets in the scene. The information provided in the scene description is then used by an application to render the 3D scene properly, using techniques such as Physically-Based Rendering (PBR) that produce realistic views.\nA scene description is typically organized as a directed acyclic graph, typically a plain tree-structure, that represents an object-based hierarchy of the geometry of a scene and its attributes/properties.  Nodes are organized in a parent-child hierarchy known informally as the node hierarchy. A node is called a root node when it doesn't have a parent. Any node may define a local space transformation.\nSpatial transformations are represented by transformation matrices or separate transform operations such as translation, rotation, and scaling. The transformations are applied hierarchically and iteratively from the root node down to the child nodes. Scene description also support animation nodes that allow to animate properties of the corresponding objects over time.\nThis structure of scene description has the advantage of reduced processing complexity, e.g. while traversing the graph for rendering. An example operation that is simplified by the graph representation is the culling operation, where branches of the graph are omitted, if deemed that the parent node’s space is not visible or relevant (level of detail culling) to the rendering of the current view frustum.\nTo address the needs of immersive applications, MPEG is finalizing the development of a scene description solution that adds extensions to glTF to support scene description. glTF 2.0 [22] provides a solid and efficient baseline for exchangeable and interoperable scene descriptions. However, glTF 2.0 has traditionally been focused on static scenes and assets, which makes it unfit to address the requirements and needs of dynamic and rich 3D scenes in immersive environments.\nAs part of its effort to define solutions for immersive multimedia, MPEG has identified the following gaps in glTF 2.0:\n-\tNo support for timed media\n-\tNo support for audio\n-\tLimited support for interactions with the scene and the assets in the scene\n-\tNo support for local and real-time media, which are crucial for example for AR experiences\nBased on this analysis, MPEG has an ongoing project to extend glTF 2.0 with the ability to add timed media to glTF 2.0-based scenes standardized in ISO/IEC 23090-14 [23].\nAdditional extensions for the support of interactivity and AR are currently being developed and will be part of the MPEG Scene Description in the next phase.\nMPEG also developed an architecture to guide the work on immersive media and scene description. Figure 4.6.5-1 depicts the MPEG-I architecture and defines the key interfaces.\nThe figure depicts the MPEG-I architecture, which is a standard for multimedia information, and defines the key interfaces for multimedia data transmission.\nFigure 4.6.5-1: MPEG-I architecture and defines the key interfaces\nThe design focuses mainly on buffers as means for data exchange throughout the media access and rendering pipeline. It also defines a Media Access Function API to request media that is referenced by the scene description, which will be made accessible through buffers.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.6.6\tMPEG-I Video Decoding Interface for Immersive Media",
                            "text_content": "The aim of VDI (MPEG-I part 13) is to address the challenges for media applications to handle multiple decoder instances running in parallel, especially in the case of immersive media. To this end, the scope of the VDI specification covers the interface between a media application and the Video Decoding Engine (VDE) sitting on the device.\nTypically, hardware decoder is exposed via API to the application. Proprietary APIs exist but also standardised one such as Khronos® OpenMaxTM and Khronos® Vulkan® Video extension. However, those APIs only allow the instantiation of video decoder instances independently from each other up to the point where the hardware decoding platform may no longer sustain the application requests, for instance due to lack of memory.\nExtensions specified in MPEG-I VDI (ISO/IEC 23090-13) allow the AR/MR application to query the capacity to simultaneously decode multiple operation points (generally specified by profile, tier and levels). This allows a better predictability of what bitstreams may be decoded by the application.\nAdditionally, VDI also defines bitstream manipulation functions for the video codecs HEVC, VVC and EVC that enable the merging and the splitting of bitstreams. This aspect of elementary stream manipulation is covered by the so-called input formatting function in MPEGI VDI. This way, an application may adapt the number of the decoder needed when several input bitstreams are to be decoded to the extent the merging operations has been enabled by the proper encoding constraints.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.6.7\tMPEG-I Carriage of Point Cloud Compression Data",
                            "text_content": "For the encapsulation and storage of coded volumetric media, two MPEG systems standards may be considered as potential technologies: ISO/IEC 23090-10 [24] and ISO/IEC 23090-18 [25]. ISO/IEC 23090-10 and ISO/IEC 23090-18 define how to structure and carry the various components in a V3C bitstream or G-PCC bitstream, respectively, in an ISOBMFF media container to support flexible and partial access (e.g., using multiple component tracks and tile tracks) as well as adaptive streaming. Both specifications support single track encapsulation as well as multi-track encapsulation, where different components of the bitstream are carried in separate tracks in the container. In addition, these standards also define metadata tracks that may be associated with the main media tracks and carry additional timed information that signal changes in the spatial partitioning of the volumetric content and the mapping to different independently decodable tiles as well as viewport-related information.\nIn addition, ISO/IEC 23090-10 and ISO/IEC 23090-18 define how to signal V3C and G-PCC content in a DASH MPD file. This includes defining new DASH descriptors that signal metadata about the type of component carried by each adaptation set and using pre-selections to group the adaptation sets of the different components associated with the volumetric media content. Other descriptors are also defined for signalling independently decoded spatial sub-divisions of the content to support partial streaming. In addition to signalling for DASH-based delivery, ISO/IEC 23090-10 and ISO/IEC 23090-18 also define descriptors for signalling volumetric media assets for delivery over MMT.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.6.8\tWeb Real-Time Communication (WebRTC)",
                            "text_content": "The Web Real-Time Communication (WebRTC) is an API and set of protocols that enable real-time communication. The WebRTC protocols enable any two WebRTC agents to negotiate and setup a bidirectional and secure real-time communication channel. The WebRTC API exposes a JavaScript-based API to enable the development of applications that make use of the user’s existing multimedia capabilities to establish real-time communication sessions. However, access to the WebRTC set of protocols is possible through other programming languages.\nThe WebRTC protocols are developed and maintained by the rtcweb group in IETF. The WebRTC API is developed by W3C.\nThe WebRTC API is decomposed into three layers:\n-\tAPI for web developers that consists mainly of the MediaStream, RTCPeerConnection, and RTCDataChannel objects.\n-\tAPI for browser and user agent implementers and providers\n-\tOverridable API for audio/video capture and rendering and for network input/output, which the browser implementers may hook their own implementations to.\nThe main WebRTC stack components are the voice engine, the video engine, and the transport component.\nThe transport component ensures a secure transport channel for both parties of the call to communicate. It relies on an RTP protocol stack that runs over DTLS and leverages the SRTP profile.\nThe following diagram depicts the WebRTC protocol stack:\nThe figure depicts the WebRTC protocol stack, which is a set of components used for real-time communication over the internet. It includes the WebRTC client, server, and various APIs for audio and video communication. The figure illustrates the flow of data between the client and server, with the client initiating a connection and the server responding with audio and video streams. The figure also includes a diagram of the network stack, showing the various components and their connections.\nFigure 4.6.8-1: WebRTC protocol stack\nWebRTC delegates the signalling exchange to the application. The signalling protocol and format may be chosen by the application freely. However, the offer and answer are generated in the SDP format. The ICE candidates may be provided as strings or in JSON format.\nWebRTC needs negotiation for the following purposes:\n-\tNegotiation of the media streams and formats: this relies on the SDP offer/answer mechanism to generate and validate media streams and parameters.\n-\tNegotiation of the transport parameters: this relies on ICE to identify and test ICE candidates. Whenever a higher priority ICE candidate is validated, the connection will switch to it.\nThe following call flow shows an example of the ICE negotiation process:\nThe figure depicts the WebRTC ICE negotiation process, which is a crucial step in the communication process for real-time communication applications. The process involves two parties, the sender and the receiver, exchanging information to establish a connection. The figure shows the various stages of the negotiation process, including the initiation of the connection, the exchange of data, and the establishment of a secure connection. The figure is a visual representation of the complex process, making it easier to understand and interpret.\nFigure 4.6.8-2: WebRTC ICE negotiation process\nDue to the separation of the negotiation of the transport parameters from the media parameters, appropriate QoS negotiation needs to consider consecutive and asynchronous changes to the connection parameters. In case of a relay server, such as a TURN server, is deployed, the QoS negotiation is to be extended to appropriately cover the outbound streams as well.\nA subset of WebRTC, limited to a protocol stack and implementation excluding codecs and other media processing functions defined in W3C and/or IETF, is considered in clauses 6.5 and 8.3 to define an instantiation of AR conversational services.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.6.9\tJoint workshop with Khronos and MPEG on \"Streamed Media in Immersive Scene Descriptions\"",
                            "text_content": "3GPP also participated in a joint workshop with Khronos and MPEG on the topic of “Streamed Media in Immersive Scene Descriptions” in September 2021 in order to identify common and complementary aspects on defining networked media. All presentations are provided in [53][54]. The workshop attracted more than 200 participants. A survey was conducted among the participants and there was broad positive feedback on the event with a request to a follow-up in 2022. Details are available in [54]. An initial summary of main observations is provided as follows:\n-\tComplementary work – many touch points - collaboration seems to be beneficial\n-\tSpecific topics identified, but may be digested further\n>\tglTF and extensions by MPEG-I Scene description\n>\tTools and implementation support\n>\tVulkan video and VDI\n>\tExtended Realities: OpenXR, MPEG-I Phase 2 including AR, Interactivity, and Haptics\n>\tSystems and Split Rendering: OpenXR, 3GPP connectivity, MPEG codecs\n-\tChallenges: timelines, publication rules, IPR policies, membership\n-\tOpportunities: complementary expertise, implementation and developer support, joint promotion, focus\n-\tProposed next steps:\n>\tcontinue the discussion\n>\tset up some kind of discussion platform\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "5\tCore Use Cases",
            "description": "This clause documents the core use cases and scenarios for AR/MR devices, which serve to extract requirements, functional structure, related media format, and protocols for the 5G systems. Parts of the use cases are derived from XR use cases in TR26.928 [2] based on the relevance to AR/MR device type. In addition, the other use cases and scenarios are collected in Annex A of this document.\nTable 5-1 provides a list of all the collected use cases.\nTable 5-1: List of use cases for AR/MR services\n\nThe use cases may be grouped into several categories based on the similar requirements for media flow and device functional structure.\n",
            "summary": "",
            "tables": [
                {
                    "description": "Table 5-1: List of use cases for AR/MR services",
                    "table number": 7,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "6\tMapping to 5G System Architecture",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "6.1\tGeneral",
                    "description": "",
                    "summary": "",
                    "text_content": "Based on the identified use cases in clause 5, this clause documents how AR/MR service scenarios are supported in 5G system architecture.\nThere already exist developed 5G system architectures relevant to deliver immersive media depending on the underlying functionalities, such as real-time communications, adaptive delivery, QoS guarantee, and a support of network node processing. An architecture of 5G Media Streaming (5GMS) for both downlink and uplink is specified in TS 26.501 [26] and it is under development to support the edge media processing. In addition, MTSI architecture extended to 5G system [15] may be applied to AR/MR conversational scenarios to guarantee the specific service QoS. In the following clauses, these relevant architectures will be analysed to identify potential standardisation areas for each scenario.\nNote that only STAR UE and EDGAR UE in Table 4.2.2.1-1 are taken into account, as WLAR UE as well as WTAR UE have similar functionalities with STAR UE from a 5G system perspective. Specifically, STAR UE (and WLAR/WTAR UEs) possibly has an on-device decoding and rendering capability for immersive media and may rely on support from 5G cloud/edge for a certain condition. On the other hand, EDGAR UE always requires 5G cloud/edge for immersive media decoding and rendering, and the conventional 2D media is exchanged in Uu interface.\nTable 6.1-1 provides a list of AR/MR service scenarios and the associated use cases for each. Note that some use cases may be duplicated as they address multiple features.\nTable 6.1-1: List of service scenario mapping to use cases\n\n",
                    "tables": [
                        {
                            "description": "Table 6.1-1: List of service scenario mapping to use cases",
                            "table number": 8,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "6.2\tImmersive media downlink streaming",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.2.1\tIntroduction",
                            "text_content": "This clause introduces the case where immersive AR/MR media is streamed to a 5G AR UE using basic functionalities as defined in 5G Media Streaming for downlink (5GMSd).\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.2\tRelevant use cases",
                            "text_content": "The following use cases are relevant to this scenario.\n-\tUC#2: AR sharing\n-\tUC#14: AR Streaming with Localization Registry\n-\tUC#17: AR remote advertising\n-\tUC#18: Streaming of volumetric video for glass-type MR Devices\nAn immersive video which was pre-captured or pre-generated are stored in the server of an application provider. On a user’s request, the desired immersive video is streamed to 5G AR UE throughout 5GMS architecture. The user is able to play, pause, stop, and enjoy the trick play while watching the video. In this use case, the scene description of the pre-generated video may get updated as the video progresses. However, these updates are independent to the user’s interaction or change of pose.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.3\tArchitectures",
                            "text_content": "Figure 6.2.3.1-1 provides a basic extension of 5G Media Streaming for immersive media downlink using a STAR UE, when all essential AR/MR functions in a UE are available for typical media processing use cases. In addition to media delivery, also scene description data delivery is included.\n\nFigure 6.2.3.1-1: STAR-based 5GMS Downlink Architecture\nNOTE:\tMicrophones are not shown in Figure 6.2.3.1-1, the focus is on the receiving side of an AR UE.\nFigure 6.2.3.2-1 provides a basic extension of 5G Media Streaming download for immersive media using an EDGAR UE. In this context, it is expected that the edge will pre-render the media based on pose and interaction information received from the 5G EDGAR UE. It is also highlighted, that the 5G EDGAR UE may consume the same media assets from an immersive media server as the STAR UE according to Figure 6.2.3.1-1, but the communication of the edge server to this immersive server is outside of the considered 5G Media Streaming architecture.\nFigure 6.2.3.2-1: EDGAR-based 5GMS Downlink Architecture\nNOTE:\tMicrophones are not shown in Figure 6.2.3.2-1, the focus is on the receiving side of an AR UE.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.4\tProcedures and call flows",
                            "text_content": "Figure 6.2.4.1-1 illustrates the procedure diagram for 5G immersive media downlink streaming using a STAR-based UE when all essential AR/MR functions in a UE are available without an assist by an edge.\n\nFigure 6.2.4.1-1: STAR-based procedure for 5G downlink streaming\nPrerequisites and assumptions:\n-\tThe AR/MR Scene Manager includes immersive media rendering and scene graph handling functionalities.\n-\tThe Media Player includes immersive content delivery and immersive media decoding functionalities.\n-\tThe AR/MR Application in the UE is run by the user.\n-\tThe STAR UE initialises AR registration (starts analysing the surroundings where a user/UE is located), it namely:\na.\tcaptures its surroundings via camera(s)\nb.\tanalyses where the device is located\nc.\tregisters the device into the analysed surroundings.\n-\tAR/MR Application and AR/MR Application Provider have exchanged some information, such as device capability or content configuration, for content rendering. The exchange procedures for device capability and content configuration are FFS.\n-\tAR/MR Application Provider has established a Provisioning Session and its detailed configurations has been exchanged.\n-\tAR/MR Application Provider has completed to set up ingesting immersive contents.\nProcedures:\n1.\tThe scene content is ingested by the 5GMSd AS.\n2.\tService Announcement is triggered by AR/MR Application. Service Access Information including Media Client entry or a reference to the Service Access Information is provided through the M8d interface.\n3.\tDesired media content is selected.\n4.\tOptionally, the Service Access information is acquired or updated.\n5.\tThe AR/MR Application initializes the Scene Manager with the entry point (full scene description) URL.\n6.\tThe Media Client establishes the transport session for receiving the entry point (scene description).\n7.\tThe Media Client requests and receives the full scene description.\n8.\tThe entry point (scene description) is processed.\n9.\tThe AR/MR Scene Manager requests the creation of a new AR/MR session from the AR Runtime.\n10.\tThe AR Runtime creates a new AR/MR session.\nAR Media Delivery Pipeline, steps 11~23 requests, receives and renders scenes and scene updates:\n11.\tThe Media Client and/or AR/MR Scene Manager notifies the necessary QoS information required to the Media Session Handler.\n12.\tThe Media Session Handler shares the information with the 5GMSd AF, in some cases including desired QoS information.  Based on existing provisioning by the AR/MR Application Provider, the 5GMSd AF may request QoS modifications to the PDU sessions.\nSteps 13~15 establish the transport sessions, receives, and process the delivery manifests:\n13.\tFor the required media content, the Media Client establishes the transport session(s) to acquire delivery manifest(s) information.\n14.\tThe Media Client requests and receives the delivery manifest(s) from the 5GMSd AS.\n15.\tThe Media Client processes the delivery manifest(s).  It determines for example the number of needed transport sessions for media acquisition.  The Media Client is expected to be able to use the delivery manifest(s) information to initialize the media pipelines for each media stream.\n16.\tThe AR/MR Scene Manager and Media Client configures the rendering and delivery media pipelines.\n17.\tThe Media Client establishes the transport session(s) to acquire the media content.\nMedia session loop, steps 18~23 provide the latest pose information, request, receive and render the media objects of the immersive scene:\n18.\tThe latest pose information is acquired by the AR/MR Scene Manager and shared to the Media Client.\n19.\tThe Media Client requests the immersive media data according to the delivery manifest processed, possibly taking into account pose information (e.g., viewport dependent streaming).\n20.\tThe Media Client receives the immersive media data and triggers the media rendering pipeline(s), including the registration of AR content into the real world accordingly.\n21.\tThe Media Client decodes and processes the media data. For encrypted media data, the Media Client may also perform decryption.\n22.\tThe Media Client passes the media data to the AR/MR Scene Manager.\n23.\tThe AR/MR Scene Manager renders the media, and passes the rendered media to the AR Runtime, which performs further processing such as registration of the AR content into the real world, and pose correction.\nFigure 6.2.4.2-1 illustrates the procedure diagram for 5G immersive media downlink streaming using an EDGAR-based UE.\n\nFigure 6.2.4.2-1: EDGAR-based procedure for 5G downlink streaming\nPrerequisites and assumptions:\n-\tIdentical to those from the STAR UE case.\nProcedures:\n1~8.\tIdentical to steps 1~8 from the STAR UE case (figure 6.2.4.1-1).\n9.\tBased on the processed scene description and the device capabilities, the 5GMSd AS/EAS is selected, and edge processes are instantiated using the processes defined in 5GMS_EDGE:\na.\tThe AR/MR Lightweight Scene Manager sends the scene description and the device capabilities to 5GMS AS. The 5GMS AS derives the EAS KPIs and if needed selects a new AS/EAS (through AF) based on the new KPI.  Then the edge processes are started, and a new entry point URL is provided to the AR/MR Lightweight Scene Manager.\nb.\tThe AR/MR Lightweight Scene Manager derives the EAS KPIs from the scene description and device capabilities, requests the AF to provide the list of suitable EAS. Then the AR/MR Lightweight Scene Manager selects the AS/EAS and requests to start the edge processes in the EAS. The edge processes are started, and a new entry point URL is provided to the AR/MR Lightweight Scene Manager.\n10.\tThe AR/MR Lightweight Scene Manager requests the lightweight scene description. The edge processes derive the lightweight scene description from the full scene description and provide it to AR/MR Lightweight Scene Manager.\n11.\tThe simplified entry point (lightweight scene description) is processed.\n12~19.\t\tIdentical to steps 9~16 from the STAR UE case (figure 6.2.4.1-1).\n20.\tThe Media Client establishes the transport session(s) to acquire the media content.\n21.\tThe 5GMSd AS initiates and starts a media session. This media session forms a stateful session loop specific to the UE, containing steps 22~25:\nStateful media session loop (steps 22~28):\n22.\tThe latest pose information is acquired by the AR/MR Lightweight Scene Manager and shared to the Media Client.\n23.\tThe Media Client sends the latest pose information to the 5GMSd AS.\n24.\tThe 5GMSd AS performs pre-rendering of the media based on the latest received pose information and possibly any original scene update. Pre-rendering may typically consist of decoding and rendering immersive media, and encoding the rendered (2D) media.\n25.\tThe pre-rendered media is sent by the 5GMSd AS to the Media Client.\n26.\tThe Media Client decodes and processes the media data. For encrypted media data, the Media Client may also perform decryption.\n27.\tThe Media Client passes the media data to the AR/MR Lightweight Scene Manager.\n28.\tThe AR/MR Lightweight Scene Manager renders the media, and passes the rendered media to the AR Runtime, which performs further processing such as registration of the AR content into the real world, composition, and pose correction.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.5\tContent formats and codecs",
                            "text_content": "Based on the use cases, the following formats, codecs, and packaging formats are of relevance for media streaming of AR:\n-\tGeneral\n>\tBasic scene graph and scene description\n>\t2D uncompressed video formats and video compression codecs\n>\tRegular audio formats and audio compression codecs\n-\tIn addition, for STAR-based UE\n>\tRicher scene graph data\n>\t3D formats such as static and dynamic point clouds or meshes\n>\tSeveral video decoding instances\n>\tDecoding tools for such formats\n>\tDASH/CMAF based delivery\n-\tIn addition, for EDGAR-based UE\n>\t2D compression tools for eye buffers as defined in clause 4.5.2\n>\tDecoding tools for such formats\n>\tAt least two video decoding instances\n>\tLow-latency downlink real-time streaming of the above media\n>\tUplink streaming of pose information and other relevant information, such as input actions\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.6\tKPIs and QoS",
                            "text_content": "The above scenarios relate to the following cases in TR 26.928 [2], clause 6. In particular:\n-\tFor STAR:\n>\tViewport-independent streaming based on clause 6.2.2 as defined TR 26.928 [2],\n>\tViewport-dependent streaming based on clause 6.2.3 as defined TR 26.928 [2],\n-\tFor EDGAR:\n>\tRaster-based split rendering based on clause 6.2.5 as defined TR 26.928 [2].\nFor STAR-based devices and viewport-independent streaming, processing of updated pose information is only done locally in the XR device. Delivery latency requirements are independent of the motion-to-photon latency. Initial considerations on QoE parameters are provided in TR 26.928 [2], clause 6.2.2.5. The XR media delivery are typically built based on download or adaptive streaming such as DASH such that one may adjust quality to the available bitrate to a large extent. Compared to the viewport independent delivery, for viewport dependent streaming, updated tracking and sensor information impacts the network interactivity. Typically, due to updated pose information, HTTP/TCP level information and responses are exchanged every 100-200ms. For more details, refer to clause 6.2.3 as defined TR 26.928 [2]. Such approaches may reduce the required bitrate compared to viewport independent streaming by a factor of 2 to 4 at the same rendered quality. It is important to note that viewport-dependent streaming technologies are typically also built based on adaptive streaming allowing to adjust quality to the available bitrate. The knowledge of tracking information in the XR Delivery receiver just adds another adaptation parameter. However, generally such systems may be flexibly designed taking into account a combination/tradeoff of bitrates, latencies, complexity and quality. Suitable 5QI values for adaptive streaming over HTTP are 6, 8, or 9 as defined in TS 23.501 [55], clause 5.7.4 and also indicated in clause 4.3.3 of TR 26.928 [2].\nFor EDGAR-based devices, raster-based split rendering based on clause 6.2.5 as defined TR 26.928 [2] applies. With the use of pose corrections, the key latency for the network is the motion-to-render-to-photon delay as introduced in clause 4.5.2 and 4.5.3, i.e. the end-to-end latency between the user motion and the rendering is 50-60ms. The formats are defined in clause 4.5.2 as follows\n-\tfor 30 x 20 degrees, 1.5K by 1K per eye is required and 1.8K by 1.2K per eye is desired\n-\tfor 40 x 40 degrees, 2K by 2K required and 2.5 K by 2.5 K desired\nColours are typically RGB but may be converted to YUV.  Framerates are typically 60fps to 90fps. The above formats result in typically in maximum 4K content at 60 fps. Modern compression tools compress this to 30 to 50 Mbit/s. Stereo audio signals are considered, requiring bitrates that are negligible compared to the video signals. In order to support warping and late stage reprojection, some depth information may be added. For communication a real-time capable content delivery protocol is needed, and the network needs provide reliable delivery mechanisms. 5QI values exist that may address the use case, such 5QI value number 80 with 10ms, however this is part of the non-GBR bearers (see clause). In addition, it is unclear whether the 10ms with such high bitrates and low required error rates may be too stringent and resource consuming.\nHence, for simple split rendering in the context of the requirements in this clause, suitable 5QIs 89 and 90 have been defined in Rel-17 in TS 23.501 in Rel-17 addressing the latency requirements in the range of 10-20ms and bitrate guarantees to be able to stream up to 50 Mbps consistently. Significant opportunities exist to support split rendering with advanced radio tools, see for example TR 26.926 [48] for performance evaluation.\nThe uplink is predominantly the pose information. Data rates are several 100 kbit/s and the latency need to be small in order to not add to the overall target latency. Suitable 5QIs 87 and 88 have been defined in Rel-17 in TS 23.501 to stream uplink pose information.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.7\tStandardization areas",
                            "text_content": "The list of potential standardization area that has been collected is provided in the following:\n-\tHTTP-Streaming of immersive scenes with 2D and 3D media formats and objects to STAR-based devices including\n>\tImmersive media format and profile with integration into 5GMS for STAR-based devices\n>\tScene description format, functionality, and profile as an entry point of immersive media\n>\tRelevant subset of media codecs for different media types and formats\n>\tCMAF encapsulation of immersive media for 5G media streaming\n>\tViewport independent and viewport dependent streaming\n-\tSplit rendering delivery of immersive scenes to EDGAR-based devices\n>\tMedia payload format to be mapped into RTP streams\n>\tCapability exchange mechanism and relevant signalling\n>\tProtocol stack and content delivery protocol\n>\tCross-layer design, radio and 5G system optimizations for QoS support\n>\tUplink streaming of predicted pose information and input actions\n-\tRequired QoE metric\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.3\tInteractive immersive services",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.3.1\tIntroduction",
                            "text_content": "This clause introduces the case where interactive immersive service. In this case, pose and other interactions are sent uplink for the Interactive Immersive Server to render the scene accordingly.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.3.2\tRelevant use cases",
                            "text_content": "The following use cases are relevant to this scenario.\n-\tUC#1: 3D Image Messaging\n-\tUC#2: AR Sharing\n-\tUC#4: AR guided assistant at remote location (industrial services)\n-\tUE#5: Police Critical Mission with AR\n-\tUE#15: 5G Shared Spatial Data\n-\tUE#16: AR remote cooperation\n-\tUC#21: AR gaming\nIn this scenario, a user interaction is sent from a UE to a server, so that the server handles the user’s request to the immersive media scene (e.g., changing the context such as translation, rotation, and scaling). The processed scene is sent back to a UE in a similar manner of immersive media streaming case.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.3.3\tArchitectures",
                            "text_content": "Figure 6.3.3.1-1 provides an architecture for immersive interactive media distribution using a STAR UE.\n\nFigure 6.3.3.1-1: STAR-based 5G interactive immersive service architecture\nNOTE:\tMicrophones are not shown in Figure 6.3.3.1-1, the focus is on the receiving side of an AR UE.\nFigure 6.3.3.2-1 provides an architecture for Interactive Immersive Media distribution using a EDGAR UE. In this case, similar as before, most of the rendering needs to accomplished on the server.\n\nFigure 6.3.3.2-1: EDGAR-based 5G interactive immersive service architecture\nNOTE:\tMicrophones are not shown in Figure 6.3.3.2-1, the focus is on the receiving side of an AR UE.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.3.4\tProcedures and call flows",
                            "text_content": "Figure 6.3.4.1-1 illustrates the procedure diagram for interactive immersive services using a STAR-based UE when all essential AR/MR functions in a UE are available without an assist by an edge.\n\nFigure 6.3.4.1-1: STAR-based procedure for interactive immersive service\nPrerequisites and assumptions:\n-\tThe AR/MR Scene Manager includes immersive media rendering and scene graph handling functionalities.\n-\tThe Media Player includes immersive content delivery and immersive media decoding functionalities.\n-\tThe AR/MR Application in the UE is run by the user.\n-\tThe STAR UE initialises AR registration (starts analysing the surroundings where a user/UE is located), it namely:\na.\tcaptures its surroundings via camera(s)\nb.\tanalyses where the device is located\nc.\tregisters the device into the analysed surroundings.\n-\tAR/MR Application and AR/MR Application Provider have exchanged some information, such as device capability or content configuration, for content rendering. The exchange procedures for device capability and content configuration are FFS.\n-\tAR/MR Application Provider has established a Provisioning Session and its detailed configurations has been exchanged.\n-\tAR/MR Application Provider has completed to set up ingesting immersive contents.\nProcedures:\n1.\tThe Scene Server context is established, and scene content is ingested by the Media AS.\n2.\tService Announcement is triggered by AR/MR Application. Service Access Information including Media Client entry or a reference to the Service Access Information is provided through the M8d interface.\n3.\tDesired media content is selected.\n4.\tOptionally, the Service Access information is acquired or updated.\n5.\tThe AR/MR Application initializes the Scene Manager with the entry point (full scene description) URL.\n6.\tThe Media Client establishes the transport session for the scene session between the Scene Manager in the UE and the Scene Server.\n7.\tThe Media Client requests and receives the full scene description. The entry point (scene description) is processed by the AR/MR Scene Manager, and a scene session is created.\n8.\tThe AR/MR Scene Manager requests the creation of a new AR/MR session from the AR Runtime.\n9.\tThe AR Runtime creates a new AR/MR session.\nAR Media Delivery Pipeline, steps 10~24, send the interaction and pose information and receives and renders the updated scenes accordingly:\n10.\tThe latest interaction and pose information are acquired by the AR/MR Scene Manager and shared to the Media Client. The Media Client sends this information to the Media AS and Scene Server.\n11.\tThe Scene Server processes the scene according to the interaction and pose information from the UE. Depending on the level of processing, the current scene may be updated or replaced.\n12.\tWhen needed, one of the following steps:\n12a.\tThe Scene Server sends a new scene entry points to the AR/MR Scene Manager through the Media AS and Media Client (go to step 7), or\n12b.\tThe Scene Server sends a scene update (updating streams/objects) to the AR/MR Scene Manager through the Media AS and Media Client.\n13.\tThe AR/MR Scene requests to create additional streaming sessions if needed for new media objects in the scene.\nNOTE:\tThe number of the additional streaming sessions depends on the delivery mechanism. One or more media objects may be delivered through a single manifest and/or use of the same connection. Therefore, introduction of every new media object may not need an additional streaming session.\n14. The Media Session Handle establishes the additional streaming sessions based on the received request.\nStreaming session, steps 15~18 establish the transport sessions for media objects and configure the media pipelines\n15.\tFor the required media content, the Media Client establishes the transport session(s) to acquire delivery manifest(s) information.\n16.\tThe Media Client requests and receives the delivery manifest(s) from the Media AS.\n17.\tThe Media Client processes the delivery manifest(s).  It determines for example the number of needed transport sessions for media acquisition.  The Media Client is expected to be able to use the delivery manifest(s) information to initialize the media pipelines for each media stream.\n18.\tThe AR/MR Scene Manager and Media Client configures the rendering and delivery media pipelines.\n19.\tThe Media Client establishes the transport session(s) to acquire the media content.\nMedia session loop includes steps 20~24 which are for streaming, decoding and rendering media components:\n20.\tThe Media Client requests the immersive media data according to the delivery manifest processed, possibly taking into account pose information (e.g., viewport dependent streaming).\n21.\tThe Media Client receives the immersive media data and triggers the media rendering pipeline(s), including the registration of AR content into the real world accordingly.\n22.\tThe Media Client decodes and processes the media data. For encrypted media data, the Media Client may also perform decryption.\n23.\tThe Media Client passes the media data to the AR/MR Scene Manager.\n24.\tThe AR/MR Scene Manager renders the media, and passes the rendered media to the AR Runtime, which performs further processing such as registration of the AR content into the real world, and pose correction.\nFigure 6.3.4.2-1 illustrates the procedure diagram for interactive immersive services using an EDGAR-based UE.\n\nFigure 6.3.4.2-1: EDGAR-based procedure for interactive immersive service\nPrerequisites and assumptions:\n-\tIdentical to those from the STAR UE case.\nProcedures:\n1~7.\tIdentical to steps 1~7 from the STAR UE case (figure 6.3.4.1-1).\n8.\tBased on the processed scene description and the device capabilities, the Media AS/EAS is selected, and edge processes are instantiated using the processes defined in EDGE:\na.\tThe AR/MR Lightweight Scene Manager sends the scene description and the device capabilities to Media AS. The Media AS derives the EAS KPIs and if needed selects a new AS/EAS (through AF) based on the new KPI.  Then the edge processes are started, and a new entry point URL is provided to the AR/MR Lightweight Scene Manager.\nb.\tThe AR/MR Lightweight Scene Manager derives the EAS KPIs from the scene description and device capabilities, requests the AF to provide the list of suitable EAS. Then the AR/MR Lightweight Scene Manager selects the AS/EAS and requests to start the edge processes in the AS. The edge processes are started, and a new entry point URL is provided to the AR/MR Lightweight Scene Manager.\n9.\tThe AR/MR Lightweight Scene Manager requests the lightweight scene description. The edge processes derive the lightweight scene description from the full scene description and provide it to AR/MR Scene Manager.\n10.\tThe simplified entry point (lightweight scene description) is processed.\n11~21.\tIdentical to steps 8~18 from the STAR UE case (figure 6.3.4.1-1).\nNOTE:\tAfter step 15a (12a in Figure 6.3.4.1-1): go to step 10\n22.\tThe Media Client establishes the transport session(s) to acquire the media content.\n23.\tThe Media AS initiates and starts a media session. This media session forms a stateful session loop specific to the UE, containing steps 25~28:\nStateful media session loop (steps 24~30):\n24.\tThe latest pose information is acquired by the AR/MR Lightweight Scene Manager and shared to the Media Client.\n25.\tThe Media Client sends the latest pose information to the Media AS.\n26.\tThe 5GMSd AS performs pre-rendering of the media based on the latest received pose information. Pre-rendering may typically consist of decoding and rendering immersive media, and encoding the rendered (2D) media.\n27.\tThe pre-rendered media is sent by the Media AS to the Media Client.\n28.\tThe Media Client decodes and processes the media data. For encrypted media data, the Media Client may also perform decryption.\n29.\tThe Media Client passes the media data to the AR/MR Lightweight Scene Manager.\n30.\tThe AR/MR Lightweight Scene Manager renders the media, and passes the rendered media to the AR Runtime, which performs further processing such as registration of the AR content into the real world, composition, and pose correction.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.3.5\tContent formats and codecs",
                            "text_content": "Based on the use cases, the following formats, codecs and packaging formats are of relevance for interactive immersive media distribution of AR:\n-\tScene graph and scene description\n-\t2D video formats\n-\t3D formats such as static and dynamic point clouds or meshes\n-\t2D video formats with depth\n-\tAudio formats supporting mono, stereo, and/or spatial audio\n-\tSeveral video decoding instances\n-\tDecoding tools for such formats\n-\tLow-latency downlink real-time streaming of the above media\n-\tUplink streaming of pose information and interaction data\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.3.6\tKPIs and QoS",
                            "text_content": "The above scenarios relate to the following cases in TR 26.928 [2], clause 6. In particular:\n-\tFor STAR:\n>\tViewport-dependent streaming based on clause 6.2.3 as defined TR 26.928 [2],\n>\tRaster-based split rendering based on clause 6.2.5 as defined TR 26.928 [2],\n>\tGeneralized XR split rendering based on clause 6.2.6 as defined TR 26.928 [2].\n-\tFor EDGAR:\n>\tRaster-based split rendering based on clause 6.2.5 as defined TR 26.928 [2].\nFor STAR-based delivery, a basic architecture as shown in Figure 6.3.6-1 applies. Two important latency considerations are important:\n-\tUser interaction latency, i.e. the time duration between the moment at which a user action is initiated and the time such an action is taken into account by the stage performer or content creation engine. In the context of gaming, this is the time between the moment the user interacts with the game and the moment at which the game engine processes the player’s response.\n-\tEnd-to-End Latency (EEL): The latency for an action that is originally presented in the scene or captured by camera until its visibility on the remote display.\n-\tRound-trip Interaction Delay (RID): The time of an action by the user until it sees the action reflected on its screen. This delay is the sum of the user interaction delay and End-to-End Latency.\n\nFigure 6.3.6-1: Architecture and latencies for interactive immersive service\nThe maximum RID depends on the type of scene.  A typical example is the Stadia cloud gaming platform and an excellent introduction is provided here [52]. Some extracted high-level requirements on user experience for RID are provided between time 700 to 800 ms [52].\nSimilar data is collected in TR 26.928 [2], clause 4.5. Typically, systems have maximum delay requirements between 60ms and 500ms. In terms of formats and bitrates, similar considerations as for clause 6.2.6 apply. However, note that in many cases a pre-rendering is applied in the network, such that data rates and formats are more similar to the split-rendering considerations. Similar considerations as for clause 6.2.6 apply on raster-based split rendering.\nFor EDGAR-based devices, raster-based split rendering based on clause 6.2.5 as defined TR 26.928 [2] applies. Similar considerations as for clause 6.2.6 apply.\nThe uplink is predominantly the pose information and user interactions. Data rates are several 100 kbit/s and the latency need to be small in order to not add to the overall target latency.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.3.7\tStandardization areas",
                            "text_content": "The list of potential standardization area that has been collected is provided in the following:\n-\tStreaming of immersive scenes with 2D and 3D media formats and objects to STAR-based devices including\n>\tLow-latency streaming protocols to support latencies in the range between 50 to 500ms, typically using RTP-based real-time streaming based on cloud rendering\n>\tScene description format, functionality, and profile as an entry point of immersive media\n>\tSimplified 3D media formats and 2D media formats with integration for STAR-based devices\n>\tRelevant subset of media codecs for different media types and formats\n>\tRTP encapsulation of media formats\n>\t5G System and 5G Media Streaming support\n-\tSplit rendering delivery of immersive scenes to EDGAR-based devices\n>\tSimple 2D media formats that match AR glass display capabilities\n>\tMedia payload format to be mapped into RTP streams\n>\tCapability exchange mechanism and relevant signalling\n>\tProtocol stack and content delivery protocol\n>\tCross-layer design, radio and 5G system optimizations for QoS support\n>\tUplink streaming of predicted pose information\n-\tRequired QoE metrics\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.4\t5G cognitive immersive service",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.4.1\tIntroduction",
                            "text_content": "This clause introduces the case of cognitive immersive service. In this case, media and other interactions are sent uplink in order for the cognitive server to create semantical perception.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.2\tRelevant use cases",
                            "text_content": "The following use cases are relevant to this scenario.\n-\tUC#4: AR guided assistant at remote location (industrial services)\n-\tUE#5: Police Critical Mission with AR\n-\tUE#14: AR Streaming with Localization Registry\n-\tUE#16: AR remote cooperation\n-\tUC#20: AR IoT control\nIn this scenario, a media captured in a UE may be sent to a cognitive server to request semantical perception. The server processes and outputs the perception results, then responds the outputs to the UE. For example, a UE regularly scans his/her environments and sends the captured media such as video, depth-maps, sensor output, and XR Spatial Description data (if SLAM and XR Spatial Compute processing is involved) to the cognitive server. The server identifies each component in the environments and sends back to the UE the identified perception outputs so that the UE may render in textual or visual overlays. The server may also send to the UE XR Spatial Description data, such as spatial anchors and trackables, in order to facilitate rendering.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.3\tArchitectures",
                            "text_content": "Figure 6.4.3.1-1 provides an architecture for immersive interactive media distribution using a STAR UE.\n\nFigure 6.4.3.1-1: STAR-based 5G cognitive immersive service architecture\nNOTE:\tMicrophones are not shown in Figure 6.4.3.1-1, the focus is on the receiving side of an AR UE.\nFigure 6.4.3.2-1 provides an architecture for Cognitive Immersive Media distribution using a EDGAR UE. In this case, similar as before, most of the rendering needs to be accomplished on the server.\n\nFigure 6.4.3.2-1: EDGAR-based 5G cognitive immersive service architecture\nNOTE:\tMicrophones are not shown in Figure 6.4.3.2-1, the focus is on the receiving side of an AR UE.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.4\tProcedures and call flows",
                            "text_content": "Figure 6.4.4-1 illustrates the generic procedure diagram for cognitive immersive services for both STAR-based and EDGAR-based UEs.\nFigure 6.4.4-1: Generic procedure for cognitive immersive service\nPrerequisites and assumptions:\n-\tThe AR/MR Scene Manager includes immersive media rendering and scene graph handling functionalities.\n-\tThe Media Player includes immersive content delivery and immersive media decoding functionalities.\n-\tThe AR/MR Application in the UE is run by the user.\n-\tThe UE initialises AR registration (starts analysing the surroundings where a user/UE is located), it namely:\na.\tcaptures its surroundings via camera(s)\nb.\tanalyses where the device is located\nc.\tregisters the device into the analysed surroundings.\n-\tAR/MR Application and AR/MR Application Provider have exchanged some information, such as device capability or content configuration, for content rendering. The exchange procedures for device capability and content configuration are FFS.\n-\tAR/MR Application Provider has established a Provisioning Session and its detailed configurations has been exchanged.\n-\tAR/MR Application Provider has completed to set up ingesting immersive contents.\nProcedures:\n1.\tThe Scene Server context is established, and scene content is ingested by the Media AS.\n2.\tService Announcement is triggered by AR/MR Application. Service Access Information including Media Client entry or a reference to the Service Access Information is provided through the M8d interface.\n3.\tDesired media content is selected.\n4.\tOptionally, the Service Access information is acquired or updated.\n5.\tThe AR/MR Application initializes the Scene Manager with the entry point (full scene description) URL.\n6.\tThe Media Client establishes the transport session for the scene session between the Scene Manager in the UE and the Scene Server.\n7.\tThe Media Client requests and receives the full scene description. The entry point (scene description) is processed by the AR/MR Scene Manager, and a scene session is created.\n8.\tThe AR/MR Scene Manager requests the creation of a new AR/MR session from the AR Runtime.\n9.\tThe AR Runtime creates a new AR/MR session.\nScene session loop, steps 10~24, send the interaction and pose information and receives and renders the updated scenes accordingly:\n10. The latest sensor data (e.g. captured media) is acquired by the AR/MR Scene Manager and shared with the Media Client. The Media Client sends this information to the Media AS and AR/MR Application.\n11.\tThe AR/MR Application performs cognitive processing according to the sensor data from the UE. Depending on the outcome, the current scene may be updated or replaced.\n12.\tWhen needed, one of the following steps:\n12a. The Scene Server sends a new scene entry point to the AR/MR Scene Manager through the Media AS and Media Client (go to step 7), or\n12b. The Scene Server sends a scene update (updating streams/objects) to the AR/MR Scene Manager through the Media AS and Media Client.\n13.\tThe AR/MR Scene requests to create additional streaming sessions if needed for new media objects in the scene.\nNOTE:\tThe number of the additional streaming sessions depends on the delivery mechanism. One or more media objects may be delivered through a single manifest and/or use of the same connection. Therefore, introduction of every new media object may not need an additional streaming session.\n14. The Media Session Handle establishes the additional streaming sessions based on the received request.\nStreaming session, steps 15~18 establish the transport sessions for media objects and configure the media pipelines\n15.\tFor the required media content, the Media Client establishes the transport session(s) to acquire delivery manifest(s) information.\n16.\tThe Media Client requests and receives the delivery manifest(s) from the Media AS.\n17.\tThe Media Client processes the delivery manifest(s).  It determines for example the number of needed transport sessions for media acquisition.  The Media Client is expected to be able to use the delivery manifest(s) information to initialize the media pipelines for each media stream.\n18.\tThe AR/MR Scene Manager and Media Client configures the rendering and delivery media pipelines.\n19.\tThe Media Client establishes the transport session(s) to acquire the media content.\nMedia session loop includes steps 20~24 which are for streaming, decoding and rendering media components:\n20.\tThe Media Client requests the media data according to the delivery manifest processed, possibly taking into account pose information (e.g., viewport dependent streaming).\n21.\tThe Media Client receives the media data and triggers the media rendering pipeline(s), including the possible registration of AR content into the real world accordingly (depending on the device type).\n22.\tThe Media Client decodes and processes the media data. For encrypted media data, the Media Client may also perform decryption.\n23.\tThe Media Client passes the media data to the AR/MR Scene Manager.\n24.\tThe XR Spatial Compute Pipeline as specified in clause 4.3.3.\n25.\tThe AR scene data and XR Spatial Compute data are combined for composition and rendering.\n6.4.5\tContent formats and codecs\nBased on the use cases, the following formats, codecs and packaging formats are of relevance for cognitive immersive media distribution of AR:\n-\tScene graph and scene description\n-\tSpatial description\n-\t2D video formats\n-\t3D formats such as static and dynamic point clouds or meshes\n-\t2D video formats with depth\n-\tAudio formats supporting mono, stereo, and/or spatial audio\n-\tSeveral video decoding instances\n-\tDecoding tools for such formats\n-\tEncoding tools for 2D formats\n-\tLow-latency downlink and uplink real-time streaming of the above media\n-\tUplink streaming of pose information\n-\tUplink streaming of media\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.6\tKPIs and QoS",
                            "text_content": "In the downlink this scenario is equivalent to clause 6.3.6 and similar KPIs and QoS aspects apply.\nFor the uplink, the above scenarios relate to the following cases in TR 26.928 [2], clause 6. In particular:\n-\tXR distributed computing based on clause 6.2.7 as defined TR 26.928 [2].\nDetails on uplink streaming of sensor data are for future study.\n6.4.7\tStandardization areas\nThe list of potential standardization area that has been collected is provided in the following:\n-\tSimilar functionalities as identified in clause 6.3.7 for downlink\n-\tFor the uplink, streaming of sensor information to the network\n>\tLow-latency streaming protocols to support latencies in the range between 50 to 500ms, typically using RTP-based real-time streaming\n>\tSimple 2D media formats to stream match AR sensor data\n>\tPayload format to be mapped into RTP streams\n>\tCapability exchange mechanism and relevant signalling\n>\tProtocol stack and content delivery protocol\n>\tCross-layer design, radio and 5G system optimizations for QoS support\n-\tSpatial description format for downlink and uplink\n-\tRequired QoE metrics\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.5\tAR conversational services",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.5.1\tIntroduction",
                            "text_content": "AR Conversational services are end-to-end use-cases that include communication between two or more parties. The following building blocks to realize AR conversational services are identified:\na)\tCall setup and control: this building block covers the\n-\t\tsignalling to setup a call or a conference.\n-\tfetching of the entry point for the AR experience. The protocol needs to support upgrading and downgrading to/from an AR experience. It also needs to support adding and removing media. This also includes the device type (Type-1, Type-2, or Type-3) as well as non-AR experience, e.g., tablet.\nb)\tFormats: The media and metadata types and formats for AR calls need to be identified. The format for the entry point, namely the scene description, and any extensions to support AR telephony need to be identified. Also, the format for media capturing, e.g., 2D video, depth map, 3D point clouds, colour attributes, etc. need to be identified. For AR telephony media types, the necessary QoS characteristics need to be defined, as well as format properties and codecs.\nc)\tDelivery: the transport protocols for the AR media need to be identified. AR telephony and conferencing applications require low latency exchange of real-time media. A protocol stack, e.g. based on RTP, will be required.\nd)\t5G system integration: offering the appropriate support by the 5G system to AR telephony and conferencing applications includes:\n-\tsignalling for QoS allocation,\n-\tdiscovery and setup of edge resources to process media for AR telephony,\n-\tusage of MBS and MTSI,\n-\tdata collection and reporting.\nThe building blocks may have different instantiations and/or options. For example, the delivery may be mapped to a WebRTC protocol stack or to an MTSI protocol stack. Furthermore, a single session may combine several delivery methods to accommodate the different media types supported by an AR conversational service.\nIn addition, AR telephony and conferencing applications may support asymmetrical and symmetrical experiences. In an asymmetrical case, one party is sending AR immersive media and the backchannel from other participants may be audio only, 2D video, etc. In a symmetrical case, all involved parties are sending and receiving AR immersive media.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.2\tRelevant use cases",
                            "text_content": "The use case relevant to this scenario may be further categorized into AR two-party call use cases and AR conferencing use cases. The AR two-party call use cases include:\n-\tUC#3: Real-time 3D Communication\n-\tUC#4: AR guided assistant at remote location (industrial services)\n-\tUC#7: Real-time communication with the shop assistant\n-\tUC#11: AR animated avatar calls\n-\tUC#16: AR remote cooperation\n-\tUC#19: AR conferencing\nThe AR conferencing use cases include:\n-\tUC#8: 360-degree conference meeting\n-\tUC#9: XR meeting\n-\tUC#10: Convention / Poster Session\n-\tUC#12: AR avatar multi-party calls\n-\tUC#13: Front-facing camera video multi-party calls\n-\tUC#19: AR conferencing\n3GPP TR 22.873 [14] also addresses use cases relevent to this scenario, namely conference call with AR holography and AR call. Note that the first use case has similarity with the UC#19 and the second use case has similarity with the UC#4 as listed in Table 5-1.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.3\tBasic architecture and call flows",
                            "text_content": "There are different options for mapping to 5G system:\na)\tThe MTSI architecture (TS 26.114 [15]) supports audio and 2D video conversational services. Extending the MTSI architecture to support AR signalling and immersive media. This includes both MTSI/RTP and MTSI/Data channel (DC) stack options.\nb)\tExtending the 5GMS architecture (TS 26.501 [26]) to support AR conversational services by combining live uplink and live downlink. 5GMS offers basic functionality such as QoS support, reporting, and in the future also edge, which will be beneficial for all types of applications. The typical/expected QoS parameters (especially delay) need to be clarified.\nc)\tAn architecture based on something different than MTSI / IMS or 5GMS, for example, browser implementations such as WebRTC. WebRTC is widely deployed today for conversational services and is built on flexible ecosystem on the device side, which is important in this case since conversational AR will require significant device-side changes.\nTable 6.5.3-1: Comparison of different architecture options for supporting AR conversational services\nNOTE:\tThere is no support of WebRTC media stack in 3GPP today, except for the WebRTC data channel stack in MTSI. WebRTC access to IMS was studied in TR 23.701 [41] and TR 23.706, [42] and OTT WebRTC client access to 3GPP core network through a gateway is specified in TS 24.371 [43].\nTo describe the functional architecture for AR conversational use-cases such as clause Annex A.4 and identify the content delivery protocols and performance indicators an end-to-end architecture is addressed. The end-to-end workflow for AR conferencing (one direction) is shown in Figure 6.5.3-1. Camera is capturing the participant in an AR conferencing scenario. The camera is connected to a UE (e.g. laptop) via a data network (wired/wireless). Live camera feed, sensors and audio signals are provided to a UE/Edge node (or split) which processes, encodes, and transmits immersive media content to the 5G system for distribution. The immersive media processing function in UE may include pre-processing of the captured 3D video, format conversion, and any other processing needed before compression. Immersive media content includes 3D representation, such as in form of meshes or point clouds, of participants in an AR conferencing scenario.  After processing and encoding, the compressed 3D video and audio streams are transmitted over the 5G system. A 5G STAR UE decodes, processes and renders the 3D video and audio stream.\nThe use-case may be extended to bi-directional/symmetric case by adding a 3D camera on the receiver side and AR glasses on the sender side and applying a similar workflow. For an asymmetrical case of EDGAR UE, the immersive media is further pre-rendered by the immersive media processing function in the 5G System and transmitted to the UE. Depending on the device capability, further media processing such as main scene management, composition, and rendering partial scene for individual participant are processed in cloud/edge.\n\nFigure 6.5.3-1: Extensions to device architecture of conversational services for STAR UE\nWe consider an immersive AR two party call between Alice and Bob. The end-to-end call flow is described:\n1.\t[STAR UE Alice - STAR UE Bob]:  Either one of the UEs may initiate an AR immersive call by starting an application on the phone or AR glasses.\n2.\t[STAR UE Alice - STAR UE Bob]: Both UEs communicate with a signalling server to establish the AR call. During the session establishment, both parties agree on the format (e.g. point clouds, triangular/polygon meshes). The exact session type and configuration depends on the capabilities of STAR UE.\n3.\t[STAR UE Alice]:  Alice is captured by a depth camera embedded within the STAR UE or an external camera which generates an immersive 3D media stream (audio and video).\n4.\t[STAR UE Alice]: The immersive 3D media is encoded and transmitted in real-time to Bob over the 5G system. Additional pre-processing may be applied before encoding such as format conversion.\n5.\t[STAR UE Bob]: The immersive 3D media is received on Bob’s STAR UE. The immersive 3D media stream is decoded and rendered on AR glasses. Additional post-processing may be applied before rendering such as format conversion, customization to match the stream to rendered environment e.g. filling holes.\n6.\t[STAR UE Bob]: Bob is captured by a depth camera generating an immersive 3D media which is encoded and transmitted in real-time to Alice’s AR glasses.\n7.\t[STAR UE Alice]: The immersive 3D media which is received, decoded and rendered on Alice’s AR glasses.\n8.\t[STAR UE Alice - STAR UE Bob]: Both UEs terminate the service at the end of the call.\nNOTE:\tAdditional call-flows that cover other AR conferencing use-cases listed in Table 6.1-1 may be added.\nWe consider an immersive AR asymmetrical call between Alice and Bob, where Bob is transmitting immersive media to be consumed on the AR glasses of Alice (STAR UE). Bob (non-STAR UE) is receiving content from Alice via other means such as audio, 2D video, etc. The end-to-end call flow is described:\n1.\t[STAR UE Alice – non-STAR UE Bob]:  Alice initiates an AR immersive call by starting an application on the phone or AR glasses.\n2.\t[STAR UE Alice - non-STAR UE Bob]: Alice communicates with a signalling server to establish the AR call. During the session establishment, the format is identified (e.g., point clouds, triangular/polygon meshes). The exact session type and configuration depends on the capabilities of STAR UE.\n3.\t[non-STAR UE Bob]:  Bob is captured by a depth camera embedded within the STAR UE or an external camera which generates an immersive 3D media stream (audio and video).\n4.\t[non-STAR UE Bob]: The immersive 3D media is encoded and transmitted in real-time to Alice over the 5G system. Additional pre-processing may be applied before encoding such as format conversion.\n5.\t[STAR UE Alice]: The immersive 3D media is received on Alice’s STAR UE. The immersive 3D media stream is decoded and rendered on AR glasses. Additional post-processing may be applied before rendering such as format conversion, customization to match the stream to rendered environment e.g., filling holes.\n6.\t[STAR UE Alice]: Alice is transmitting audio, 2D video or other media content as a back channel to Bob.\n7.\t[non-STAR UE Bob]: The 2D video or other media content which is received, decoded and rendered on Bob’s device.\n8.\t[STAR UE Alice – non-STAR UE Bob]: Alice terminates the service at the end of the call.\nNOTE:\tAdditional call-flows that cover other AR conferencing use-cases listed in Table 6.1-1 may be added.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.5.3-1: Comparison of different architecture options for supporting AR conversational services",
                                    "table number": 9,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "6.5.4\tInstantiation #1: MTSI-based architecture extension",
                            "text_content": "This instantiation provides the detailed architecture and procedures for the case of extending the current MTSI architecture. Figure 6.5.4-1 provides an MTSI-based architecture of conversational services for STAR UE.\nAn MTSI client specified in TS 26.114 [15] may be extended to an AR-MTSI client which supports AR immersive media and take a role of Media Access Functions. A data channel application, an HTML web page including JavaScript(s) provided by a data channel server through a bootstrap data channel, also may be used to provide rich user experiences such as sitting side by side on a bench. Support of data channel media is optional for an MTSI client. An AR-MTSI client supporting data channel is denoted as an AR-DCMTSI client. Note that the data channel server may be implemented in IMS core or outside of it. The figure depicts a 4-1 1-1 MTSI-based conversational service architecture for a Star UE, illustrating the various components and their interactions. The architecture supports AR immersive media and AR-DCMTSI clients, providing rich user experiences such as side-by-side seating on a bench. The data channel server can be implemented in IMS core or outside of it.\nFigure 6.5.4-1: MTSI-based conversational service architecture for STAR UE\nFigure 6.5.4-2 illustrates the procedure diagram for an immersive AR two party call using STAR UEs including an AR-MTSI client.\nThe figure depicts a call establishment between two AR-MTSI clients, with the client on the left initiating the call and the client on the right accepting it. The call is established using the AR-MTSI protocol, which is a standard for managing multimedia services over IP networks. The figure shows the AR-MTSI client's signaling and the AR-MTSI server's response, illustrating the communication process between the two devices.\nFigure 6.5.4-2: AR-MTSI client to AR-MTSI client call establishment (STAR UE)\nAssumptions:\n-\tAR immersive media is sent over RTP/UDP/IP.\n-\tAR immersive media format (e.g. point clouds, triangular/polygon meshes) is negotiated and configured using SDP.\nProcedures:\n1.\tA STAR UE initiates a SIP INVITE request, containing the SDP offer with AR media capabilities.\n2.\tThe call propagates to the terminating STAR UE.\n3.\tThe called party’s STAR UE returns an SDP answer in a SIP 183 progress message. The P-CSCF uses the SDP answer to allocate the required resources.\n4.\tThe originating STAR UE generate a PRACK which is transited to the terminating side of the call.\n5.\tThe originating STAR UE receives an associated 200 OK (PRACK).\n6.\tThe STAR UE reserves internal resources to reflect the SDP answer and configures media pipelines.\n7.\tThe STAR UE sends a SIP UPDATE message with a new SDP offer confirming the selected media parameters.\n8.\tThe 200 OK (UPDATE) response is received for the terminating STAR UE containing the SDP answer.\n9.\tThe terminating STAR UE is now alerted and sends a SIP 180 Ringing response.\n10.\tWhen the called party’s STAR UE has answered the call, it sends a 200 OK to the calling party STAR UE.\n11.\tThe STAR UE receives the 200 OK, and sends a SIP ACK message to acknowledge that the call has been established.\n12.\tThe STAR UE processes the immersive media to be transmitted.\na.\tThe AR runtime function captures and processes the immersive media to be sent.\nb.\tThe AR runtime function passes the immersive media data to the AR-MTSI client.\nc.\tThe AR-MTSI client encodes the immersive media to be sent to the called party’s STAR UE.\nNOTE:\tThe capturing may be done by an external camera. In that case, the processing and encoding may be done outside STAR UE (i.e. AR-MTSI client)\n13.\tThe STAR UE has an AR call established with AR media traffic.\n14.\tThe STAR UE processes the received immersive media.\na.\tThe AR-MTSI client decodes and process the received immersive media.\nb.\tThe AR-MTSI client passes the immersive media data to the Scene Manager.\nc.\tThe Scene Manager renders the immersive media, which includes the registration of the AR content into the real world accordingly.\nFigure 6.5.4-3 illustrates the procedure diagram for an immersive AR two party call using STAR UEs including an AR-DCMTSI client.\nThe figure depicts a call establishment process between two AR-DCMTSI clients, with the client initiating the call and the other client responding. The call is established using the STAR protocol, which is a key feature of the AR-DCMTSI standard. The figure illustrates the steps involved in establishing the call, including the client initiating the call, the other client acknowledging the call, and the initiation of the call. The figure also shows the AR-DCMTSI protocol, which is used to establish the call, and the STAR protocol, which is used to establish the call.\nFigure 6.5.4-3: AR-DCMTSI client to AR-DCMTSI client call establishment (STAR UE)\nAssumptions:\n-\tAR immersive media is sent over RTP/UDP/IP.\n-\tAR immersive media is negotiated and configured using SDP.\n-\tA data channel application provides rich user experiences by utilizing both user’s underlying scene and pose of objects representing users in the scene.\nProcedures:\n1-14.\tSame as the procedures for AR-MTSI client to AR-MTSI client call establishment except that the SDP contains a data channel media description for the bootstrap data channel.\n15.\tThe STAR UE retrieve a data channel application through the bootstrap data channel.\n16.\tAny additional data channels created and used by the data channel application itself are requested.\n17.\tThe AR-DCMTSI client initiate SIP re-INVITE request, containing an updated SDP offer to establish those data channels.\n18.\tThe data channels for the data channel application has been established.\n19.\tThe established data channel may be used by the data channel application JavaScript(s).\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.5\tInstantiation #2: DCMTSI-based architecture extension with immersive media processing",
                            "text_content": "Compared with the instantiation for MTSI-based architecture extension, this instantiation emphasises that the IMS-AGW/MRF may support immersive media processing. It is necessary for 5G EDGAR UEs with poor media capabilities. Figure 6.5.5-1 provides an DCMTSI-based architecture of AR conversational services for EDGAR UE. A 5G EDGAR UE integrated with DCMTSI client in terminal is denoted as an EDGAR-DCMTSI client. An EDGAR-DCMTSI client may request an AR application (i.e., an entry point) via a bootstrap data channel from the data channel server. An EDGAR-DCMTSI client may also generate or retrieve some AR specific data (e.g., pose and viewpoint information) which is transmitted via additional data channels, given that non-media data is handled by using SCTP as specified in IETF RFC 8831 [44]. When an EDGAR-DCMTSI client initiates an AR call with another one, the IMS-AGW/MRF with a support of immersive media processing may perform pre-rendering with the media stream originated from the parties of this AR session if they receive the corresponding AR-specific data (i.e. the pose and viewpoint information).\nEDGAR-DCMTSI clients negotiate the properties such as reliable or unreliable message transmission, in-order or out-of-order message delivery and an optional protocol for data channel using SDP as defined in IETF RFC 8864 [45]. Based on the user plane protocol stack for a basic MTSI client defined in clause 4.2 of TS 26.114 [15] and the clause 6.5 of IETF RFC 8827 [46], all data channels (e.g., both an AR application via bootstrap data channels and AR-specific data via additional data channels) are secured via DTLS.\nThe figure depicts a DCMTSI-based conversational service architecture for an EDGAR UE, illustrating the communication flow and components involved in the service.\nFigure 6.5.5-1: DCMTSI-based conversational service architecture for EDGAR UE\nFurthermore, the IMS-AGW/MRF with a support of immersive media processing are also desirable to 5G STAR UEs due to saving power consumption. Note that the IMS-AGW/MRF with a support of immersive media processing may perform pre-rendering based on the request of the STAR UEs carried in these additional data channels. Particularly, the logical function of immersive media processing may be integrated in the MRF or other media functions.\nFigure 6.5.5-2 illustrates the procedure diagram for an immersive AR conversational with two party using EDGAR UEs including an EDGAR-DCMTSI client in the context of the IMS-AGW/MRF with a support of immersive media processing. The procedure is also applicable to establish an immersive AR call where the two parties of a session are STAR UEs or one is STAR UE and the other is EDGAR UE.\n\nFigure 6.5.5-2: AR-DCMTSI client to AR-DCMTSI client call establishment for EDGAR UE\nAssumptions:\n-\tAR immersive media may be sent over RTP/UDP/IP and/or SCTP/UDP/IP.\n-\tAR immersive media may be negotiated and configured using SDP.\n-\tA data channel application may provide rich user experiences by utilizing both user’s underlying scene and pose of objects representing users in the scene.\nProcedures:\n1.\tAn EDGAR UE initiates a SIP INVITE request, containing the SDP offer with AR media capabilities.\n2.\tThe call propagates to the terminating EDGAR UE.\n3.\tThe terminating EDGAR UE returns an SDP answer in a SIP 183 progress message. The P-CSCF uses the SDP answer to allocate the required resources.\n4.\tThe originating EDGAR UE generate a PRACK which is transited to the terminating side of the call.\n5.\tThe originating EDGAR UE receives an associated 200 OK (PRACK).\n6.\tThe terminating EDGAR UE reserves internal resources to reflect the SDP answer and configures media pipelines.\n7.\tThe originating EDGAR UE sends a SIP UPDATE message with a new SDP offer confirming the selected media parameters.\n8.\tThe 200 OK (UPDATE) response is received for the terminating STAR UE containing the SDP answer.\n9.\tThe terminating EDGAR UE is now alerted and sends a SIP 180 Ringing response.\n10.\tWhen the terminating EDGAR UE has answered the call, it sends a 200 OK to the originating EDGAR UE.\n11.\tThe terminating EDGAR UE receives the 200 OK, and sends a SIP ACK message to acknowledge that the call has been established.\n12.\tThe EDGAR UEs retrieve a data channel application for AR through the bootstrap data channel. If the EDGAR UE enables to provide native AR application, this step is not required.\n13.\tAny additional data channels created and used by the data channel application for AR itself are requested.\n14.\tThe originating EDGAR UE initiates SIP re-INVITE request, containing an updated SDP offer to establish those additional data channels.\n15.\tThe AS/S-CSCF identify an updated SDP offer for additional data channels and modify the \"c=\" as the IP address of the MRF, and then send this SDP offer to the remote party.\n16.\tThe AS/S-CSCF send this updated SDP offer to the remote party.\n17.\tThe AS/S-CSCF receive an updated SDP answer from the remote party.\n18.\tThe AS/S-CSCF identify this updated SDP answer for additional data channels and modify the \"c=\" as the IP address of the MRF, and then send this SDP answer to the remote party.\n19.\tThe additional data channels for the data channel application has been established.\n20.\tThe EDGAR UE processes the immersive media to be transmitted.\na.\tThe AR runtime function captures and processes the immersive media to be sent.\nb.\tThe AR runtime function passes the immersive media data to the AR-DCMTSI client.\nc.\tThe AR-DCMTSI client encodes the immersive media to be transmitted to the IMS-AGW/MRF supporting immersive media processing.\nNOTE:\tThe capturing process may be done by an external camera. In this case, the processing and encoding processes are done outside EDGAR UE (e.g., AR-DCMTSI client)\n21.\tThe data channel application for AR collects the AR-specific data, and decides to send them to the AR-DCMTSI client if the AR experiences requires assistance from the network side.\n22.\tThe AR-DCMTSI client sends the AR-specific data (e.g., virtual objects info) to the IMS-AGW/MRF via the designated data channel 1 based on the previous SDP negotiation.\n23.\tThe IMS-AGW/MRF composes, renders and encodes the AR immersive media based on the received media stream and the AR-specific data from the originating party, and finally send them to the terminating party.\n24.\tThe data channel application for AR collects the AR-specific data, and decides to send them to the AR-DCMTSI client if the AR experiences requires assistance from the network side.\n25.\tThe AR-DCMTSI client sends the AR-specific data (e.g. pose info and/or viewport info) to the IMS-AGW/MRF via the designated data channel 2 based on the previous SDP negotiation.\n26.\tThe IMS-AGW/MRF decodes and pre-renders media stream based on the received media stream from the terminating party and the AR-specific data from the originating party, and finally sends them to the originating party.\n27.\tThe EDGAR UE processes the received immersive media.\na.\tThe AR-DCMTSI client decodes and process the received immersive media.\nb.\tThe AR-DCMTSI client passes the immersive media data to the Lightweight Scene Manager.\nThe Lightweight Scene Manager renders the immersive media, which includes the registration of the AR content into the real world accordingly.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.6\tContent formats and codecs",
                            "text_content": "Based on the use cases, the following formats, codecs, and packaging formats are of relevance for AR conversational:\n-\tGeneral\n>\t2D Video Formats and video compression codecs\n>\tAudio Formats and audio compression codecs supporting mono, stereo, and/or spatial audio\n-\tIn addition, for downlink\n>\tImmersive media 3D Formats such as static and dynamic point clouds or meshes\n>\tDecoding tools for such formats\n>\tComposed Scene Graph and Scene Description\n-\tIn addition, for uplink\n>\tImmersive media 2D Video Formats with depth\n>\tImmersive media 3D Formats such as static and dynamic point clouds or meshes\n>\tEncoding tools for such formats\n>\tStreaming of sensor information (e.g., gyroscope, accelerometer) as well as pose information\nNOTE 1:\tDetails on uplink delivery of immersive media 3D formats are for further study, to take into account the specific latency requirements of each conversational use case.\nNOTE 2:\tIt is not necessary to support all media formats listed, depending on the device type and/or application.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.7\tSummary of AR conversational instantiations",
                            "text_content": "Table 6.5.7-1 shows the list of potential instantiations and how they may be composed from each building block described in clause 6.5.1.\nTable 6.5.7-1: Summary of each instantiation for AR conversational services\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.5.7-1: Summary of each instantiation for AR conversational services",
                                    "table number": 10,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "6.5.8\tStandardization areas",
                            "text_content": "The list of potential standardization area that has been collected is provided in the following:\n-\tImmersive media format and profile with integration into relevant 5G architecture\n-\tScene description format, functionality, and profile as an entry point of immersive media\n-\tScene description update mechanism\n-\tRelevant subset of media codecs for different media types and formats\n-\tCMAF encapsulation of immersive media for 5G media streaming\n-\tMedia payload format to be mapped into RTP streams\n-\tCapability exchange mechanism and relevant signalling (e.g., SDP)\n-\tProtocol stack and content delivery protocol for various architecture options as identified in Table 6.5.3-1\n-\tFunctionalities to support split rendering and network-based media processing allocation with 5G edge/MRF\n-\tRequired QoS and QoE for AR/MR conversational service\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.6\tShared AR conversational experience",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.6.1\tIntroduction",
                            "text_content": "Shared AR Conversational experience is an end-to-end conversational service that includes communication between two or more parties through a network/cloud entity that creates a shared experience, meaning that every party in the call in its AR experience sees the same relative arrangements of the other participants relative to each other. Therefore, for instance the interaction between two parties seating next to each other in the virtual space (e.g. when these parties turn to each other when talking) is seen by all participants in the same way. Note that the AR runtime in each device customizes and updates the arrangement of the people in the virtual room. The absolute positioning of people or objects in a user’s scene may vary based on the physical constraints of the user’s room. This shared experience distinguishes this use case from the AR conversational experience of clause 6.5.\nIn addition to the building blocks listed in clause 6.5.1, an immersive media processing function is needed to create the shared virtual experience. This requirement is discussed as an abstract functionality. In various deployments, this functionally may be implemented in different ways or by different entities, in a centralized or distributed fashion, and other possible arrangements.\nThis experience may be deployed with a combination of AR and non-AR devices. In this context, an AR device is capable of laying over received media object on a see-through glass (e.g. AR glasses) or the display of the device while capturing live content through its camera and rendering on its display (e.g. a table or phone). A non-AR device only receives one or multiple 2D video streams each representing one of the other participants but is incapable of laying over received media object with the see-through or captured by its camera scene. In such a scenario, each AR device creates an AR scene as mentioned above. But an application running on the edge/cloud may create one or multiple 2D videos (i.e. a VR video or multi-view videos) of a virtual room which includes all other participants and streams one or more of them to a non-AR device, based on its user’s preference. The user on non-AR devices can also change its viewport to the virtual room by changing the position of its device or using navigation devices such as keyboard or mouse, but the device does not provide an AR experience.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.6.2\tRelevant use cases",
                            "text_content": "The use cases relevant to this scenario may be further categorized including:\n-\tUC#8: 360-degree conference meeting\n-\tUC#9: XR meeting\n-\tUC#10: Convention / Poster Session\n-\tUC#12: AR avatar multi-party calls\n-\tUC#13: Front-facing camera video multi-party calls\n-\tUC#19: AR conferencing\n-\tUC#22: shared AR conferencing experience\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.6.3\tBasic architecture",
                            "text_content": "To describe the functional architecture for shared AR conversational experience use case such as clause Annex A.7 and identify the content delivery protocols and performance indicators, an end-to-end architecture is addressed. The end-to-end architecture for shared AR conferencing (one direction) is shown in Figure 6.6.3-1. To simplify the architecture, only 5G STAR UE is considered in this figure.\nThe figure depicts a 6-node STAR (Shared AR Conversational Service) network, with each node serving as a STAR UE (User Equipment). The network is designed to provide a conversational service, allowing users to engage in real-time, interactive conversations with each other. The figure illustrates the network's architecture, including the use of AR (Artificial Reality) technology to enhance the user experience.\nFigure 6.6.3-1: Shared AR conversational service for STAR UE.\nCamera(s) are capturing the participant(s) in an AR conferencing scenario. The camera(s) for each participant are connected to a UE (e.g. laptop or mobile phone or AR glasses) via a data network (wired/wireless). Live camera feeds, sensors, and audio signals are provided to a UE which processes, encodes, and transmits immersive media content to the 5G system for distribution.  In multi-party AR conversational services, the immersive media processing function on the cloud/network receives the uplink streams from various devices and composes a scene description defining the arrangement of individual participants in a single virtual conference room.  The scene description as well as the encoded media streams are delivered to each receiving participant. A receiving participant’s 5G STAR UE receives, decodes, and processes the 3D video and audio streams, and renders them using the received scene description and the information received from its AR Runtime, creating an AR scene of the virtual conference room with all other participants.\nAlso note that if the format conversion is desired, the immersive media processing function on the cloud may optionally use media services such as pre-processing of the captured 3D video, format conversion, and any other processing before compression of immersive media content including 3D representation, such as in form of meshes or point clouds, of participants in an AR conferencing scenario.\nNOTE:\tAs an example of the composite scene generation, the immersive media processing function may take the input from the participants’ physical constraints, so that the generated scene is consistent with every participants’ environment and can be rendered at each device consistently.\nFigure 6.6.3-2 illustrates the architecture for shared AR conversational experience use case when an 5G EDGAR UE (receiver) is used. While the functionalities of the sender and the network/cloud shown in Figure 6.6.3-1 are identical in the STAR and EDGAR devices, an EDGAR device uses a split-rendering function on Cloud/Edge.\nThe figure depicts a shared AR conversational service for EDGAR  UE and cloud/edge pre-rendering, showcasing the integration of AR technology into the communication process.\nFigure 6.6.3-2: Shared AR conversational service for EDGAR UE and cloud/edge pre-rendering.\nThe AR session management may be done by AR/MR application on device. In this case, it is a responsibility of the device to connect and to acquire an entry point for edge/cloud during the session management. AR Scene Manager on cloud/edge generates the lightweight scene description and simple format of AR media that match AR glass display capabilities of the individual participant’s 5G EDGAR device. The lightweight scene description and encoded rendered scene are delivered to the UE. The UE receives the simple format of AR media and audio streams, decodes and renders them using the received lightweight scene description and the information received from its AR Runtime, creating an AR scene of the virtual conference room with all other participants.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.6.4\tGeneric Call flow",
                            "text_content": "Figure 6.6.4-1 illustrates the call flow for an immersive AR conversational for a receiving EDGAR UE. Only one sender is shown in this diagram without showing its detailed call flow.\n\nFigure 6.6.4-1: Shared AR conversational experience call flow for a receiving EDGAR UE\nProcedures:\n1.\tSession Establishment:\na.\tThe AR/MR Application requests to start a session through EDGE.\nb.\tThe EDGE negotiates with the Scene Composite Generator (SCG) and the sender UE to establish the session.\nc.\tThe EDGE acknowledges the session establishment to the UE.\n2.\tMedia pipeline configuration:\na.\tMAF configures its pipelines.\nb.\tEDGE configures its pipelines.\n3.\tThe AR/MR Application requests the start of the session.\nLoops 4, 5, 6, and 7 are run in parallel:\n4.\tAR uplink loop:\na.\tThe AR Runtime sends the AR data to the AR/MR Application.\nb.\tThe AR/MR Application processes the data and sends it to the MAF.\nc.\tThe MAF streams up the AR data to the EDGE.\n5.\tShared experience loop:\na.\tParallel to 9, the sender UE streams its media streams up to Media Delivery (MD).\nb.\tThe sender UE streams its AR data up to the Scene Graph Compositor (SGC).\nc.\tUsing the AR data from various participants, the SCG creates the composted scene.\nd.\tThe composted scene is delivered to the EDGE.\ne.\tThe media streams are delivered to the EDGE.\n6.\tMedia uplink loop:\na.\tThe AR Runtime captures the media components and processes them.\nb.\tThe AR Runtime sends the media data to the MAF.\nc.\tThe MAF encodes the media.\nd.\tThe MAF streams up the media streams to the EDGE.\n7.\tMedia downlink loop:\na.\tThe EDGE parses the scene description and media components, partially renders the scene, and creates a simple scene description as well as the media component.\nb.\tThe simplified scene is delivered to the Media Client and Scene Manager.\nc.\tMedia stream loop:\ni.\tThe pre-rendered media components are streamed to the MAF.\nii.\tThe MAF decodes the media streams.\niii.\tThe Scene Manager parses the basic scene description and composes the scene.\niv.\tThe AR manager after correcting the pose, renders the immersive scene including the registration of AR content into the real world.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.6.5\tVarious instantiations",
                            "text_content": "Similar to clause 6.5.3, the shared AR conversational experience may be instantiated in various 5G systems:\na)\tThe MTSI architecture (TS 26.114 [15]) supports audio and 2D video conversational services.\nb)\tExtending the 5GMS architecture (TS 26.501 [26]) to support AR conversational services by combining live uplink and live downlink.\nc)\tAn architecture based on something different than MTSI / IMS or 5GMS, for example WebRTC.\nFor the comparison between different instantiations, please refer to Table 6.5.3-1.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.6.6\tContent formats and codecs",
                            "text_content": "Based on the use cases, the following formats, codecs and packaging formats are of relevance for Media Streaming of AR:\n-\tScene Graph/Description\n-\t2D Video Formats\n-\t3D Formats such as static and dynamic point clouds or meshes\n-\tAnimated 3D meshes\n-\t2D Video Formats with depth\n-\tMono, stereo, and spatial audio formats\n-\tSeveral video decoding instances\n-\tDecoding tools for such formats\n-\tEncoding tools for 2D formats\n-\tLow-latency downlink and uplink real-time streaming of the above media\n-\tUplink streaming of pose information\n-\tUplink streaming of media\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.6.7\tStandardization areas",
                            "text_content": "The list of potential standardization area that has been collected is provided in the following:\n-\tImmersive media format and simplified media formats with integration into relevant 5G architecture\n-\tScene description format, functionality, and profile as an entry point of immersive media\n-\tScene description update mechanism\n-\tRelevant subset of media codecs for different media types and formats\n-\tCMAF encapsulation of immersive media for 5G media streaming\n-\tMedia payload format to be mapped into RTP streams\n-\tCapability exchange mechanism and relevant signalling\n-\tLow-latency uplink streaming of captured AR data\n-\tFunctionalities and session management to support split rendering and network-based media processing allocation with 5G edge/MRF\n-\tRequired QoS and QoE for shared AR/MR conversational experience service\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "7\tConsiderations on Devices Form-factor",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "7.1\tGeneral",
                    "description": "",
                    "summary": "",
                    "text_content": "The components of AR glasses are same or similar with those of mobile phones which may launch and execute AR/MR applications. However, AR glasses have rather different requirements and limitations compared with mobile phones.\nFrom a form factor perspective, AR glasses have several different design considerations. For example, AR glasses have two separate see-through displays for each eye. They also usually include more than two vision cameras which are spatially separated in order to achieve better disparity for depth estimation. In addition, AR glasses are worn and closely attached to a user’s face and contain IMU sensors to estimate where the user’s focal point is. Most of the included components are designed and placed in order to meet requirements which differ to those for mobile phones.\nFrom a media processing perspective, AR/MR applications consume far more energy than non-AR/MR applications [27]. Multiple, as well as different types of cameras are always turned on to track the features detected in 2D and 3D video every second. In the case when AR/MR objects are augmented into the real world, the objects need to be rendered frame by frame with different view frustum positions and directions. In the case when the AR/MR objects are rendered in a server, the AR/MR device is expected to upload the user's pose in a millisecond frequency, then download, decode, correct, and composite the pre-rendered image sequences streamed from the server.\nBesides, from an ergonomics perspective, restrictions need to be considered to place the components of the AR glasses in a limited space and under the manageable range of user neck joint torque.\nThis clause addresses form-factor related issues from the components of AR glasses device architectures, such as battery/power consumption, camera, display, heat dissipation, and weight.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.2\tBattery/Power consumption",
                    "description": "",
                    "summary": "",
                    "text_content": "The run time of a typical battery is proportional to its physical size, capacity, and weight, while they are proportional to user discomfort and neck torque. A study on the characteristics of AR applications [27] measured battery consumption of commercially available applications on AR, streaming, and social networking which shows that AR applications consume around at least 46% more energy than non-AR applications. The capacity of the battery needs to be designed to support a fair amount of running time for the everyday use of AR/MR applications. The amount of running time could be from tens of minutes for shopping of products via AR remote advertising in Annex A.2, 1-2 hours for streaming of volumetric video in Annex A.3, or even several hours for AR gaming in Annex A.6. However, as capacity is typically proportional to weight, and as the AR glasses is expected to be worn and equipped under the consideration of human ergonomics such as neck strain, there are clear limitations on extending the capacity of the battery. Such limitations may be relaxed by dynamically offloading some energy-intensive workloads to 5G cloud/edge. In this case, local processing power consumption is exchanged with power consumption for 3GPP/non-3GPP connectivity and an always on connectivity as well. For connectivity Discontinuous Reception (DRX) and Reduced Capability (RedCap) may be one of examples looking for lower power consumption for the radio for AR/MR application.\nThe following KPI is related with battery and power consumption and listed in clause 4.5.2.\n-\tMaximum Available Power\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.3\tCamera",
                    "description": "",
                    "summary": "",
                    "text_content": "Augmented reality may be realized by SLAM. To understand the physical world through SLAM, various types of multiple cameras need to be continuously turned on and always need to be acquiring image sequences.\nAmong the various components contributing to heat, such as CPU, GPU, camera and display, it is measured that the cameras are one of major sources of heat dissipation for AR applications [27]. AR/MR applications may need to be aware of the available run time remaining, and the amount of heat dissipation felt by the user.\nIn addition, as multiple cameras may be equipped in AR glasses for various purposes, they need to be designed and placed optimally to process the required functions in AR Runtime. Camera related parameters, such as for calibration, pose correction, Vision Engine, SLAM etc. are expected to have a big impact on the quality of service for AR glasses. AR/MR applications may need to be aware of intrinsic and extrinsic parameters for the cameras to properly process the required functions. Such parameters may be delivered to the server whenever there is any change in camera configurations.\nThe following KPI is related with camera and listed in clause 4.5.2\n-\tMaximum Available Power\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.4\tDisplay",
                    "description": "",
                    "summary": "",
                    "text_content": "There is at least one display for each eye on a pair of immersive AR glasses. The AR glasses estimates the position of each eye then presents pixels of the rendered AR/MR objects on the display in order to combine the ray of light reflected from the surface of real-world objects with each pixel. A renderer in the AR scene manager may take into consideration the shape and optical distortion characteristics of the displays, pixel arrangements, and the estimated position of each eye of the user. At least one of the view frustum models that represents either an AR glasses, each display, or each eye, with a 3D map of the surroundings may be provided to the AR scene manager in order to minimize the post processing of customizing a generic rendered image to fit to a certain pair of AR glasses.\nThe following KPI(s) are related with display and listed in clause 4.5.2.\n-\tMaximum Available Power\n-\tPersistence – Duty time\n-\tDisplay refresh rate\n-\tSpatial Resolution per eye\n-\tContent frame rates\n-\tBrightness\n-\tField of View\n-\tEye Relief\n-\tCalibration\n-\tDepth perception\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.5\tHeat dissipation",
                    "description": "",
                    "summary": "",
                    "text_content": "It has been studied that AR applications may generate 4-5 degrees (in Celsius) higher heat than non-AR applications on the same device [27]. Another study shows that a user’s heat sensation and discomfort increase with temperature. Overheated components have not only degraded performance but also power leakage through thermal throttling [28].\nThe following KPI may be related with heat dissipation and listed in clause 4.5.2.\n-\tMaximum Available Power\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.6\tWeight",
                    "description": "",
                    "summary": "",
                    "text_content": "AR glasses consists of displays, sensors, cameras, batteries and so on. The weight of AR glasses puts constant pressure on a user’s skin and changes the amount of torque applied to the neck joints and muscles in a neutral posture.\nA study shows that a user’s posture may be changed from a neutral to a look-up posture, a look-down posture, or a body-bending posture because of the relative placement of virtual objects [29]. Those different postures increase the moment arm between the Centre of Mass (CoM) of the wearable device and the neck joint.\nThere are different characteristics between HMD type and glasses type devices, as the CoM of glasses type devices is biased towards the front of the device, by design. As a result, AR/MR applications need to consider the issues due to the differences in the ergonomics between the two different types of wearable devices.\nThe following KPI is related with weight and listed in clause 4.5.2.\n-\tMaximum Weight\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.7\tAudio",
                    "description": "",
                    "summary": "",
                    "text_content": "Audio requirements in TS 26.131 and 26.132 or in the scope of the ATIAS work item are not considering explicitly AR glasses. The form factor may require specific definitions of terminal audio performance requirements and objectives for AR glasses.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "8\tPotential New Work and Study Area",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "8.1\tGeneral",
                    "description": "",
                    "summary": "",
                    "text_content": "This clause documents and clusters potential new work and study areas identified in the context of this Technical Report. In particular, two areas have been identified as crucial for supporting AR type of services and applications that impact network and terminal architectures:\n-\t5G Generic Architecture for Real-Time Media Delivery as introduced in clause 8.2.\n-\tSupport for Media Capabilities for Augmented Reality Glasses as introduced in clause 8.5.\nIn order to separate the work areas of these potential work topics, Figure 8.1-1 and Figure 8.1-2 provides the high-level scope of these two work topics for STAR-based and EDGAR-based UEs, respectively.\nThe figure depicts a schematic representation of the separation of work topics between AR media capabilities, terminal architecture, and network architecture for STAR-type devices. It highlights the importance of these components in ensuring the efficient and reliable operation of AR systems.\nFigure 8.1-1: Work topic separation between AR media capabilities, terminal architecture and network architecture for STAR-type devices.\nThe figure depicts a schematic representation of the separation of work topics between AR media capabilities, terminal architecture, and network architecture for EDGAR-type devices. It highlights the importance of these components in ensuring efficient and reliable communication systems.\nFigure 8.1-2: Work topic separation between AR media capabilities, terminal architecture and network architecture for EDGAR-type devices.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "8.2\t5G Generic Architectures for Real-Time Media Delivery",
                    "description": "",
                    "summary": "",
                    "text_content": "Based on the initial conclusions in TR 26.928 [2], clause 7, and the evaluation of architectures in clause 4 and 6 of this report, it is clear that for the integration of AR services and experiences into 5G Networks, the approach taken in 5GMS to separate the data plane and the control plane, and enable access of third-party services getting access to 5G System functionalities, is a major benefit. The basic concept is the extension of 5GMS principles. to any type of service including real-time communication and split-rendering. While the work is motivated by XR and AR experiences discussed in this TR, it is neither specific nor limited to those experiences. In principle, the basic control plane similar/identical to 5GMS, and the media plane is generic, permitting different types of operator and third-party services supported by the 5G System. The following aspects are identified:\n-\t5GMS-like network architectures to support any type of media services including real-time communication, split rendering and spatial computing\n-\tOperator and third-party services need to be supported\n-\tSeparation of user and control plane functionalities\nBased on the above, it is considered to specify 5G generic architectures for real-time media delivery addressing the following stage-2 work objectives:\n-\tA generic media delivery architecture to define relevant core building blocks, reference point, and interfaces to support modern operator and third-party media services based on the 5GMS architecture\n-\tProvide all relevant reference points and interfaces to support different collaboration models between 5G System operator and third-party media service provider, including but not limited to an AR media service provider.\n-\tCall flows and procedures for different service types, for example real-time communication, shared communication, etc., based on the context of clause 6\n-\tSpecify support for AR relevant functionalities such split-rendering or spatial computing on top of a 5G System based on this architecture\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "8.3\t5G-Media Service Enablers",
                    "description": "",
                    "summary": "",
                    "text_content": "AR applications rely on functionalities provided by devices and networks. On devices, such functionalities are typically bundled in software development kits (SDKs) in order to get access to complex hardware functionalities. SDKs typically expose APIs to simplify the communication with the underlying hardware and network functionalities.\nWhat is clearly needed for AR and provided for example by Khronos with OpenXR, are standardized APIs to access underlying AR hardware functions. However, the standardized APIs and functions in OpenXR are restricted to local device processing. In order to enable and simplify the access to 5G network, system and media functionalities for AR, it is beneficial to provide packages and bundles for application providers. Typical assets for media service enablers are:\n-\tSet of functions that may be used to develop applications on top of 5G Systems\n-\tSet of robust features and functionalities which reduce the complexity of developing applications\n-\tFunctions to leverage system and radio optimizations as well as features defined in 5G System (5G Core Network and 5G NR)\n-\tProvision and documentation of APIs to enable or at least simplify access to these functionalities\n-\tProvision of network interfaces to connect to the 5G System\n-\tA testable set of functions. Testing and conformance may be addressed outside 3GPP by an appropriate Marketing and Public Relations (MPR) or Industry Forum.\n-\tGuidelines and examples to make use of the functionalities\nIt is proposed to use the concept of 5G-media service enablers to define relevant specifications for AR and possibly other applications. A common set of properties and functionalities for Media Service Enabler specifications is needed and hence it is proposed to provide a 3GPP internal report that:\n-\tDefine the principal properties of media service enablers\n-\tDefine minimum and typical functionalities of media service enablers\n-\tDefine a specification template for media service enablers\n-\tIdentify possibly relevant stage-2 and stage-3 work for media service enablers\n-\tCollect a set of initially relevant media service enablers for normative work\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "8.4\t5G Real-time Communication",
                    "description": "",
                    "summary": "",
                    "text_content": "As documented in clause 4.2.6 and further developed in the context of clause 6, there are several use cases that require a 5G Real-time communication. The use cases include:\n1)\tEDGAR-based UEs relying on rendering on the network. In this case, the downlink requires sending pre-rendered viewports with lowest latency, typically in the range below 50ms.\n2)\tUplink streaming of camera and sensor information for cognitive/spatial computing experiences, in case the environment tracking data and sensor data is used in creating and rendering the scene.\n3)\tConversational AR services require real-time communication both in the downlink and the uplink, even independent from MTSI for app integration of the communication.\nIn order to provide adequate QoS as well as possible optimizations when using a 5G System for media delivery, an integration of real-time communication into the 5G System framework is essential.\nAs identified in clause 4.2.6 and clause 6.5, there is a need for supporting third-party applications in 5G real-time communication as well as server-based real-time streaming. From an app developer perspective, an enabler is preferable, especially to support real-time streaming, for example split-rendering.\nDifferent options may be considered, for example re-use of parts of MTSI such as the IMS data channel and 5G Media Streaming for managed services, or re-use of WebRTC for OTT services. A 5G real-time communication is expected to be aligned with either IMS or WebRTC but provides additional functions to integrate with the 5G System.\nIt is proposed to define a general 5G real-time communication media service enabler that includes, among others, the following functionalities:\n-\tA protocol stack and content delivery protocol for real-time communication based on RTP\n-\tA common session and connection establishment framework, with instantiations based on SIP and SDP for IMS or SDP and ICE for WebRTC, including further possible investigation of control plane\n-\tA capability exchange mechanism\n-\tA security framework, for example based on SRTP and DTLS for WebRTC\n-\tUplink and downlink communication\n-\tSuitable control protocols for end-to-end adaptation\n-\tQoS and 5G System integration framework\n-\tReporting and QoE framework\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "8.5\tMedia Capabilities for Augmented Reality Glasses",
                    "description": "",
                    "summary": "",
                    "text_content": "In TR 26.928 [2] and this report, XR and AR device architectures have been developed and details on relevant media formats are documented, for example in, clause 4.4. In particular, it is identified that for design AR glasses, implementation and operational requirements are significantly more stringent than for smart phones (see clause 4.5.2 and clause 7). As an example, consuming media on AR glasses requires functionalities to address very low power consumption, low area size, low latency options, new formats, operation of multiple decoders in parallel, etc.\nTo support basic interoperability for AR applications in context of 5G System based delivery, a set of well-defined media capabilities are essential. These capabilities may be used in different services and applications and hence service-independent capabilities are relevant. The media capabilities typically address three main scenarios:\n-\tSupport of basic media services on such glasses with simple rendering functionalities\n-\tSupport of split-rendering, e.g. a pre-rendering of eye buffers is carried out in the cloud/edge\n-\tSupport of sensor and device data streaming to the network in order to support network-based processing or device sensor information\nMedia functions are relevant for the Media Access Function as defined in clause 4.2.6. The media capabilities are importantly driven by realistic deployment options addressing device capabilities, as documented in clause 4.5.2, as well as the relevant KPIs.\nIn particular, the following objectives need to be considered:\n-\tDefine a reference terminal architecture for AR devices\n-\tDefine at least one AR device category that addresses the constraints of an EDGAR-type AR glasses\nNOTE:\tAdditional device categories may be defined, but with lower priority\n-\tFor each AR device category\n>\tDefine media types and formats, including scene description, audio, 3D/2D graphics and video, as well as sensor information and metadata of user and environment.\n>\tDefine decoding capabilities, including support for multiple parallel decoders\n>\tDefine encoding capabilities\n>\tDefine security aspects related to media capabilities\n-\tSupport signalling (e.g., SDP and MPD) of AR media for generic capability exchange mechanisms\n-\tDefine capability exchange mechanisms based on complexity of AR media and capability of device to support EAS KPIs for provisioning of edge/cloud resources\nNOTE:\tIdentify a suitable existing capability framework, or if it does not exist, collaboration with broader industries (e.g., IETF, Khronos, W3C) is required.\n-\tDefine relevant KPIs and QoE Metrics for AR media\n-\tEncapsulation into RTP and ISOBMFF/CMAF\nThe media capabilities may be referenced and added to 3GPP Media service enablers and/or 3GPP service specifications such as 5G Media Streaming or MTSI.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "8.6\tSplit Rendering Media Service Enabler with AR profile",
                    "description": "",
                    "summary": "",
                    "text_content": "In the context of this report, it was clearly identified that AR glasses depend on cloud or edge-based pre-rendering. However, not only AR glasses benefit from such a functionality, also for VR, XR and gaming, as identified in TR 26.928 and TR 26.926, would benefit from split rendering approaches. Hence, a basic Media Service Enabler for split rendering is paramount, in particular in combination with 5G new radio and 5G System capabilities.\nBased on this discussion it is proposed to specify a generic raster-based split rendering media service enabler that includes, among others, the following functionalities:\n-\tA content delivery protocol defined as a profile of 5G real-time communication for downlink with possible extension\n-\tA relevant subset of codecs for different media types\n-\tA scene description functionality to support a scene manager end point\n-\tRelevant edge compute capabilities, for example Edge procedures, EAS profiles and KPIs for rendering, and rendering context relocation\n-\tRelevant APIs and network communication\n-\tIntegration into 5GS and RAN, possibly with support of cross-layer optimizations\n-\tOperational requirements and recommendations for low-latency communications\n-\tGuidelines and examples\nIn addition to the generic enabler for split rendering a specific profile for AR is recommended to be defined that includes special considerations for:\n-\tThe formats to be supported on AR glasses\n-\tThe post-processing for pose correction and the integration with XR runtimes\n-\tThe power consumption challenge for AR glasses\n-\tThe metrics and KPIs for AR glasses\n-\tThe required QoS and QoE for AR type of applications as defined in clause 4.5\n-\tOther AR specific considerations\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "8.7\tTethering AR Glasses",
                    "description": "",
                    "summary": "",
                    "text_content": "In clause 4.2.2.4, the important aspect of wireless tethering of AR glasses was introduced. The tethering technology between a UE and an AR glass may use different connectivity. Wireless tethered connectivity is provided through WiFi or 5G sidelink. BLE (Bluetooth Low Energy) connectivity may be used for audio. Two main types are identified:\n-\tFunctional structure for Type 3a: 5G Split Rendering WireLess Tethered AR UE\n-\tFunctional structure for Type 3b: 5G Relay WireLess Tethered AR UE\nIn the first case, the motion-to-render-to-photon loop runs from the glasses to the phone, whereas in the second case the 5G phone acts as a relay to forward IP packets. The architectures result in different QoS requirements, session handling properties, and also media handling aspects. For enhanced end-to-end QoS and/or QoE, AR glasses may need to provide functions beyond the basic tethering connectivity function, and the resulting AR glasses may be referred to as smartly tethering AR glasses. Generally, smartly tethering AR glasses is an important aspect. Based on these observations, it is proposed to further study this subject including specific topics such as:\n-\tDefining different tethering architectures for AR Glasses including 5G sidelink and non-5G access based on existing 5G System functionalities\n-\tDocumenting end-to-end call flows for session setup and handling\n-\tIdentify media handling aspects of different tethering architectures\n-\tIdentify end-to-end QoS-handling for different tethering architectures and define supporting mechanisms to compensate for the non-5G link between the UE and the AR glasses\n-\tProvide recommendations for suitable architectures to meet typical AR requirements such as low power consumption, low latency, high bitrates, security and reliability.\n-\tCollaborate with relevant other 3GPP groups on this matter\n- \tIdentify potential normative work for stage-2 and stage-3\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "8.8\tIMS-based AR conversational services",
                    "description": "",
                    "summary": "",
                    "text_content": "As identified in Table 6.1-1, AR conversational and shared AR conversational services have a number of related use cases. 3GPP TR 22.873 [14] also addresses use cases relevant to AR conversational services, namely conference calls with AR holography and AR calls, which have similarities with UC#19 and UC#4 in this study, respectively.\nAs documented in clause 6.5 and clause 6.6, AR conversational services and shared AR conversational experiences may be realized using various building blocks, including call setup and control, formats, delivery and 5G system integration, and these building blocks may have different instantiations and/or options.\nIn this study, the MTSI architecture is identified as one of the options to map those services to the 5G system. Furthermore, SA1’s Rel-18 eMMTEL work item introduced new service requirements for 5G IMS Multimedia Telephony Service, including the support of AR media processing in TS 22.261[13] and it is expected that enhancements on the IMS architecture and/or IMS procedures to fulfil new requirements will be handled by SA2 in Rel-18.\nIt is proposed to define an IMS-based instantiation for a complete AR communication service, including:\n-\tTerminal architecture(s) for various device types integrated with an MTSI client based on the work in summarized in clause 8.2, 8.4, 8.5, and 8.7\n-\tIMS session setup, control, and capability exchange procedures for AR media in an IMS communication session\n-\tReal-time transport of AR media, scene description, and metadata, as addressed in clause 4.4, via IMS media path including Data Channel\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "8.9\tAudio Media Pipelines for AR Experiences",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "8.9.1\tGeneric functional device architecture with audio subsystem (STAR-based UE)",
                            "text_content": "In AR experiences the audio media consists of spatial audio in the form of audio sources as well as acoustic characteristics of the scene. Example device functional architectures, including audio subsystems, are shown in Figure 8.9-1, 8.9-2 and 8.9-3. In receiving, the audio subsystem may be implemented as a monolithic block (Figure 8.9-1); alternatively, the audio decoding and rendering may be closely integrated, e.g. in the XR Scene Manager Audio Subsystem (Figure 8.9-2) or split into an encoding-decoding process by the Media Access Functions with separated rendering by the XR Scene Manager Audio Subsystem (Figure 8.9-3). The separation of audio decoding and audio rendering may depend on the implementation and/or use cases.\n\nFigure 8.9-1: Immersive service architecture with audio subsystem – monolithic block\n\n\nFigure 8.9-2: Immersive service architecture with audio subsystem – integrated decoding/rendering\n\n\n\nFigure 8.9-3: Immersive service architecture – separated audio decoding and rendering\nIsolation of the audio path from the rest of the immersive scene and the other media types as in Figure 8.9-1 may be an issue. A single XR anchor space should ideally be used by the XR scene manager, similarly as a shared spatial system across audio and video was established for VR in TS 26.118.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "8.9.2\tConsiderations on split rendering (EDGAR-based UE)",
                            "text_content": "In the split rendering case, audio quality including end-to-end delay should ideally be preserved as compared to non-split rendered case. There is currently no codec / payload type specified in 3GPP to transport (decoded) spatial audio from the edge or cloud to an AR device for split rendering. The required characteristics (e.g., audio format, bit rate, frame length) of such a codec / payload type have to be defined to ensure adequate performance. The input audio format (e.g., channel-based, scene-based, object-based audio, MASA or combinations) may typically have to be preserved in the encoding-decoding process together with the associated metadata structure (when present in the audio input) to allow full rendering control, as expected from content creation.\n\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "9\tConclusions",
            "description": "AR/MR experiences involve augmenting visual/auditory contents into the real world to improve the user’s experience with better immersiveness, unlike VR, which provides an entirely virtual world. To realize these experiences, glass-type AR/MR devices may be a good candidate device, easily combining the lights from the real world and those from the display without a need of holding a device in one’s hand.\nIn this study, the generic finding for eXtended Reality (XR) in TR 26.928 [2] have been further analysed with specific focus on Augmented Reality (AR) experiences and in particular also with a new device type, AR glasses. Different device centric functions of AR glasses are defined, and different device types are defined. Of particular relevance are 5G STandalone AR (STAR) UEs, i.e. devices that have sufficient capabilities to render rich AR experiences on the device as well as 5G EDGe-Dependent AR (EDGAR) UEs for which edge-based rendering support is a must to provide rich AR experiences. Three basic functions are introduced, the AR Runtime, the Scene Manager and the 5G Media Access Function. Basic AR processes are defined, and a comprehensive summary of AR related media formats is provided. The relevant work in external organizations is summarized.\nBased on core use cases, different scenarios are mapped to the 5G System architecture, namely (i) Immersive media downlink streaming (ii) Interactive immersive services (iii) 5G cognitive/spatial computing immersive services as well (iv) AR conversational services. Potential normative work is identified and summarized in clause 8.\nBased on the details in the report, the following next steps are proposed.\nIn the short-term:\n-\tDocument the relevant 5G generic architecture for real-time media delivery based on the 5GMS architecture as addressed in clause 8.2.\n-\tEstablish the concept of 5G media service enablers as introduced in clause 8.3 and make use of the concept to define relevant AR media service enablers.\n-\tDefine a 5G real-time communication media service enabler to support different low-latency streaming and conversational AR related services based on the considerations in clause 8.4.\n-\tDefine media capabilities for AR glasses in a service-independent manner based on the considerations in clause 8.5. The outcomes may affect the other items, especially the 5G real-time communication media service enabler and the IMS-based conversational services.\n-\tBased on the work on above, define a split rendering media service enabler to support EDGAR devices, as addressed in clause 8.6.\n-\tStudy options for smartly tethering AR glasses based on the discussion in clause 8.7.\n-\tDevelop the extension of IMS-based AR conversational services and shared AR experiences, including an extended MTSI terminal architecture, as addressed in clause 8.8.\nIn the mid-term:\n-\tAdd issues around semantical perception and spatial mapping to an AI/ML study, taking into account the findings in clause 4.2.3 and 4.2.5 as well as TR 22.874.\nAll work topics will benefit to be carried out in close coordination with other groups in 3GPP on 5G System and radio related matters, edge computing and rendering as well in communication with experts in MPEG on the MPEG-I project as well as with Khronos on their work on OpenXR, glTF and Vulkan/OpenGL. A follow-up workshop based on the information in clause 4.6.9 may be conducted in order to explore additional synergies and complementary work in different organizations in the XR/AR domain.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.1\tUse Case 16: AR remote cooperation",
            "description": "\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 11,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.2\tUse Case 17: AR remote advertising",
            "description": "\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 12,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.3\tUse Case 18: Streaming of volumetric video for glass-type MR devices",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "A.3.1\tUse case description",
                    "description": "",
                    "summary": "",
                    "text_content": "\n\n",
                    "tables": [
                        {
                            "description": "",
                            "table number": 13,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "A.3.2\tCall flow for STAR UE",
                    "description": "",
                    "summary": "",
                    "text_content": "The call flow of the establishment of the AR session for STAR UE is shown in Fig. A.3.2-1:\nFig. A.3.2-1: Call flow for STAR UE\nA description of the steps is provided:\n1.\tUser starts the app. The app connects to the cloud to fetch a list of exercise routines for the user\n2.\tThe AP sends a list of routines to the app. Each routine is associated with an entry point for that routine. The entry point is typically a scene description that describes the objects in the scene and anchors the scene with the world space.\n3.\tThe user selects a routine in the app\n4.\tThe app fetches the scene description for the selected routine from the application provider\n5.\tThe app initializes the Scene Manager and passes the entry point to it.\n6.\tThe Scene Manager parses entry point to extract information about the required objects in the scene. In this case, the coach, the student, and a speaker are the 3 objects that will be rendered in the scene.\n7.\tThe Scene Manager informs the application about the required media that needs to be accessed.\n8.\tThe app parses the information about the media objects and determines how and when to access each of them.\n9.\tThe app informs the MSH that it will start 2 streaming sessions for the 2 dynamic objects.\n10.\tThe MSH shares the information with the AF and based on existing provisioning by the Application Provider, the AF may request QoS and charging modifications to the PDU sessions.\n11.\tThe App creates a new XR session and anchors the scene to a selected space in the XR session.\n12.\tThe media exchange begins:\na.\tThe static object in the scene, the loudspeaker, is fetched by the App\nb.\tThe manifest for object 1 is retrieved\nc.\tThe manifest for object 2 is retrieved\nd.\tThe App configures the immersive video decoders based on the components of each object\ne.\tMedia segment for each component of each object is fetched\nf.\tThe segment is decoded and passed to the immersive media renderer\n13.\tThe Scene Manager periodically renders a frame by iteratively reconstructing each object and rendering it to a swapchain image. The swapchain image is passed to the AR Runtime for rendering to the HMD device.\nThe following media is assumed for this use case:\n-\tAn entry point: a scene description that describes the objects in the scene.\n-\tDynamic objects for the coach and student: these can be dynamic meshes, animated meshes, or point clouds\n-\tStatic object for the loudspeaker: this can be a static mesh\n-\tSpatial audio: representing the vocal instructions associated with the dynamic object of the coach\n-\tSpatial audio: representing the music for which the loudspeaker is the source\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "A.3.3\tCall flow for EDGAR UE",
                    "description": "",
                    "summary": "",
                    "text_content": "The call flow of the establishment of the AR session for EDGAR UE is shown in Fig. A.3.3-1:\nFig. A.3.3-1: Call flow for EDGAR UE\nA description of the steps is provided:\n1.\tUser starts the app. The app connects to the cloud to fetch a list of exercise routines for the user\n2.\tThe AP sends a list of routines to the app. Each routine is associated with an entry point for that routine. The entry point is typically a scene description that describes the objects in the scene and anchors the scene with the world space.\n3.\tThe user selects a routine of preference in the app\n4.\tThe application sends a request for the entry point to the selected content. The Application Provider responds with an entry point to a scene description and a list of requirements for optimal processing of the scene.\n5.\tThe application determines that EDGE support is required and sends a request to the MSH to discover an appropriate Edge AS that can serve the application.\n6.\tThe MSH sends the requirements to the AF and receives a list of candidate EAS(s)\n7.\tThe MSH selects an appropriate EAS from the list of candidates.\n8.\tThe MSH provides the location of the EAS to the application.\n9.\tThe application connects to the EAS and provides initialization information. The initialization information contains: the URL to the scene description entry point or the actual scene description, its current processing capabilities, supported formats and protocols, etc.\n10.\tThe EAS configures the server application accordingly and generates a customized entry point for the client. The formats depend on the capabilities of the UE. The EAS adjusts the amount of processing performed by the EAS based on the current capabilities of the application. For example, The EAS may perform scene lighting and ray tracing and then generate a lightweight 3D scene description for the application. A less-capable UE may receive a more flattened scene, that contains stereo eye views and some depth information.\n11.\tThe App initializes the Scene Manager with the new low-complexity entry point.\n12.\tThe rest of the steps are similar to steps 6-10 from the STAR call flow.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "A.4\tUse Case 19: AR Conferencing",
            "description": "\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 14,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.5\tUse Case 20: AR IoT control",
            "description": "\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 15,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.6\tUse Case 21: AR gaming",
            "description": "\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 16,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "A.7\tUse Case 22: Shared AR Conferencing Experience",
            "description": "\n\n\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 17,
                    "summary": "",
                    "name": ""
                },
                {
                    "description": "",
                    "table number": 18,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        }
    ]
}