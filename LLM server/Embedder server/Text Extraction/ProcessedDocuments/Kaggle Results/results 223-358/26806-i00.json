{
    "document_name": "26806-i00.docx",
    "content": [
        {
            "title": "Foreword",
            "description": "This Technical Report has been produced by the 3rd Generation Partnership Project (3GPP).\nThe contents of the present document are subject to continuing work within the TSG and may change following formal TSG approval. Should the TSG modify the contents of the present document, it will be re-released by the TSG with an identifying change of release date and an increase in version number as follows:\nVersion x.y.z\nwhere:\nx\tthe first digit:\n1\tpresented to TSG for information;\n2\tpresented to TSG for approval;\n3\tor greater indicates TSG approved document under change control.\ny\tthe second digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, etc.\nz\tthe third digit is incremented when editorial only changes have been incorporated in the document.\nIn the present document, modal verbs have the following meanings:\nshall\t\tindicates a mandatory requirement to do something\nshall not\tindicates an interdiction (prohibition) to do something\nThe constructions \"shall\" and \"shall not\" are confined to the context of normative provisions, and do not appear in Technical Reports.\nThe constructions \"must\" and \"must not\" are not used as substitutes for \"shall\" and \"shall not\". Their use is avoided insofar as possible, and they are not used in a normative context except in a direct citation from an external, referenced, non-3GPP document, or so as to maintain continuity of style when extending or modifying the provisions of such a referenced document.\nshould\t\tindicates a recommendation to do something\nshould not\tindicates a recommendation not to do something\nmay\t\tindicates permission to do something\nneed not\tindicates permission not to do something\nThe construction \"may not\" is ambiguous and is not used in normative elements. The unambiguous constructions \"might not\" or \"shall not\" are used instead, depending upon the meaning intended.\ncan\t\tindicates that something is possible\ncannot\t\tindicates that something is impossible\nThe constructions \"can\" and \"cannot\" are not substitutes for \"may\" and \"need not\".\nwill\t\tindicates that something is certain or expected to happen as a result of action taken by an agency the behaviour of which is outside the scope of the present document\nwill not\t\tindicates that something is certain or expected not to happen as a result of action taken by an agency the behaviour of which is outside the scope of the present document\nmight\tindicates a likelihood that something will happen as a result of action taken by some agency the behaviour of which is outside the scope of the present document\nmight not\tindicates a likelihood that something will not happen as a result of action taken by some agency the behaviour of which is outside the scope of the present document\nIn addition:\nis\t(or any other verb in the indicative mood) indicates a statement of fact\nis not\t(or any other negative verb in the indicative mood) indicates a statement of fact\nThe constructions \"is\" and \"is not\" do not indicate requirements.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "Introduction",
            "description": "For initial AR experiences, an expected prominent setup will be wirelessly tethered of AR glasses, typically connected to a 5G UE. The tethering technology between a UE and an AR Glasses may use different connectivity, for example provided through WiFi or 5G sidelink. Different architectures for tethering result in different QoS requirements, session handling properties, and also media handling aspects. For enhanced end-to-end QoS and/or QoE, AR glasses may need to provide functions beyond the basic tethering connectivity function. Generally, smartly tethering AR glasses is an important aspect for successful AR experiences using the 5G System.\nBased on this, the present document introduces Smartly Tethering AR Glasses (SmarTAR) in such user experience may be maximized using the 5G System.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "1\tScope",
            "description": "The present document addresses architectures, QoS and media handling aspects of when tethering AR Glasses to 5G UEs based on initial discussions in TR 26.998 [2]. In particular, the following aspects are in scope:\n-\tDefinition of different tethering architectures for AR Glasses including 5G sidelink and non-5G access based on existing 5G System functionalities\n-\tDocumentation of the relationship between AR Glasses tethering and AR glasses considered as PIN (Personal IoT Network) elements according to TR 22.859 [3] and the derived service requirements in TS 22.261 [4].\n-\tDocumentation of end-to-end call flows for session setup and handling\n-\tIdentification media handling aspects of different tethering architectures\n-\tIdentification of end-to-end QoS-handling for different tethering architectures and define supporting mechanisms to compensate for the non-5G link between the UE and the AR glasses\n-\tProviding recommendations for suitable architectures to meet typical AR requirements such as low power consumption, low latency, high bitrates, security and reliability.\n-\tCollaboration with relevant other 3GPP groups on this matter\n-\tIdentification of potential follow-up work on this matter\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "2\tReferences",
            "description": "The following documents contain provisions which, through reference in this text, constitute provisions of the present document.\n-\tReferences are either specific (identified by date of publication, edition number, version number, etc.) or non-specific.\n-\tFor a specific reference, subsequent revisions do not apply.\n-\tFor a non-specific reference, the latest version applies. In the case of a reference to a 3GPP document (including a GSM document), a non-specific reference implicitly refers to the latest version of that document in the same Release as the present document.\n[1]\t3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".\n[2]\t3GPP TR 26.998: \"Support of 5G glass-type Augmented Reality / Mixed Reality (AR/MR) devices\".\n[3]\t3GPP TR 22.859: \"Study on Personal Internet of Things (PIoT) networks\".\n[4]\t3GPP TS 22.261: \"Service requirements for the 5G system\".\n[5]\t3GPP TR 23.700-78: \"Study on Application layer support for Personal IoT and Residential Networks\".\n[6]\t3GPP TR 23.700-88: \"Study on architecture enhancements for Personal IoT Network (PIN)\"\n[7]\t3GPP TS 23.304: \"Proximity based Services (ProSe) in the 5G System (5GS)\".\n[8]\t3GPP TS 23.287: \"Architecture enhancements for 5G System (5GS) to support Vehicle-to-Everything (V2X) services\".\n[9]\t3GPP TS 23.501: \"System Architecture for the 5G System; Stage 2\".\n[10]\tThe Khronos Group, \"The OpenXR Specification\",\n[11] \t3GPP TS26.531, \"Data Collection and Reporting; General Description and Architecture\", V17.1.0, Sept. 2022.\n[12]\tIETF RFC 792, Internet Control Message Protocol, 1981.\n[13] \tIEEE-1588-2019, \"Standard for a Precision Clock Synchronization Protocol for Networked Measurement and Control Systems\".\n[14] \tIETF RFC 8285, A General Mechanism for RTP Header Extensions, 2017.\n[15] \tIETF draft-mlichvar-ntp-ntpv5-07, Network Time Protocol Version 5, March 2023.\n[16] \tIETF RFC6051, Rapid Synchronisation of RTP Flows, 2010.\n[17]\t3GPP TS 26.119: \"Media Capabilities for Augmented Reality\".\n[18]\t3GPP TS 26.522: \"5G Real-time Media Transport Protocol Configurations\".\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "3\tDefinitions of terms, symbols and abbreviations",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "3.1\tTerms",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the terms given in 3GPP TR 21.905 [1] and the following apply. A term defined in the present document takes precedence over the definition of the same term, if any, in 3GPP TR 21.905 [1].\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.2\tSymbols",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the following symbols apply:\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.3\tAbbreviations",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the abbreviations given in 3GPP TR 21.905 [1] and the following apply. An abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in 3GPP TR 21.905 [1].\nAPI\tApplication Programming Interface\nAR\tAugmented Reality\nBLE\tBluetooth Low Energy\nDASH\tDynamic Adaptive Streaming over HTTP\nFFS\tFor Further Study\nFLUS\tFramework for Live Uplink Streaming\nGPS\tGlobal Positioning System\nGPU\tGraphics Processing Unit\nGTP\tGPRS Tunneling Protocol\nICMP\tInternet Control Message Protocol\nMAF\tMedia Access Function\nMCS\tModulation and Coding Scheme\nMSH\tMedia Session Handler\nNAT\tNetwork Address Translation\nNEF\tNetwork Exposure Function\nNTP\tNetwork Time Protocol\nPCF\tPolicy Control Function\nPCM\tPulse Code Modulation\nPDU\tProtocol Data Unit\nPEGC \tPIN Element with Gateway Capability\nPEMC \tPIN Element with Management Capability\nPIN \tPersonal IoT Network\nPINAPP \tPersonal IoT Network Application\nPQI\tPC5 QoS Identifier\nPSA\tPDU Session Anchor\nPTP\tPrecision Time Protocol\nQoS\tQuality-of-Service\nQFI\tQoS Flow Identifier\nRAN\tRadio Access Network\nRGB\tRed-Green-Blue\nRGBA\tRed-Green-Blue-Alpha\nRTP\tReal-Time Protocol\nRTT\tRound-Trip Time\nSMF\tSession Management Function\nSRTP \tSecure RTP\nSTAR\tStandalone AR glasses\nUDP\tUser Datagram Protocol\nUPF\tUser Plane Function\nURL\tUniversal Resource Locator\nURLLC\tUltra Reliable and Low Latency Communications\nWLAR\tWireLess tethered AR glasses\nWTAR\tWired Tethered AR glasses\n\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "4\tMotivation and Background",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "4.1\tSummary of TR 26.998",
                    "description": "",
                    "summary": "",
                    "text_content": "The 5G WireLess Tethered AR UE is introduced in TR26.998 [2] as one functional structural device type. It is further split into two sub-types, Type 3a: 5G Split Rendering WireLess Tethered AR UE and Type 3b: 5G Relay WireLess Tethered AR UE. For Type 3a, the tethering 5G Phone provides both the network connectivity and the rendering/pre-rendering assistant functionalities to the AR glasses. For Type 3b, the tethering 5G Phone only provides the IP network connectivity to the AR glasses.\nTable 4.1.1-1 provides an overview of functionality splitting for Wireless Tethered AR Glasses devices.\nNOTE:\tThe 5G Phone, as a tethering device, initiates the \"tether\" action to an AR Glasses which belongs to the tethered device.\nTable 4.1-1:\tFunctionality splitting for Wireless Tethered AR Glasses device\nDifferent from other types of AR UE, the end-to-end path includes one more wireless/wireline tethering link between AR Glasses and the tethering 5G Phone. In order to fulfil the end-to-end QoS requirements for the AR session, the AR UE need to acquire the tethering link status via measurement tests or empirical values, and takes it into account when determining the QoS for the 5G system link. With the tethering link status, the Media Access Function may communicate with AF for dynamic QoS policy adjustment accordingly.\n",
                    "tables": [
                        {
                            "description": "Table 4.1-1:\tFunctionality splitting for Wireless Tethered AR Glasses device",
                            "table number": 3,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.2\tGuiding Use Cases",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.2.1\tIntroduction",
                            "text_content": "This clause provides several guiding use cases in order to simplify the analysis in the course of this report.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.2.2\tCycling Glasses",
                            "text_content": "Assume the following use case: Thomas got new AR glasses for his 51st birthday. He plays around and can connect the glasses to his mobile phone using a dedicated WiFi connection. Thomas wants to go cycling so he uses the AR glasses. When Thomas goes cycling, he does use four main apps in parallel:\n-\tA navigation app that provides directions on where to go\n-\tA fitness app that tracks all different kinds of environmental as well as body data\n-\tA music app to play his favourite live music radio station\n-\tA live video sharing app such that his family and friends can track him and cheer him\nIn addition, as Thomas is a nerd, he also loves to get notifications from major apps such as on social media, work e-mails, Teams notifications from Imed, and so on. All these applications run on the phone and the phone sends notifications to the peripherals. The notifications create certain A(udio)/V(isual)/H(apical) signals. Of these additional apps, you may even then kick off and create and render additional information on the peripherals (headset, glass). Now wearing a Glasses during cycling, including an audio headset and some haptical notification.\nThomas’s wife is scared when he wears smart glasses on his bike and she wants to be absolutely sure that he does not get motion sick on the bike and possibly falls off.\nIn addition, the navigation app uses the camera feeds from the Glasses and GPS location of the phone to send exact navigation instructions. The camera feeds may have to use an edge for exact detection of the environment and may make the AR glasses provide overlays and information on the environment. But at the same time it will need to send notifications from other apps. These notifications may be any sense or combined senses. So the phone needs to render eye buffers, audio output and provide haptical information to the glasses.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.3\tPIN (Personal IoT Network)",
                    "description": "",
                    "summary": "",
                    "text_content": "4.3.1 \tMedia share within PINs use case\n\"Personal IoT networks\" (PINs) in 3GPP TR 22.859 [3] are of a type of private network typically consisting of a user smartphone, wearables and home automation devices. These networks are very different to commercial IoT device, they are usually less rugged, most highly battery constrained and lifespan of the battery typically a couple of days or weeks. User plane traffic typically stays with a constrained environment, around the body or in the home i.e., within the PIN. Notifications can be received on smartphones that events have occurred within the PIN. A typical wearable PIN is depicted in Figure 4.3.1-1.\nThe figure depicts wearable PINs, which are a type of biometric authentication technology. The figure shows a series of small, rectangular devices that are worn on the wrist and are used to verify the identity of the wearer. The PINs are designed to be small, lightweight, and easy to wear, making them suitable for a wide range of applications. The figure also includes a diagram of the PINs' components, such as the circuit board, battery, and sensor, which helps to understand how they work and how they can be used for authentication purposes.\nFigure 4.3.1-1: Wearable PINs (from [3])\nOne use case in clause 5.3 of [3] is called \"Media share within PINs\". A sub-use case to be noted consists of watching a movie on a smartphone and then switching to watching the movie on AR glasses.\n3GPP TR 22.859 [3] also includes Potential Consolidated Requirements to be considered for 5GS evolution. These potential requirements include control plane requirements such as Device and Service Discovery, Privacy and Security, PIN Management and Charging. It also includes user plane type of potential requirements such as Gateway capability, Direct Communications, Connectivity and QoS.\n4.3.2 Definitions and service requirements of PIN\nAccording to Service requirements for the 5G system 3GPP TS 22.261 [4], A Personal IoT Network (PIN) consists of PIN Elements that communicate using PIN Direct Connection or direct network connection and is managed locally (using a PIN Element with Management Capability). Examples of PINs include networks of wearables and smart home / smart office equipment.  Via a PIN Element with Gateway Capability, PIN Elements have access to the 5G network services and can communicate with PIN Elements that are not within range to use PIN Direct Connection. A PIN includes at least one PIN Element with Gateway Capability and at least one PIN Element with Management Capability.\nPIN Element related definitions can be found from TS 22.261 [4]:\nPIN Element: UE or non-3GPP device that can communicate within a PIN.\nPIN Element with Gateway Capability: a UE PIN Element that has the ability to provide connectivity to and from the 5G network for other PIN Elements.\nNOTE 5C:\tA PIN Element can have both PIN management capability and Gateway Capability.\nPIN Element with Management Capability: A PIN Element with capability to manage the PIN.\nService requirements for the 5G system TS 22.261 [4] include both control and user plane requirements such as:\n-\tGeneral: including user plane connectivity requirements such as \"the 5G system shall support a data path not traversing the 5G network for intra-PIN communications via direct connections.\" And \"The communication path between PIN Elements may include licensed and unlicensed spectrum as well as 3GPP and non-3GPP access.\"\n-\tGateways: including e.g. \"The 5G system shall be able to support access to the 5G network and its services via at least one gateway (i.e. PIN Element with Gateway Capability […]) for authorised UEs and authorised non-3GPP devices in a PIN[…].\"\n-\tOperation without 5G core network connectivity: \"The 5G system shall allow PIN Elements to communicate when there is no connectivity between a PIN Element with Gateway Capability and a 5G network.  For a Public Safety PIN licensed spectrum may be used for PIN direct communications otherwise unlicensed spectrum shall be used.\"\n-\tPIN discovery: e.g. \"The 5G system shall enable a UE or non-3GPP device in a  […] PIN to discover other UEs or non-3GPP devices within the same  […] PIN subject to access rights.\" and PIN element capability discovery.\nNOTE: Although discovery is in the TS 22.261 [4], the agreement for normative work is to divide discovery into two layers: if transport layer is based on 3GPP PC5, the existing procedures defined for 5G ProSe Direct Communication are re-used. If the transport layer functionality based on non-3GPP communication specification is outside the 3GPP scope. The application layer for PIN and PIN Element discovery and selection is not specified by SA2. Further updates may provided once the work in 3GPP has progressed,\n-\tRelay selection for PIN direct connection: \"The 5G system shall support a mechanism for a PIN Element to select a relay for PIN direct connection that enables access to the target PIN Element.\"\n-\tAuthentication, Privacy and Security: \"The 5G system shall provide user privacy; location privacy, identity protection and communication confidentiality for non-3GPP devices and UEs that are using the PIN Element with Gateway Capability […].\"\n-\tQoS monitoring and control requirements don’t apply to PINs but to UEs in CPNs (Customer Premises Networks)\nNOTE:\tDespite the lack of specific Stage 1 QoS requirements on PIN, Stage 2 includes a solution on QoS management for PINs: \"Solution #11: Differentiated QoS between a PINE and 5GS when a PEGC is used for the relay\"\n-\tCharging: \"The 5G system shall support charging data collection for data traffic to/from individual UEs in a […] PIN (i.e., UEs behind the PIN Element with Gateway Capability […])\"\n-\tPIN Creation and Management: \"The 5G system shall support mechanisms for a network operator or authorized 3rd party (e.g., a PIN User) to create, remove and manage a PIN\"\n4.3.3 PIN Architecture related study in 3GPP\nThere’s existing study in 3GPP regarding PIN network and application architectures can be potentially considered for AR Glasses tethering and media delivery. The Figure 4.3.3-1 shows the application architecture for enabling PINAPP (\"Personal IoT Network Application\") as described in 3GPP TR 23.700-78 [5].\nThe PINAPP architecture, as depicted in Figure 4.3, is a three-tier architecture that is designed to support the integration of various network functions. The architecture is based on the principles of Open Network Interworking Platform (ONIP) and is intended to provide a flexible and scalable solution for network management and operation. The architecture consists of three tiers: the Application Layer, the Network Layer, and the Infrastructure Layer. The Application Layer is responsible for the application layer functions, such as data processing, user interface, and application services. The Network Layer is responsible for the network layer functions, such as routing, switching, and network management. The Infrastructure Layer is responsible for the infrastructure layer functions, such as network infrastructure, network management, and network security. The architecture is designed to be modular and scalable, allowing for easy expansion and modification as needed.\nFigure 4.3.3-1: PINAPP architecture (from [5])\nFigure 4.3.3-2 shows the PIN network architecture in 5GS as described in TR 23.700-88[6].\nThe figure depicts a personal IoT network architecture in 5G, showcasing the various components and their interconnections. It highlights the importance of network slicing, network slicing, and network slicing in enabling efficient and secure communication for IoT devices.\nFigure 4.3.3-2: Personal IoT Networks Architecture in 5GS\nAs shown in Figure 4.3.3-2, the system architecture contains the following reference points:\nP1:\tReference point between the PINE and the PEGC. This reference point is based on non-3GPP access (e.g. WIFI, Bluetooth).\nP2:\tReference point between the PEMC and the PEGC. This reference point is based on non-3GPP access (e.g. WIFI, Bluetooth) or 5G ProSe Direct Communication.\nP3:\tReference point between the PEMC and the PIN Application Server. This reference point can be based on the direct user plane path to 5GS, relay path via the PEGC, or other communication path via Internet.\nP4:\tReference point between the PEGC and the PIN Application Server. This reference point is based on the user plane path between PEGC and 5GS.\nWhen considering the device architectures for Tethered Glasses in clause 4.4, and the PIN architecture aspects described in 3GPP TR 23.700-88 [6] there is a potential solution to use PINs for AR glasses. tethering and media delivery, when wireless tethered connectivity between 5G device and AR glasses device is provided through non-3GPP access e.g., WiFi or BLE:\n-\tThe 5G Phone is a 3GPP UE which can act as a PEGC (PIN Element with Gateway Capability) and PEMC (PIN Element with Management Capability)\n-\tThe AR glasses device can act as a PINE (PIN element).\n-\tThe AR glasses device and the 5G device can then be considered a PIN.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.4\tDevice Architectures for Tethered Glasses",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.4.1\tGeneral",
                            "text_content": "Based on the guiding use case in clause 4.2.2 as well as the discussions in TR 26.998 [2], this clause identifies the architectures and media handling for different tethered AR glasses.\nLooking at existing AR Glasses, based on the study in TR 26.998 [2] and based on information from chipset manufacturers on existing and emerging devices, an AR Glasses designed for AR experiences does integrate complex functionalities and many of those relate to capabilities. Figure 4.4.1-1 is a picture providing an overview of an AR glasses.\nThe figure depicts an AR glass, which is a type of transparent display technology used in augmented reality (AR) applications. It features a series of layers, including a transparent layer, a reflective layer, and a reflective layer, allowing for the display of visual information in a three-dimensional space. The transparent layer is used to display the real-world environment, while the reflective layer is used to display the augmented information. The reflective layer is designed to be transparent, allowing the user to see the real-world environment while the AR information is displayed in the reflective layer. The figure shows the AR glass in a three-dimensional perspective, highlighting its various layers and their functions.\nFigure 4.4.1-1 - Overview of an AR glass\nTypical functions of such a AR Glasses consists of:\n- \tPeripherals including\n-\tDisplays\n-\tCameras\n-\tMicrophones\n-\tSensors\n-\tCamera/Sensor Aggregators\n-\tPerception functionality: Eye Tracking, Face Tracking, etc.\n-\tSoC Media\n-\tDisplay Processing\n-\tGPU functionalities: Composition/Reprojection\n-\tDecoding\n-\tDecryption\n-\tCamera Front ends\n-\tPerception functionality: 6DoF, etc.\n-\tEncoding\n-\tConnectivity\n-\tWi-Fi, Bluetooth, 5G, etc.\nAn interesting aspect to consider from the above is that the device consists of different thermal islands, hence division in multiple chips in the headset is highly desirable. This means that both minimizing the power consumption per thermal island as well as minimizing the overall power consumption is an essential design constraint for the device battery life. Such devices require to partition workloads to remote devices or the cloud to some extent to balance the power load. Based on this, media capabilities are also possibly required on UE that acts as a hub for a tethered glasses. Architectures and processing for this will the main subject of discussion in this Technical Report.\nIt should be noted that such AR glasses are predominantly served with media that can directly be rendered by the peripherals, or produce media captured on the device and sent to remote processing. Initial System-on-Chip (SoC) media will likely rely on existing hardware, for example from lower end mobile chipsets. Some people consider XR even a hack that uses existing components in a smart manner. However, a core aspect of XR experiences different from traditional mobile devices is the concurrent operation of multiple encoders and/or decoders to address different sensors, eye buffers, layers and so on, as well as the rendering to GPU instead of directly going to the display. Only over time, such hardware will get added specific functionalities, but not in the near and mid-term. Expected in the future are higher render and display resolutions, multi-layer composition, etc.\nFigure 4.4.1-2 provides an 5G AR UE as a framework. In this context the XR application is offered several functionalities on the device as well as connectivity options to create an XR experiences as defined in TR 26.998, clause 4.2, namely:\n-\tXR Runtime: The XR Runtime is a device-resident software or firmware that implements a set of XR APIs to provide access to the underlying AR/MR hardware, including capability discovery, session management, input and sensors, composition, and other XR functions. An example for such APIs is provided by OpenXR.\n-\tXR Scene Manager: A Scene Manager is a software component that is able to process a scene description and renders the corresponding 3D scene. To render the scene, the Scene Manager typically uses a Graphics Engine that may be accessed by well-specified APIs such as defined by Vulkan, OpenGL, Metal, DirectX, etc. Spatial audio is also handled by the Scene Manager based on a description of the audio scene. Other media types may be added as well.\n-\tMedia Access Functions: supports the application to access and stream media. For this purpose, a Media Access Function includes: media processing, codecs, content delivery protocols, content protection, QoS control, metrics collection and reporting, etc.\n-\t5G System: supports the XR application to access the network through the 5G system, either directly or through the MAF.\nThe figure depicts a 5G AR (Augmented Reality) UE (User Equipment) framework, illustrating the various components and their interactions. The framework includes a base station (gNB), user equipment (UE), and scatterers. The diagram highlights beamforming techniques to mitigate interference, and the use of SDN (Software-Defined Networking) principles for network management and optimization.\nFigure 4.4.1-2 – 5G AR UE Framework\nGiven that many functionalities are defined through Khronos OpenXR [10], defining capabilities for example by mandating or recommending support of certain APIs or parameter settings on API may be relevant. In some cases it may not even be possible to define capabilities, but for example rely on test signals and benchmarking requirements that estimate the performance of a device.\nIn the following, two different approaches for tethering AR Glasses are identified, identifying how:\n-\tTethered Standalone AR Glasses: In this case, the AR Glasses runs an XR application that uses the capabilities of the Glasses to create a service. The AR Glasses is tethered to a 5G device and potentially uses the capabilities of the phone to support the application. For details refer to clause 4.4.2.\n-\tDisplay AR Glasses: In this case, the AR Glasses is tethered to a 5G device that includes the application and the XR functions. The 5G device runs the application that uses the capabilities of the 5G device to run an AR/MR experience. The AR Glasses is connected to the 5G Device, but the XR runtime API is exposed to the 5G device/phone. For details refer to clause 4.4.3.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.4.2\tTethered Standalone AR Glasses",
                            "text_content": "Figure 4.4.2-1 provides the technical architecture of a typical stand-alone AR Glasses device. The AR Glasses runs an XR application that uses the capabilities of the Glasses to create a service. The AR Glasses is tethered to a 5G device and potentially uses the capabilities of the phone to support the application.\nThe tethered standalone AR glass-based device architecture in Figure 4.4.2-1 is a complex and innovative solution for augmented reality (AR) applications. It combines the benefits of tethered and standalone AR systems, offering a unique and versatile approach to AR experiences. The architecture is designed to be flexible and adaptable, allowing for easy integration with various AR platforms and devices. The use of glass-based AR devices provides a high-quality visual experience, while the tethered nature ensures that the device remains connected to the user's device, providing a seamless and uninterrupted AR experience. The architecture also incorporates advanced features such as motion tracking and gesture recognition, enhancing the overall user experience.\nFigure 4.4.2-1 – Tethered Standalone AR glass-based device architecture\nThe AR/MR Application is responsible for orchestrating the various device resources to offer the AR experience to the user. In particular, the AR/MR Application can leverage three main internal components on the device which are:\n-\tThe Media Access Functions (MAF)\n-\tThe XR Runtime\n-\tThe XR Scene Manager\nThe AR/MR Application can communicate with those three components via dedicated APIs called the MAF-API, the XR Scene Manager API and the XR Runtime API. Among other functionalities, those APIs enables the AR/MR Application to discover and query the media capabilities in terms of support as well as available resources at runtime.\nThe XR runtime features several sensors and user controllers relevant for AR experiences such as cameras, microphones, speakers, display and generic user input. The XR Runtime typically also deals with the composition of primitive buffers that are mapped to the eye buffer display taking into account device characteristics as well as the latest pose information to apply late stage reprojection.\nThe XR scene manager is typically very lightweight and with no or very limited GPU capabilities. It maps raw media primitive buffers such as texture and depth information\nOnce the XR application is running, the downlink media is accessed by the MAF in compressed form and then from then MAF to the XR Scene Manger in primitives. The device may also establish an uplink data flow from the XR Runtime to the MAF wherein the data may be in an uncompressed form and then from the MAF to the remote device, it is typically compressed the data in order to facilitate the expected transmission over the network.\nIn order to analyse the use cases and tethering architectures in more details in terms of bitrates and processing, the following assumptions may be made on Tethered Standalone AR Glasses:\n-\tVideo Playback and decoding:  H.265 Main 10 Profile with maximum processing: up to 8,294,400 Macroblocks per second (corresponding to 8192x4320 @ 60fps)\n-\tVideo recording and encoding: H.265 Main 10 Profile with maximum processing: up to 3,888,000 Macroblocks per second (corresponding to 3840x2160 @ 120fps), low-latency encoding, error-robustness, slicing, intra refresh, long term prediction.\n-\tMaximum number of combined encoding and decoding instances: 16 for video, 16 for audio\n-\tAudio capabilities that allow to encode several audio inputs with low-latency and to decode multiple audio streams in parallel for binaural playback.\n-\tThe scene manager is very lightweight and passes through primitive buffers to be consumed by swap chains of the XR run-time. Swapchain images are typically 2D RGB.\nNOTE: For more detailed assumptions on the rendering capabilities for AR devices, refer to TS 26.119 [17].\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.4.3\tTethered Display AR Glasses",
                            "text_content": "Figure 4.4.3-1 provides the technical architecture of a typical tethered display AR Glasses device. The AR Glasses is tethered to a 5G device that includes the application and the XR functions. The 5G device runs the application that uses the capabilities of the 5G device to run an AR/MR experience. The AR Glasses is connected to the 5G Device, but the XR runtime API is exposed to the 5G device/phone.\nThe tethered display AR glass-based device architecture in Figure 4.4.3-1 showcases a unique design that combines the benefits of tethered and AR (Augmented Reality) technology. The device is tethered to a base station via a flexible tether, allowing for extended viewing angles and a more immersive experience. The AR glass-based display provides a high-resolution, interactive viewing experience, while the tethered tether ensures the device remains securely connected to the base station. This innovative design is a testament to the potential of AR technology in enhancing the user experience in various industries, such as healthcare, education, and entertainment.\nFigure 4.4.3-1 – Tethered Display AR glass-based device architecture\nIn this case, the connection between the phone and Glasses is hidden to the application and tethers the XR Runtime API on the 5G phone to the XR Runtime core functions on the glasses. The overall function is referred to as XR Link.\nIn order to analyse the use cases and tethering architectures in more details, it is assumed that the media access and rendering functions of a high-end smart phone can be used.\nNOTE: For more detailed assumptions on the capabilities for AR devices, refer to TS 26.119 [17].\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.4.4\tTethered AR Glasses with 5G Relay",
                            "text_content": "This architecture corresponds to the \"Type 3b: 5G Relay WireLess Tethered AR UE\" from TR26.998 [3], but is redrawn in line with the 5G AR UE Framework shown in Figure 4.4.1-2.\nA Basic AR/MR Application runs on the AR Glasses. It performs functions such as initiating the XR application that triggers a corresponding XR application in the Cloud/Edge, and initiating the setup of the tethering connection.\nThe XR runtime is split between the AR Glasses and the Cloud/Edge. An XR Runtime API is located in the Cloud/Edge. From the perspective of the XR application on the Cloud/Edge, the XR Runtime including the core functions (which are located on the AR glasses device) appears local.\nThe Media Access Function on the 5G Device/Phone performs QoS measuring and reporting for the tethering connection.\nTethered AR glasses with 5G relay, enabling seamless communication and immersive experiences.\nFigure 4.4.4-1 – Tethered AR glasses with 5G relay\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "5\tSystem Architectures and Call Flows",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "5.1\tSystem Architecture",
                    "description": "",
                    "summary": "",
                    "text_content": "The basic problem to solve is shown in Figure 5.1-1 below. An Application Server, for example on the edge, uses the 5G System to distribute content (e.g., content generated in response to the video/audio/pose input from the user) to a 5G phone. The 5G phone is connected with a pair of tethered glasses. The question is now how to set up connectivity and media call flows to provide a best user experience under latency and processing constraints on different links.\nThe system architecture of Figure 5.1-1 illustrates the multi-layered architecture of a 5G network, with key components including the base station (gNB), user equipment (UE), and scatterers. The diagram highlights beamforming techniques to mitigate interference, and the layered design aligns with SDN principles.\nFigure 5.1-1 The system architecture.\nArchitectural decisions can be made based on different applications, capabilities of the glasses and UE, link qualities and so on.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "5.2\tCall flows",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.2.1\tCall flows for standalone AR glasses-based device architecture",
                            "text_content": "For the network architecture for the standalone AR glasses-based device architecture as shown in Figure 5.2.1-1, the call flow is provided in Figure 5.2..1-1, which is similar to the call flow in subclause 4.3.1 of TR 26.998.\nNote that the 5G device serves as a relay, and may optionally be offloaded to perform some \"phone based processing functions\".\nThe call flow is described below.\n1.\tThe AR glasses device and the 5G device/phone sets up the tethering link.\n2.  The application contacts the application provider to fetch the entry point for the content. The acquisition of the entry point may be performed in different ways and is considered out of scope. An entry point may for example be a URL to a scene description.\n3.\tSession set up:\n3a.\tIn case when the entry point is a URL of a scene description, the application initializes the Scene Manager using the acquired entry point.\n3b.\tThe Scene Manager retrieves the scene description from the scene provider based on the entry point information.\n3c.\tThe Scene Manager parses the entry point and creates the immersive scene.\n3d.\tThe Scene Manager requests the creation of a local AR/MR session from the XR Runtime.\n3e.\t\tThe XR Runtime creates a local AR/MR session and performs registration with the local environment.\nThen steps 4 and 5 run in parallel:\n4:\tXR Media Delivery Pipeline: In case when entry point is a scene URL, a delivery session - for accessing scenes (new scenes or scene updates) and related media over the network is established. This can basically use the MAF as well as the scene manager and the corresponding network functions. Details are introduced in Figure 5.2.1-2.\nNOTE:\tThe realization of XR media delivery pipeline may vary in different architectures.\n5:\tXR Spatial Compute Pipeline: A pipeline that uses sensor data to provide an understanding of the physical space surrounding the device to determine the device’s position and orientation and placement of AR objects in reference to the real world and uses XR Spatial Description information from the network to support this process. Details are introduced in Figure 5.2.1-3.\n6:\tSteps 4 and 5 run independently, but the results of both pipelines (e.g., media organized in a scene graph and pose of the AR device) are inputs of the AR/MR Scene Manager function. This function handles the common processing of the two asynchronous pipelines to create an XR experience.\n\nThe tethering architecture of the standalone AR glass-based device is depicted in Figure 5.2.1-1, illustrating the call flow for the device's tethering capabilities. The architecture is designed to support seamless communication between the device and the tethering service provider, ensuring a reliable and efficient communication experience for the user.\nFigure 5.2.1-1: Call flow for the tethering architecture standalone AR glasses-based device architecture.\nThe XR media delivery pipeline (step 4 of Figure 5.2.1.-1) in Figure 5.2.1-1 is provided in Figure 5.2.1-2, which is similar to the media delivery pipeline in subclause 4.3.2 of TR 26.998.\nFor an XR Media Delivery Pipeline:\n1.\tThe Scene Manager initializes XR scene delivery session.\n2.\tThe MAF establishes XR scene delivery session.\n3.\tThe MAF may receive updates to the scene description from the scene provider\n4.\tThe MAF passes the scene update to the Scene Manager.\n5.\tThe Scene Manager updates the current scene.\n6.\tThe Scene Manager acquires the latest pose information and the user’s actions\n6.\tThe Scene Manager in the device shares that information with the Scene Manager in edge/cloud\nThe media rendering loop consists of the following steps. Note that steps 8, 9 and 10 are running as 3 parallel loops:\n8.\tFor each new object in the scene:\na.\tThe Scene Manager triggers the MAF to fetch the related media.\nb.\tThe MAF creates a dedicated media pipeline to process the input.\nc.\tThe MAF establishes a transport session for each component of the media object.\n9.\tFor each transport session:\na.\tThe media pipeline fetches the media data. It could be static, segmented, or real-time media streams.\nb.\tThe media pipeline processes the media and makes it available in buffers.\n10.\tFor each object to be rendered:\na.\tThe Scene Manager gets processed media data from the media pipeline buffers\nb.\tThe Scene Manager reconstructs and renders the object\n11.\tThe Scene Manager passes the rendered frame to the XR Runtime for display on the tethered standalone AR glass-based device.\nThe figure depicts a media delivery pipeline for call flow in Figure 5.2.1, illustrating the various stages of data transmission from the source to the destination. The pipeline includes media transport, network transport, and network transport, with each stage represented by different colored lines and symbols. The figure highlights the importance of network transport in ensuring the efficient delivery of data, with the use of optical fibers and other technologies.\nFigure 5.2.1-2: Media delivery pipeline for call flow in Figure 5.2.1.\nThe XR spatial compute pipeline (step 5 of Figure 5.2.1) in Figure 5.2.1 is provided in Figure 5.2 1-3 (below), which is similar to the XR spatial compute pipeline in subclause 4.3.3 of TR 26.998.\nFigure 5.2.1-3 Functional diagram for XR Spatial Compute Pipeline for call flow in Figure 5.2.1\nFor a XR Spatial Compute downlink delivery session:\n1.\tThe XR Spatial Compute function in the XR Runtime asks the MAF to establish a XR Spatial Compute downlink delivery session\n2. The MAF communicates with the network to establish the proper resources and QoS\n3. The XR Spatial Compute function requests access to XR Spatial Description information\n4. An XR Spatial Description downlink delivery session is established across the XR Spatial Compute server, the media delivery function, the media access function and XR Spatial Compute function on the device.\n5. XR Spatial Description information is delivered in this downlink delivery session\nFor a XR Spatial Compute uplink delivery session:\n6. The XR Spatial Compute function in the XR Runtime asks the MAF to establish a XR Spatial Compute uplink delivery session\n7. The MAF communicates with the network to establish the proper resources and QoS\n8. The MAF established an appropriate uplink delivery pipeline\n9. An XR Spatial Description uplink delivery session is established across the XR Spatial Compute function on the device, the media access function, the media delivery function and the XR Spatial Compute server.\n10. Spatial compute information is upstreamed to the XR Spatial Compute server.\n11. Data is continuously exchanged between the Scene Manager and the XR Runtime\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.2.2\tCall flows for display AR glasses-based device architecture without edge rendering",
                            "text_content": "For the network architecture corresponding to the display AR glasses-based device architecture without edge rendering, as shown in Figure 5.2.-2, the call flow is shown in Figure 5.2.1.-3. The call flow is different from the one in Figure 5.2.1.-1 mainly in that the AR/MR Application, Scene Manager and Media Access Function reside on the 5G Device. Additionally, an XR Runtime API is on the 5G Device, the XR Runtime is on the AR Glasses Device, and the interactions between the XR Runtime API and the XR Runtime are proprietary. This simplifies the design from the point of view of the AR/MR application because it only needs to concern about the XR Runtime API.\nThe call flow is similar to the one in Figure 5.2.1-3 and the main difference is on the procedures related to XR Runtime. For example, step 3d: Establish AR/MR session now points to XR Runtime API, instead of XR Runtime.\nThe figure depicts a call flow for a tethered display AR (Augmented Reality) glass-based device without edge rendering. The flow includes various stages such as device setup, display setup, and user interaction. The figure illustrates the communication between the device, the display, and the user, highlighting the importance of network architecture for seamless user experience.\nFigure 5.2.1-1-: Call flow for the network architecture for tethered display AR glasses-based device without edge rendering.\nThe XR media delivery pipeline (step 4 of Figure 5.2.-3) in Figure 5.2-3 is provided in Figure 5.2-4.\nThe figure depicts a media delivery pipeline for call flow in Figure 5.2-3, illustrating the various stages of communication from the source to the destination. The pipeline includes media transport, network control, and call control, with key components such as media servers, network switches, and call control centers. The diagram highlights the importance of network control and call control in ensuring efficient and reliable communication.\nFigure 5.2.2-2-: Media delivery pipeline for call flow in Figure 5.2-3.\n\nThe XR spatial compute pipeline (step 5 of Figure 5.2-3) in Figure 5.2-3 is provided in Figure 5.2-x below.\n\nThe figure depicts a call flow for XR spatial compute in Figure 5.2-3, illustrating the various stages of a call, including initiation, routing, and termination. The flow is represented by a series of arrows, indicating the progression of the call. The figure is a visual representation of the complex process involved in XR spatial compute, which is crucial for enabling high-quality spatial computing experiences.\nFigure 5.2.2-3 Call flow for XR spatial compute for call flow in Figure 5.2-3.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.2.3\tCall flows for tethered AR glasses with 5G relay with edge rendering",
                            "text_content": "For the network architecture corresponding to the tethered AR glasses with 5G relay with edge rendering, as shown in Figure 4.2.2 2, the call flow is shown in Figure 5.2.3-1.\nThe call flow is different from the one in Figure 5.2-1 and Figure 5.2.3 mainly in that the  XR Scene Manager, XR Runtime Functions are located within the AR Glasses and the Cloud/Edge. An XR Runtime API is located in the Cloud/Edge.\nThe call flow is similar to the one in Figure 5.2-3 and the main difference is on the procedures related to XR Runtime. For example, step  3, the XR link initialization and Step 4: session set-up takes place between XR Runtime and XR Runtime API. Establish AR/MR session now points to XR Runtime API, instead of XR Runtime. Step 5 and Step 6 follows similar procedure as the previous architectures.\nThe figure depicts a call flow for a tethered AR glasses network with 5G relay, illustrating the communication process between the glasses and the network. The figure shows the various stages of the call, including the initiation of the call, the transmission of data, and the reception of the response. The use of 5G relay technology ensures reliable and high-speed communication, while the tethered AR glasses provide a seamless user experience.\nFigure 5.2.3-1 Call flow for the network architecture for Tethered AR glasses with 5G relay.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "6\tIdentified Key Issues and Potential Solutions",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "6.1\tKey Issue #1: How to provide End-to-End QoS for the 5G relay architecture",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.1.1\tDescription of the key issue",
                            "text_content": "To an XR application, the QoS metrics that matter are the end-to-end QoS metrics, including:\n-\tDelay\n-\tPacket loss rate\n-\tBit rate\nHowever, for the tethered AR glasses with 5G relay architecture, as shown in figure 6.1-1, a typical AR/MR end-to-end path traverses both the 5G network and non-5G networks. The non-5G networks, e.g., Wi-Fi, the Internet, do not provide guaranteed QoS. A key challenge is how to provide end-to-end QoS with a mix of a 5G network and non-5G networks.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.1.2\tPotential solution",
                            "text_content": "Figure 6.1-1 depicts a breakdown of the end-to-end delay and suggests a solution: the 5G network adjusts the delay within the 5G network to compensate for the delays incurred in non-5G networks such that the end-to-end delay meets an end-to-end delay requirement. Note that this solution has its limitation in that it is impossible for the 5G network to compensate for the delay in the non-5G networks if the aggregate delay in the non-5G network exceeds the delay imposed by the end-to-end delay requirement.\nThe figure depicts the end-to-end delay breakdown of a 5G network, illustrating the various components and their interactions. The figure shows the signal propagation in a 5G network, with key components such as the base station (gNB), user equipment (UE), and scatterers. The diagram highlights beamforming techniques to mitigate interference, and the layered design aligns with SDN principles.\nFigure 6.1-1: End-to-end delay breakdown\nTo meet the end-to-end latency requirement for the AR/MR session, some entities must determine the delay on the tethering link , and the delay on the Internet (between the UPF and the edge application server) .\nAdditionally, the determined delay components need to be reported to the 5G system which uses the reported delays and the target end-to-end delay to determine the delay to be provisioned within the 5G system.\nThen, Figure 6.1-1 suggests that to meet a desired end-to-end latency requirement , the 5G network provides a delay as follows:\n\nThe above description can be extended to support delay provisioning within the 5G network in a statistical sense. For example, by taking into account the variation of the non-5G delays, the 5G network can determine the delay in the 5G network to make the end-to-end delay below a desired delay value 99% of the time.\nFor other end-to-end QoS metrics, such as packet error rate and bit rate, the 5G network can adjust the metrics within the 5G network to achieve desired end-to-end metrics.\nFirst, consider the packet error rate. Assuming that the packet error rate in the 5G network (denoted ) and the effective packet error rate in the non-5G networks (denoted ) are independent, the end-to-end packet error rate (denoted ) is then\n\nThis suggests that if  can be estimated, then the 5G network can adjust to achieve a desired value for  The above equation also suggests a method to estimate : if  is measured, then  can be solved for as\n\nThus a method for the 5G network to set a packet loss rate to achieve a desired end-to-end packet loss rate is as follows:\n1.\tThe 5G network estimates the packet error rate in the 5G network .\n2.\tThe end-to-end packet error rate  is measured.\n3.\tThe collective packet error rate of the non-5G networks  is computed according to\n4.\tThe 5G network adjusts the packet error rate within he 5G network to  to meet the desired end-to-end packet error rate\n\nNote that to measure the end-to-end packet error rate, the required number of increases with the packet error rate. For example, if the end-to-end packet error rate is 0.1%, there is one packet error event expected to occur in 1000 packets. This implies a slow response when the packet error rate is low. This is different from delay measurement, where a single packet can provide a valid measurement regardless of the range of the delay.\nNext consider the bit rate. The bit rate depends on the bit rate allocation of the network segments in the end-to-end path and rate control and congestion control. However, there is a simple relationship among the maximum allowed bit rates that can be exploited to determine the bit rate allocation. It is observed that the maximum allowed end-to-end bit rate () is the minimum of the maximum allowed bit rate in the 5G network () and the maximum allowed bit rate in the non-5G networks (, i.e.,  . Thus the 5G network only needs to provide an maximum allowed bit rate that is higher than the desired end-to-end bit rate.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.2\tKey Issue #2: How to determine the non-5G delay for the 5G relay architecture",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.2.1\tDescription of the key issue",
                            "text_content": "In an end-to-end connection that includes a tethering link (e.g., Wi-Fi link), a 5G network and the Internet, the Wi-Fi segment and the Internet segment typically cannot guarantee latency. To achieve low end-to-end latency, one approach is to make the latency in the 5G network very conservative such that the end-to-end latency is below a target value. This, however, comes at a cost, because provisioning an unnecessarily low latency in the 5G network means excessive resource allocation (e.g., to support a more robust modulation-and-coding scheme (MCS)) or pre-empting many other traffic flows.\nAn alternative approach is to dynamically adjust the delay in the 5G network in accordance with the total delay incurred elsewhere on the end-to-end path. The delay on a Wi-Fi link may change over time depending on the interference generated by other nearby Wi-Fi networks operating on the same frequency. Similarly, the delay between the UPF and the application server depends on the location of this selected UPF and the network congestion level. Therefore, measurements may be used to estimate these time-varying delays on the non-5G segments.\nFor delay measurement, it is important that the measured delay is representative of the delay to be experienced by the data packets. Delay measurements based on delay measurement messages such as the ping message (ICMP Echo and Echo Reply) may not accurately reflect the delay experienced by the data packets for two reasons: (1) the delay measurement message uses a protocol number (e.g., 1 for ping) that is different from the protocol number for the data packet (e.g., 17 if the data packet is sent with RTP/UDP), resulting in different IP 5-tuples and consequently different QoS treatment in the communication network; (2) the packet size of a delay measurement typically is much smaller than that of a data packet, resulting in different transmission delays which are part of the overall delay.\nThere are two ways to measure the latency and they fill in the details for step 10 in Figure 5.2-5 in clause 5.2.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.2\tSolution: Segment-by-segment delay measurement",
                            "text_content": "The delay on Wi-Fi link and the delay between the UPF and the application server are measured separately. One simple solution is to use the ICMP ping protocol (ICMP Echo and Echo Reply, IETF RFC792 [12]). The 5G phone sends a ping request to the AR glasses, which replies with a ping response. The 5G phone then obtains the RTT over the Wi-Fi link. Similarly, the UPF sends a ping request to the application server, which replies with a ping response, and the UPF obtains the RTT between the UPF and the application server. The respective RTTs can then be halved to get estimates of the one-way delays for the two non-5G segments.\nIn step 4, the MAF reports the one-way delay estimate  to the AF.\nIn step 7, the UPF reports the one-way delay estimate  to the SMF, which forwards the estimate to the AF.\nNOTE:\tHow UPF retrieves the RTT between the UPF and the application server and further exposes the latency results to the AF are not supported in SA2 in current release.\nIn step 9, the AF determines the desired value for the delay in the 5G network needed to compensate for the variation in the delay in the non-5G segments in order to meet the end-to-end latency requirement for the application, and sends a delay request to the PCF.\nThe figure depicts a segment-by-segment delay measurement in a 5G network, illustrating the impact of various factors such as signal strength, distance, and path loss on the delay. The visual representation shows the delay values for different segments, highlighting the importance of optimizing these parameters for optimal performance.\nFigure 6.2.2-1: segment-by-segment delay measurement\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.3\tSolution: End-to-end delay measurement",
                            "text_content": "The delay measurement is carried out in an end-to-end fashion. This avoids the potential rejection of a measurement message that originates from the UPF and reaches the application server. The AR glasses sends a ping request message to the application server, which replies with a ping response. The AR glasses then estimate the UL one-way end-to-end delay  by halving the RTT. The 5G network estimates the UL one-way delay within the 5G network , e.g., by recording the time when the ping request arrives at the phone and the time when the ping request reaches the UPF and takes the difference. or using the QoS monitoring mechanism to obtain the UL one-way delay within the 5G network.\nNOTE:\tHow UPF detects and reports the arrival time of the ping test is not supported yet in SA2.\nThe estimated UL one-way delay on the non-5G segments is then  as shownin Figure 6.2.3-1.\nThe figure depicts a 5G network signal propagation, illustrating the multi-path signal path and beamforming techniques to mitigate interference. The diagram highlights the role of base station (gNB), user equipment (UE), and scatterers in the network. The figure also shows the use of redundancy paths and layered architecture to ensure reliability. \nFigure 6.2.3-1: End-to-end delay measurement\nTo make the end-to-end delay measurements accurately reflect the end-to-end delay experienced by the data packets, one potential solution is to use in-band measurement as shown in Fig. 6.2.3-2. A timestamp message is piggybacked to an RTP packet that carries data when needed. The payload type field in the RTP packet header is determined by the data, and an RTP header extension is added to instruct the receiver how to separate the timestamp message and the data. This way, the RTP packet is treated as a data packet with packet size substantially the same as an RTP packet that carries data only. Similarly, a timestamp reply message is piggybacked to an RTP packet in the reverse direction. This mechanism is payload-format independent, i.e., it can be applied to all payload formats. The order of the Data and the Timestamp msg (or Timestamp Reply Msg) may be switched.\nThe figure depicts a detailed in-band end-to-end delay measurement using piggybacking a delay measurement message to an RTP packet. The measurement is conducted in the presence of a delay source, which is represented by the delay source message. The RTP packet is used to transmit the delay measurement message, and the delay measurement is then received by the receiver. The figure provides a clear and concise representation of the process, allowing for accurate and reliable in-band delay measurement.\nFig. 6.2.3-2 In-band end-to-end delay measurement by piggybacking a delay measurement message to an RTP packet.\nThe timestamp message may be an ICMP Timestamp message (as defined in RFC792 [12]) with a timestamp field of 32 bits, which represents the number of milliseconds with respect to midnight Universal Time (UT). However, the achievable accuracy is limited by the 1ms granularity of the timestamp format. Alternatively, Precision Time Protocol or PTP (IEEE 1588-2019 [13]) may be used for better accuracy due to a finer granularity of 1 nanosecond of its timestamp format.\nThe target precision for the measurement depends on the overall latency and the use case. TR 26.998 listed 50-60ms for the pose-to-render-to-photon latency. For better user experience, we may need to consider better target precision. .\nInstead of using RTP, using RTCP for delay measurement may be a candidate solution (as allowed in RFC3550), but it may have the following drawbacks:\n-\tFirst, although RTCP and RTP may use the same port number as allowed by RFC 5761 which was motivated to simplify NAT and firewall management, it is not guaranteed that they always use the same port number. When they use different port numbers, RTCP and RTP packets will be mapped to different QoS flows (in the 5G core network) and receive different QoS treatment.\n-\tSecond, the payload type for RTCP packets (200 for sender report (SR) and 201 for receiver report (RR)) are different from those of RTP packets, and if a network takes into account the payload type in provisioning QoS (e.g., in Wi-Fi an RTP packet carrying video may be mapped to the Video access category, while RTCP packets may be mapped to the Best Effort access category), RTCP packets and RTP packets may receive different QoS.\nThird, the overhead of RTCP packets may be greater, because a separate packet, which includes the UDP packet header, the IP packet header and lower layer packet headers, needs to be sent.\nTS 23.501 [9] offers two measurement methods for measuring the delay in the 5G system , originally intended for QoS monitoring to assist URLLC service. The first method, termed \"Per QoS Flow per UE QoS Monitoring\", leverages the GTP-U headers to carry the timestamps, and the second method, termed \"GTP-U Path Monitoring\", leverages the GTP-U Echo protocol. The first method is shown in Figure 6.2.3-3.\nThe figure depicts a flow-based delay measurement in the 5G system, specifically in TS23.501, which is a standard for QoS monitoring in 5G networks. The flow-based delay measurement is used to evaluate the performance of the 5G system in terms of delay, which is a critical metric for ensuring high-quality user experience. The figure shows the flow-based delay measurement for different QoS flows, with each flow represented by a different color and a different flow type (e.g., voice, video, etc.). The figure also includes a legend to help users understand the different flow types and their corresponding flow-based delay values. Overall, the figure provides a comprehensive view of the 5G system's QoS monitoring capabilities and helps in understanding the performance of the network in terms of delay.\nFigure 6.2.3-3: Measuring the delay in the 5G system: Per QoS Flow per UE QoS Monitoring in TS23.501  [9]\nThe PCF generates the QoS monitoring policy based on the request from the AF (directly or via NEF) (step 2).\nThe SMF initiates a QoS monitoring request to the NG-RAN (step 3) and the PSA UPF (step 4).\nStep 6: Time stamp T1 is taken in the PSA UPF, indicating the time when the PSA UPF sends a monitoring packet to the NG-RAN (i.e., gNB).\nStep 7: the PSA UPF sends a monitoring packet to the NG-RAN, containing T1, QFI and QoS Monitoring Packet (QMP) indicator in the GTP-U header.\nStep 8: Time stamp T2 is taken when the monitoring packet is received by the NG-RAN.\nStep 10: Time stamp T3 is taken when the NG-RAN forwards an UL packet, or generate a dummy UL packet, where for either case the NG-RAN puts UL/DL packet delay results of RAN part, T1, T2, T3 and the QMP indicator in the GTP-U header.\nStep 12: Time stamp T4 is taken when the UL packet is received.\nStep 13: Between the NG-RAN and the PSA UPF, if they are synchronized, then the UL delay will be T4-T3, and the DL delay will be T2-T1. If they are not synchronized, then the procedure computes the average one-way delay (T2-T1 + T4-T3)/2.\nStep 14: The delay on the access network (between the UE and the NG-RAN) can be added to the results in step 13 to get the total delays in the 5G system, i.e. UL, DL or RT latency.\nFinally, the UPF reports the QoS monitoring results to SMF and SMF further reports to PCF. The PCF then exposes the QoS monitoring results to the AF directly or via NEF as requested and the AF eventually obtains the UL/DL or average delay within the 5G System.\nPiggybacking timestamps rather than the timestamp message or timestamp reply message to an RTP data packet may reduce the communication overhead. For example, the Timestamp message or Timestamp Reply message are of the following format (RFC792 [12]) as shown in Figure 6.2.3-4.\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|     Type      |      Code     |          Checksum             |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|           Identifier          |        Sequence Number        |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|     Originate Timestamp                                       |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|     Receive Timestamp                                         |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|     Transmit Timestamp                                        |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nFigure 6.2.3-4: The packet format of Timestamp Reply message in RFC792[12]\nFor the Timestamp message, the size is 12 bytes. If only the timestamp portion – the Originate Timestamp – is piggybacked, the size is reduced to 4 bytes. To let the receiver know the type of the information (i.e., how many timestamps are contained), the Type field can be added. As a result, the total size is 5 bytes. The savings is 7 bytes. This may not look much but can be significant if frequent measurements are needed.\nSimilarly, for the Timestamp Reply message, the size is 20 bytes. If again only the timestamps and the Type information is included in the piggybacked RTP packet, the size is reduced from 20 bytes to 13 bytes. Assuming the number of Timestamp messages are the same as the number of Timestamp Reply messages, the average saving is 44%.\nRTP header extensions defined in RFC8285 [14] can be used to do timestamp piggybacking. There are two formats of RTP header extension, namely the one-byte and the two-byte formats [14].\nFigure 6.2.3-3 shows the timestamp piggybacking in the RTP packet payload that allows for measuring the one-way delays in both directions as well as the round-trip time. Specifically, uplink one-way delay = T2 – T1, downlink one-way delay = T4 – T3, and round-trip time = T2 – T1 + T4 – T3 = T4 – T1 – (T3 – T2).\nThe reason for putting the timestamp(s) in the RTP packet payload rather than in the RTP header is to accurately capture the processing delay (e.g., encryption of the payload in the case of SRTP) and other delays (e.g., wait time experienced by the media in the processing pipeline) experienced by the media.\nNote that RFC6051[16] specifies an RTP header extension that carries a timestamp, which has shortcomings compared to putting the timestamp(s) in the RTP packet payload. Although the motivation of the technique in RFC6051, i.e., putting a timestamp in the RTP header extension, was to speed up the synchronization between multiple RTP sessions, the technique has the benefit of offering more accurate delay measurement than the RTCP approach and ICMP approach described earlier because the latter approaches lead to a different treatment between the measurement packets and the RTP packets in the network. However, the technique in RFC6051 is not preferred because the timestamp fails to capture the processing delay and other delays experienced by the media, and, as a lesser problem, the technique currently supports only one timestamp to be carried in the RTP header extension.\nThe figure depicts a method for measuring in-band end-to-end delay in a video streaming system. By piggybacking timestamps onto an RTP packet, the delay can be measured in real-time, providing a more accurate representation of the system's performance.\nFigure 6.2.3-3 In-band end-to-end delay measurement by piggybacking timestamps to an RTP packet.\nThe one-byte RTP header extension for piggypacking timestamps is shown in Figure 6.2.3-4. The header extension consists of a single RTP header extension element, the ID of which is set to 1, and the length L field of which is set to 2 (which indicates a size of 3 bytes rather than 2 bytes for the ‘data’ field of the RTP header extension element in the one-byte format according to RFC8285 [14]), and the data of which is labeld \"#timestamps, start, size\" occupying 24 bits. Specifically,\n-\tThe \"#timestamps\" field specifies the number of timestamps. In the case of piggybacking the originate timestamp (T1), this field is set to 1; in the case of piggybacking the originate timestamp (T1), the receive timestamp (T2) and the transmit timestamp (T3), this field is set to 3.\n-\tThe \"start\" field specifies whether the timestamps are at the beginning of the RTP payload or at the end of the RTP payload.\n-\tThe \"size\" field specifies the size of the timestamps. The size depends on the timestamp formats. Considering the granularity and overhead, the 32-bit short NTP timestamp format [15] or a truncated version of it seems a good choice.\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|       0xBE    |    0xDE       |           length=1            |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|  ID=1 | L=2   |     #timestamps, start, size                  |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nFigure 6.2.3-4 The one-byte RTP header extension for piggybacking timestamps.\nThe two-byte RTP header extension for piggypacking timestamps is shown in Figure 6.2.3-5. The difference is that the ID and the L fields together occupy 2 bytes rather than 1 byte as in the one-byte format. Note that the value in the L field indicates the size of the data of the RTP header extension element literally, i.e., L=2 means 2 bytes.\n0                   1                   2                   3\n0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|       0xBE    |    0xDE       |           length=1            |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n|  ID=1         |         L=2   | #timestamps, start, size      |\n+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\nFigure 6.2.3-5 The two-byte RTP header extension for piggybacking timestamps.\nFor both the one-byte format and the two-byte format, the timestamp(s) are put in the RTP payload according to the respective \"#timestamps, start, size\" fields.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.2.4\tTime measurement protocol",
                            "text_content": "The ICMP ping protocol uses two timestamps generated at the transmitter to get an estimate of the RTT. The measured delay includes the time gap between the reception of the ping request message and the transmission of the ping response message at the receiver. The time gap contributes to the estimation error, and it depends on the operating system used at the receiver and may become significant for low latency applications.\nAlternatively, ICMP timestamp approach (IETF RFC792 [12]) can be used, which, compared to ICMP ping, provides the source two timestamps, one for the reception of the timestamp message and the other for the transmission of the timestamp reply message. The two timestamps are carried back to the source, which can use the difference to calculate the time gap and get a more accurate estimate of the RTT.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.3\tKey Issue #3: What and how to report the non-5G delay",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.3.1\tDescription of the key issue",
                            "text_content": "In order for the 5G network to provide an appropriate delay over the 5G segment of the end-to-end path to meet a target end-to-end delay requirement, the non-5G delay needs to be reported to the 5G system.\nThe delay over a non-5G network may vary over time. A key issue is what should be reported to the 5G network, and how.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.3.2\tPotential solutions",
                            "text_content": "In the segment-by-segment measurement method (clause 6.2.2), the 5G device (e.g., 5G phone) can report the delay between the AR glasses and the 5G device via  the Direct Data Collection Client as indicated in 3GPPTS26.531 [11], Other 5G functions on the 5G device is not excluded. The the UPF can report the delay between the UPF and the application server to SMF.\nIn the end-to-end measurement method (clause 6.2.3), the AR glasses measures the end-to-end delay, forwards the measurement to the 5G device. The 5G device reports it through MAF to the 5G core network.\nThe reporting methodology may depend on the tethering architectures. Figure 6.3-1 shows a possible call flow for delay reporting for the end-to-end measurement approach. The reported statistics of the end-to-end delay may include the mean and standard deviation.\nFigure 6.3-2 shows a possible call flow for delay reporting for the partial path measurement approach, where\n-\tDWM: delay between Wi-Fi and Media Access Function (MAF) on the 5G Device\n-\tDWW: delay between Wi-Fi on the AR Glasses Device and Wi-Fi on the 5G Device\n-\tDME: delay between MAF and the Edge/Cloud\nThe end-to-end delay De2e = DWW+ DWM + DME.\nIn step 15, the Direct Data Collection Client reports to the Data Collection AF a desired value for the delay within the 5G network Dc.\nThe figure depicts a graph with various data points representing the end-to-end delay of a communication system. The x-axis represents the time in seconds, and the y-axis represents the delay in milliseconds. The data points are color-coded to indicate the different types of delays, such as packet loss, jitter, and latency. The figure provides a visual representation of the correlation between the delay and the different factors affecting the system, such as the number of users, the distance between the source and the destination, and the type of network protocol used. The figure is useful for understanding the performance of the communication system and identifying potential bottlenecks.\nFigure 6.3-1: reporting statistics of end-to-end delay based on end-to-end measurement\nThe figure depicts a graph with various data points representing delays in a network. The x-axis represents the partial path measurement, and the y-axis represents the delay. The data points are color-coded to indicate the type of delay (e.g., packet delay, jitter, etc.). The figure provides a visual representation of the relationship between the partial path measurement and the delay, allowing for a quick understanding of the data.\nFigure 6.3-2: reporting statistics of delays based on partial path measurement\nNote that in Figure 6.3-2, the Media Access Function is mapped to an instance of the UE Application in the reference architecture for data collection and reporting in 3GPP TS26.531 [11], i.e., the reporting is via the R7 reference point. This also applies to step 8, where an application may be used to read the log on Wi-Fi and the application is an instance of the UE Application via the R7 reference point.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.4\tKey Issue #4: Formats and Connectivity of Tethered Glass",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.4.1\tDescription",
                            "text_content": "Split Rendering across a proprietary link may have limitations in terms of formats that can be used, as well as on the supported connectivity and associated bitrates. The knowledge of the capabilities of a tethered glasses, accessible through the XR Runtime API on the phone, can support the operation of the phone over the 5G Network in terms of required bitrates as well as in terms of preferable formats. This clause discusses the workflow and provides relevant conclusions in terms of capabilities and related signalling.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.2\tBackground",
                            "text_content": "The architecture of relevance for the device is shown in Figure 4.4.1-2. The key issue is the handling of media data in the workflow. In a typical implementation aligned with OpenXR processing, the following applies for visual media:\n-\tTo present images to the user, the runtime provides images organized in swapchains for the application to render into.\n-\tThe XR runtime may support different swapchain image formats and the supported image formats may be provided to the application through the runtime API. XR runtimes is expected to at least support R8G8B8A8 and R8G8B8A8 sRGB formats. Details may depend on the graphics API specified in xrCreateSession. Options include DirectX or OpenGL. For example, support for OpenGL ES as a reference may be assumed, i.e. an extension equivalent to the functionalities provided in XR_KHR_opengl_es_enable. OpenGL ES is platform independent and suited for embedded systems.\n-\tSwapchain images can be 2D or 2D Array. Arrays allow to extract a subset of the 2D images for rendering\n-\tThe application or scene manager can offload the composition of the final image to a XR runtime-supplied compositor. By this, the rendering complexity is significantly lower since details such as frame-rate interpolation and distortion correction are performed by the XR runtime. It is assumed that the XR Runtime provides these functionalities.\n-\tA runtime on a XR device typically supports OpenXR composition, namely Projection, Quad, Cube, Cylinder, Equirectangular.\nAn OpenXR application life cycle is shown in Figure 6.4.2-1. In this case, after creating an OpenXR session, the application starts a frame loop. The frame loop is executed for every frame. The frame loop consists of the following steps:\n1)\tSynchronize actions: this step consists of retrieving the action state, e.g. the status of the controller buttons and the associated pose. During this step, the application also establishes the location of different trackables. The application may also send haptics feedback.\n2)\tStart a new frame: this step starts with waiting for a frame to be provided by the XR runtime. This step is necessary to synchronize the application frame submission with the display. The xrWaitFrame function returns a frame state for the requested frame that includes a predictedDisplayTime, which is a prediction of when the corresponding composited frame will be displayed. This information is used by the application to request the predicted pose at display. Once the xrWaitFrame function completes, the application calls xrBeginFrame to signal the start of the rendering process.\n3)\tRetrieve rendering resources: the application starts by locating the views in space and time by calling the xrLocateViews function, provided with the predicted display time and the XR space. It then acquires the swap chain image associated with every view of the composition layer. It waits for the swap chain image to be made available so it can write into it.\n4)\tRendering: the application then performs its rendering work. It iterates over the scene graph nodes and renders each object to the view. This step usually uses a Graphics Framework such Vulkan, OpenGL, or Direct3D to perform the actual graphics operations.\n5)\tRelease resources: once the rendering is done for a view, the application releases the corresponding swap chain image. Once all views are rendered, it sends them for display by calling the xrEndFrame function.\nThe figure depicts the life cycle of an OpenXR application, including its development, testing, and deployment stages. It illustrates the various steps involved in creating, testing, and deploying an OpenXR application, highlighting the importance of testing and ensuring compatibility with different hardware and software platforms.\nFigure 6.4-2-1: OpenXR application life cycle\nFor audio media, similar processes as video typically apply. OpenXR and OpenSL ES aligned terminology is used as a reference, A typically possible decomposition of steps for immersive audio rendering is as follows: An interface to the XR runtime is available to hand over raw audio buffers to determine how the XR application and scene manager would access a device’s audio capabilities. In order address a concrete implementation example, the model of OpenSL ES is used as a reference for. OpenSL ES supports both file-based and in-memory data sources, as well as buffer queues, for efficient streaming of audio data from memory to the audio system. Buffer queues in OpenSL may be viewed as equivalent to visual swap chains. OpenSL ES may be viewed as companion to 3D graphic APIs such as OpenGL ES. The 3D graphics engine will render the 3D graphics scene to a two-dimension display device, and the OpenSL ES implementation will render the 3D audio scene to the audio output device. In today’s implementations, in addition to the functionalities from such buffer queues, different types of audio signals may be provided, and additional/alternative processing steps may be carried out. Audio signals (i.e. the combination of metadata and buffer queues) may be\n-\tnon-immersive or also known as non-diegitic, i.e. they are not rendered according to the pose.\n-\tImmersive and describe a full 6DoF experience in the reference space of the XR session. In this case, the XR runtime will create a rendered signal according to the latest pose.\n-\tImmersive and pre-rendered for a specific render pose. In this case, the signals have been prepared such that the runtime can use the audio signal and the associated render pose and supplementary data for a pose correction to the latest pose.\n-\ta mixture of such signals that are jointly presented.\n-\tthe signals may originate from different source, for example some may be generated locally, others may be part of a pre-rendering or a full scene created in the network\nThe audio data is considered to be uncompressed.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.3\tAssumptions",
                            "text_content": "Assuming now a device as documented in clause 4.4.3 as a tethered AR glasses, then in a typical deployment, Figure 6.4.3-1 provides a summary of the operation.\n-\tSwap chain images are provided by phone-based XR Scene manager to OpenXR\n-\tActions and pose information are provided to the application\n-\tA proprietary system is operated south of the OpenXR API for split rendering, i.e. depicted left of the OpenXR API. This is very typical in implementations today for which the phone and the glassesare provided by the same vendor.\n-\tThe OpenXR API supports a set of typical formats, that are raw formats according to the discussion in clause 6.4.2.\nThe figure depicts a proprietary split rendering operation, which is a method used in telecommunication networks to transmit data over long distances. The operation involves splitting the data into smaller packets, which are then transmitted in parallel over different paths. This method is used to improve the efficiency of data transmission and reduce the overall latency. The figure also shows the various components involved in the split rendering operation, such as the splitters, the data buffers, and the network interface controllers (NICs). The figure provides a clear and concise representation of the proprietary split rendering operation, making it easy to understand its purpose and components.\nFigure 6.4.3-1: Tethering based on proprietary split rendering operation\nIn an extension to Figure 6.4.3-1, Figure 6.4.3-2 provides more details of the typical functions carried out in the split rendering, namely converting the raw OpenXR formats, encoding and providing a security framework, setting up a connection together with a protocol and match the bitrate of the link. On the receiving end, the signals sent over the tethered link are decrypted, decoded and the provided to the XR runtime on the glassesfor final pose correction. Similar processing happens on the uplink to provide encoded pose and actions.\nThe figure depicts a split rendering operation on a proprietary link, illustrating the process of splitting a signal into two or more paths to improve signal quality and reduce interference. The figure shows the split path, the splitter, and the receiver, highlighting the importance of proper signal splitting in 5G networks.\nFigure 6.4.3-2: Split rendering operations on proprietary link\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.4\tProblem Statements",
                            "text_content": "Now a couple of scenarios exist, in case the media is not generated on device, but received over the 5G System. In the regular operation, the formats on the two links, i.e. the 5G system and the proprietary link, would operate completely independently.\nHowever, this misses the following aspects when the raw formats are decoded and provided:\n1)\tThe formats on the AR Glass may be restricted. Hence, only a reduced set of media data, for example in terms of resolution, may be usefully provided to the AR glass\n2)\tThe bitrate on the tethered link may be restricted, or may even dynamically change. In this case, the quality of the formats provided at the input may not be maintained end-to-end\n3)\tThe security framework between the glassesand the phone is such that it is unusable for certain formats.\nExamples for such cases may be in Media Streaming, for which only formats and signals are downloaded that can be processed by the raw formats supported over the tethered link. For conversational applications, also such restrictions may apply, for example for audio only stereo signals are supported.\nEven more severe, if the content is provided from the 5G System in a compressed from, then the operation in the phone results in a typical transcoding operation with the following drawbacks\n1)\tAddition of latency because of algorithmic and processing delays\n2)\tAdditional distortion may be added in the transcoding process\n3)\tUnnecessary power consumptions for the encoding and decoding processes\n4)\tA trusted security point may be interrupted, put in clear and re-encoded\n5)\tThe bitrate on the tethered link may be restricted, or may even dynamically change. In this case, the quality of the formats provided at the input may not be maintained end-to-end\n6)\tThe security framework between the glassesand the phone is such that it is unusable for certain formats.\nExamples for such cases may be in Media Streaming, but likely to a limited attempt. For conversational applications, also such restrictions may apply, for example for audio only stereo signals are supported. More prominent is the case in conversational speech cases and most prominent is the case in split rendering context, i.e. if the buffer view is pre-rendered on the edge or the cloud. In this case, transcoding is most impactful.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.5\tPotential Solutions",
                            "text_content": "In order to avoid format or transcoding conversion on the UE in case 1 from above, a simple relay can be done according to the architecture in clause 4.4.4. However, this has several downsides:\n1)\tThe phone is excluded in the rendering and the application. No locally generated data can be added to the rendering. The application needs to reside on the edge, and the UE may upload data for rendering.\n2)\tIf the connection to the cloud/edge is not available or interrupted, no service can be provided.\n3)\tThe latency that is possibly incurred by this operation may be to high\nRaw format adaptation primarily addressed case 2 from 6.4.4. In this case, the application on the UE not only queries the supported raw formats, but it also gets information from the XR Runtime on information related to the resolution of the formats, the quality of the link in a static and dynamic fashion and other information that the phone can use in the communication between across the 5G System as well as in the rendering in the device to properly match the formats received and generated on the phone to match those of the one supported on the proprietary link.\nThe key extensions are as follows:\n-\tSupport in the runtime query for supported formats additional information that include\n-\tthe resolution of the format\n-\tthe quality degradation incurred by the combination of the link and the coding\n-\tthe security framework and capabilities of the device\n-\tdynamically providing the metrics and information of the signal quality on the proprietary link\n-\tSupport of usage of this provided static and dynamic information for example in\n-\tthe Media streaming client to select the appropriate content codecs, bitrates, possibly in a dynamic fashion\n-\ta communication client to negotiate with the network to support the appropriate content formats, bitrates and qualities, possibly in a dynamic fashion\n-\tAnnouncing appropriate information on the 5G System network side to the 5G Phone in order to be able to make such selections.\nThe details of these extensions need to be added to for example a streaming manifest, the session description protocol or to a scene description.\nThis case primarily addressed the case 2 from 6.4.4, namely the issue of transcoding. In order to address this case, it is considered according to Figure 6.4.5.3-1, the media format is passed through from the 5G network over the tethering link, using the OpenXR or the general run-time API. This allows to also add local data as an example that can be added on different layers. Pass-through may include passing only the compressed media data, but may also include the security frame work or the entire protocol. This depends on the capability of the endpoint on the glasses.\nThe figure depicts a pass-through compressed media format, which is a method of transmitting data over a network without the need for additional compression. This format is used in various applications, such as video streaming and data transmission, to reduce the size of the data being transmitted. The figure shows the different components of the format, including the header, data, and compression algorithm, which are essential for ensuring efficient data transmission.\nFigure 6.4.5.3-1: Pass-through compressed media format.\nThe key extensions are as follows:\n-\tSupport in the runtime query for supported formats additional information that include\n-\tthe resolution of the format\n-\tthe quality degradation incurred by the combination of the link and the coding\n-\tthe security framework and capabilities of the device\n-\tdynamically providing the metrics and information of the signal quality on the proprietary link\n-\tthe audio and video decoding capabilities of the glass\n-\tthe security capabilities of the glass\n-\tthe security framework and capabilities of the device\n-\tstatically and dynamically the bitrate and delay of the link\n-\tSupport of usage of this provided static and dynamic information for example in\n-\tthe Media streaming client to select the appropriate content formats, bitrates, codecs and qualities, possibly in a dynamic fashion\n-\ta communication client to negotiate with the network to support the appropriate content formats, codecs bitrates and qualities, possibly in a dynamic fashion\n-\tin split rendering for which the formats are provided accordingly as shown in Figure 6.4.5.3-2\n-\tAnnouncing appropriate information on the 5G System network side to the 5G Phone in order to be able to make such selections.\nThe details of these extensions need to be added to for example a streaming manifest, the session description protocol or to a scene description.\nThe figure depicts a compressed media format used in telecommunication systems, specifically for transmitting data over long distances. It illustrates the various components involved in the compression process, such as the encoder, decoder, and compression algorithm. The figure also shows the different types of compressed media formats, including MPEG-2, MPEG-4, and H.264, and their respective compression ratios. The figure is essential for understanding the technical aspects of telecommunication systems and their role in data transmission.The figure depicts a compressed media format used in telecommunication systems, specifically for transmitting data over long distances. It illustrates the various components involved in the compression process, such as the encoder, decoder, and compression algorithm. The figure also shows the different types of compressed media formats, including MPEG-2, MPEG-4, and H.264, and their respective compression ratios. The figure is essential for understanding the technical aspects of telecommunication systems and their role in data transmission.\nFigure 6.4.5.3-2: Pass-through compressed media format.\nA simplified approach is provided in Figure 6.4.5.3-3, for which compressed formats ate exchanged via the XR runtime API.\nThe figure depicts a simplified architecture of a compressed format exchange system, showcasing the use of the XR Runtime API for efficient data transmission.\nFigure 6.4.5.3-3: Simplified architecture: Compressed formats are exchanged via XR Runtime API\nIn order to address these extensions in the  Swapchain Image Management of OpenXR are provided here https://registry.khronos.org/OpenXR/specs/1.0/html/xrspec.html#swapchain-image-management and summarized in clause 6.4.2, the following extensions are considered:\n-\tIn a regular operation, the xrEnumerateSwapchainFormats functional call enumerates the texture formats supported by the current session. The type of formats returned are dependent on the graphics API specified in xrCreateSession.\n-\tAs an example, Vulkan permits compressed image formats  relying on compressed data formats from Khronos:\n-\tHowever, none of the APIs refer to using any video compression formats for each of the swap chain images.\n-\tHence, in order to support the above operation of pass-through compressed formats, OpenXR or any runtime is extended to allow a format that adds compressed video formats as swap chain images for which\n-\tTime stamp is target display time (for example RTP time stamp)\n-\tthe compressed format includes render pose\n-\tcompressed texture format is handed over as part of the swap chain management\nNote that this swap chain image management with compressed data only applies for parts of the submitted swap chain buffers, one layer may be sent over in compressed from, whereas a locally generated layer may be sent over in raw form. The synchronization needs to be done by the runtime.\nFor audio similar principles apply, for which a compressed bitstream is handed to the glasses.\nFor the uplink media data, action and pose data may be passed on in compressed form as received in the glassesto the network. The XR Runtime API may even provide such data in compressed (for sending to network) and in raw form (for processing on the phone)\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.4.6\tConclusions",
                            "text_content": "In order to support tethered links and glass-based endpoints properly, it is beneficial to provide a framework that allows to adapt to the end point capabilities to maximize end-to-end quality in terms of signal quality, latency, power consumption, etc. In order to support this, extensions considered above include:\n-\tSupport in the runtime query for supported formats additional information that include\n-\tthe audio and video decoding capabilities of the glass\n-\tthe security capabilities of the glass\n-\tthe security framework and capabilities of the device\n-\tstatically and dynamically the bitrate and delay of the link\n-\tSupport of usage of this provided static and dynamic information for example in\n-\tthe Media streaming client to select the appropriate content formats, bitrates and qualities, possibly in a dynamic fashion\n-\ta communication client to negotiate with the network to support the appropriate content formats, bitrates and qualities, possibly in a dynamic fashion\n-\tin split rendering which the formats are provided from the cloud/edge rendering\n-\tAnnouncing appropriate information on the 5G System network side to the 5G Phone in order to be able to make such selections.\nThese extensions are preferably addressed in extensions OpenXR, 5G Media Streaming, 5G Real-time communication and the related stage-3 protocols such as DASH, ISO BMFF, SDP, RTP/RTCP, etc.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.5\tKey Issue #5: Compute distribution across UE and network for tethered glasses",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.5.1\tIntroduction",
                            "text_content": "In the tethered display AR Glasses context, the compute functions are distributed across the AR Glasses, as well as possibly the UE (phone) and the network. Even within the network the compute may be done in an edge or in the cloud.  The decision how to distribute the compute across different network entities highly depends on the use case and application, the available capabilities in different network entities, the available network connections, economical reasons and possibly many other constraints. Generally, the situation may even change over time, for example due to changes in the application, varying network connections or load re-distribution. It is not expected that a specification will solve the distribution of the compute resources. However, what is essential is that the decision-making entity has as much static and dynamic information in order to make informed decisions.\nThis clause provides some background on different distribution scenarios. The main focus is the derivation of relevant static and dynamic status and capability information to establish proper workflows.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.2\tBackground",
                            "text_content": "Based on TS 26.119 [17], once a session is running and in focussed state a rendering loop is executed following Figure 6.5.2:\na)\tThe XR Application retrieves the action state, e.g. the status of the controllers and their associated pose. The application also establishes the location of different trackables.\nb)\tBefore an application can begin writing to a swapchain image, it first waits on the image to avoid writing to it before the Compositor has finished reading from it. Then an XR application synchronizes its rendering loop to the runtime. In the common case that an XR application has pipelined frame submissions, the application is expected to compute the appropriate target display time using both the predicted display time and predicted display interval. An XR Runtime is expected to provide and operate a swapchain that supports a specific frame rate.\nc) \tOnce the wait time completes, the application initiates the rendering process. In order to support the application in rendering different views the XR Runtime provides access to the viewer pose and projection parameters that are needed to render the different views. The view and projection info is provided for a particular display time within a specified XR space. Typically, the target/predicted display time for a given frame.\nd)\tthe application then performs its rendering work. Rendering work may be very simple, for example just directly copying data from the application into the swap chain or may be complex, for example iterating over the scene graph nodes and rendering complex objects. Once all views/layers are rendered, the application sends them to the XR Runtime for final compositing including the expected display time as well as the associated render pose.\ne) \tAn XR Runtime typically supports (i) planar projected images rendered from the eye point of each eye using a perspective projection, typically used to render the virtual world from the user’s perspective, and (ii) quad layer type describing a posable planar rectangle in the virtual world for displaying two-dimensional content. Other projection types such as cubemaps, equirectangular or cylindric projection may also be supported.\nf)\tThe XR application offloads the composition of the final image to an XR Runtime-supplied compositor. By this, the rendering complexity is significantly lower since details such as frame-rate interpolation and distortion correction are performed by the XR Runtime. It is assumed that the XR Runtime provides a compositor functionality for device mapping. A Compositor in the runtime is responsible for taking all the received layers, performing any necessary corrections such as pose correction and lens distortion, compositing them, and then sending the final frame to the display. An application may use multiple composition layers for its rendering. Composition layers are drawn in a specified order, with the 0th layer drawn first. Layers are drawn with a \"painter’s algorithm,\" with each successive layer potentially overwriting the destination layers whether or not the new layers are virtually closer to the viewer. Composition layers are subject to blending with other layers. Blending of layers can be controlled by layer per-texel source alpha. Layer swapchain textures may contain an alpha channel. Composition and blending is done in RGBA.\ng)\tAfter the compositor has blended and flattened all layers, it then presents this image to the system’s display. The composited image is then blend with the user’s view of the physical world behind the displays in one of three modes, based on the application’s chosen environment blend mode:\nh)\tMeanwhile, while the XR Runtime uses the submitted frame for compositing and display, a new rendering process may be kicked off for a different swap chain image.\nThe figure depicts a rendering loop for visual data, illustrating the process of data visualization. The loop consists of a series of steps, including data input, data processing, data display, and data storage. The figure provides a visual representation of the data flow, allowing users to understand the data processing and display process in a more intuitive manner.\nFigure 6.5.2-1 Rendering loop for visual data\nOnce this loop is running, the rendering can statically or dynamically be adjusted as long as operation northbound of the rendering loop is consistent.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.3\tAssumptions",
                            "text_content": "According to TS 26.119 [17], media to be rendered and displayed by the XR device through the XR runtime is typically not available in uncompressed form on the device. In contrast, media is accessed using a 5G System, decoded in the device using media capabilities, and the decoded media is rendered to then be provided through swapchains to the XR Runtime as shown in Figure 6.5.3-1.\nThe figure depicts various media pipelines, including access, decoding, and rendering, which are essential components in the communication process.\nFigure 6.5.3-1 Media pipelines: Access, decoding and rendering\nThe rendering function is responsible to adapt the content to be presentable by the by the XR Runtime by making use of a rendering loop and using swapchains. The application configures pipeline of different processes, namely the media access, the decoding and the rendering.\nFor certain applications and scenes, the rendering capabilities on the glasses, or the phone may not be sufficient in order to address the application requirements and pre-rendering is done in the network.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.4\tProblem Statement",
                            "text_content": "In all considered architectures, the compute functions are distributed across the AR Glasses, as well as possibly the phone and the 5G network. Even within the network the compute may be done in an edge or in the cloud.  The decision how to distribute the compute across different network entities highly depends on the use case and application, the available capabilities in different network entities, the available network connections, economical reasons and possibly many other constraints. Generally, the situation may even change over time, for example due to changes in the application, varying network connections or load re-distribution.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.5\tPotential Solutions",
                            "text_content": "The solution is obvious, namely the distribution of processing functions across different entities. However, it is not expected that a specification will solve the distribution of the compute resources, but that a decision-making entity has sufficient static and dynamic information in order to make informed decisions.\nThe decision-making entity may for example be a Media Session Handler functions that configures the workflows. In order to do such configuration, the Media Session Handler needs to collect static and real-time information on the capabilities as well as real-time metrics from:\n-\ttethering glass\n-\tphone running the 5G System and Media Session Handler\n-\t5G Edge Server\n-\tCloud Server\nCollected static and dynamic information includes, but is not limited to,\n-\tlink quality (bitrate, QoS),\n-\tavailable encoding and decoding resources,\n-\tavailable rendering capabilities,\n-\toperational QoE metrics and logs\nReconfiguration of such workflows needs to be possible.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.5.6\tConclusions",
                            "text_content": "Based on the considerations, the Split Rendering architecture is expected to be extended to address:\n-\tA workflow configuration management\n-\tThis workflow management collects static and real-time capabilities and metrics information based on metrics in clause 6.5.5 from tethering glasses, phones running the 5G System and Media Session Handler, 5G edge server as well as cloud server.\n-\tThe workflow management is able to re-configure the rendering workflow.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "6.6\tKey Issue #6: Usage of PIN for Tethered Glasses",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.6.1\tDescription",
                            "text_content": "In order to use PIN (Personal IoT Network) for AR Glasses tethering, a functional mapping between PIN architecture [6] and the device architecture for tethered AR Glasses in clause 4.4 of this document is needed.\nAR Glasses may support different connectivity solutions, e.g., it can be a 3GPP UE, or a Wi-Fi device. A key issue is to clarify the usage of PIN for AR Glasses tethering for different scenarios.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.6.2\tPotential solutions",
                            "text_content": "When AR Glasses considered as a 3GPP UE, and typically in the Tethered Standalone AR glass-based device architecture in clause 4.4.2 and assumed that a 3GPP UE can act as PEGC and/or PEMC [6], AR Glasses can act as PEGC and/or PEMC. It means that AR glasses can directly use the PC5 interface for tethering. In this situation, it is not necessary to use PIN network for AR glasses tethering.\nWhen AR Glasses considered as a non-3GPP device, wireless tethered connectivity is provided through non-3GPP access e.g., Wi-Fi or BLE, With the consideration of the Tethered Display AR glass-based device architecture in clause 4.4.3 and Tethered AR glasses with 5G relay in clause 4.4.4, a solution for functional mapping can be found as follows, shown in Figure 6.6.2-1:\n-\tThe 5G Device/Phone is a 3GPP UE which can act as a PEGC and PEMC\n-\tThe Tethered AR Glasses can act as a PINE\n-\tThe XR application server can be considered as a PIN Application server\n\nThe figure depicts a tethered augmented reality (AR) glasses system, showcasing the use of a PIN (Point-to-Interference-Narrowing) antenna for enhanced signal reception. The system utilizes a combination of a PIN antenna and a traditional antenna to provide a more stable and reliable connection. The figure illustrates the process of connecting the tethered glasses to the base station, demonstrating the importance of proper antenna selection and positioning for optimal performance.\nFigure 6.6.2-1 – Using PIN for Tethered AR glasses\nAccordingly, reference point P1 between the PINE and the PEGC, which is based on non-3GPP access, can be used for AR Glasses tethering connection.\nReference point P2 between the PEMC and the PEGC is based on non-3GPP access (e.g. Wi-Fi or BLE) or 5G ProSe Direct Communication. Reference point P3 between the PEMC and the PIN Application Server is based on the direct user plane path to 5GS, relay path via the PEGC, or other communication path via Internet. PEMC can be used for PIN management (create/modify/delete/activate/deactivate a PIN) and add/remove of PINE and PEGC via P2 and P3, to manage the tethering connection of AR Glasses.\nReference point P4 between the PEGC and the PIN Application Server, which is based on the user plane path between PEGC and 5GS, can be used for supporting the relay path between PINE and 5GS for the Tethered AR glasses with 5G relay architecture in clause 4.4.4.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "7\tSummary of Key Issues and Potential Work Topics",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "7.1\tGeneral",
                    "description": "",
                    "summary": "",
                    "text_content": "This clause documents and clusters potential new work and study areas identified in the context of this Technical Report.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.2\tEnd-to-End QoS",
                    "description": "",
                    "summary": "",
                    "text_content": "In case of an architecture, for which the phone is predominantly acting as a relay as shown in clause 4.4.4, the topics identified in Key Issue #1, key issue #2 and key issue#3 require solutions to address handling of QoS parameters and QoE monitoring for such relay systems.\nThe following is a list of work topics need to be addressed in normative specifications:\n-\tSpecification of RTP header extensions to support in-band end-to-end delay measurements as defined in clause 6.2.3.\n-\tSpecification of reporting mechanisms for the end-to-end delay measurements as defined in clause 6.2.3.\nThe RTP header extensions are preferably addressed by extending the 5G RTP protocol as defined in TS 26.522 [18] and the reporting mechanisms may be addressed by some other related work items.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.3\tCapabilities, Formats and Connectivity of Tethered Glass",
                    "description": "",
                    "summary": "",
                    "text_content": "Following the conclusions for key issue #4 in clause 6.4, in order to support tethered links and glass-based endpoints properly, it is beneficial to provide a framework that allows to adapt to the end point capabilities to maximize end-to-end quality in terms of signal quality, latency, power consumption, etc. In order to support this, extensions considered above include:\n-\tSupport in the runtime query for supported formats additional information that include\n-\tthe audio and video decoding capabilities of the glass\n-\tthe security capabilities of the glass\n-\tthe security framework and capabilities of the device\n-\tstatically and dynamically the bitrate and delay of the link\n-\tSupport of usage of this provided static and dynamic information for example in\n-\tthe Media streaming client to select the appropriate content formats, bitrates and qualities, possibly in a dynamic fashion\n-\ta communication client to negotiate with the network to support the appropriate content formats, bitrates and qualities, possibly in a dynamic fashion\n-\tin split rendering which the formats are provided from the cloud/edge rendering\n-\tAnnouncing appropriate information on the 5G System network side to the 5G Phone in order to be able to make such selections.\nThese extensions are preferably addressed in extensions OpenXR, 5G Media Streaming, 5G Real-time communication and the related stage-3 protocols such as DASH, ISO BMFF, SDP, RTP/RTCP, etc.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.4\tWorkflow management in distributed split rendering",
                    "description": "",
                    "summary": "",
                    "text_content": "Based on the considerations in Key Issue #5 as documented in clause 6.5, as Split Rendering architecture is expected to be extended to address:\n-\tA workflow configuration management\n-\tThis workflow management collects static and real-time capabilities and metrics information based on metrics in clause 6.5.5 from tethering glasses, phones running the 5G System and Media Session Handler, 5G edge server as well as cloud server.\n-\tThe workflow management is able to re-configure the rendering workflow.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.5\tUsage of PIN for Tethered Glasses",
                    "description": "",
                    "summary": "",
                    "text_content": "Clause 6.6.2 provides a functional mapping in case AR Glasses are considered as a non-3GPP device, and the wireless tethered connectivity is provided through non-3GPP access e.g., Wi-Fi or BLE. While the functional mapping is complete, a more detailed analysis on potential gaps on the user plane path P4 between PEGC and 5GS as well as P1 between the PINE and the PEGC is needed to understand whether more details would have to be specified. Additional studies are encouraged.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "8\tConclusions and Recommendations",
            "description": "The present document addressed different architectures, QoS and media handling aspects of when tethering AR Glasses to 5G UEs based on initial discussions in TR 26.998 [2].\nBased on the details in the report, the following next steps are proposed:\n-\tSpecify RTP header extensions for in-band end-to-end delay measurements to support end-to-end QoS based on the discussion in clause 7.2 using the framework of 5G RTP protocol as defined in TS 26.522 [18].\n-\tSupport capability exchange, formats and connectivity extensions to support adapting to the end point capabilities of tethered in order to maximize end-to-end quality in terms of signal quality, latency, power consumption as well as the compute distribution across different physical devices (glasses, phone, edge, cloud) based on the detailed discussion in clause 7.3.\n-\tSupport information collection for workflow management, including static and real-time capabilities and metrics information from tethering glasses, phones running the 5G System and Media Session Handler, 5G edge server as well as cloud server, including the ability to re-configure the rendering workflows based on the discussions in clause 7.4.\n-\tContinue the study of applicability and potential gaps of PIN for tethered glasses based on the discussion in clause 7.5.\nAll work topics will benefit to be carried out in close coordination with other groups in 3GPP on 5G System and radio related matters, edge computing and rendering as well in communication with experts in MPEG on the MPEG-I project as well as with Khronos on their work on OpenXR, glTF and Vulkan/OpenGL.\nFor Relay WLAR UE, the 5G sidelink communication is used for the tethering link. The tethering 5G Phone providing the IP network connectivity can be seemed as a 5G ProSe Layer-3 UE-to-Network Relay and the tethered AR Glasses can be seemed as a Remote UE as specified in TS 23.304 [7]. The QoS requirements for the end-to-end AR session can be satisfied by the corresponding QoS control for the tethering link between AR Glasses and 5G Relay UE (i.e. PC5 QoS control), and the QoS control through the 5G system (i.e. the PDU Session between UE and UPF). The tethering link QoS and the 5G System QoS are separately controlled with corresponding QoS rules and QoS parameters (e.g. 5QI, PQI) as specified in TS 23.287 [8] and TS 23.501 [9].\nAs shown in figure A-1 below, the end-to-end QoS can be met only when the QoS requirements are properly translated and satisfied over the two legs respectively.\nThe figure depicts the end-to-end QoS translation for 5G Layer-3 Relay operation, illustrating the various steps involved in ensuring high-quality communication. Key components include the relay node, the relay network, and the network slicing. The figure highlights the importance of network slicing in providing differentiated services to users, while also demonstrating the use of QoS management tools to optimize network performance.\nFigure A-1: End-to-End QoS translation for 5G Layer-3 Relay operation\nTo achieve this, the QoS mapping can be pre-configured or provided to the 5G Relay UE from the 5GC. The QoS mapping includes combinations of the 5QIs for the 5G link and the PQIs for the tethering link as entries. Both 5QIs and PQIs have standardized values as specified in TS 23.501 [9] and TS 23.287 [8].\nIf the QoS setup of 5G system link is initiated by network, the 5G Core Network can generates the QoS parameters (e.g. 5QI) and signal to the 5G Relay UE. Then the 5G Relay UE determines the tethering link QoS parameters based the pre-retrieved QoS mapping and then setup the tethering link between AR glasses and the 5G Relay UE.\nIf the AR Glasses initiates QoS setup or modification for the tethering link, it provides the QoS Info to the 5G Relay UE. The QoS Info (i.e. PQI, etc.) are interpreted as the end-to-end QoS requirements by the 5G Relay UE for the traffic transmission through the 5G system. The 5G Relay UE would check if the end-to-end QoS requirements can be supported, and decide the 5GS QoS and tethering link QoS parameters based on the QoS mapping.\n\n\n\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 4,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        }
    ]
}