{
    "document_name": "26925-i10.docx",
    "content": [
        {
            "title": "Foreword",
            "description": "This Technical Report has been produced by the 3rd Generation Partnership Project (3GPP).\nThe contents of the present document are subject to continuing work within the TSG and may change following formal TSG approval. Should the TSG modify the contents of the present document, it will be re-released by the TSG with an identifying change of release date and an increase in version number as follows:\nVersion x.y.z\nwhere:\nx\tthe first digit:\n1\tpresented to TSG for information;\n2\tpresented to TSG for approval;\n3\tor greater indicates TSG approved document under change control.\ny\tthe second digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, etc.\nz\tthe third digit is incremented when editorial only changes have been incorporated in the document.\nIn the present document, modal verbs have the following meanings:\nshall\t\tindicates a mandatory requirement to do something\nshall not\tindicates an interdiction (prohibition) to do something\nThe constructions \"shall\" and \"shall not\" are confined to the context of normative provisions, and do not appear in Technical Reports.\nThe constructions \"must\" and \"must not\" are not used as substitutes for \"shall\" and \"shall not\". Their use is avoided insofar as possible, and they are not used in a normative context except in a direct citation from an external, referenced, non-3GPP document, or so as to maintain continuity of style when extending or modifying the provisions of such a referenced document.\nshould\t\tindicates a recommendation to do something\nshould not\tindicates a recommendation not to do something\nmay\t\tindicates permission to do something\nneed not\tindicates permission not to do something\nThe construction \"may not\" is ambiguous and is not used in normative elements. The unambiguous constructions \"might not\" or \"shall not\" are used instead, depending upon the meaning intended.\ncan\t\tindicates that something is possible\ncannot\t\tindicates that something is impossible\nThe constructions \"can\" and \"cannot\" are not substitutes for \"may\" and \"need not\".\nwill\t\tindicates that something is certain or expected to happen as a result of action taken by an agency the behaviour of which is outside the scope of the present document\nwill not\t\tindicates that something is certain or expected not to happen as a result of action taken by an agency the behaviour of which is outside the scope of the present document\nmight\tindicates a likelihood that something will happen as a result of action taken by some agency the behaviour of which is outside the scope of the present document\nmight not\tindicates a likelihood that something will not happen as a result of action taken by some agency the behaviour of which is outside the scope of the present document\nIn addition:\nis\t(or any other verb in the indicative mood) indicates a statement of fact\nis not\t(or any other negative verb in the indicative mood) indicates a statement of fact\nThe constructions \"is\" and \"is not\" do not indicate requirements.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "Introduction",
            "description": "The document presents typical media traffic characteristics (including bandwidth and latency requirements) that are of importance for 3GPP standardization work. This includes demands based on current services, but also expectations for new services or emerging services, considering developments in the industry in terms of efficiency improvements.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "1\tScope",
            "description": "The present document includes following information:\n-\tMedia centric Third-Party and Operator services currently deployed or expected to be deployed until 2025 on 3GPP defined 4G and 5G networks.\n-\tTypical deployment characteristics today, such as bandwidth requirements, client buffered and rate adaptation reception characteristics, codecs, protocols in use and latency requirements.\n-\tAn overview on technological developments for existing and emerging services and their impact on typical traffic characteristics of media services, e.g. evolution of compression technologies, new demands for high quality, new experiences, etc.\n-\tA summary on typical characteristics and requirements for different media services on 3GPP networks.\n-\tAn identification of the applicability of existing 5QIs for such services and potentially identify requirements for new 5QIs or QoS related parameters.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "2\tReferences",
            "description": "The following documents contain provisions which, through reference in this text, constitute provisions of the present document.\n-\tReferences are either specific (identified by date of publication, edition number, version number, etc.) or non-specific.\n-\tFor a specific reference, subsequent revisions do not apply.\n-\tFor a non-specific reference, the latest version applies. In the case of a reference to a 3GPP document (including a GSM document), a non-specific reference implicitly refers to the latest version of that document in the same Release as the present document.\n[1]\t3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".\n[2]\t3GPP TS 26.234: \"Transparent end-to-end Packet-switched Streaming Service (PSS); Protocols and codecs\".\n[3]\t3GPP TS 26.247: \"Transparent end-to-end Packet-switched Streaming Service (PSS); Progressive Download and Dynamic Adaptive Streaming over HTTP (3GP-DASH)\".\n[4]\t3GPP TS 26.346: \"Multimedia Broadcast/Multicast Service (MBMS); Protocols and codecs \".\n[5]\t3GPP TS 26.116: \"Television (TV) over 3GPP services; Video profiles\".\n[6]\t3GPP TS 26.118: \"3GPP Virtual reality profiles for streaming applications\".\n[7]\t3GPP TS 26.114: \"IP Multimedia Subsystem (IMS); Multimedia telephony; Media handling and interaction\".\n[8]\t3GPP TS 26.223: \"Telepresence using the IP Multimedia Subsystem (IMS); Media handling and interaction \".\n[9]\tRecommendation ITU-R BT.1872-3 (10-2019): \" User requirements for broadcast auxiliary services including digital television outside broadcast, electronic/satellite news gathering and electronic field production\".\n[10]\t3GPP TS 23.501: \"System Architecture for the 5G System \".\n[11]\t3GPP TS 26.238: \"Uplink streaming\".\n[12]\tST 2110-10:2017 - SMPTE Standard - Professional Media Over Managed IP Networks: System Timing and Definitions.\n[13]\tVideo Services Forum (VSF) Technical Recommendation TR-05, Essential Formats\tand Descriptions for Interoperability of\tSMPTE ST 2110-20 Video Signals.\n[14]\tRecommendation ITU-T H.264 (04/2017): \"Advanced video coding for generic audiovisual services\" | ISO/IEC 14496-10:2014: \"Information technology - Coding of audio-visual objects - Part 10: Advanced Video Coding\".\n[15]\tRecommendation ITU-T H.265 (12/2016): \"High efficiency video coding\" | ISO/IEC 23008-2:2015: \"High Efficiency Coding and Media Delivery in Heterogeneous Environments - Part 2: High Efficiency Video Coding\".\n[16]\t3GPP TR 26.949: \"Video formats for 3GPP services\".\n[17]\tIETF RFC 793: \"TCP\"\n[18]\thttps://www.ietf.org/id/draft-ietf-quic-transport-19.txt.\n[19]\tDVB BlueBook A176: \"Adaptive media streaming over IP multicast - reference architecture\".\n[20]\tCableLabs OC-TR-IP-MULTI-ARCH: \"IP Multicast Adaptive Bit Rate Architecture Technical Report\".\n[21]\t\"How youtube led to Google's cloud-gaming service\", spectrum.ieee.org | SEP 2019 | 09.\n[22]\t3GPP TR 22.842: \"Study on Network Controlled Interactive Services (Release 17)\".\n[23]\t\"Cloud Gaming: Architecture and Performance\", Ryan Shea and Jiangchuan Liu, Simon Fraser University; Edith C.-H. Ngai, Uppsala University; Yong Cui, Tsinghua University; IEEE Network-July/August 2013.\n[24]\tJens-Rainer Ohm, Gary J. Sullivan, Heiko Schwarz, Thiow Keng Tan, and Thomas Wiegand, \"Comparison of the Coding Efficiency of Video Coding Standards—Including High Efficiency Video Coding (HEVC)\" IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, VOL. 22, NO. 12, DECEMBER 2012.\n[25]\tT.K. Tan, M. Mrak, R. Weerakkody, N. Ramzan, V. Baroncini, G.J. Sullivan, J.-R. Ohm, K.D. McCann, \"HEVC subjective video quality test results\", IBC2014 Conference, 2014.\n[26]\tThiow Keng Tan ; Rajitha Weerakkody ; Marta Mrak ; Naeem Ramzan ; Vittorio Baroncini, Jens-Rainer Ohm, Gary J. Sullivan, \"Video Quality Evaluation Methodology and Verification Testing of HEVC Compression Performance\" IEEE Transactions on Circuits and Systems for Video Technology, Volume: 26, Issue: 1, Jan. 2016.\n[27]\tMinhua Zhou, Jianle Chen, Kiho Choi and Dmytro Rusanovskyy, \"Tool comparison between VVC (VTM3.0) and NVC\", ISO/IEC JTC1/SC29/WG11 MPEG2019/ m46554, Marrakech, Morocco, January 2019.\n[28]\tFrank Bossen, Xiang Li, Karsten Suehring, \"AHG report: Test model software development (AHG3)\", JVET-M0003, Joint Video Experts Team (JVET) of ITU-T SG 16 WP 3 and ISO/IEC JTC 1/SC 29/WG 11, 13th Meeting: Marrakech, MA, 9-18 Jan. 2019.\n[29]\tK. Choi et al., \"Description of video coding technology proposal by Samsung, Huawei, and Qualcomm for New Video Coding Standard\", MPEG-M46354, Marrakech, Morocco, January 209.\n[30]\t3GPP TR 26.928: \"Extended Reality over 5G\".\n[31]\tST 297:2006 - SMPTE Standard - For Television — Serial Digital Fiber Transmission System for SMPTE 259M, SMPTE 344M, SMPTE 292 and SMPTE 424M Signals.\n[32]\tST 2081-10:2018 - SMPTE Standard - 2160-line and 1080-line Source Image and Ancillary Data Mapping for 6G-SDI.\n[33]\tST 2082-1:2015 - SMPTE Standard - 12 Gb/s Signal/Data Serial Interface — Electrical ST 2082-1:2015.\n[34]\tSMPTE ST-2083, 24G-SDI, In development.\n[35]\t3GPP TR 22.827: \"Study on Audio-Visual Service Production\".\n[36]                      ST 2042-1:2017 - SMPTE Standard - VC-2 Video Compression\n[37]                      ITU-T/T.802 | ISO/IEC 15444-3 \"Information technology - JPEG 2000 image coding system - Part 3: Motion JPEG 2000\n[38]                     ITU-T G.1032 \"Influence factors on gaming quality of experience\" ()\n[39]                     ITU-T P.809 \"Subjective evaluation methods for gaming quality\" ()\n[40]                     M. Mathis, J. Semke, J. Mahdavi, The macroscopic behavior of the TCP congestion avoidance algorithm. Comput. Commun. Rev. (ACM SIGCOMM) 27(3), 67–82 (1997)\n[41]                     Yusuke Miki, Tsuyoshi Sakiyama, Kenichiro Ichikawa, Mayumi Abe, Seiji Mitsuhashi, Masayuki Miyazaki , \"Ready for 8K UHDTV broadcasting in Japan\", IBC 2015 Conference.\n[42]\tHirokazu Kamoda, \"NHK Launched World’s First 8K Broadcasting in Japan\", NAB Pilot guest blog post ()\n[43]\tThierry Fautier, Eric Mazieres, France Television Lab blog post \"8K EXPERIMENT AT ROLAND GARROS 2019\" (https://www.francetelevisions.fr/lab/projets/8K-Experiment-at-Roland-Garros-2019)\n[44]\tM. Aracena, T. Fautier, O. Oyman, \"Live VR end-to-end workflows: real-life deployments and advances in VR and network technology\", SMPTE 2020 Annual Technical Conference & Exhibition, November 2020, .\n[45]\t3GPP TS 26.511, \"5G Media Streaming (5GMS); Profiles, codecs and formats\".\n[46]\t3GPP TR 26.955, \" Video codec characteristics for 5G-based services and applications\".\n[47]\t3GPP TR 26.926, \"Traffic Models and Quality Evaluation Methods for Media and XR Services in 5G Systems\".\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "3\tDefinitions of terms, symbols and abbreviations",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "3.1\tTerms",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the terms and definitions given in 3GPP TR 21.905 [1] and the following apply. A term defined in the present document takes precedence over the definition of the same term, if any, in 3GPP TR 21.905 [1].\n360 VR: Virtual Reality 360° video and 3D audio content.\n720p: a video with an image resolution of 1 280 × 720 pixels.\n4K UHD: a video with an image resolution of 3 840 x 2 160 pixels a.k.a. 2160p.\n8K UHD: a video with an image resolution of 7680 x 4 320 pixels a.k.a. 4320p.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.2\tSymbols",
                    "description": "",
                    "summary": "",
                    "text_content": "Void.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.3\tAbbreviations",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the abbreviations given in 3GPP TR 21.905 [1] and the following apply. An abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in 3GPP TR 21.905 [1].\nA/V\tAudio/Visual\nFEC\tForward Error Correction\nFFS\tFor Further Study\nHD\tHigh Definition\nPSS\tPacket Switched Streaming\nSTB\tSet-Top-Box\nTV\tTelevision\nUHD\tUltra High Definition\nVR\tVirtual Reality\n\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "4\tMedia centric Third-Party and Operator services",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "4.1\tA/V Streaming - Enhanced TV",
                    "description": "",
                    "summary": "",
                    "text_content": "Live and on-demand audio-visual streaming and enhanced TV services are considered according to 3GPP PSS [2], 3GP-DASH [3], MBMS [4], 5G Media Streaming for downlink [45]  and the media profiles in TS 26.116 [5].\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.2\tVR 360° Streaming",
                    "description": "",
                    "summary": "",
                    "text_content": "Live and on-demand VR 360° streaming are considered according to 3GPP PSS [2], 3GP-DASH [3], MBMS [4], 5G Media Streaming [45] and the media profiles in TS 26.118 [6].\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.3\tConversational Multimedia Telephony and Telepresence",
                    "description": "",
                    "summary": "",
                    "text_content": "Conversational Multimedia Telephony and Telepresence are considered according to MTSI [7] and IMS Telepresence [8].\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.4\tLive uplink A/V streaming",
                    "description": "",
                    "summary": "",
                    "text_content": "Professional and consumer live uplink streaming contribution of A/V content are considered according to FLUS [11] and 5G Media Streaming for uplink [45].\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.5\tCloud gaming",
                    "description": "",
                    "summary": "",
                    "text_content": "Cloud gaming (a.k.a. game streaming) implies that, while the game is being played by a user on a UE or on a device attached to a UE, game processing and rendering is totally or partly performed in a network entity, potentially at the edge of the network. The traffic typically consists of uplink and downlink game status/control information traffic between a client and a server and of downlink streaming of rendered and encoded 2D or VR360 video.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "4.6\tXR Media Services",
                    "description": "",
                    "summary": "",
                    "text_content": "For details on XR Services including AR and VR, refer to 3GPP TR 26.928 [30] and 3GPP TR 26.926 [47]. Identified scenarios are:\n-\tViewport-independent 6DOF streaming, see TR 26.928 [30], clause 6.2.2.\n-\tViewport-dependent 6DOF streaming, see TR 26.928 [30], clause 6.2.3.\n-\tRaster-based split rendering, see TR 26.928 [30], clause 6.2.5.\n-\tGeneralized split rendering, see TR 26.928 [30], clause 6.2.6.\n-\tXR conversational, see TR 26.928 [30], clause 6.2.7.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "5\tTypical current deployment characteristics",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "5.1\tTypical streaming/broadcast video and audio bitrates",
                    "description": "",
                    "summary": "",
                    "text_content": "These figures are valid for both HDR/non-HDR video:\n-\t720p HD: 2 - 5 Mbps\nNOTE 1: \tToday typically 3 Mbps for HEVC [15] and 5Mbps for AVC [14], but bitrate reductions expected with better encoding and coding tools. See Clause 6.1.2.\n-\tFull HD: 3 - 12 Mbps\nNOTE 2: \tToday typically 5-7 Mbps for HEVC [15] and 10-12 Mbps for AVC [14], but bitrate reductions expected with better encoding and coding tools. See Clause 6.1.2 as well as TR 26.955 [46].\n-\t4k UHD: 5- 25Mbps\nNOTE 3: Today typically 8-16 Mbps for HEVC [15] and 15-25 Mbps for AVC [14], but bitrate reductions expected with better encoding and coding tools. See Clause 6.1.2 as well as TR 26.955 [46].\n-\t8k UHD: 25 - 80 Mbps\nNOTE 4: \tInitially up to 80 Mbps for HEVC [15], but bitrate reductions expected with better encoding and coding tools. More advances with new codecs are expected, See Clause 6.1.2.\n\nThese figures apply for audio:\n-\tNormal quality audio: mono/stereo: 24-48 kbps\n-\tHigh quality audio: mono/stereo/immersive 24-512 kbps\n-\tExtreme quality audio: mono/stereo/immersive 512 kbps\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "5.2\tTypical streaming/broadcast 360 VR bitrates",
                    "description": "",
                    "summary": "",
                    "text_content": "The following bitrates apply for streaming/broadcast 360 VR.\n-\tBasic 360 VR: 2.5 - 25 Mbps\nNOTE 1: For viewport agnostic, the 4k UHD numbers as defined in clause 5.1 apply. For viewport dependent, the bitrates likely can be reduced to half.- \tAccording to [44], with HEVC at 1080p@30fps between 2.5 - 5 Mbps on average with caps at 5 – 8 Mbps.\n-\tHD 360 VR: 10 - 80 Mbps\nNOTE 2: For viewport agnostic, the 8k UHD numbers as defined in clause 5.1 apply. For viewport dependent, the bitrates likely can be reduced to half.\n- \tAccording to [44], with HEVC at 4K@30fps between 10 - 18 Mbps on average with caps between 15 – 25 Mbps.\n-\tRetinal VR: 15 - 150 MbpsNOTE 3: For viewport agnostic, the 8k UHD numbers multiplied by 4 as defined in clause 5.1 apply. For viewport dependent, the bitrates likely can be reduced to half or even on third.\n- \tAccording to [44], with HEVC at 8K@30fps between 30 - 35 Mbps on average with caps at 42 Mbps.\nNote 4 on framerates: cinema content is usually captured and distributed at 24 fps. TV content is usually captured and distributed at 50 or 60 fps depending on the region. The above video bitrates include frame rates of up to 60fps.\nNote 5 on bitrate ranges: the bitrates are dependent on codec (e.g. AVC/H.264 [14] or HEVC/H.265 [15]), on content type and whether it is live or on-demand. Bitrates are expected to be reduced with expected encoder implementation enhancements and new codecs. In the past, that reduction has been observed to be in the order of 50 % every 10 years.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "5.3\tTypical conversational speech/audio and video bitrates",
                    "description": "",
                    "summary": "",
                    "text_content": "The following bitrates are typical in commercial services according to clause 4.3:\n-\tNarrowband voice (mono): 7.2-13.2 kbps\n-\tWideband voice (mono): 7.2-24.4 kbps\n-\tSuper-wideband voice (mono): 9.6-24.4 kbps\n-\tFullband voice (mono): 16.4-[TBD] kbps\nNOTE: \tUnlike NB, WB and SWB which all are audio bandwidths used in commercial networks, no FB usage has yet been seen for commercial conversational services.\n-\tVGA video: 300 - 900 kbps\n-\t720p HD video: 800 - 1500 kbps\n-\tTelepresence video 1080p: 1500 - 3000 kbps\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "5.4\tTypical uplink A/V streaming of A/V, including 360 VR content",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "5.4.1\tProfessional production content bitrates",
                            "text_content": "Table 5.4.1-1 lists common content formats from professional cameras that are output in uncompressed form over the SDI interface, and their corresponding bit rates:\nNOTE 1: The formal standard references are listed in table 5.4.1-1.\nNOTE 2: See also 3GPP TR 22.827 [35].\nNOTE 3: \tThe SDI rates given below are for wired connections within professional production environments.\nTable 5.4.1-1: SDI interface, and their corresponding bit rates\n\nSMPTE ST 2110 [12] can support transport of compressed or uncompressed streams from cameras with the resolutions above. According to [13], in case of uncompressed streams, the video rates for YCbCr 4:2:2 10b format can be derived as follows.\nTable 5.4.1-2: ST 2110 Video uncompressed streams bitrates\n\nAudio uncompressed channels come in addition, but their bitrates are some magnitude smaller than uncompressed video.\nLight compression can also be used for such production environments using for example VC-2 (ST 2042-1:2017 - SMPTE Standard - VC-2 Video Compression) [36] which is typically used with a coding ratio of 4:1 and MJ2 - Motion JPEG 2000 (ISO/IEC IS 15444-3 | ITU-T T.802)[37] which is typically used with a coding ratio of 6:1.\nCompression is used for portable cameras equipped with wireless (Wireless LAN and 4G LTE) modules. The bitrates are 9, 6, 3, 2Mbps for up to 720p@60fps and 9, 6, 3 Mbps for 1080@30fps with AVC/H.264 [14].\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 5.4.1-1: SDI interface, and their corresponding bit rates",
                                    "table number": 3,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "Table 5.4.1-2: ST 2110 Video uncompressed streams bitrates",
                                    "table number": 4,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "5.4.2\tLive uplink contribution professional content bitrates",
                            "text_content": "Contribution feeds as seen in real deployed systems:\n-\t80 Mbps 422 10 bits for HD @60fps\n-\t165 Mbps 422 10 bits for UHD (4K, AVC/H.264 [14]) @60fps\n-\t120 Mbps 422 10 bits for UHD (4K, HEVC/H.265 [15]) @60fps\n-\tNormal quality 360 VR: [TBD 96 Mbps]\n-\tHigh quality 360 VR: [TBD 140 Mbps]\nNOTE 1: \tThe above video profiles include frame rates of up to 60fps.\n-\tBroadcast auxiliary services (e.g. electronic news gathering) according to Recommendation ITU-R BT.1872-3 [9]:21-35 Mbps for HD TV (AVC/H.264 [14])\n-\t18-30 Mbps for HD TV (HEVC/H.265 [15])\n-\t96-145 Mbps for UHD TV (4K, HEVC/H.265 [15])\n-\t140-285 Mbps for UHD TV (8K, HEVC/H.265 [15])\n-\tCompressed audio: 96-180 kbps per channel\n-\tUncompressed audio: 768-1152 kbps per channel\nNOTE 2: \tThe lower video rates from BT.1872-3 concern a single codec while the higher rates have headroom for 3 coding steps in tandem.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "5.4.3\tUser generated content",
                            "text_content": "The following bitrates are typical for user generated content:\n-\t720p HD: 3 - 10 Mbps\nNOTE 1: Today typically 5-10 Mbps for AVC [14] depending on frame rate and dynamic range, but bitrate reductions expected with better encoding and coding tools. See Clause 6.1.2.\n-\tFull HD: 5 - 15 Mbps\nNOTE 2: Today typically 8-15 Mbps for AVC [14], but bitrate reductions expected with better encoding and coding tools. See Clause 6.1.2.\n-\t4k UHD: 10 - 85 Mbps\nNOTE 3: Today typically 8-16 Mbps for HEVC [15] and 15-25 Mbps for AVC [14], but bitrate reductions expected with better encoding and coding tools. See Clause 6.1.2.\n-\t8k UHD: 20 - 150 Mbps\nNOTE 4: No information is available today, but the numbers are extrapolated from 4K by multiplying with a factor 4.\n-\tBasic 360 VR: the numbers for 4k UHD apply\n-\tHD 360 VR: the numbers for 8k UHD apply\n-\tRetinal VR: this is roughly 16k as expected, so a factor 4 to 8k UHD is applicable, i.e. 40-300 Mbps\n-\tNormal quality audio: 24.4kbps/channel\nNOTE 5: Super-wideband 3GPP audio codecs may be used.\n-\tHigh quality audio: 128kbps/channel\nNOTE 6 on user generated content: the video bitrate ranges are wide as user generated content may have different quality expectations including semi-professional content.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "5.5\tTypical Traffic Characteristics for Cloud gaming",
                    "description": "",
                    "summary": "",
                    "text_content": "For cloud gaming, the downlink streaming of 720p/1080p/4k @60fps encoded A/V typically consists of a 5-35Mbps bitstream. One instance of a cloud gaming service requires a minimum uplink bitrate of 1.5 Mbps [21].\nIn the future the cloud gaming is presumed to reach up to 8k resolutions and up to 120fps downlink bitstreams. No information on such currently deployed services are available to formulate typical bitrates. However, Clause 6 provides indication that allows estimation of bitrates. Annex A provides background information on deployed cloud gaming services.\nDifferent game types result in different round-trip user interaction delay requirements (sometimes referred also as acceptable game latency). As discussed in TR 26.928 [30], clause 4.2, with regards to such requirements, games may be divided into the following 4 types: games requiring (i) at most 50 ms, (ii) at most 100 ms, (iii) at most 200ms, and (iv) games with no latency requirements. The game latency impacts the traffic model as well as the requirements on the delivery system. The shorter the latency requirements, the higher the expected bitrate.\nCloud gaming traffic characteristics are also discussed in 3GPP TR 26.928 [30], clause 6 as well was TR 26.926 [47].\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "5.6\tXR Traffic Characteristics",
                    "description": "",
                    "summary": "",
                    "text_content": "Initial typical bitrates and traffic characteristics for XR services are collected in 3GPP TR 26.928 [30], clause 6 as well was TR 26.926 [47]. A summary of expected bitrates and traffic characteristics are provided:\n-\tViewport-independent 6DOF streaming\n-\tDownlink only\n-\tHTTP Streaming\n-\tUp to 100 Mbps to address high-quality 6DOF VR services to allow 2k per eye streaming at 90 fps (see TR 26.928, clause 4.2 and 6.2.2)\n-\tViewport-dependent 6DOF streaming\n-\tDownlink only\n-\tHTTP streaming, HTTP/TCP level information and responses are exchanged every 100-200 ms in viewport-dependent streaming.\n-\tUp to 25 - 50 Mbps to address high-quality 6DOF VR services to allow 2k per eye streaming at 90 fps (see TR 26.928, clause 4.2 and 6.2.3)\n-\tRaster-based split rendering\n-\tPrimarily downlink\n-\tfor H.264/AVC the bitrates are in the order of up to 50 Mbps per eye buffer, i.e. up to 100 Mbit/s.\n-\tfor H.265/HEVC the bitrates are in the order of 20 - 30 Mbps per eye buffer, i.e. 40 – 60 Mbit/s\n- \tpacket latency requirements are in the range of 15ms\n-\tApplication Layer FEC may be used with overhead from 10-50%, typically something like 30%\n-\tuplink pose information  (see TR 26.926 [47], clause 5.8)\n- \ttypically every 10 -15ms constant packet size of up to 100 bytes\n-\tpacket latency requirements are 10 – 15 ms\n-\tGeneralized split rendering\n-\tdetails are FFS\n-\tXR conversational\n-\tdetails are FFS\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "6\tOverview on technological developments for existing and emerging services",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "6.1\tTechnology Developments",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "6.1.1\tOverview",
                            "text_content": "This clause collects developments in the industry on compression advances, content formats, protocol improvements and other advances that may impact the traffic characteristics documented in clause 4.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.1.2\tCompression Improvements",
                            "text_content": "Due to the increasing consumption of video content with higher resolutions, the need for more efficient video compression techniques is growing. The first version of the High Efficiency Video Coding (HEVC) standard [15], jointly developed by the ITU-T VCEG and the ISO MPEG, was finalized in 2013. A wide range of products and services support HEVC [15] for video encoding/decoding, especially for Ultra High Definition (UHD) content, where HEVC [15] can provide around 50% bitrate savings for the same subjective quality as its predecessor H.264/AVC [14].\nBoth codecs are defined as part of the TV Video Profiles in TS 26.116 [5] and are also the foundation of the VR Video Profiles in TS 26.118 [6].\nWork on video compression technologies beyond the capabilities of HEVC [15] are continued by the MPEG/ITU, with the creation of the Joint Video Exploration Team (JVET) on future video coding in October 2015. Many new coding tools have been proposed in the context of JVET, which eventually led to a Call for Proposals on video coding technologies with video compression capabilities beyond HEVC [15]. The reference software used in the exploration phase of JVET, called Joint Exploration Model (JEM), was leveraged as the base for the majority of responses to the call. Results included responses demonstrating compression efficiency gains of around 40 % or more with respect to HEVC [15]. This initiated the work by the Joint Video Experts Team (JVET) on the development of a new video coding standard, to be known as Versatile Video Coding (VVC).\nMPEG has started working on a new video coding standard to be known as MPEG-5 Essential Video Coding (EVC) in January 2019. MPEG-5 EVC aims to provide a standardized video coding solution to address business needs in some use cases, such as video streaming, where existing ISO video coding standards have not been as widely adopted as might be expected from their purely technical characteristics. In addition, a main profile adds a small number of additional tools, each of which is individually capable of being either properly deactivated or switched to the corresponding basic tool. The target coding efficiency for the call for proposals was to be at least as efficient as HEVC. This target was exceeded by approximately 24 % in the responses to the call for proposals, which were evaluated at this meeting. The development of the MPEG-5 EVC standard is expected to be completed in 2020.\nFigure 6.1.2-1 shows the typical improvements of video compression rates over time as well as the target for the VVC standard. It is also observed that compression technologies have enabled the reduction of bitrates by 50 % in a time frame of 7-10 years. Most of the gains come by the increase of encoding and decoding complexity, spurred according to Moore's law.\nThe figure depicts a graph illustrating the video bitrate efficiency improvements and target for the final VVC standard. The graph shows the relationship between video bitrate and the target bit rate, with the target bit rate set at 1000 kbps. The graph also includes a target bit rate of 1000 kbps, which is the target bit rate for the final VVC standard. The graph is reproduced with appropriate permission from Fraunhofer.\nFigure 6.1.2-1: Video bitrate efficiency improvements and target for the final VVC standard [reproduced with appropriate permission from Fraunhofer]\nTable 6.1.2-1 provides a summary of the expected compression efficiency of different codecs and expectations on target bitrates for different video technologies.\nTable 6.1.2-1: Expected Video coding standards performance and bitrate target\n\nAlso noteworthy is the improvement of encoders over time even for existing standards which also leads to bitrate reductions at the same quality.\nBased on this information it can be expected that within the time frame until 2025, video compression technology permit bitrate reductions by a factor of 50 % compared to what is today possible with HEVC [15].\nHowever, not to forget according to Jevons Paradox, stating that the efficiency with which a resource is used tends to increase (rather than decrease) the rate of consumption of that resource. This may well mean that the compression efficiency gains spur even more traffic.\nAdditional updates to video chararacteristics and bitrates are available in 3GPP TR 26.955 [46].\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.1.2-1: Expected Video coding standards performance and bitrate target",
                                    "table number": 5,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "6.1.3\tNew Media Formats",
                            "text_content": "New media formats in media generation and distribution are developed on a continuous basis, taking into account improvements in capturing and display systems. TR 26.949 [16] collects TV distribution formats as they were considered in the time frame of 2015 to 2017. The parameters are summarized in clause 5.4 of the present document in terms of:\n-\tSpatial Resolution\n-\tFrame Rates\n-\tColorimetry\n-\tOther parameters\nThe latest versions, referred to as UHD-1 Phase 2 is summarized according to TR 26.949, clause 5.4.8:\n-\tUHD-1 phase 2 exclusively operates with HEVC [15] and bit depth of 10 bit.\n-\tonly square pixel resolutions and only progressive scan are supported.\n-\tOnly BT.2100 [33] non-constant luminance YCbCr is supported.\n-\tHigh Dynamic Range is added either through the PQ10 or the HLG10 system, relying on HEVC Main-10 Level 5.1 [15].\n-\tOptional SEI metadata may be provided in the bitstream using HEVC specified SEI messages [15].\n-\tAddition of High Frame Rate (HFR), i.e. addition of 100, 120 000/1 001 and 120 Hz.\n-\tHFR can be supported by two means:\n-\tSingle PID HFR bitstream.\n-\tDual PID and temporal scalability when the bitstream is intended to be decodable by UHD phase 1 receivers at half frame rate.\n-\tFor HFR on HEVC Main-10 Level 5.2 [15] is required.\n-\tSupport for Next Generation Audio (NGA) to enable immersive and personalized audio content.\nThe TV Video Profiles in TS 26.116 [5] address coded representations of the UHD-1 phase 2 signals to the most extent. Table 6.1.3-1 provides an overview of the TV relevant formats considered in the context of 3GPP TV Video Profiles.\nTable 6.1.3-1: TV over 3GPP services Video Profile Operation Points (TS 26.116 [5])\n\nLooking further into the future, there is currently only one 8K broadcast service being supported by the Japanese public service broadcaster NHK. This runs 12 hours a day and is to be utilised as part of the promotion for the upcoming Olympics in Japan. This service is supported by a government initiative. A summary of the service launched by NHK was published in [42] and a more detailed description can be found in the IBC paper \"Ready for 8K UHDTV broadcasting in Japan\" [41].\nFrance Television performed a trial of 8K delivery of the Roland Garros Tennis Championship by 5G in June 2019, details can be found in [43].\n\n\nFor VR production and distribution, the ITU recommends [TBA] that for eye not to perceive pixels for 360VR, 30K by 15K images should be provided, though other criteria may be used in system design.\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 6.1.3-1: TV over 3GPP services Video Profile Operation Points (TS 26.116 [5])",
                                    "table number": 6,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "6.1.4\tProtocol Improvements",
                            "text_content": "Most of the video traffic used HTTP and TCP for the delivery of both wired and wireless Internet. The transmission control protocol, mostly known as TCP or TCP/IP [17], has been invented over 40 years ago. Over the years, it has been evolving steadily and it became the number transport protocol on the Internet and finally also on mobile networks. Today, around 75 % of the traffic is encrypted in mobile data networks. TCP, in combination with TLS, requires three round trips before the actual data can be sent. However, innovation in the context became very hard.\nIn response to these challenges with slow innovation of TCP, Internet companies started experimenting with proprietary protocols build on top of UDP. UDP is a very basic protocol. It only provides the bare minimum functionality and is suitable for new protocols to be built on top of it. It is well supported by all infrastructure on the Internet. Protocols on top of UDP can be implemented in applications so it allows for rapid deployment of new versions. The experiments with UDP protocols evolved into an effort to bring in the engineering community and openly collaborate on a single protocol framework on top of UDP. An Internet Engineering Task Force working group has been established to specify a protocol called QUIC (Quick UDP Internet Connections). Shortly it is likely to see the first QUIC Internet standard published [18].\nThe QUIC Protocol is the next generation of transport for the Internet and is on track to become ubiquitous on end-user platforms and within all server-side workflows. Unlike prior standard technologies, which are struggling to innovate due to compatibility issues with existing infrastructure, QUIC operates on top of UDP in the application layer and that brings flexibility to deploy new features in rapid iterations.\nGoogle introduced QUIC back in 2013 as an experimental protocol to reduce TCP connection and transport latencies. QUIC, on the other hand, minimizes the number of set-up round trips by combining UDP transport and its own cryptographic handshake. For connections to the same origin server, QUIC facilitates a zero round trip time.\nQUIC also re-implements TCP loss recovery, over UDP and, by using multiplexed connections, it eliminates TCP's head-of-line blocking. This ensures that lost packets do not block any other stream but only those with data in it. Last, but not least, QUIC moves congestion control to the application and the user space, enabling a rapid evolution for the protocol, as opposed to kernel space TCP.\nQUIC has been nicely designed to allow being enabled seamlessly for existing video workflows without any changes needed in the video formats or players. Akamai reports as an example that for a measurement is from a soccer event in 2018 a 25 % throughput improvement for median viewer was achieved compared to TCP for desktop. QUIC performances gains for mobile compared to TCP/TLS, while still positive, have found not to be that significant as for desktop, in particular for video content.\nThere are several features of the protocol that contribute to improved video quality, among others:\n-\tWith usual TCP&TLS session which forms HTTPS streaming, there has to be 3 or 4 exchanges of information back and forth between the client and the server, before the video request is made. With QUIC there's none needed, assuming the client communicated with the server previously. The first packet from the client already contains the video request, and the first packet from the server already contains the video content. Or in case the client has not communicated with the server before, it only takes one roundtrip between the client and server. This reduces latency greatly. TCP has similar features available with new version of TLS, TLS-1.3, and TCP Fast Open feature but its deployment is complicated. The 0-RTT (zero round trip time) connection establishment for returning connections applies to about 50 % of connections.\n-\tQUIC also uses more efficient loss recovery - due to more information being provided about the lost packets and timing.\n-\tQUIC allows Multiplexing without head-of-line blocking, i.e. if one packet gets lost then all requests affected.\n-\tQUIC also allows connection migration across different access networks.\n-\tIt also allows to measure the performance of each feature and use and apply it selectively for specific deployments.\nOn mobile QUIC benefits seem to also be tangible. On mobile Android devices, Google claims that QUIC has helped to reduce latency of Google Search responses by 3.6 % and YouTube video buffering by 15.3 %. It was found that by November 2017, QUIC represented 20 % of the total mobile traffic and expected to grow to 35 % by end of 2018. According to MVI data, video accounts for approximately 58 % of the total mobile internet traffic and video represents 64 % of the total QUIC traffic. By November 2018, approximately 90 % of internet traffic will be encrypted and QUIC will be 32 % of global Internet traffic.\nQUIC being an encryption-based protocol such as HTTP/TLS, traffic is typically encrypted over the mobile network delivery, meaning that the use of traditional traffic management tools is limited. On top of what is also observed for HTTP/TLS, QUIC allows multiplexing multiple streams over a single connection, but this comes with the added downside that it is impossible to differentiate between the different streams as the signalling header with the stream identifier is also encrypted.\nAnother technology and protocol in media distribution is developed under the umbrella of ABR Multicast. With the recent proliferation of live streaming, especially of premium sports, over the Internet, issues in scale and quality have been exposed. The challenge partially lies in scaling the services to millions of simultaneous users, but also in the associated peering and delivery costs as well as the end-to-end latency. Multicast ABR addresses a problem that had already been solved in the context of managed networks for IPTV with IGMP. However, it is quite difficult to replicate IGMP exactly over an unmanaged network. However, in practice it is possible to control delivery of streams quite precisely over a large part of their journey and multicast ABR effectively provides a tunnel through which multiple unicast streams are combined into single ones, just as with traditional multicast. Multiple unicast streams are converted at the entrance of the tunnel into a single stream and then back to multiple unicast as they exit in a process known as transcasting. These processes have since been documented either as a standard or guidelines towards a standard, by both the DVB [19] and by Cable Labs [20]. Products are being built around these guidelines, with many operators currently in proof of concept or field trials of the technology.]\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "6.1.5\tImpact on Media Services",
                            "text_content": "The impact of these new developments on Media Service is FFS.\n\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "7\tCharacteristics and requirements for different media services on 3GPP networks",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "7.1\tIntroduction",
                    "description": "",
                    "summary": "",
                    "text_content": "This clause collects characteristics and requirements for different media services on 3GPP networks.\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.2\tCollection process for requirements for different media services on 3GPP networks",
                    "description": "",
                    "summary": "",
                    "text_content": "Use cases are collected in the context of the present document. The use cases come with an analysis of certain characteristics. For this purpose a template is provided below.\n\nTable 7.2-1: Standardized 5QI to QoS characteristics mapping (from TS 23.501)\n\nNOTE 1:\tFor Standardized 5QI to QoS characteristics mapping, the table will be extended/updated to support service requirements for 5G, e.g. ultralow latency service.\nNOTE 2:\tIt is preferred that a value less than 64 is allocated for any new standardised 5QI of non-GBR Resource Type. This is to allow for option 1 to be used as described in clause 5.7.1.3 of TS 23.501 (as the QFI is limited to less than 64).\n",
                    "tables": [
                        {
                            "description": "Use cases are collected in the context of the present document. The use cases come with an analysis of certain characteristics. For this purpose a template is provided below.",
                            "table number": 7,
                            "summary": "",
                            "name": ""
                        },
                        {
                            "description": "Table 7.2-1: Standardized 5QI to QoS characteristics mapping (from TS 23.501)",
                            "table number": 8,
                            "summary": "",
                            "name": ""
                        }
                    ],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "7.3\tSummary of Responses for Streaming Services",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "7.3.1\tIntroduction",
                            "text_content": "This clause provides a summary of the information that was collected for streaming services. The current information is based on responses from:\n-\tComcast VIPER\n-\tAWS Media Services\n-\tHulu\n-\tBitmovin\n-\tARD Mediathek\nThe services address live and On-demand streaming services.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.3.2\tUsed Technologies",
                            "text_content": "The pre-dominant video codec for the services is still H.264/AVC [14]. H.265/HEVC [15] is also in deployments. Non-MPEG video codecs such as VP9 or AV1 are in use or expected to be in use, but much less prominent than MPEG codecs according to the responses.\nThe pre-dominant audio codec is AAC, with some variants. Also, AC-3 is used quite often to support beyond stereo experiences. No other audio codec was mentioned.\nMost of the services run HD video. Experiments with UHD and HDR are ongoing. Different HDR formats (i.e. HDR10, Dolby Vision, HLG, and SL-HDR) are in use.\nFor audio, stereo is deployed and 5.1 is also broadly used. Some initial experiments are done with multichannel audio, NGA (Next Generation Audio).\nAll services deploy DASH and HLS. On HLS, the MPEG-2 TS still is maintained. For DASH, exclusively ISO BMFF based distribution is used but a convergence towards a common segment format based on CMAF for DASH and HLS is expected.\nOther formats such as RTMP or RTP are maintained to some extent for legacy reasons.\nHTTP/1.1 with TCP/IP is used almost exclusively. Some initial tests are ongoing on HTTP/2.0 and HTTP/3 (also known as HTTP over QUIC). No received response indicates the use of multicast protocols.\nAll services provide client application running on iOS and Android for Browser-based playback as well as applications or native integration into set-top boxes, HDMI sticks (i.e. Apple TV™, Fire TV™, Chromecast™, Tizen™) and TV Sets. Game consoles are also targeted.\nOther mentioned deployed technologies are:\n-\tDRM (Widevine, PlayReady, Fairplay) based on common encryption\n-\tAnalytics collection and QoE Measurement\n-\tDynamic Ad Insertion\n-\tWatermarking\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "7.3.3\tTraffic Characteristics",
                            "text_content": "Average streaming bitrate\n-\tSTBs: 4-6Mbps\n-\tMobile applications and browsers: less than 4-6Mbps\n-\t4K content: 10-15Mbps\nAverage segment durations\n-\t6 seconds\nOther KPIs such as video start time, video playback failure, rebuffering ratio, connection induced rebuffering, minimum/average bitrates, ABR adaptation frequency were not reported.\nCommonly targeted 5QI values are 6 and 8. Others are considered interesting for specific services.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                }
            ]
        },
        {
            "title": "8\tApplicability of existing 5Qis",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "8.1\tQoS Model",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "8.1.1\tOverview",
                            "text_content": "Clause 5.7 of TS 23.501 [10] explains the QoS Model for 5G. The 5G QoS model is based on QoS Flows. The 5G QoS model supports both QoS Flows that require guaranteed flow bit rate (GBR QoS Flows) and QoS Flows that do not require guaranteed flow bit rate (Non-GBR QoS Flows). The 5G QoS model also supports Reflective QoS (see clause 5.7.5 of TS 23.501 [10]).\nA QoS Flow ID (QFI) is used to identify a QoS Flow in the 5G System. User Plane traffic assigned to the same QoS Flow within a PDU Session receives the same traffic forwarding treatment (e.g. scheduling, admission threshold).\nThe QFI may be dynamically assigned or may be equal to the 5QI, for more details on existing 5QI see clause 8.1.2.\nA QoS Flow may either be 'GBR', 'Non-GBR' or \"Delay Tolerant GBR\" depending on its QoS profile and it contains QoS parameters as follows:\n-\tFor each QoS Flow, the QoS profile includes the QoS parameters:\n-\t5G QoS Identifier (5QI); and\n-\tAllocation and Retention Priority (ARP).\n-\tFor each Non-GBR QoS Flow only, the QoS profile can also include the QoS parameter:\n-\tReflective QoS Attribute (RQA).\n-\tFor each GBR QoS Flow only, the QoS profile also include the QoS parameters:\n-\tGuaranteed Flow Bit Rate (GFBR) - UL and DL; and\n-\tMaximum Flow Bit Rate (MFBR) - UL and DL; and\n-\tIn the case of a GBR QoS Flow only, the QoS profile can also include one or more of the QoS parameters:\n-\tNotification control;\n-\tMaximum Packet Loss Rate - UL and DL\nThe usage of a dynamically assigned 5QI for a QoS Flow requires in addition the signalling of the complete 5G QoS characteristics (described in clause 5.7.3 of TS 23.501 [10]) as part of the QoS profile.\nThe principle for classification and marking of User Plane traffic and mapping of QoS Flows to Access Network (AN) resources is illustrated in Figure 8-1-1.\nThe figure depicts a schematic representation of the classification and user plane marking process for QoS Flows and mapping them to AN Resources. It illustrates the process of categorizing and assigning specific resources to QoS Flows, which are essential for ensuring high-quality service delivery. The figure includes key components such as the classification matrix, user plane marking, and mapping to AN Resources, all of which are crucial for managing and optimizing network resources.\nFigure 8.1-1: The principle for classification and User Plane marking for QoS Flows and mapping to AN Resources\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "8.1.2\t5G QoS Parameters",
                            "text_content": "A 5QI is a scalar that is used as a reference to 5G QoS characteristics defined in clause 5.7.4 of TS 23.501 [10], i.e. access node-specific parameters that control QoS forwarding treatment for the QoS Flow (e.g. scheduling weights, admission thresholds, queue management thresholds, link layer protocol configuration, etc.).\nStandardized 5QI values have one-to-one mapping to a standardized combination of 5G QoS characteristics as specified in Table 5.7.4-1 of TS 23.501 [10] and shown below in Table 8.1.4-1.\nA summary of the most relevant QoS Parameters is provided as follows. For details, please refer to TS 23.501 [10], clause 5.7.2:\n-\tGuaranteed Flow Bit Rate (GFBR) - UL and DL, for GBR QoS Flows only: denotes the bit rate that is guaranteed to be provided by the network to the QoS Flow over the Averaging Time Window.\n-\tMaximum Flow Bit Rate (MFBR) -- UL and DL, for GBR QoS Flows only limits the bit rate to the highest bit rate that is expected by the QoS Flow (e.g. excess traffic may get discarded or delayed by a rate shaping or policing function).\n-\tThe Maximum Packet Loss Rate (UL, DL) indicates the maximum rate for lost packets of the QoS flow that can be tolerated in the uplink and downlink direction, if the flow is compliant to the GFBR and is only used for voice media traffic in rel.16.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "8.1.3\t5G QoS Characteristics",
                            "text_content": "In addition to 5G QoS parameters, also 5G QoS characteristics are defined in TS 23.501 [10]. The characteristics describe the packet forwarding treatment that a QoS Flow receives edge-to-edge between the UE and the UPF in terms of the following performance characteristics:\n1)\tResource Type (GBR, Delay critical GBR or Non-GBR): determines if dedicated network resources related to a QoS Flow-level Guaranteed Flow Bit Rate (GFBR) value are permanently allocated (e.g. by an admission control function in a radio base station).\n2)\tPriority level; indicates a priority in scheduling resources among QoS Flows.\n3)\tPacket Delay Budget; defines an upper bound for the time that a packet may be delayed between the UE and the UPF that terminates the N6 interface. In uncongested scenarios, 98 percent of the packets are expected to be within the packet delay budget.\n4)\tPacket Error Rate: The Packet Error Rate (PER) defines an upper bound for the rate of PDUs (e.g. IP packets) that have been processed by the sender of a link layer protocol (e.g. RLC in RAN of a 3GPP access) but that are not successfully delivered by the corresponding receiver to the upper layer (e.g. PDCP in RAN of a 3GPP access).\n5)\tAveraging window (for GBR and Delay-critical GBR resource type only); The Averaging window represents the duration over which the GFBR and MFBR are calculated.\n6)\tMaximum Data Burst Volume (for Delay-critical GBR resource type only): denotes the largest amount of data that the 5G-AN is required to serve within a period of 5G-AN PDB.\nStandardized or pre-configured 5G QoS characteristics, are indicated through the 5QI value. Signalled 5G QoS characteristics are provided as part of the QoS profile and include all of the characteristics listed above.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "8.1.4\tStandardized 5QI to QoS characteristics mapping",
                            "text_content": "The one-to-one mapping of standardized 5QI values to 5G QoS characteristics is specified in table 5.7.4-1 if TS 23.501 [10] and shown below in Table 8.1.4-1.\nTable 8.1.4-1: Standardized 5QI to QoS characteristics mapping \n(identical to Table 5.7.4.1-1 in TS 23.501 [10])\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table 8.1.4-1: Standardized 5QI to QoS characteristics mapping \n(identical to Table 5.7.4.1-1 in TS 23.501 [10])",
                                    "table number": 9,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        },
                        {
                            "title": "8.1.5\tConsiderations for Media Services",
                            "text_content": "For media traffic considered in the present document, it is appropriate to understand, how they map to QoS parameters and characteristics, including the mapping to the 5QIs as indicated above.\nMaximum achievable throughput for a single TCP connection is determined by different factors. One trivial limitation is the maximum bandwidth of the slowest link in the path. But there are also other, less obvious limits for TCP throughput. Packet loss can create a limitation for the connection as well as can the roundtrip time of acknowledgements.\nThe TCP Receive Window is the amount of data that a receiver can accept without acknowledging the sender. If the sender has not received acknowledgement for the first packet it sent, it will stop and wait and if this wait exceeds a certain limit, it may even retransmit. This is how TCP achieves reliable data transmission. Even if there is no packet loss in the network, this windowing limits throughput. Because TCP transmits data up to the window size before waiting for the acknowledgements, the full bandwidth of the network may not always get used. At any given time, the window advertised by the receive side of TCP corresponds to the amount of free receive memory it has allocated for this connection.\nWhen packet loss occurs in the network, an additional limit is imposed on the connection. In the case of light to moderate packet loss when the TCP rate is limited by the congestion avoidance algorithm.\nThe Mathis equation is a formula that approximates the actual impact of loss on the maximum throughput rate:\nMax Rate in bps < (MSS/RTT)*(1 / √p)\nwhere\nMSS = maximum segment size in bytes\nRTT = round trip time in seconds\np = the probability of packet loss\nNOTE: \tThe formula is known as the Mathis equation given in [40] from a 1997 paper titled  [40].\nWhile this equation obviously has limits in the details, it provides an excellent estimate on the estimated TCP throughput. Figure 8.1.5-1 shows the estimated TCP Throughput over One Way Latency for different packet loss rates and MSS 1500 bytes based on this equation, assuming that the RTT is twice the one way latency.\nThe figure depicts the TCP throughput over one-way latency for different packet loss rates and MSS (Maximum Segment Size) values. The graph shows the relationship between packet loss rate (λ) and TCP throughput (T) for different MSS values (1500 bytes). The figure provides insights into the impact of packet loss on TCP performance, allowing network designers to optimize their network configurations for optimal throughput and reliability.\nFigure 8.1.5-1: Estimated TCP Throughput over One Way Latency for different packet loss rates and MSS 1500 bytes\nBased on this equation, the estimated TCP throughput for some relevant streaming related Non-GBR 5QIs as documented in Table 8.1.4-1 are as follows:\n-\t5QI = 7 with Latency=100ms and Packet loss rate 10e-3 results in estimated TCP Throughput of 237 kbit/s\n-\t5QI = 8 and 9 with Latency=300ms and Packet loss rate 10e-6 results in estimated TCP Throughput of 2.5 Mbit/s\n-\t5QI = 7 with Latency=10ms and Packet loss rate 10e-6 results in estimated TCP Throughput of 75 Mbit/s\nThe equation shows that typically TCP based streaming traffic is more susceptible to loss rates than to delay. It is also important that packet loss rates and latencies are not guaranteed and may therefore result in lower or higher throughput, for example in case of congestion or cell handoff. Hence, media services using these 5QI assignments preferably apply protocols that enable to adapt to these bitrates and changes.\nIf the service would be able to benefit from GBR QoS, then the GFBR and MFBR are relevant. A suitable characterization for a media service is the required minimum bitrate that it needs in order to maintain a good service quality. At the same time, a service is well characterized up to which bitrate it would still provide noticeable quality improvements. Such information may be static or may even change over time, depending on the complexity of the service.\nBased on this discussion, it is proposed that for Media services that use TCP-based distribution systems and permit rate adaptation (such as adaptive streaming services), the following information is worthwhile to be provided:\n-\tThe typical bitrate at which the service preferable operates, possibly providing a range of the bitrate. It should also be mentioned if such information is static over the service or would change, and if such information may be configurable.\n-\tIf there exists a minimum bitrate that the service should not fall below to maintain a sufficient quality. If this exists, a range would be preferable. It should also be mentioned if such information is static over the service or would change, and if such information may be configurable.\n-\tIf there exists a maximum bitrate that the service beyond which the services does not created additional quality. If this exists, a range would be preferable. It should also be mentioned if such information is static over the service or would change, and if such information may be configurable.\n\nInteractive online game applications require connectivity between the game application client and a game application server. The game application client typically runs on a UE or on a device attached to a UE. The type of traffic generated depends on the type of game. The Quality of Experience is influenced by the adequacy of the QoS of the connectivity and the traffic requirements from the game application.\nFor example, Ref. [23] provides delay thresholds per game type in table A-1:\nTable A-1: Delay tolerance in traditional gaming (from [23]).\n\nNOTE: \tThe delay tolerance figures of Table A-1. are applicable to regular 2D games and do not take into account any motion-to-photon requirements of VR gaming applications.\nCloud gaming implies that, while the game is being played by a user on a client, game application processing and rendering is totally or partly performed in a remote network entity, potentially at the edge of the network. The rendered 2D or VR360 video is then encoded and streamed to the client. These extra steps in game workflow influence the overall interactive delay and audio/video quality (e.g. encoding type/profile/level and performances, resolution and frame rate), and hence influence the Quality of Experience.\nThere is work in ITU-T Study Group 12 regarding assessment of gaming quality. Two recommendations are published:\n-\tITU-T G.1032 \"Influence factors on gaming quality of experience\" () [38]\n-\tITU-T P.809 \"Subjective evaluation methods for gaming quality\" (). [39]\nThis last recommendation could be used to produce comparable subjective evaluations of the influence of connectivity characteristics over QoE.\nClause 5.3.1 of [22] discusses cloud gaming requirements. It states that \"Taking out the delay for rendering and encoding/decoding processing, the round-trip time (RTT) delay over 5GS should be less than 5 ms\".\nCloud gaming services already exist, and new ones are being launched, enabling users to play any type of game (whether it is originally offline or online, single player or multiplayer etc.) on a device like a lite smartphone, tablet or TV which would typically be unable to process such a game. The following is a brief look at several of these services and the public information about their requirements.\nCloud gaming typical traffic characteristics are evolving as services are being deployed. But a tendency can be derived from the data available. Bitrates in the range of 5-35Mbps are expected. Although latency requirements vary from game types and users, cloud gaming services are expected to offer a Quality of Experience which is at least as good as the experience of regular (locally processed) gaming.\n\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "Table A-1: Delay tolerance in traditional gaming (from [23]).",
                                    "table number": 10,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "",
                                    "table number": 11,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                }
            ]
        }
    ]
}