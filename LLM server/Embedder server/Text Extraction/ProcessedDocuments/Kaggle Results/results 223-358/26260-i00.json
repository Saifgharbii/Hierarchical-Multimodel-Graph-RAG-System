{
    "document_name": "26260-i00.docx",
    "content": [
        {
            "title": "Foreword",
            "description": "This Technical Specification has been produced by the 3rd Generation Partnership Project (3GPP).\nThe contents of the present document are subject to continuing work within the TSG and may change following formal TSG approval. Should the TSG modify the contents of the present document, it will be re-released by the TSG with an identifying change of release date and an increase in version number as follows:\nVersion x.y.z\nwhere:\nx\tthe first digit:\n1\tpresented to TSG for information;\n2\tpresented to TSG for approval;\n3\tor greater indicates TSG approved document under change control.\ny\tthe second digit is incremented for all changes of substance, i.e. technical enhancements, corrections, updates, etc.\nz\tthe third digit is incremented when editorial only changes have been incorporated in the document.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "Introduction",
            "description": "Audio is a key component of an immersive multimedia experience and 3GPP systems are expected to deliver immersive audio with a high Quality of Experience. However, industry agreed methods to assess the Quality of Experience for immersive audio are relatively few and the present document seeks to address this gap by providing objective test methods for the assessment of immersive audio.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "1\tScope",
            "description": "The present document specifies objective test methodologies for 3GPP immersive audio systems including channel based, object based, scene-based and hybrids of these formats. The subjective evaluation methods described in the present document are applicable to audio capture, coding, transmission and rendering as indicated in their corresponding clauses.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "2\tReferences",
            "description": "The following documents contain provisions which, through reference in this text, constitute provisions of the present document.\n-\tReferences are either specific (identified by date of publication, edition number, version number, etc.) or non-specific.\n-\tFor a specific reference, subsequent revisions do not apply.\n-\tFor a non-specific reference, the latest version applies. In the case of a reference to a 3GPP document (including a GSM document), a non-specific reference implicitly refers to the latest version of that document in the same Release as the present document.\n[1]\t3GPP TR 21.905: \"Vocabulary for 3GPP Specifications\".\n[2]\tJ. Fliege und U. Maier: \"A two-stage approach for computing cubature formulae for the sphere,\" Dortmund University, 1999.\n[3]\tISO 3745 - Annex A: \"Acoustics - Determination of sound power levels and sound energy levels of noise sources using sound pressure -- Precision methods for anechoic rooms and hemi-anechoic rooms - Annex A: General procedures for qualification of anechoic and hemi-anechoic rooms\".\n[4]\tISO 1996 Acoustics: \"Description, measurement and assessment of environmental noise\".\n[5]\tANSI S1.4: \"Specifications for Sound Level Meters\".\n[6]\tISO 3: \"Preferred numbers – Series of preferred numbers\".\n[7]\tB. Rafaely, “Analysis and design of spherical microphone arrays,” IEEE Transactions on Speech and Audio Processing, no. 13, 2005, pp. 135 – 143\n[8]\tM. Poletti, “Unified Description of Ambisonics Using Real and Complex Spherical Harmonics,” Ambisonics Symposium 2009, June 25-27, 2009, Graz, Austria.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "3\tDefinitions, symbols and abbreviations",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "3.1\tDefinitions",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the terms and definitions given in 3GPP TR 21.905 [1] and the following apply. A term defined in the present document takes precedence over the definition of the same term, if any, in 3GPP TR 21.905 [1].\nspherical coordinates: The coordinate system used in this document is defined such that the x-axis points to the front, the y-axis to the left and the z-axis to the top (see Figure 0). Spherical coordinates are the distance  from the origin, the azimuth  in mathematical positive orientation (counter-clockwise) and the elevation angle  relative to the z-axis (with 0 degrees pointing to the equator and +90 degrees pointing to the North pole).\n\nThe figure depicts a spherical coordinate system, which is a mathematical representation of a three-dimensional space. It is used to describe the position and orientation of objects in a three-dimensional space. The figure includes axes, points, and lines that represent the spherical coordinates. The axes are labeled with the spherical coordinates, and the points are labeled with the spherical coordinates of the objects. The lines represent the lines of sight between the points. The figure is useful for understanding the geometry of a three-dimensional space and for calculating distances and angles between objects.\nFigure 0: Spherical coordinate system\ndBFS: dB full-scale, where 0 dBFS refers to the RMS level of a DC-free sinusoidal signal exercising the full scale of the digital interface/file.\n\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.2\tSymbols",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the following symbols apply:\nLAeq\tthe sound level in decibels equivalent to the total A-weighted sound energy measured over a stated period of time.\nazimuth\nelevation\n\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                },
                {
                    "title": "3.3\tAbbreviations",
                    "description": "",
                    "summary": "",
                    "text_content": "For the purposes of the present document, the abbreviations given in 3GPP TR 21.905 [1] and the following apply. An abbreviation defined in the present document takes precedence over the definition of the same abbreviation, if any, in 3GPP TR 21.905 [1].\n",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": []
                }
            ]
        },
        {
            "title": "4\tObjective Test Methodologies for Immersive Audio Systems",
            "description": "",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": [
                {
                    "title": "4.1\tObjective Test Methodologies for Assessment of Immersive Audio Systems in the Sending Direction",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.1.1\tDiffuse-field Send Frequency Response for Scene-based Audio",
                            "text_content": "This test is applicable to UEs capturing scene-based audio (e.g. First and Higher Order Ambisonics).\nNOTE: Currently, the test method uses a periphonic loudspeaker array for generation of a diffuse-field. Additional loudspeaker setups for the derivation of the diffuse sound field are under consideration.\nGeneral test conditions\nFree-field propagation conditions\n-\tThe test environment shall contain a free-field volume, wherein free-field sound propagation conditions shall be observed.\n-\tThe free-field sound propagation conditions shall be observed down to a frequency of 200 Hz or less.\n-\tQualification of the free-field volume shall be performed using the method and limits for deviation from ideal free-field conditions described in [3].\nTest environment noise floor\nWithin the free-field volume, the equivalent continuous sound level of the test environment in each 1/3rd octave band, Leq(f), shall be less than the limits of the NR10 curve, following the noise rating determination procedures in [4].\nThe Diffuse-field Send Frequency Response for Scene-based Audio is defined as the transfer function, The Diffuse-field Send Frequency Response for Scene-based Audio is defined as the transfer function, which is estimated using a scene-based audio capture and reference synthesis at the geometric center of a free-field volume. The estimated sound pressure magnitude spectrum obtained from this process provides a valuable tool for analyzing and optimizing audio systems.The figure depicts the estimated sound pressure magnitude spectrum obtained from a diffuse-field scene-based audio capture and reference synthesis at the geometric center of a free-field volume. This information is crucial for analyzing and optimizing audio systems, as it provides a valuable tool for understanding the audio signal's characteristics and potential improvements., between:\nThe figure depicts a sound pressure magnitude spectrum obtained from a diffuse-field microphone recording the same diffuse field at the origin of a spherical coordinate system. The spectrum is obtained by analyzing the sound pressure levels at different frequencies, which can be used to understand the acoustic properties of the environment.The figure depicts a sound pressure magnitude spectrum obtained from a diffuse-field microphone recording the same diffuse field at the origin of a spherical coordinate system. The spectrum is obtained by analyzing the sound pressure levels at different frequencies, which can be used to understand the acoustic properties of the environment., the estimated sound pressure magnitude spectrum obtained from a diffuse-field scene-based audio capture and reference synthesis at the geometric center of a free-field volume; and\nThe figure depicts a typical block diagram for scene-based audio sending direction with measurement points when using a periphonic loudspeaker array. The diagram illustrates the various components involved in the audio transmission process, including the loudspeaker array, measurement points, and the periphonic loudspeaker. The diagram also shows the connections between the components, such as the audio signal, the measurement points, and the loudspeaker array. This diagram is used to understand the audio transmission process and to identify any potential issues or improvements that can be made.The figure depicts a typical block diagram for scene-based audio sending direction with measurement points when using a periphonic loudspeaker array. The diagram illustrates the various components involved in the audio transmission process, including the loudspeaker array, measurement points, and the periphonic loudspeaker. The connections between the components, such as the audio signal, the measurement points, and the loudspeaker array, are shown in dashed lines. This diagram is used to understand the audio transmission process and to identify any potential issues or improvements that can be made., the sound pressure magnitude spectrum obtained from a diffuse-field microphone recording the same diffuse field at the origin of a spherical coordinate system.\nFigure 1 describes a typical block diagram for the scene-based audio sending direction with measurement points when using a periphonic loudspeaker array.\nThe scene-based audio capture block diagram in Figure 1 illustrates the process of sending direction measurements to a receiver. The diagram shows the various components involved in the audio capture process, including the audio source, audio capture device, and the receiver. The diagram also includes a schematic representation of the audio capture device, which includes a microphone array and a directional microphone. The diagram is used to demonstrate the process of capturing audio in a scene-based audio capture system, which is a method of capturing audio in a real-world environment.\nFigure 1: Scene-based audio capture block diagram for sending direction measurements\nDefinition of Equivalent Spatial Domain\nThe equivalent spatial domain representation, w(t), of a Nth order Ambisonics soundfield representation c(t) is obtained by rendering c(t) to K virtual loudspeaker signals wj (t), 1  j  K, with K = (N+1)2. The respective virtual loudspeaker positions are expressed by means of a spherical coordinate system, where each position lies on the unit sphere, i.e., a radius of 1. Hence, the positions can be equivalently expressed by order-dependent directions j(N)=(j(N),  j(N)), 1  j  K, where j(N) and  j(N) denote the inclinations and azimuths, respectively. These directions are defined according to [2] and reproduced in Annex A for convenience.\nThe rendering of into the equivalent spatial domain can be formulated as a matrix multiplication:\nw(t) = ((N,N))-1 c(t),\nwhere ()-1 denotes the inversion.\nThe matrix (N,N) of order N with respect to the order-dependent directions j(N) is defined by:\n(N,N) := [S1(N)    S2(N)    …    SK(N)],\nwith:\nj(N) := [S00(j(N))    S-1-1(j(N))    S-10(j(N))    S-11(j(N))    S-11(j(N))    …    SNN(j(N))]T ,\nwhere Snm() represents the real valued spherical harmonics of the order n and degree m as defined in [8].\nThe matrix (N,N) is invertible so that the HOA representation c(t) can be converted back from the equivalent spatial domain by:\nc(t) = (N,N)·w(t)\nPeriphonic loudspeaker array\na)\tA periphonic loudspeaker array shall be placed within the free-field volume with the geometric center of the periphonic loudspeaker array coinciding with the geometric center of the free-field volume.\nb)\tThe periphonic loudspeaker array shall have a radius greater or equal than 1 meter.\nc)\tThe periphonic loudspeaker array shall be composed of (N+1)2 coaxial loudspeaker elements. Each of the (N+1)2 coaxial loudspeaker elements shall be equalized (if necessary) and level compensated to conform with the operational room response curve limits given in [5] Section 8.3.4.1. N should be equal or greater than the maximum ambisonics order supported by the device under test (DUT), e.g. N>=4 for a DUT supporting 4th order Ambisonics capture.\nd)\tThe (N+1)2 coaxial loudspeaker elements shall be positioned according to the azimuth and elevation coordinates given in Annex B.\ne)\tAll coaxial loudspeaker elements shall be oriented such that their acoustic axis intersects at the geometric center of the free field volume.\nf)\tThe radius of each coaxial loudspeaker element shall be such that, at the geometric center of the free-field volume, the far field approximation for the coaxial loudspeaker axial pressure amplitude decay holds true.\nReference Spectrum measurement for periphonic loudspeaker array method\na)\tA diffuse-field / random incidence, or multi-field microphone is mounted in the free-field volume such that the tip of the microphone corresponds to the geometric center of the free-field volume and the geometric center of the periphonic loudspeaker array.\nNOTE 1:\tDiffuse-field / random incidence microphones, are described in [5].\nb)\t(N+1)2 decorrelated pink noise signals are played simultaneously over each of the (N+1)2 coaxial loudspeakers of the periphonic loudspeaker array.\nc)\tThe playback level is adjusted such that the LAeq, measured over a 30s time window at the geometric center of the periphonic loudspeaker array, is equal to 78dBSPL(A)  0.5dB.\nd)\tThe reference sound pressure at the geometric center of the free-field volume, p(t), is captured with the diffuse-field or multi-field microphone.\ne)\tThe magnitude spectrum of the reference sound pressure, P(f), is calculated for the 1/12th octave intervals as given by the R40 series of preferred numbers in [6].\nNOTE 2:\tFor ideal (calibrated) loudspeakers, the P(f) spectra should have equal energy in each 1/12th octave intervals.\nEstimated Spectrum measurement\na)\tThe scene-based audio capture device under test is mounted in the free-field volume such that its geometric center coincides with the geometric center of free-field volume and the geometric center of the periphonic loudspeaker array.\nb)\t(N+1)2 decorrelated pink noise signals are played simultaneously over each of the (N+1)2 coaxial loudspeakers of the periphonic loudspeaker array. The pink noise signals shall be identical to the signals used for the reference spectrum measurement.\nc)\tThe B-format scene-based audio format representation (compressed or uncompressed, depending on the use case being tested) is stored for offline analysis.\nd)\tThe B-format scene-based audio format representation is uncompressed (if necessary) and converted to an equivalent spatial domain representation of order NDUT (B-Format to ESD conversion in Figure 1), where NDUT corresponds to the Ambisonics order of the device under test.\ne) The figure depicts a 5G network signal propagation scenario, illustrating the multi-path signal path and highlighting beamforming techniques to mitigate interference. The diagram highlights the role of base stations (gNB), user equipment (UE), and scatterers in the network. The figure also shows the fiber-optic backbone architecture, with core switches, optical line terminals (OLTs), and distributed nodes, aligning with SDN principles.The figure depicts a 5G network signal propagation scenario, illustrating the multi-path signal path and highlighting beamforming techniques to mitigate interference. The diagram highlights the role of base stations (gNB), user equipment (UE), and scatterers in the network. The figure also shows the fiber-optic backbone architecture, with core switches, optical line terminals (OLTs), and distributed nodes, aligning with SDN principles., the estimate of the sound field at the geometric center of the free-field volume and periphonic loudspeaker array, is synthesized using the equivalent spatial domain representation of order NDUT.\nNOTE 3:\t can be taken from the W component of the B-Format signal, as an alternative to implementing the B-Format to ESD conversion in step d).\nf)\tThe magnitude spectrum of the estimated sound pressure, , is calculated for the 1/12th octave intervals as given by the R40 series of preferred numbers in [6].\nCalculation of send frequency response for scene-based audio\nThe send frequency response for scene-based audio, G(f), is calculated as .\nLoudspeaker array\na)\tA calibrated loudspeaker array shall be placed within the free-field volume.\nb)\tThe loudspeaker array shall comprise one or several semi-arcs having a radius greater or equal than 1 meter. The radius shall be reported.\nc)\tThe loudspeaker array shall be composed of N+1 loudspeaker elements. The ambisonic order N shall be reported.\nd)\tEach loudspeaker in the array shall be calibrated with a frequency response of [at least 100 Hz-20,000 Hz] and minimum phase response.\ne)\tThe coordinates of the loudspeaker elements are defined according to a Gaussian spherical grid [7] of order N. Directions shall comply with Annex B.1 and the N+1 elevations of the spherical grid shall be reported.\nTurn table\na)\tA turn table with a resolution of 0.5 degrees shall be used. The rotation axis of the turn table and the vertical axis of the semi-arcs shall be aligned The turn table shall be adjusted in height so that the device under test is positioned at the geometric center of the loudspeaker array.\nb)\tFor measurement, an azimuth step of 180/(N+1) degrees shall be used.\n\nReference Spectrum measurement\na)\tA diffuse-field / random incidence, or multi-field microphone is mounted in the free-field volume such that the tip of the microphone corresponds to the geometric center of the free-field volume and the geometric center of the loudspeaker array.\nNOTE 1:\tDiffuse-field / random incidence microphones, are described in [5].\nRepeat steps b-c) with an azimuth angular resolution of 180/(N+1) degrees:\nb)\tAn exponential sweep sine signal is played over each of the N+1 loudspeakers of the loudspeaker array.\nc)\tThe impulse response at the geometric center of the loudspeaker array The figure depicts the impulse response of a loudspeaker array at the geometric center, with the magnitude spectrum of the reference sound pressure, P(f), calculated for the 1/12 th  octave intervals as given by the R40 series of preferred numbers in [6]. The figure illustrates the frequency response of the loudspeaker array, which is essential for understanding the sound quality and performance of the system.The figure depicts the impulse response of a loudspeaker array at the geometric center, with the magnitude spectrum of the reference sound pressure, P(f), calculated for the 1/12 th  octave intervals as given by the R40 series of preferred numbers in [6]. This figure is essential for understanding the sound quality and performance of the system. is measured for each loudspeaker position.\nd)\tThe magnitude spectrum of the reference sound pressure, P(f), is calculated for the 1/12th octave intervals as given by the R40 series of preferred numbers in [6].\nNOTE 2:\tFor ideal (calibrated) loudspeakers, the P(f) spectra should have equal energy in each 1/12th octave intervals.\nEstimated Spectrum measurement\na)\tThe scene-based audio capture device under test is mounted in the free-field volume such that its geometric center coincides with the geometric center of free-field volume and the geometric center of the loudspeaker array.\nb)\tRepeat steps b-c) with an azimuth angular resolution of 180/(N+1) degrees::\nc)\tAn exponential sweep sine signal is played over each of the N+1 loudspeakers of the loudspeaker array. The sweep signals shall be identical to the signals used for the reference spectrum measurement.\nd)\tThe impulse response at the geometric center of the loudspeaker array  is measured for each loudspeaker position.\ne)\tThe magnitude spectrum of the estimated sound pressure, , is calculated for the 1/12th octave intervals as given by the R40 series of preferred numbers in [6].\nCalculation of send frequency response for scene-based audio\nThe send frequency response for scene-based audio, G(f), is calculated as .\nDue to practical constraints (e.g. reflections on turn table), measurements for specific elevations (e.g. < - degrees) may be unreliable and discarded. In this case, the above measurement procedure may be conducted in two phases by measuring only directions for one hemisphere (e.g. top hemisphere, with elevations >0) in each phase. The device under test shall be flipped upside down between the two phases, and this two-phase approach shall be reported.\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.1.2\tDirectional response measurement for scene-based audio",
                            "text_content": "The directional response for scene-based audio is defined as the transfer function, represented as an impulse response, h(iibetween a device under test and a loudspeaker located at an equal distance r and L predefined directions, (ii i=1,...,L.\nFree-field propagation conditions\n-\tThe test environment shall contain a free-field volume, wherein free-field sound propagation conditions shall be observed.\n-\tThe free-field sound propagation conditions shall be observed down to a frequency of 200Hz.\nTest environment noise floor\nThe equivalent continuous sound level of the test environment in each 1/3rd octave band, Leq(f), shall be less than the limits of the NR10 curve, following the noise rating determination procedures in [4].\nLoudspeaker array\nA real or simulated loudspeaker array comprising L loudspeakers located be a set of predefined directions (ii, i=1,...,L, from the geometric center of the loudspeaker array shall be used.\nFor each loudspeaker position (ii, i=1,...,L , the following procedure shall be used:\na)\tAn exponential sweep sine test signal is played over the loudspeaker.\nNOTE:\tThe impact of codec on the exponential sweep sine test signal needs to be verified before performing the measurements. An activation signal may be needed.\nb)\tThe impulse response h(ii at the geometric center of the loudspeaker array is measured.\n",
                            "figures_meta_data": [],
                            "tables": []
                        }
                    ]
                },
                {
                    "title": "4.2\tObjective Test Methodologies for Assessment of Immersive Audio Systems in the Receiving Direction",
                    "description": "",
                    "summary": "",
                    "text_content": "",
                    "tables": [],
                    "figures_meta_data": [],
                    "subsubsections": [
                        {
                            "title": "4.2.1\tHeadset Binaural Diffuse-field Receive frequency response for Scene-based audio",
                            "text_content": "This test is applicable to UEs rendering scene-based audio (e.g. First and Higher Order Ambisonics) over a binaural headset.\nThe Headset Binaural Diffuse-field Receive Frequency Response for Scene-based Audio (for left and right ears) is defined as the transfer function, GL,R (f), between:\na)\tPL,R(f), the binaurally recorded sound pressure magnitude spectra, obtained when a diffuse field signal in the equivalent spatial domain representation, w(t), is played on the DUT; and\nb)\tPref L,R(f), the reference sound pressure magnitude spectra, obtained by direct convolution of the diffuse field signal in the equivalent spatial domain representation, w(t) with its corresponding set of HRTFs.\nTest environment noise floor\nThe equivalent continuous sound level of the test environment in each 1/3rd octave band, Leq(f), shall be less than the limits of the NR10 curve, following the noise rating determination procedures in [4].\nThe set of HRTFs used by the UE shall be documented and available to the test lab.\nReference sound pressure magnitude spectra\nThe reference sound pressure magnitude spectra are derived offline. The reference sound pressure magnitude spectra for the left and right ears, Pref L,R (f) is the frequency domain representation of the convolution between the set of equivalent spatial domain signals, w(t), with its corresponding set head related transfer functions h L,R(t), for each direction j in an equivalent spatial domain of order NDUT , i.e.:\n\nThe signals wj(t), for 1  j  (NDUT +1)2, are uncorrelated pink noise signals of 30s length.\nBinaurally recorded sound pressure magnitude spectra\nThe binaurally recorded sound pressure magnitude spectra is obtained as follows:\na)\tThe binaural headset is placed on a HATS.\nb)\tThe DUT shall be configured such that the set of HRTFs used for binaural rendering correspond to the HATS used for testing.\nc)\tThe DUT volume control (if any) is adjusted for its nominal setting.\nd)\tThe binaural time-domain signals are recorded with HATS.\ne)\tThe binaurally recorded sound pressure magnitude spectra, PL,R(f) is obtained by taking the Fourier transform of the binaurally recorded time-domain signals.\nCalculation of headset binaural diffuse-field receive frequency response for scene-based audio\nThe headset binaural diffuse-field frequency response for scene-based audio, G(f), is calculated for each supported Ambisonics order NDUT as:\n\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.2.2\tNominal System Sensitivity in Receive Direction for Channel-based audio",
                            "text_content": "This test is applicable to UEs rendering channel-based audio (e.g. 7.1.4) over a binaural headset.\nThe nominal system sensitivity in receive direction for channel-based audio is defined as the difference between the sound pressure level (in dBSPL(A)) produced by the DUT on HATS and the root mean square of the digital test signal (in dBFS).\nTest environment noise floor\nThe equivalent continuous sound level of the test environment in each 1/3rd octave band, Leq(f), shall be less than the limits of the NR10 curve, following the noise rating determination procedures in [4].\nThe specific HATS used for the recording shall be described in the test report. The set of HRTFs used by the UE shall be documented and available to the test lab.\nFor each audio channel supported by the DUT, a pink noise signal with -18 dBFS RMS level is played, with the signals played only one channel at a time.\nThe LAeq (in dBSPL(A)) is measured continuously for a period of 30 s for each of the left and right ears.\nThe sensitivity Gi L,R is expressed as the difference of the recorded sound pressure levels at the left and right ears and the root mean square digital level of the pink noise test signal, i.e. -18 dBFS.\n\n",
                            "figures_meta_data": [],
                            "tables": []
                        },
                        {
                            "title": "4.2.3\tMotion to Sound Latency in Dynamic Binaural Rendering Systems",
                            "text_content": "Motion to Sound latency is the time difference between the event of a change in head rotation and when the immersive audio signal is finally compensated for the head motion. The method in this specification is intended to verify that the overall motion-to-sound latency that a user experiences upon rotating their head is within acceptable limits.\nThe method allows full measurement of motion to sound, i.e. including both the latency of the head tracking sensor as well as the audio playback. This includes all components of a real setup and therefore contains all possible causes of additional latency that a user may experience.\nThe method also provides a latency value for the isolated audio processing of the binaural renderer without the aforementioned external hardware, assuming that the binaural renderer can process audio data as an audio processing plugin that can be evaluated in isolation.\nNOTE:\tThis method requires synchronized playback of two renderer instances and may not be suitable for the measurements of UEs where such synchronization is not possible.\nThe following will be required:\nSoftware:\n-\tAudio processing software to run and record output of two renderers simultaneously\n-\tHead tracker software\nHardware:\n-\tHost machine for audio processing\n-\tHead tracker hardware\n-\tStereo audio recording interface\n-\tStereo audio playback interface\n-\tMechanical setup to rotate the head tracking sensor in a precise and reproducible way\nAn exemplary hardware setup can be seen below in Figure 2, the method however can also be implemented using different systems under test and accompanying equipment:\nThe figure depicts a setup in Position 1 on the left and Position 2 on the right, illustrating the hardware components and their arrangement in a 5G network. Key components include the base station (gNB), user equipment (UE), and scatterers. The diagram highlights beamforming techniques to mitigate interference.\nFigure 2: Hardware Overview (Setup in Position 1 on the left, Position 2 on the right)\nThe audio processing environment uses two parallel signal chains, each containing its own instance of the same binaural renderer being tested. The test is concerned only with yaw angles, so values of pitch and roll should be set to zero at the beginning of the test and can be ignored thereafter.\nFigure 3 illustrates a generic audio processing environment, showcasing a multi-path signal propagation in a 5G network. The figure depicts the signal path, including the base station (gNB), user equipment (UE), and scatterers. Key components include the base station (gNB), user equipment (UE), and scatterers. The diagram highlights beamforming techniques to mitigate interference.\nFigure 3: Generic Audio Processing Environment\nThe initial conditions are that Rendering Chain 1 (RC1) has a static yaw head rotation angle of 0 degrees and RC2 uses the physical rotation of the head tracker to get its yaw value. A white noise signal is virtually placed directly in front of the listener (0 degrees azimuth, elevation), meaning that rotation of the arm directly affects how the white noise source is rendered.\nThe first step is to calibrate the final position of the rotating arm (Position 2 / P2). The rotating arm is moved manually and requires only a limited range of motion - from some small rotation away from the table (Position 1 / P1), 20 to 30 degrees will be ample, through until contact with the table (P2). The arm should be placed at P2 and set up so that this position also corresponds to 0 degrees yaw.\nAn object within the evaluation environment, e.g. using Max/MSP, should be created to set the value of yaw to exactly 0 degrees once the real value of yaw (received from the head tracker) is <0.2 degrees.\nNOTE:\tThis tolerance value was chosen to be as small as possible while ensuring that it does not bounce (dependent on the accuracy of the tracking system)\nThis object should be designed to latch to zero once the actual value is under the tolerance threshold, so that any small accidental rebound of the rotating arm does not affect the yaw angle fed to the renderer - it artificially remains at exactly 0, which is important to ensure that both rendering chains have exactly the same head rotation when the arm is in its final position (P2). The output from the evaluation environment is captured by the recording audio interface, which therefore includes any latency introduced by playback.\nA test run begins by starting to record on the recording audio interface. The rotating arm is set to Position 1, then the audio processing set running and starts feeding the input source to both renderer instances. A microphone is positioned near the contact point at the table. This mono room microphone will be recorded synchronously with the output from the evaluation environment, with its purpose being to log the point of contact of the arm with the table, which should be done with a good amount speed and vigour so that the microphone picks up a loud knock at the table. Shortly after this (one or two seconds for example), with the test run now complete, playback and recording can be stopped.\nSome milliseconds after the collision, the latest yaw value detected by the tracking system will have been passed into the evaluation environment (tTracking). With the target yaw value now reached (latched to zero in Max), both rendering chains will have the identical values of head rotation and therefore, after some further short delay, the output of both renderer instances will be identical.\nThe overall motion-to-sound latency (tM2S) is taken as the time from the moment of collision until the point at which the two output signals are identical.\nTo easily visually inspect when this point occurs, one output channel of one signal chain (e.g. RC1-left) is subtracted from the same output channel of the complementary signal chain (RC2-left).\nNOTE 1:\tThis could be done manually in audio editor software after processing, but this would require recording at least three channels synchronously (one from each renderer chain, and one of the room microphone). Instead, the subtraction of signals can be done within Max/MSP, meaning only the output of this operation (one channel) and the room microphone can conveniently be recorded with a stereo audio interface. In addition to the stereo WAVE file recorded by a separate audio application, the Max/MSP application also writes to a separate mono WAVE file once it detects that it is in the final tolerated yaw position (latched on). This mono WAVE contains only the subtracted signals as described above, from which the tMspProc time can also be measured.\nEvaluation is performed offline in audio editor software. The tMspProc time is measured from the start of the file until the point at which consecutive zero samples begin. This value encompasses any motion-to-sound latency caused by the tested renderer chain as well as any other latency caused by Virtual Studio Technology (VST) plugin framework buffering. The tMspProc time shall be measured from the audio frame boundary at which the latched-on yaw value is activated and applied within that audio frame.\nNOTE 2:\tSince the yaw rotation update rate of the tracker is typically in the range of a few milliseconds, there is a framing mismatch when compared to the audio framing, but this mismatch will not be incorporated in the tMspProc value but rather only in the tM2S measurement.\nAn example measurement of the tMspProc is displayed in Figure 4. For tM2S this is measured by selecting the duration between the visible collision peak in the microphone channel and the point at which the other channel reduces to silence. Figure 5 shows an example measurement for the motion-to-sound latency.\nThe figure depicts a tMspProc latency measurement, showing the measured latency for different scenarios. The figure illustrates the impact of various factors such as network congestion, packet loss, and network latency on the measured latency.\nFigure 4: tMspProc latency measurement\nThe figure depicts a tM2S latency measurement, which is a crucial metric in evaluating the performance of wireless communication systems. The measurement is conducted using a motion sensor, capturing the time it takes for sound to travel from the source to the receiver. This data is crucial for understanding the efficiency of wireless communication systems and for optimizing their performance.\nFigure 5: Motion-to-sound (tM2S) latency measurement\nIn Figure 5 the room recording is on the top, subtracted renderer output is on the bottom. Marked region is the time passed since the arm hits the table (recorded knock) and when the subtracted binaural renderer output reaches silence.\nNOTE 3:\tUnlike the tMspProc measurement, the tM2S measurement is taken from signals recorded from hardware audio interfaces, hence it is not possible to look for continuous silence since the resulting file will always contain some noise added by the digital-to-analog and analog-to-digital converters. For this reason, it is important to ensure a high signal-to-noise ratio in the signal provided to audio interfaces, to make it easier to inspect where the cancellation occurs.\nThe following tables order-dependent directions , where  and  denote the elevations and azimuths in radians, respectively.\n\n\n\n\n\n\n\n\n",
                            "figures_meta_data": [],
                            "tables": [
                                {
                                    "description": "",
                                    "table number": 3,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "",
                                    "table number": 4,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "",
                                    "table number": 5,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "",
                                    "table number": 6,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "",
                                    "table number": 7,
                                    "summary": "",
                                    "name": ""
                                },
                                {
                                    "description": "",
                                    "table number": 8,
                                    "summary": "",
                                    "name": ""
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        {
            "title": "B.1\tDefinition",
            "description": "A Gaussian grid of order N consists of 2(N+1)2 points associated to directions ,  and , where  and   denote the elevation and azimuth, respectively. These directions are defined as follows [7]: The elevations  are computed as the zeros of the (N+1)-th degree Legendre polynomial \n, while the azimuths are given by  (in radians) or j.180/(N+1) (in degrees), .\nDirections in test setup shall comply with the theoretical values   with an accuracy of +/-0.5 degree for all azimuths and +/-0.5 degree for elevations in the range [-80,+80] degrees. For elevation >80 degrees and <-80 degrees, the accuracy shall be respectively +4/-0.5 degrees and +0.5/-4 degrees.\n",
            "summary": "",
            "tables": [],
            "figures_meta_data": [],
            "subsections": []
        },
        {
            "title": "B.2\tExample loudspeaker array",
            "description": "An example implementation with an ambisonic order N = 29 is described below:\n-\tA turn table with constant step size of 6 degrees and starting at 0 degree (to obtain 60 positions in azimuth).\n-\tTwo fixed semi-arcs of radius 2.5 meters separated in azimuth by 90 degrees with 15 loudspeakers on each semi-arc; the elevations of loudspeakers are given (in degrees) by -85, -80, -74, -68, -62, -56, -50, -44, -38, -32, -27, -21, -15,  -9,  -3,   3,   9,  15,  21,  27,  32,  38, 44,  50,  56,  62,  68,  74,  80,  85, where succesive values are alternatively allocated to each semi-arc.\nNOTE: In practice, the elevation of -85 degrees may be replaced by a nearby value (e.g. -82 degrees) to leave room for the mounting structure at the bottom of the loudspeaker array.\n\n\n\n",
            "summary": "",
            "tables": [
                {
                    "description": "",
                    "table number": 9,
                    "summary": "",
                    "name": ""
                }
            ],
            "figures_meta_data": [],
            "subsections": []
        }
    ]
}